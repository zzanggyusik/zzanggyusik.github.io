---
title: "1.Network Traffic Classification"
excerpt: "네트워크 트래픽 분류"
toc: true
toc_label: "Network Traffic Classification"
toc_sticky: true

categories:
  - Project
tags:
  - 트래픽 분류
last_modified_at: 2021-08-24
---

### 트래픽 분류

캐글에 있는[Labeled Network Traffic flows - 141 Applications](https://www.kaggle.com/jsrojas/labeled-network-traffic-flows-114-applications) 데이터 세트를 이용해 트래픽 분류를 해보겠습니다.

파일명을 traffic.csv로 변경한 뒤 진행했습니다.

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split 
from sklearn.tree import DecisionTreeClassifier

traffic_df = pd.read_csv('./traffic.csv')
traffic_df.info()
```

![image](https://user-images.githubusercontent.com/76269316/130161645-8fd66637-1fb7-45d3-980e-9a898dd5f336.png)

데이터 세트는 총 2,704,839의 레코드와 50개의 피처로 구성돼 있습니다.

이 중 category가 레이블 값입니다.

<br><br><br>

### 데이터 전처리

##### 1. Null 값 처리

사이킷런 머신러닝 알고리즘은 Null 값을 허용하지 않으므로 먼저 null 값이 있는지 확인해보니, 현재 데이터 세트에는 Null 값이 없습니다.

```python
print('Traffic 데이터 세트의 Null 값: ', traffic_df.isnull().sum().sum())
```

![image](https://user-images.githubusercontent.com/76269316/130161858-2a720eb7-4e59-45b1-9e3c-ac9b45045393.png)

<br>

##### 2. 문자열 피처 처리

문자열 피처가 flow_key, src_ip, dst_ip, application_protocol, web_service 다섯 개가 있는데 마찬가지로 사이킷런 머신러닝 알고리즘은 문자열 값을 허용하지 않으므로 일단은 drop하고 나머지 피처들로 분류하도록 하겠습니다.

또한 최근에는 포트를 동적으로 사용하는 프로그램들이 늘어났기 때문에 소스와 데스티네이션의 ip 주소, 포트 번호 피처(src_ip_numeric, src_port, dst_port)도 삭제하도록 하겠습니다. 

```python
drop_columns = ['flow_key', 'src_ip_numeric', 'src_ip', 'dst_ip', 'src_port', 'dst_port', 'application_protocol', 'web_service']
traffic_df.drop(drop_columns, axis=1, inplace=True)
traffic_df.head()
```

![image](https://user-images.githubusercontent.com/76269316/130163003-fa7178a6-d83b-4016-9ed4-a62474b63995.png)

<br>

##### 3. 레이블 값 처리

먼저 데이터 세트의 레이블 값 분포를 확인해 보겠습니다.

```python
print(traffic_df['category'].value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/130163140-6575df6e-bda5-4209-a833-6257112f267f.png)

총 24개의 클래스 값이 존재하는데, 일단 이 중 Unspecified를 제외한 상위 10개의 레이블 값만 사용하고, category 이름을 target으로 바꾼 뒤 분류해 보도록 하겠습니다.

```python
traffic_df.rename(columns = {'category': 'target'}, inplace = True)  #category 피처 이름을 target으로 변경
print('변경 전 target 값\n', traffic_df['target'].value_counts())

#상위 10개의 레이블 값만 target에 저장
web = traffic_df['target'] == 'Web'
network = traffic_df['target'] == 'Network'
socialnetwork = traffic_df['target'] == 'SocialNetwork'
chat = traffic_df['target'] == 'Chat'
download_filetransper_filesharing = traffic_df['target'] == 'Download-FileTransfer-FileSharing'
media = traffic_df['target'] == 'Media'
cloud  = traffic_df['target'] == 'Cloud'
voip = traffic_df['target'] == 'VoIP'
collaborative = traffic_df['target'] == 'Collaborative'
system  = traffic_df['target'] == 'System'

top10_label = web | network | socialnetwork | chat | download_filetransper_filesharing | media | cloud | voip | collaborative | system
traffic_df = traffic_df[top10_label]
print('\n변경 후 target 값\n', traffic_df['target'].value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/130168118-7993c478-ac96-436f-a223-1c2af1723a34.png)

<br>

레이블 값도 문자열이 있는 것을 허용하지 않기 때문에 [레이블 인코딩](https://seominseok4834.github.io/machine%20learning/2.scikit-learn-machine-learning-in-python/#%EB%A0%88%EC%9D%B4%EB%B8%94-%EC%9D%B8%EC%BD%94%EB%94%A9-label-encoding)을 적용해 int형으로 변경해 줬습니다.

```python
from sklearn.preprocessing import LabelEncoder

#LabelEncoder를 객체로 생성한 후, fit_transform()으로 target 피처 인코딩
encoder = LabelEncoder()
encoder = encoder.fit(traffic_df['target'])
traffic_df['target'] = encoder.transform(traffic_df['target'])
print('레이블 인코딩 적용\n', traffic_df['target'].value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/130180114-eb8a7d1d-d3c8-4c92-980f-2b910433dc93.png)

<br><br><br>

### 결정 트리, 로지스틱 회귀(Decision Tree, Logistic Regression)

먼저 사이킷런 결정 트리, 로지스틱 회귀 클래스로 트래픽 분류를 해봤습니다.

```python
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import time

y_traffic_df = traffic_df['target']
X_traffic_df = traffic_df.drop('target', axis=1)

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_traffic_df, y_traffic_df, test_size=0.2, random_state=156)

#결정트리, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성
dt_clf = DecisionTreeClassifier(random_state=11)
lr_clf = LogisticRegression()

#결정트리 수행 시간 측정을 위한 시작 시간 설정
dt_start_time = time.time()

#DecisionTreeClassifier 학습/예측/평가
dt_clf.fit(X_train, y_train)
dt_pred = dt_clf.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)
print('DecisionTreeClassifier 정확도: {0:.4f}'.format(dt_accuracy))
print('결정트리 수행시간: {0:.1f}초\n'.format(time.time() - dt_start_time))

#로지스틱 회귀 수행 시간 측정을 위한 시작 시간 설정
lr_start_time = time.time()

#LogisticRegression 학습/예측/평가
lr_clf.fit(X_train, y_train)
lr_pred = lr_clf.predict(X_test)
lr_accuracy = accuracy_score(y_test, lr_pred)
print('LogisticRegression 정확도: {0:.4f}'.format(lr_accuracy))
print('로지스틱 회귀 수행시간: {0:.1f}초'.format(time.time() - lr_start_time))
```

![image](https://user-images.githubusercontent.com/76269316/130182375-ce7183a5-eed7-4ef6-b02f-c8da42b9ce32.png)

결정 트리는 약 91.5% 정도의 정확도가 나온 반면, 로지스틱 회귀는 약 55.4%로 굉장히 저조한 정확도가 나왔습니다.

+학교 컴퓨터로 돌렸을 때는 엄청 오래 걸리네요 ㅠ

![image](https://user-images.githubusercontent.com/76269316/130182974-1e33cc97-51dd-45c8-b4a2-280677f1f791.png)

<br><br><br>

### 앙상블 학습(Ensemble Learning)

##### Voting

이번에는 앙상블 학습 중 [소프트 보팅 방식](https://seominseok4834.github.io/machine%20learning/4.classification/#%EB%B3%B4%ED%8C%85-%EC%9C%A0%ED%98%95---%ED%95%98%EB%93%9C-%EB%B3%B4%ED%8C%85hard-voting%EA%B3%BC-%EC%86%8C%ED%94%84%ED%8A%B8-%EB%B3%B4%ED%8C%85soft-voting)을 사용해 트래픽 분류를 수행해 봤습니다.

사이킷런 VotingClassifier 클래스를 이용했고, 개별 모델은 로지스틱 회귀와 KNN을 사용했습니다.

```python
import pandas as pd
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#개별 모델은 로지스틱 회귀와 KNN임
lr_clf = LogisticRegression()
knn_clf = KNeighborsClassifier(n_neighbors=8)

#개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기
vo_clf = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf)], voting='soft')

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_traffic_df, y_traffic_df, test_size=0.2, random_state=156)

#VotingClassifier 학습/예측/평가
vo_clf.fit(X_train, y_train)
pred = vo_clf.predict(X_test)
vo_accuracy = accuracy_score(y_test, pred)
print('Voting 분류기 정확도: {0:.4f}'.format(vo_accuracy))

#개별 모델의 학습/예측/평가
classifiers = [lr_clf, knn_clf]
for classifier in classifiers:
    classifier.fit(X_train, y_train)
    pred = classifier.predict(X_test)
    class_name = classifier.__class__.__name__
    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test, pred)))
```

![image](https://user-images.githubusercontent.com/76269316/130182160-d7a0df7c-7e12-49e4-af61-78d49d9fd56d.png)

<br>

##### Bagging

다음으로는 앙상블 학습 중 배깅 방식(같은 알고리즘으로 여러 개의 분류기를 만들어서 Voting으로 최종 결정하는 알고리즘)으로 트래픽 분류를 수행해 봤습니다.

배깅의 대표적인 알고리즘인 [랜덤 포레스트](https://seominseok4834.github.io/machine%20learning/4.classification/#%EB%9E%9C%EB%8D%A4-%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8)를 사용했습니다. 

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_traffic_df, y_traffic_df, test_size=0.2, random_state=156)

#랜덤 포레스트를 위한 사이킷런 Classifier 클래스 생성
rf_clf = RandomForestClassifier(random_state=11)

#랜덤 포레스트 수행 시간 측정을 위한 시작 시간 설정
rf_start_time = time.time()

#RandomForestClassifier 학습/예측/평가
rf_clf.fit(X_train, y_train)
rf_pred = rf_clf.predict(X_test)
rf_accuracy= accuracy_score(y_test, rf_pred)
print('RandomForestClassifier 정확도:{0:.4f}'.format(rf_accuracy))
print('랜덤 포레스트 수행시간: {0:.1f}초\n'.format(time.time() - rf_start_time))
```

![image](https://user-images.githubusercontent.com/76269316/130183215-23d89cc6-c252-4168-8bf6-2a632341b823.png)

약 94.2%의 높은 정확도를 보여줍니다.

<br><br><br>

### GBM(Gradient Boosting Machine)

이번에는 [GBM](https://seominseok4834.github.io/machine%20learning/4.classification/#gbmgradient-boosting-machine)을 사용해서 트래픽 분류를 수행해 봤습니다.

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
import time
import warnings
warnings.filterwarnings('ignore')

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_traffic_df, y_traffic_df, test_size=0.2, random_state=156)

#GBM 수행 시간 측정을 위한 시작 시간 설정
start_time = time.time()

gb_clf = GradientBoostingClassifier(random_state=0)
gb_clf.fit(X_train, y_train)
gb_pred = gb_clf.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)

print('GBM 정확도: {0:.4f}'.format(gb_accuracy))
print('GBM 수행시간: {0:.1f}초'.format(time.time() - start_time))
```

![image](https://user-images.githubusercontent.com/76269316/130183623-a8ff9ba5-1fe3-46d7-be37-bb3b7a6870bf.png)

<br><br><br>

### XGBoost(eXtra Gradient Boost)

다음으로 [XGBoost](https://seominseok4834.github.io/machine%20learning/4.classification/#xgboostextra-gradient-boost)를 사용해 트래픽 분류를 수행했습니다. 

```python
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
import time
import warnings
warnings.filterwarnings('ignore')

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_traffic_df, y_traffic_df, test_size=0.2, random_state=156)

#XGBoost 수행 시간 측정을 위한 시작 시간 설정
start_time = time.time()

xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)
xgb_wrapper.fit(X_train, y_train)
xgb_pred = xgb_wrapper.predict(X_test)
xgb_accuracy = accuracy_score(y_test, xgb_pred)

print('XGBoost 정확도: {0:.4f}'.format(xgb_accuracy))
print('XGBoost 수행시간: {0:.1f}초'.format(time.time() - start_time))
```

![image](https://user-images.githubusercontent.com/76269316/130184350-db3fa2ca-94db-4f36-be2b-286617e2eb29.png)

<br><br><br>

### LightGBM

마지막으로 [LightGBM](https://seominseok4834.github.io/machine%20learning/4.classification/#lightgbm)을 사용해 트래픽 분류를 수행해 봤습니다.

```python
from sklearn.model_selection import train_test_split
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score
import time
import warnings
warnings.filterwarnings('ignore')

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_traffic_df, y_traffic_df, test_size=0.2, random_state=156)

#LightGBM 수행 시간 측정을 위한 시작 시간 설정
start_time = time.time()

#앞서 XGBoost와 동일하게 n_estimators=400으로 설정
lgbm_wrapper = LGBMClassifier(n_estimators=400)

#조기 중단 기능 설정
evals = [(X_test, y_test)]

lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="logloss", eval_set=evals, verbose=True)
lgbm_pred = lgbm_wrapper.predict(X_test)
lgbm_accuracy = accuracy_score(y_test, lgbm_pred)

print('LightGBM 정확도: {0:.4f}'.format(lgbm_accuracy))
print('LightGBM 수행시간: {0:.1f}초'.format(time.time() - start_time))
```

![image](https://user-images.githubusercontent.com/76269316/130184714-f5667370-2e95-43dd-a360-3d58b82ccd84.png)

<br><br><br>

### 실행 결과

```python
import matplotlib.pyplot as plt
%matplotlib inline

accuracies = [dt_accuracy, lr_accuracy, vo_accuracy, rf_accuracy, gb_accuracy, xgb_accuracy, lgbm_accuracy]
classifiers = ['DecisionTree', 'LogisticRegression', 'Voting', 'RandomForest', 'GBM', 'XGBoost', 'LightGBM']

plt.figure(figsize=(13, 5))
plt.bar(classifiers, accuracies)

for i, v in enumerate(classifiers):
  plt.text(v, accuracies[i], str(np.round(accuracies[i] * 100, 2)), fontsize=9, color='black', horizontalalignment='center', verticalalignment='bottom')

plt.title('Accuracy by Classifier')
plt.xlabel('Classifier')
plt.ylabel('Accuracy (%)')
```

![image](https://user-images.githubusercontent.com/76269316/130189377-f125390e-b511-431a-9834-38f799dd9ab2.png)

특별한 데이터 가공 없이 분류를 수행한 결과 다음과 같은 결과가 나왔습니다.

![image](https://user-images.githubusercontent.com/76269316/130190142-f70fc15c-96b0-4b48-a329-18db027a50ab.png)

참고한 논문의 결과와 거의 비슷한 결과가 도출됐습니다.

<br><br><br>

### 피처 중요도 확인

각 classifier별 피처 중요도를 확인해 보겠습니다.

피처 개수가 많으므로 상위 11개 피처만 출력했고, 이를 위해 **print_top11_feature_importances(model)** 메소드를 사용했습니다. 

**print_top11_feature_importances(model)**

학습을 완료한 classifier를 파라미터로 받아 피처 중요도 상위 11개의 피처를 출력해주는 함수

```python
import seaborn as sns
import numpy as np
%matplotlib inline

def print_top11_feature_importances(model):
    columns = traffic_df.drop('target', axis=1, inplace=False).columns
    
    ftr_importances_values = model.feature_importances_
    ftr_importances = pd.Series(ftr_importances_values, index=columns)
    
    ftr_top11 = ftr_importances.sort_values(ascending=False)[:11]
    
    #feautre importance를 column 별로 시각화하기
    plt.figure(figsize=(8, 6))
    plt.title('{0} importances Top 11'.format(model.__class__.__name__))
    sns.barplot(x=ftr_top11, y=ftr_top11.index)
    plt.show()
```

```python
models = [dt_clf, rf_clf, gb_clf, xgb_wrapper, lgbm_wrapper] 

for model in models:
  print_top11_feature_importances(model)
```



**DecisionTree**

![image](https://user-images.githubusercontent.com/76269316/130193639-3085a5d2-a94d-41a3-91ea-bd0d540666c3.png)

<br>

**RandomForest**

![image](https://user-images.githubusercontent.com/76269316/130193759-07a9725b-c075-4394-82c4-cf46d74fdbdc.png)

<br>

**GBM**

![image](https://user-images.githubusercontent.com/76269316/130193822-1a4a9865-bba6-4b0e-b933-d2eebbc83891.png)

<br>

**XGBoost**

![image](https://user-images.githubusercontent.com/76269316/130193922-14589d1c-7f98-4d5f-a983-2c9dc1b17a0e.png)

<br>

**LightGBM**

![image](https://user-images.githubusercontent.com/76269316/130194006-a05a68bd-6f58-45cc-a081-0c1b491cc4fe.png)

<br>

위 5개 classifier들의 중요 피처가 모두 다르기 때문에 피처들이 몇 번 나왔는지 카운트해 봤습니다.

```python
import matplotlib.pyplot as plt
%matplotlib inline

ferquency = [5, 5, 5, 5, 3, 5, 2, 3, 4, 3, 5, 4, 1, 2, 1, 1, 1]
features = ['f_max_ps', 'std_dev_ps', 'max_ps', 'b_min_ps', 'proto', 'f_octetTotalCount', 'min_piat', 'octetTotalCount', 'b_max_ps', 'b_octetTotalCount', 'avg_ps', 'min_ps', 'b_avg_ps', 'f_avg_ps', 'b_pktTotalCount', 'flowStart','f_flowEnd']

plt.figure(figsize=(25, 5))         
plt.bar(features, ferquency)
   
plt.title('Feature Importance Frequency')
plt.xlabel('Feature')
plt.ylabel('Frequency (count)')
            
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/130532375-5c9eb5bd-88f4-486c-8f2d-eb0ab9029354.png)

이 중 가장 많이 나온 f_max_ps, std_dev_ps, max_ps, b_min_ps, f_octerTotalCount, avg_ps, b_max_psFeature, min_ps, proto, octerTotalCount, b_octerTotalCount 피처만 사용해 다시 학습/예측/평가해 보겠습니다.

<br><br><br>

### 피처 중요도 상위 11개 피처로 학습/예측/평가

**데이터 로딩 및 전처리**

트래픽 데이터 세트 재로딩

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split 
from sklearn.tree import DecisionTreeClassifier

traffic_df = pd.read_csv('./traffic.csv')
```

<br>

문자열 피처 및 불필요 피처 드롭

```python
drop_columns = ['flow_key', 'src_ip_numeric', 'src_ip', 'dst_ip', 'src_port', 'dst_port', 'application_protocol', 'web_service']
traffic_df.drop(drop_columns, axis=1, inplace=True)
```

<br>

상위 10개의 레이블 값만 사용

```python
traffic_df.rename(columns = {'category': 'target'}, inplace = True)  #category 피처 이름을 target으로 변경
print('변경 전 target 값\n', traffic_df['target'].value_counts())

#상위 10개의 레이블 값만 target에 저장
web = traffic_df['target'] == 'Web'
network = traffic_df['target'] == 'Network'
socialnetwork = traffic_df['target'] == 'SocialNetwork'
chat = traffic_df['target'] == 'Chat'
download_filetransper_filesharing = traffic_df['target'] == 'Download-FileTransfer-FileSharing'
media = traffic_df['target'] == 'Media'
cloud  = traffic_df['target'] == 'Cloud'
voip = traffic_df['target'] == 'VoIP'
collaborative = traffic_df['target'] == 'Collaborative'
system  = traffic_df['target'] == 'System'

top10_label = web | network | socialnetwork | chat | download_filetransper_filesharing | media | cloud | voip | collaborative | system
traffic_df = traffic_df[top10_label]
```

<br>

Target 값에 레이블 인코딩 적용

```python
from sklearn.preprocessing import LabelEncoder

#LabelEncoder를 객체로 생성한 후, fit_transform()으로 target 피처 인코딩
encoder = LabelEncoder()
encoder = encoder.fit(traffic_df['target'])
traffic_df['target'] = encoder.transform(traffic_df['target'])
```

<br>

피처 중요도 상위 11개 피처만 사용

```python
feature11 = ['f_max_ps', 'std_dev_ps', 'max_ps', 'b_min_ps', 'f_octetTotalCount', 'avg_ps', 'b_max_ps', 'min_ps', 'proto', 'octetTotalCount', 'b_octetTotalCount']
traffic11_df = traffic_df[feature11]
traffic11_df['target'] = traffic_df['target']
traffic11_df.head()
```

![image](https://user-images.githubusercontent.com/76269316/130532939-7b62d41e-0ecc-458d-8bbd-1781d461e24e.png)

<br>

데이터 분포도가 왜곡돼 있어 로그 변환을 적용했습니다.

<details markdown="1">
<summary>접기/펼치기</summary>
|                         원본 데이터                          |                          로그 변환                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![output_7_0](https://user-images.githubusercontent.com/76269316/130569211-cd603426-cb78-48b5-ae59-beb6355b30be.png) | ![output_10_0](https://user-images.githubusercontent.com/76269316/130569184-b5d23897-c4dc-4537-886f-0c98eca8b74f.png) |
| ![output_7_1](https://user-images.githubusercontent.com/76269316/130566747-7dcf3c24-62bf-404b-abdd-6ce02943e5a0.png) | ![output_10_1](https://user-images.githubusercontent.com/76269316/130568493-3fdbf1c2-524d-4c85-8e83-36ae9bf65410.png) |
| ![output_7_2](https://user-images.githubusercontent.com/76269316/130566749-16023983-728a-4ea4-bebe-49e5e35776b8.png) | ![output_10_2](https://user-images.githubusercontent.com/76269316/130568495-54cd269f-a1e7-4554-b0c5-4e63a51db8f0.png) |
| ![output_7_3](https://user-images.githubusercontent.com/76269316/130566751-c2df6537-3170-4f43-9ad7-0a6d1f4bc875.png) | ![output_10_3](https://user-images.githubusercontent.com/76269316/130568498-751f78f5-c153-4910-88b4-22269e0c3c95.png) |
| ![output_7_4](https://user-images.githubusercontent.com/76269316/130566753-0d97ebbb-9ddb-4ef8-92cd-39c082bc2639.png) | ![output_10_4](https://user-images.githubusercontent.com/76269316/130568499-717dd349-95aa-42e8-8038-95334b5de1e3.png) |
| ![output_7_5](https://user-images.githubusercontent.com/76269316/130566755-332fe89b-df03-42df-a3ea-ab973c151e56.png) | ![output_10_5](https://user-images.githubusercontent.com/76269316/130568502-d908536d-474f-4763-9fd6-401596670328.png) |
| ![output_7_6](https://user-images.githubusercontent.com/76269316/130566756-d82277b0-ed42-4eb0-b90f-993692b92cc3.png) | ![output_10_6](https://user-images.githubusercontent.com/76269316/130568505-d62d7bb3-4083-42b8-8575-924a685bbf26.png) |
| ![output_7_7](https://user-images.githubusercontent.com/76269316/130566757-280b73f5-0f0b-4b0a-b03f-141400445a51.png) | ![output_10_7](https://user-images.githubusercontent.com/76269316/130568507-4418884b-aa71-418e-b611-183e48b03563.png) |
| ![output_7_8](https://user-images.githubusercontent.com/76269316/130566759-b86e8346-3018-4d0a-ac0d-9077c32d318c.png) | ![output_10_8](https://user-images.githubusercontent.com/76269316/130568508-bc311315-9eb2-4371-bb7b-f8b5bf260046.png) |
| ![output_7_9](https://user-images.githubusercontent.com/76269316/130566761-e85de009-705d-4f68-8c5e-c18a0397a500.png) | ![output_10_9](https://user-images.githubusercontent.com/76269316/130568511-361a3b0a-b04b-40d3-a1ce-b6db2639ecff.png) |
| ![output_7_10](https://user-images.githubusercontent.com/76269316/130566764-d875113f-fa4b-407e-a8ae-f6c5dcdbb736.png) | ![output_7_10](https://user-images.githubusercontent.com/76269316/130566764-d875113f-fa4b-407e-a8ae-f6c5dcdbb736.png) |

</details>

로그 변환 적용

```python
def log1_transform(features):
    traffic11_log_df = pd.DataFrame()
    for feature in features:
        traffic11_log_df[feature] = np.log1p(traffic11_df[feature])
        
    return traffic11_log_df

features = ['f_max_ps', 'std_dev_ps', 'max_ps', 'b_min_ps', 'f_octetTotalCount', 'avg_ps', 'b_max_ps', 'min_ps', 'proto', 'octetTotalCount', 'b_octetTotalCount']

traffic11_log_df = log1_transform(features)
traffic11_log_df['target'] = traffic11_df['target']
traffic11_log_df.head()
```

![image](https://user-images.githubusercontent.com/76269316/130569761-33c49050-52b0-4aa3-8831-77fe46a38787.png)

<br><br>

**결정 트리, 로지스틱 회귀(Decision Tree, Logistic Regression)**

```python
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import time

y_traffic_df = traffic11_log_df['target']
X_traffic_df = traffic11_log_df.drop('target', axis=1)

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_traffic_df, y_traffic_df, test_size=0.2, random_state=156)

#결정트리, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성
dt_clf = DecisionTreeClassifier(random_state=11)
lr_clf = LogisticRegression()

#결정트리 수행 시간 측정을 위한 시작 시간 설정
dt_start_time = time.time()

#DecisionTreeClassifier 학습/예측/평가
dt_clf.fit(X_train, y_train)
dt_pred = dt_clf.predict(X_test)
dt_accuracy = accuracy_score(y_test, dt_pred)
print('DecisionTreeClassifier 정확도: {0:.4f}'.format(dt_accuracy))
print('결정트리 수행시간: {0:.1f}초\n'.format(time.time() - dt_start_time))

#로지스틱 회귀 수행 시간 측정을 위한 시작 시간 설정
lr_start_time = time.time()

#LogisticRegression 학습/예측/평가
lr_clf.fit(X_train, y_train)
lr_pred = lr_clf.predict(X_test)
lr_accuracy = accuracy_score(y_test, lr_pred)
print('LogisticRegression 정확도: {0:.4f}'.format(lr_accuracy))
print('로지스틱 회귀 수행시간: {0:.1f}초'.format(time.time() - lr_start_time))
```

|                            colab                             |                           jupyter                            |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/130729483-5af20f58-dd26-48cc-a448-44ab9a2f7382.png) | ![image](https://user-images.githubusercontent.com/76269316/130730064-59c8ea9f-47af-4a2c-8bf3-9c8d8fe26e15.png) |

<br>

**앙상블 학습(Ensemble Learning)**

**Voting**

```python
import pandas as pd
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#개별 모델은 로지스틱 회귀와 KNN임
lr_clf = LogisticRegression()
knn_clf = KNeighborsClassifier(n_neighbors=8)

#개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기
vo_clf = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf)], voting='soft')

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_traffic_df, y_traffic_df, test_size=0.2, random_state=156)

#VotingClassifier 학습/예측/평가
vo_clf.fit(X_train, y_train)
pred = vo_clf.predict(X_test)
vo_accuracy = accuracy_score(y_test, pred)
print('Voting 분류기 정확도: {0:.4f}'.format(vo_accuracy))

#개별 모델의 학습/예측/평가
classifiers = [lr_clf, knn_clf]
for classifier in classifiers:
    classifier.fit(X_train, y_train)
    pred = classifier.predict(X_test)
    class_name = classifier.__class__.__name__
    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test, pred)))
```

| colab | jupyter |
| :---: | :-----: |
|       |         |

<br>

**Bagging**

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_traffic_df, y_traffic_df, test_size=0.2, random_state=156)

#랜덤 포레스트를 위한 사이킷런 Classifier 클래스 생성
rf_clf = RandomForestClassifier(random_state=11)

#랜덤 포레스트 수행 시간 측정을 위한 시작 시간 설정
rf_start_time = time.time()

#RandomForestClassifier 학습/예측/평가
rf_clf.fit(X_train, y_train)
rf_pred = rf_clf.predict(X_test)
rf_accuracy= accuracy_score(y_test, rf_pred)
print('RandomForestClassifier 정확도:{0:.4f}'.format(rf_accuracy))
print('랜덤 포레스트 수행시간: {0:.1f}초\n'.format(time.time() - rf_start_time))
```

| colab | jupyter |
| :---: | :-----: |
|       |         |

<br>

**GBM(Gradient Boosting Machne)**

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
import time
import warnings
warnings.filterwarnings('ignore')

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_traffic_df, y_traffic_df, test_size=0.2, random_state=156)

#GBM 수행 시간 측정을 위한 시작 시간 설정
start_time = time.time()

gb_clf = GradientBoostingClassifier(random_state=0)
gb_clf.fit(X_train, y_train)
gb_pred = gb_clf.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)

print('GBM 정확도: {0:.4f}'.format(gb_accuracy))
print('GBM 수행시간: {0:.1f}초'.format(time.time() - start_time))
```

| colab | jupyter |
| :---: | :-----: |
|       |         |

<br>

**XGBoost**

```python
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
import time
import warnings
warnings.filterwarnings('ignore')

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_traffic_df, y_traffic_df, test_size=0.2, random_state=156)

#XGBoost 수행 시간 측정을 위한 시작 시간 설정
start_time = time.time()

xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)
xgb_wrapper.fit(X_train, y_train)
xgb_pred = xgb_wrapper.predict(X_test)
xgb_accuracy = accuracy_score(y_test, xgb_pred)

print('XGBoost 정확도: {0:.4f}'.format(xgb_accuracy))
print('XGBoost 수행시간: {0:.1f}초'.format(time.time() - start_time))
```

| colab | jupyter |
| :---: | :-----: |
|       |         |

<br>

**LightGBM**

```python
from sklearn.model_selection import train_test_split
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score
import time
import warnings
warnings.filterwarnings('ignore')

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_traffic_df, y_traffic_df, test_size=0.2, random_state=156)

#LightGBM 수행 시간 측정을 위한 시작 시간 설정
start_time = time.time()

#앞서 XGBoost와 동일하게 n_estimators=400으로 설정
lgbm_wrapper = LGBMClassifier(n_estimators=400)

#조기 중단 기능 설정
evals = [(X_test, y_test)]

lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="logloss", eval_set=evals, verbose=True)
lgbm_pred = lgbm_wrapper.predict(X_test)
lgbm_accuracy = accuracy_score(y_test, lgbm_pred)

print('LightGBM 정확도: {0:.4f}'.format(lgbm_accuracy))
print('LightGBM 수행시간: {0:.1f}초'.format(time.time() - start_time))
```

| colab | jupyter |
| :---: | :-----: |
|       |         |

<br><br><br>

### 이상치 데이터 제거 후 모델 학습/예측/평가

```python
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(9, 9))
corr = traffic11_df.corr()
sns.heatmap(corr, annot=True, cmap='RdBu')  #양의 상관관계가 높을수록 진한 파란색, 음의 상관관계가 높을 수록 진한 빨간색으로 표현
```

![image](https://user-images.githubusercontent.com/76269316/130588577-b7076b1f-d940-4b94-a9b2-6a333ee95cee.png)

