---

title:  "1.Satellite Semantic Segmentation"
excerpt: "MMSegmentation을 사용해서 semantic segmentation 구현하기"
toc: true
toc_label: "Satellite Semantic Segmentation"
toc_sticky: true
published: true

categories:
  - Project
tags:
  - Semantic Segmentation
  - Satellite Image
last_modified_at: 2021-12-13


---

인공지능 수업에서 했던 프로젝트를 정리한 포스트입니다.

<br>

### 0. Introduce Project

다음과 같은 인공위성 사진에서

<img src="https://user-images.githubusercontent.com/76269316/145778196-6bfb9809-9a6b-4a2d-b2d9-1c5aad222bed.png" alt="BLD01552_PS3_K3A_NIA0373" style="zoom: 50%;" />

아래와 같이 빌딩과 도로를 검출하는 프로젝트였습니다.

<img src="https://user-images.githubusercontent.com/76269316/145778324-c584aa87-fe97-458b-8220-c2f43c2d7a98.png" alt="BLD01552_PS3_K3A_NIA0373 (1)" style="zoom: 50%;" />

<br>

사용한 데이터세트는 [여기](https://aihub.or.kr/aidata/7982)에서 다운받을 수 있습니다.

데이터세트는 다음과 같이 이미지 파일과 마스킹 정보가 담긴 json 파일 형태로 주어졌습니다.

<img width="1032" alt="스크린샷 2021-12-14 오후 2 54 50" src="https://user-images.githubusercontent.com/76269316/145941251-1d7f861a-6c89-4296-bffc-38e47213fc19.png" style="zoom:67%;" >

<img width="1032" alt="스크린샷 2021-12-14 오후 2 57 57" src="https://user-images.githubusercontent.com/76269316/145941596-29becea6-ba88-4115-94e2-442375db3b7e.png" style="zoom:67%;" >

<br>

<img width="421" alt="스크린샷 2021-12-13 오후 5 46 59" src="https://user-images.githubusercontent.com/76269316/145780258-41b2a9dc-5170-4042-a7aa-2006f27ebaf7.png">

모델 구현을 위해서 [MMSegmentation](https://github.com/open-mmlab/mmsegmentation)을 사용했습니다.

MMSegmentation을 사용한 이유는 컴퓨터 비전을 처음 접했기 tensorflow나 keras처럼 네트워크를 직접 구성하는데에 무리가 있다고 판단돼, MMSegmentation을 사용했습니다.

MMSegmentation은 중국 칭화 대학에서 구현한 open source toolbox로 configuration 파일을 변경하는 것만으로 mmsegmentation에서 구현해놓은 다양한 모델들을 사용할 수 있습니다.

<br>MMsegmentation을 사용하기 위해선 ground truth 파일이 gray scale로 마스킹 돼 있어야합니다.

[Creating custom data for training #185](https://github.com/open-mmlab/mmsegmentation/issues/185)

<br>

자세히 설명하면,

마스킹 정보가 들어있는 json 파일을 사용해서 이미지를 모두 0으로 채운(zero masking) 다음, 빌딩과 건물이 있는 부분만 마스킹 해주면 다음과 같이 마스킹 되게 됩니다.

<img src="https://user-images.githubusercontent.com/76269316/145782389-6e487e19-3188-4761-8980-0b2db1b94399.png" alt="BLD01552_PS3_K3A_NIA0373" style="zoom: 50%;" />

하지만 이 상태로는 mmsegmentation에서 사용할 수가 없습니다. 현재 배경은 모두 0인 상태이지만 건물과 도로는 모두 0이 아닌 어떤 픽셀값을 갖고 있습니다.

이 때 빌딩의 픽셀값을 1, 도로의 픽셀값을 2로 바꿔주면 다음과 같이 흑백 이미지(gray scale)를 얻을 수 있습니다.

<img src="https://user-images.githubusercontent.com/76269316/145782846-feb620c9-ddd4-404b-862e-ab96785be6df.png" alt="BLD01552_PS3_K3A_NIA0373 (1)" style="zoom:50%;" />

저희가 보기에는 온통 검은색이지만 실제로 이 안에는 아래와 같이 클래스 별로 건물 부분은 1, 도로 부분은 2로 채워져있습니다.

![image](https://user-images.githubusercontent.com/76269316/145783066-7c60e945-7781-4bb9-be33-3e2c0d8b8ea3.png)

[출처: Semantic Segmentation 첫걸음!](https://medium.com/hyunjulie/1%ED%8E%B8-semantic-segmentation-%EC%B2%AB%EA%B1%B8%EC%9D%8C-4180367ec9cb)

<br>

따라서 저는 먼저 건물은 (128, 128, 0)로, 도로는 (128, 64, 128)로 마스킹 한 뒤, 이를 다시 1, 2로 mapping 시켜주었습니다.

<br><br>

### 1. Masking (gray scale)

```json
    "features": [
      {
        "geometry": {
          "coordinates": [
            [
              31.3609468532,
              30.140372558,
              0
            ],
            [
              31.3667921448,
              30.1404454243,
              0
            ],
            [
              31.366708448,
              30.1455259019,
              0
            ],
            [
              31.3608628572,
              30.1454530208,
              0
            ]
          ],
          "type": "Polygon"
        },
        "properties": {
          "object_imcoords": "EMPTY",
          "building_imcoords": "EMPTY",
          "road_imcoords": "211.13825632612296,0,228.25757440661943,0,236.73506587440733,0,222.73960447617443,8.736311155155173,205.13056959253697,20.22717757755483,156.48327523790732,56.01530199102005,107.5950759842507,91.32161660643132,51.870298799909584,131.75963478574357,-2.8421709430404014e-14,170.75222357089405,-2.8421709430404014e-14,145.7946338998848,80.57492376553665,89.70522674180145,146.761359523997,42.5906896754829",
          "image_id": "BLD00596_PS3_K3A_NIA0277.png",
          "ingest_time": "2020-11-11T08:29:37.297611Z",
          "type_id": "3",
          "type_name": "Secondary"
        },
        "type": "Feature"
      }
```

json 파일은 다음과 같이 구성돼 있습니다.

도로 마스킹 좌표 정보가 담긴 json 파일이라 road_imcoords key에만 좌표 정보가 있고 building_imcoords는 EMPTY로 돼 있는 것을 확인할 수 있습니다.

<br>

pandas read_json을 사용해서 이미지 이름과 동일한 json 파일을 읽어들여 좌표 정보에 해당하는 부분을 fill_poly로 마스킹해줬습니다.

**masking.py**

json 파일을 읽어들여 마스킹하는 코드

건물만 있는 경우, 건물+도로 같이 있는 경우 마스킹

```python
import os
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import pylab
%matplotlib inline

 
building_imgs_path = '/content/drive/MyDrive/LV2_dataset/LV2_validation_set/images'
building_labels_path = '/content/drive/MyDrive/LV2_dataset/LV2_validation_set/labels/building'

road_imgs_path = '/content/drive/MyDrive/LV2_dataset/LV2_validation_set/images'
road_labels_path = '/content/drive/MyDrive/LV2_dataset/LV2_validation_set/labels/road'

save_dir = '/content/drive/MyDrive/SIA/labels'  # 마스킹 이미지를 저장할 위치

building_label_list = os.listdir(building_labels_path)

def list_chunk(lst, n):
    return [lst[i:i+n] for i in range(0, len(lst), n)]

for building_label in building_label_list:
    # json 파일인지 체크
    if building_label.split('.')[1] != 'json':
        continue

    building_img = os.path.join(building_imgs_path, building_label.split('.')[0] + '.png')
    if os.path.isfile(building_img):  # 이미지 파일이 존재하는지 확인
        image_array = cv2.imread(building_img)
        zero_mask = np.zeros(image_array.shape[0:3])
        masked_image = zero_mask

        cur_build_json = pd.read_json(building_labels_path + "/" + building_label)

        for feature in cur_build_json['features']:
            building_imcoords = feature['properties']['building_imcoords']

            if len(building_imcoords) == 0:  # 좌표 정보가 없을 경우 다음 좌표 정보 확인
                continue

            building_imcoords_list = building_imcoords.split(',')
            building_imcoords_list = list_chunk(building_imcoords_list, 2)

            polygon = np.array(building_imcoords_list)
            polygon = np.array(polygon, np.float32)
            polygon = np.array(polygon, np.int32)
            cv2.fillPoly(masked_image, np.int32([polygon]), [0, 128, 128])

        road_json = os.path.join(road_labels_path, building_label)
        if os.path.isfile(road_json):  # 도로 json 파일이 존재하는지 확인
            cur_road_json = pd.read_json(road_json)

            for feature in cur_road_json['features']:
                road_imcoords = feature['properties']['road_imcoords']

                if len(road_imcoords) == 0:  # 좌표 정보가 없을 경우 다음 좌표 확인
                    continue
                
                road_imcoords_list = road_imcoords.split(',')
                road_imcoords_list = list_chunk(road_imcoords_list, 2)

                polygon = np.array(road_imcoords_list)
                polygon = np.array(polygon, np.float32)
                polygon = np.array(polygon, np.int32)
                cv2.fillPoly(masked_image, np.int32([polygon]), [128, 64, 128])

        print('current file: {}'.format(building_label))
        plt.imshow(masked_image)
        plt.show()

        save_path = os.path.join(save_dir, building_label.split('.')[0] + '.png')
        result = cv2.imwrite(save_path, masked_image)

        if result == True:
            print('File saved successfully\n')
        else:
            print('{} file: Error in saving file\n'.format(save_path))
```

도로만 있는 경우 마스킹

```python
road_label_list = os.listdir(road_labels_path)

for road_label in road_label_list:
    # json 파일인지 확인
    if road_label.split('.')[1] != 'json':
        continue

    road_img = os.path.join(road_imgs_path, road_label.split('.')[0] + '.png')
    existBuildJson = os.path.join(building_labels_path, road_label)
    if os.path.isfile(road_img) and not os.path.isfile(existBuildJson): # road 이미지가 있는지 확인 & building json 파일이 없는지 확인
        image_array = cv2.imread(road_img)
        zero_mask = np.zeros(image_array.shape[0:3])
        masked_image = zero_mask

        cur_road_json = pd.read_json(road_labels_path + "/" + road_label)

        for feature in cur_road_json['features']:
            road_imcoords = feature['properties']['road_imcoords']

            if len(road_imcoords) == 0:  # 좌표 정보가 없을 경우 다음 좌표 정보 확인
                continue

            road_imcoords_list = road_imcoords.split(',')
            road_imcoords_list = list_chunk(road_imcoords_list, 2)

            polygon = np.array(road_imcoords_list)
            polygon = np.array(polygon, np.float32)
            polygon = np.array(polygon, np.int32)
            cv2.fillPoly(masked_image, np.int32([polygon]), [128, 64, 128])

        print('current file: {}'.format(road_label))
        plt.imshow(masked_image)
        plt.show()

        save_path = os.path.join(save_dir, road_label.split('.')[0] + '.png')
        result = cv2.imwrite(save_path, masked_image)

        if result == True:
            print('File saved successfully\n')
        else:
            print('{} file: Error in saving file\n'.format(save_path))
```

처음 제공받은 데이터 세트는 마스킹 정보가 건물만 있는 것, 도로만 있는 것, 건물+도로만 있는 것 이렇게 세가지라

먼저 건물 디렉토리에 있는 이미지를 읽어 건물 json 파일로 마스킹 한 다음, 해당 이미지의 도로 json 파일도 있는지 확인한 다음 있으면 마스킹을 해주었습니다.

이렇게 하면, json 파일이 건물만 있거나 건물과 도로 모두 있는 경우는 마스킹 되므로 아래 코드를 통해 도로 json 파일만 있는 경우도 마스킹 해주었습니다.

**✓ fillpoly 함수는 RGB가 아닌 BGR 순서로 입력받기 때문에 색깔 지정시 유의해야 합니다.**

<br>

변환 코드는 [Semantic Segmentation of Aerial Images](https://www.kaggle.com/alexalex02/semantic-segmentation-of-aerial-images)을 참조했습니다.

그런 다음, 마스킹 색깔을 1, 2로 맵핑시켜주었습니다.

**mask2class.py**

```python
clr_tab = {}
clr_tab['Clutter'] = [0, 0, 0]
clr_tab['Building'] = [128, 128, 0]
clr_tab['Road'] = [128, 64, 128]

def clr2id(clr):
    return clr[0]+clr[1]*255+clr[2]*255*255
  
id_tab = {}
for k, v in clr_tab.items():
    id_tab[k] = clr2id(v)

print(id_tab)
```

이때는 RGB 순서로 입력해주면 됩니다.

```python
masking_path = '/content/drive/MyDrive/SIA/labels'
save_path = '/content/drive/MyDrive/SIA/maskings'

mask_file_list = os.listdir(masking_path)

for mask_file in mask_file_list:
    cur_file = os.path.join(masking_path, mask_file)
    gt = np.array(Image.open(cur_file))
    trainId = transform(gt, dtype=np.uint8)

    save_file_path = os.path.join(save_path, mask_file)
    print(save_file_path)
    print(np.unique(trainId))
    print('\n')
    Image.fromarray(trainId).save(save_file_path)
```

<img width="548" alt="스크린샷 2021-12-14 오전 9 51 35" src="https://user-images.githubusercontent.com/76269316/145912620-7c0faab7-e82b-43de-94ef-fc64f92cdb7e.png">

이렇게 현재 이미지에 어떤 값들로 채워져 있는지 출력되면서 변환되게 됩니다.

<br><br>

### 2. Initial Setting

[MMSegmentation Tutorial](https://colab.research.google.com/github/open-mmlab/mmsegmentation/blob/master/demo/MMSegmentation_Tutorial.ipynb), [DLCV_New/mask_rcnn](https://github.com/chulminkw/DLCV_New/tree/main/mask_rcnn)을 참고했습니다.

**1. git clone**

먼저 MMSegmentation github를 git clone 해줍니다.

```shell
!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html
!git clone https://github.com/open-mmlab/mmsegmentation.git
```

**✓ 이 이후부터 디렉토리들을 이동하면서 import하니까 cd 명령어를 그대로 사용해야 합니다.**

<br>

```python
import torch

print(f"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})")
```

<br>

torch를 import 해준뒤 mmsegmentation 디렉토리로 이동합니다.

```shell
%cd mmsegmentation
```

<br>

이후 모두 import 해줍니다.

```python
from mmseg.apis import inference_segmentor, init_segmentor, show_result_pyplot
import mmcv
```

```python
import os.path as osp
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
```

<br>

[MMSegmentation github](https://github.com/open-mmlab/mmsegmentation)에 들어가면 다음과 같이 지원되는 모델들이 정리돼 있습니다.

<img width="336" alt="스크린샷 2021-12-14 오후 12 01 35" src="https://user-images.githubusercontent.com/76269316/145925367-c00f8c7d-6208-400c-b017-879722637440.png">

이 중 사용하고자 하는 모델을 클릭해서 들어가면 다음과 같이 pretrain 데이터 세트별로 모델과 configuration 파일들을 다운받을 수 있습니다.

<img width="1028" alt="스크린샷 2021-12-14 오후 12 03 43" src="https://user-images.githubusercontent.com/76269316/145925609-2054e7f4-5543-4354-b370-3d9570e400e6.png">

<br>

checkpoints 디렉토리를 만들고 해당 디렉토리에 사용할 모델 파일을 다운받습니다.

<img width="296" alt="스크린샷 2021-12-14 오후 1 55 15" src="https://user-images.githubusercontent.com/76269316/145935540-c3c1b953-31e0-49e9-a054-cc7edc53fc64.png">

사용할 모델 다운로드 링크는 마우스 우클릭으로 링크 복사해서 붙여넣으면 됩니다.

```shell
!mkdir checkpoints
!wget -O //content/mmsegmentation/checkpoints/fpn_r101_512x1024_80k_cityscapes_20200717_012416-c5800d4c.pth https://download.openmmlab.com/mmsegmentation/v0.5/sem_fpn/fpn_r101_512x1024_80k_cityscapes/fpn_r101_512x1024_80k_cityscapes_20200717_012416-c5800d4c.pth
```

<br>

그런 다음 configuration 파일과 다운받은 모델 파일의 경로를 각각의 변수에 지정해주면 됩니다.

```python
# config 파일을 설정하고, 다운로드 받은 pretrained 모델을 checkpoint로 설정. 
config_file = '/content/mmsegmentation/configs/sem_fpn/fpn_r101_512x1024_80k_cityscapes.py'
checkpoint_file = '/content/mmsegmentation/checkpoints/fpn_r101_512x1024_80k_cityscapes_20200717_012416-c5800d4c.pth'
```

+config 파일은 mmsegmentation 디렉토리의 configs 디렉토리에서 모델에 맞는 파일을 찾아서 사용하면 됩니다.

configuration 파일을 일부 수정해야하는데 아래에서 자세히 설명하도록 하겠습니다.

<br><br>

### 3. Data Structure

MMSegmentation이나 MMDetection을 이용하려면 지원하는 데이터 포맷을 맞춰줘야 합니다.

[Custom Dataset](https://mmsegmentation.readthedocs.io/en/latest/tutorials/customize_datasets.html) 공식 문서가 있긴한데 그렇게 친절하진 않은 것 같습니다 😅

튜토리얼이 코드로 돼 있어서 도움이 더 많이 됐습니다.

<br>

데이터 세트 구조는 다음과 같이 돼 있습니다.

```
SIA
 | 
 ㅡㅡ images: train/validation에서 사용하는 모든 이미지
 |
 ㅡㅡ labels: ground truth (각각의 카테고리 값으로 마스킹된 이미지) 
 |
 ㅡㅡ splits
          |
          ㅡㅡ train.txt: training에 사용할 이미지명
          |
          ㅡㅡ val.txt: validation에 사용할 이미지명
```

images에는 train/validation에 사용할 원본 이미지들(png)이 들어가 있고,

<img src="https://user-images.githubusercontent.com/76269316/145778196-6bfb9809-9a6b-4a2d-b2d9-1c5aad222bed.png" alt="BLD01552_PS3_K3A_NIA0373" style="zoom: 50%;" />

▲images에 들어있는 이미지

labels에서는 train/validation 이름과 동일하지만 gray scale로 마스킹된 이미지들(png)이 들어있습니다.

<img src="https://user-images.githubusercontent.com/76269316/145782846-feb620c9-ddd4-404b-862e-ab96785be6df.png" alt="BLD01552_PS3_K3A_NIA0373 (1)" style="zoom:50%;" />

▲labels에 들어있는 이미지

<br>

train.txt와 valid.txt에는 다음과 같이 확장자를 제외한 이미지명이 들어있습니다.

<img width="794" alt="스크린샷 2021-12-14 오전 10 20 08" src="https://user-images.githubusercontent.com/76269316/145915091-9bf599e1-3ae7-4144-957a-8bb892f160f9.png">

<br>

### 4. Custom Dataset

Custom Dataset 포맷을 사용하기 위해선 다음과 같이 CustomDataset 클래스를 상속받아 클래스를 만들어야 합니다.

```python
from mmseg.datasets.builder import DATASETS
from mmseg.datasets.custom import CustomDataset

classes = ('background', 'building', 'road')
palette = [[0, 0, 0], [128, 128, 0], [128, 64, 128]]

@DATASETS.register_module()
class SIADataset(CustomDataset):
  CLASSES = classes
  PALETTE = palette
  def __init__(self, split, **kwargs):
    super().__init__(img_suffix='.png', seg_map_suffix='.png', 
                     split=split, **kwargs)
    assert osp.exists(self.img_dir) and self.split is not None
```

palette의 경우 나중에 predict 할 때 모델에서 사용할 색인데, 마스킹할 때와 동일한 RGB 값을 주었습니다.

<br>

### 5. Configuration File

MMSegmentation은 모델과 그 모델의 환경설정(configuration) 파일이 존재합니다.

MMSegmentation의 가장 큰 장점은 이 환경설정 파일을 수정하는 것만으로 백본(Backbone) 변경, 데이터 증대(data augmentation), 런타임 세팅(iteration을 얼마나 할 것인지, learning rate는 몇으로 할 것인지 등을 설정)을 할 수 있다는 점입니다.

configuration 파일의 구성은 모델이 달라져도 비슷하기 때문에 손쉽게 다른 모델들을 사용할 수 있습니다. (**이게 정말 큰 장점입니다**)

<br>

먼저 configuration 파일을 살펴보겠습니다.

```python
# config 파일을 설정하고, 다운로드 받은 pretrained 모델을 checkpoint로 설정. 
config_file = '/content/mmsegmentation/configs/sem_fpn/fpn_r101_512x1024_80k_cityscapes.py'
checkpoint_file = '/content/mmsegmentation/checkpoints/fpn_r101_512x1024_80k_cityscapes_20200717_012416-c5800d4c.pth'

from mmcv import Config

cfg = Config.fromfile(config_file)
print(cfg.pretty_text)
```

<br>

```python
norm_cfg = dict(type='SyncBN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained='open-mmlab://resnet101_v1c',
    backbone=dict(
        type='ResNetV1c',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        dilations=(1, 1, 1, 1),
        strides=(1, 2, 2, 2),
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        norm_eval=False,
        style='pytorch',
        contract_dilation=True),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=4),
    decode_head=dict(
        type='FPNHead',
        in_channels=[256, 256, 256, 256],
        in_index=[0, 1, 2, 3],
        feature_strides=[4, 8, 16, 32],
        channels=128,
        dropout_ratio=0.1,
        num_classes=19,
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        align_corners=False,
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'CityscapesDataset'
data_root = 'data/cityscapes/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (512, 1024)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(512, 1024), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(512, 1024), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(2048, 1024),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='CityscapesDataset',
        data_root='data/cityscapes/',
        img_dir='leftImg8bit/train',
        ann_dir='gtFine/train',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(
                type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(512, 1024), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(512, 1024), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ]),
    val=dict(
        type='CityscapesDataset',
        data_root='data/cityscapes/',
        img_dir='leftImg8bit/val',
        ann_dir='gtFine/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 1024),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CityscapesDataset',
        data_root='data/cityscapes/',
        img_dir='leftImg8bit/val',
        ann_dir='gtFine/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 1024),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0005)
optimizer_config = dict()
lr_config = dict(policy='poly', power=0.9, min_lr=0.0001, by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=80000)
checkpoint_config = dict(by_epoch=False, interval=8000)
evaluation = dict(interval=8000, metric='mIoU', pre_eval=True)
```

configuration 파일은 파이썬 파일로, dictionary 형태로 존재합니다.

<br>

이 부분은 모델 부분으로 backbone, neck, head를 설정합니다.

```python
model = dict(
    type='EncoderDecoder',
    pretrained='open-mmlab://resnet101_v1c',
    backbone=dict(
        type='ResNetV1c',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        dilations=(1, 1, 1, 1),
        strides=(1, 2, 2, 2),
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        norm_eval=False,
        style='pytorch',
        contract_dilation=True),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=4),
    decode_head=dict(
        type='FPNHead',
        in_channels=[256, 256, 256, 256],
        in_index=[0, 1, 2, 3],
        feature_strides=[4, 8, 16, 32],
        channels=128,
        dropout_ratio=0.1,
        num_classes=19,
        norm_cfg=dict(type='SyncBN', requires_grad=True),
        align_corners=False,
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
```

우선 여기서 수정해줘야 할 부분은 **norm_cfg** 입니다. (Multi GPU를 사용하신 분들은 수정하지 않아도 됩니다)

모델 생성시 Multi GPU를 사용했기 때문에 모두 **type='SyncBN'**으로 돼 있는데, 저처럼 colab을 사용하시거나 로컬로 single GPU를 사용하시는 분들은 다음과 같이 **type='BN'**으로 변경해야 합니다.

또한 decode_head의 num_classes가 19로 돼 있는데, 저희는 3개(배경, 건물, 도로)이기 때문에 3으로 변경해줍니다.

**✓ 현재 configuration 파일에는 auxiliary_head가 없는데 만약 사용하려는 모델의 configuration 파일에 auxiliary_head가 있다면, 마찬가지로 num_classes를 변경해줘야 합니다.**

<br>

Training, Validation pipeline을 구성하는 부분입니다.

이 부분을 수정함으로써 다양한 augmentation을 사용할 수 있습니다.

**참고할만한 문서**

- [Customize Data Pipelines](https://mmsegmentation.readthedocs.io/en/latest/tutorials/data_pipeline.html)
- [Image Data Augmentation Overview](https://hoya012.github.io/blog/Image-Data-Augmentation-Overview/)

```python
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (512, 1024)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(512, 1024), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(512, 1024), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(2048, 1024),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
```

주어진 데이터 세트는 모두 1024x1024 크기를 갖고 있기 때문에 dict(type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)) 이 부분에서 img_scale만 (1024, 1024)로 변경해주었습니다.

+데이터 증대를 사용하고 싶지 않은 경우 주석처리하거나 지워서 사용하지 않을 수 있다는데, resize만 남기고 주석처리 하니 에러가 발생해 Resize, RandomCrop만 사용하고 진행했습니다.

방법을 아시는 분이 계시다면 댓글 남겨주세요 😥

[How to remove augmentation pipelines in the train_pipeline and val_pipeline? #1026](https://github.com/open-mmlab/mmsegmentation/issues/1026)

<br>

데이터 세트 설정 부분인데, dataset_type을 위에서 CustomDataset 클래스를 상속해 만든 SIADataset로 변경해줬고, 이외에 경로 설정 부분을 변경해줬는데 아래 configuration setting 부분에서 설명하겠습니다.

그리고 samples_per_gpu, workers_per_gpu가 batch size를 지정하는 변수인데, 모두 8로 변경했습니다.

```python
dataset_type = 'CityscapesDataset'
data_root = 'data/cityscapes/'

data = dict(
    samples_per_gpu=2,  #batch size
    workers_per_gpu=2,  
    train=dict(
        type='CityscapesDataset',
        data_root='data/cityscapes/',
        img_dir='leftImg8bit/train',
        ann_dir='gtFine/train',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(
                type='Resize', img_scale=(2048, 1024), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(512, 1024), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(512, 1024), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ]),
    val=dict(
        type='CityscapesDataset',
        data_root='data/cityscapes/',
        img_dir='leftImg8bit/val',
        ann_dir='gtFine/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 1024),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CityscapesDataset',
        data_root='data/cityscapes/',
        img_dir='leftImg8bit/val',
        ann_dir='gtFine/val',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(2048, 1024),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
```

<br><br>

### 6. Configuration Setting

먼저 모델 부분의 norm_cfg 부분의 type을 BN으로 바꾸고, 클래스 개수를 3개로 변경했습니다.

```python
cfg.norm_cfg = dict(type='BN', requires_grad=True)
cfg.model.backbone.norm_cfg = cfg.norm_cfg
cfg.model.decode_head.norm_cfg = cfg.norm_cfg

cfg.model.decode_head.num_classes = 3
```

<br>

이후 train_pipeline과 test_pipeline의 img_scale만 (1024, 1024)로 바꿔주었고 나머지는 그대로 사용했습니다.

*RandomFlip이나 PhotoMetricDistortion, Padding을 사용하고 싶지 않은 경우 주석 처리해도 됩니다.*

*단 나머지는 그대로 두셔야합니다.*

```python
cfg.img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
cfg.crop_size = (512, 1024)
cfg.train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1024, 1024), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=cfg.crop_size, cat_max_ratio=0.75),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(type='Normalize', **cfg.img_norm_cfg),
    dict(type='Pad', size=cfg.crop_size, pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg']),
]

cfg.test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 1024),
        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75],
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **cfg.img_norm_cfg),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ])
]
```

<br>

dataset_type을 위에서 정의한 SIADataset를 사용했고, 경로를 지정해줬습니다.

```python
cfg.dataset_type = 'SIADataset'
cfg.data_root = '/content/drive/MyDrive/SIA'

cfg.data.train.type = 'SIADataset'
cfg.data.train.data_root = '/content/drive/MyDrive/SIA'
cfg.data.train.img_dir = 'images'
cfg.data.train.ann_dir = 'labels'
cfg.data.train.pipeline = cfg.train_pipeline
cfg.data.train.split = 'splits/train.txt'

cfg.data.val.type = 'SIADataset'
cfg.data.val.data_root = '/content/drive/MyDrive/SIA'
cfg.data.val.img_dir = 'images'
cfg.data.val.ann_dir = 'labels'
cfg.data.val.pipeline = cfg.test_pipeline
cfg.data.val.split = 'splits/val.txt'

cfg.data.test.type = 'SIADataset'
cfg.data.test.data_root = '/content/drive/MyDrive/SIA'
cfg.data.test.img_dir = 'images'
cfg.data.test.ann_dir = 'labels'
cfg.data.test.pipeline = cfg.test_pipeline
cfg.data.test.split = 'splits/val.txt'
```

<br>

```
SIA
 | 
 ㅡㅡ images: train/validation에서 사용하는 모든 이미지
 |
 ㅡㅡ labels: ground truth (각각의 카테고리 값으로 마스킹된 이미지) 
 |
 ㅡㅡ splits
          |
          ㅡㅡ train.txt: training에 사용할 이미지명
          |
          ㅡㅡ val.txt: validation에 사용할 이미지명
```

현재 데이터는 다음과 같은 구조로 구성돼 있기 때문에 data_root 부분을 **'/content/drive/MyDrive/SIA'**로 설정했고,

원본 이미지들은 image 디렉토리에 있기 때문에 img_dir를 **images**로, gray scale로 masking한 이미지가 labels 디렉토리 밑에 있기 때문에 ann_dir를 **'labels'**로 설정했습니다.

마찬가지로, train에 사용되는 이미지명은 splits 디렉토리 밑의 train.txt에 있기 때문에 split은 **'splits/train.txt'**로 설정했습니다.

**✓ 이 때 절대경로와 상대경로가 혼용되서 사용되기 때문에 경로 설정에 주의해야 합니다.** 

*img_dir, ann_dir, split의 시작에는 /가 붙지 않음*

```python
cfg.data.train.type = 'SIADataset'
cfg.data.train.data_root = '/content/drive/MyDrive/SIA'
cfg.data.train.img_dir = 'images'
cfg.data.train.ann_dir = 'labels'
cfg.data.train.pipeline = cfg.train_pipeline
cfg.data.train.split = 'splits/train.txt'
```

<br>

pipeline 부분은 위에서 만든 cfg.train_pipeline과 cfg.test_pipeline을 그대로 사용했습니다.

<br>

cfg.load_from은 사용할 모델 파일의 경로를 저장하는 변수이고, cfg.work_dir는 모델 학습시 지정한 iteration마다 pth 파일로 모델 파일이 생성되게 되는데 그 파일의 저장 경로를 갖고 있는 변수입니다.

```python
cfg.load_from = '/content/mmsegmentation/checkpoints/fpn_r101_512x1024_80k_cityscapes_20200717_012416-c5800d4c.pth'

# Set up working dir to save files and logs.
cfg.work_dir = '/content/drive/MyDrive/SIA/semantic_checkpoint'
```

<br>

MMSegmentation은 epoch 기반이 아닌 iteration 기반으로 동작합니다.

보통 Detection이나 Segmentation에서 epoch를 기준으로 하는데 MMSegmentation은 달라서 찾아본 결과,

epoch 기반보다 iteration 기반이 더 유연하고 (epoch는 한 epoch이 완료될 때만 저장할 수 있지만 iteration은 원할 때 저장할 수 있어서), CSAIL semantic segmentation, torchcv 같은 현장에서도 사용하기 때문에 iteration 기반으로 설계했다고 합니다.

[Why mmseg uses IterBasedRunner rather than EpochBasedRunner #76](https://github.com/open-mmlab/mmsegmentation/issues/76)

+Epoch 기반으로 변경하기 [runner type selection](https://issueexplorer.com/issue/open-mmlab/mmsegmentation/926)

```python
cfg.runner.max_iteTrs = 200
cfg.log_config.interval = 50
cfg.evaluation.interval = 1000  # 모델 학습시 평가를 몇 번째 iteration마다 할 것인지 지정
cfg.checkpoint_config.interval = 1000  # 모델 학습시 학습한 모델을 몇 번째 iteration마다 저장할 것인지 지정

cfg.runner = dict(type='IterBasedRunner', max_iters=4000)  # Iteration으로 동작, Epoch로 동작하게 변경할 수도 있음
# cfg.runner = dict(type='EpochBasedRunner', max_epochs=4000)  # Epoch로 변경
cfg.workflow = [('train', 1)]

# Set seed to facitate reproducing the result
cfg.seed = 0
set_random_seed(0, deterministic=False)
cfg.gpu_ids = range(1)

# Let's have a look at the final config used for training
print(f'Config:\n{cfg.pretty_text}')
```

변경하면 다음과 같이 출력됩니다.

```python
Config:
norm_cfg = dict(type='BN', requires_grad=True)
model = dict(
    type='EncoderDecoder',
    pretrained='open-mmlab://resnet101_v1c',
    backbone=dict(
        type='ResNetV1c',
        depth=101,
        num_stages=4,
        out_indices=(0, 1, 2, 3),
        dilations=(1, 1, 1, 1),
        strides=(1, 2, 2, 2),
        norm_cfg=dict(type='BN', requires_grad=True),
        norm_eval=False,
        style='pytorch',
        contract_dilation=True),
    neck=dict(
        type='FPN',
        in_channels=[256, 512, 1024, 2048],
        out_channels=256,
        num_outs=4),
    decode_head=dict(
        type='FPNHead',
        in_channels=[256, 256, 256, 256],
        in_index=[0, 1, 2, 3],
        feature_strides=[4, 8, 16, 32],
        channels=128,
        dropout_ratio=0.1,
        num_classes=3,
        norm_cfg=dict(type='BN', requires_grad=True),
        align_corners=False,
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
dataset_type = 'SIADataset'
data_root = '/content/drive/MyDrive/SIA'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (512, 1024)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', img_scale=(1024, 1024), ratio_range=(0.5, 2.0)),
    dict(type='RandomCrop', crop_size=(512, 1024), cat_max_ratio=0.75),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(512, 1024), pad_val=0, seg_pad_val=255),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_semantic_seg'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 1024),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=8,
    workers_per_gpu=8,
    train=dict(
        type='SIADataset',
        data_root='/content/drive/MyDrive/SIA',
        img_dir='images',
        ann_dir='labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations'),
            dict(
                type='Resize', img_scale=(1024, 1024), ratio_range=(0.5, 2.0)),
            dict(type='RandomCrop', crop_size=(512, 1024), cat_max_ratio=0.75),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(512, 1024), pad_val=0, seg_pad_val=255),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_semantic_seg'])
        ],
        split='splits/train.txt'),
    val=dict(
        type='SIADataset',
        data_root='/content/drive/MyDrive/SIA',
        img_dir='images',
        ann_dir='labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 1024),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/val.txt'),
    test=dict(
        type='SIADataset',
        data_root='/content/drive/MyDrive/SIA',
        img_dir='images',
        ann_dir='labels',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 1024),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ],
        split='splits/val.txt'))
log_config = dict(
    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = '/content/mmsegmentation/checkpoints/fpn_r101_512x1024_80k_cityscapes_20200717_012416-c5800d4c.pth'
resume_from = None
workflow = [('train', 1)]
cudnn_benchmark = True
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0005)
optimizer_config = dict()
lr_config = dict(policy='poly', power=0.9, min_lr=0.0001, by_epoch=False)
runner = dict(type='IterBasedRunner', max_iters=4000)
checkpoint_config = dict(by_epoch=False, interval=1000)
evaluation = dict(interval=1000, metric='mIoU', pre_eval=True)
work_dir = '/content/drive/MyDrive/SIA/semantic_checkpoint'
seed = 0
gpu_ids = range(0, 1)
```

<br><br>

### 7. Training

이제 모델 학습을 진행해 보겠습니다.

```python
from mmseg.datasets import build_dataset
from mmseg.models import build_segmentor
from mmseg.apis import train_segmentor

# Build the dataset
datasets = [build_dataset(cfg.data.train)]

# Build the detector
model = build_segmentor(
    cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))

# Add an attribute for visualization convenience
model.CLASSES = datasets[0].CLASSES

# Create work_dir
mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))
train_segmentor(model, datasets, cfg, distributed=False, validate=True, 
                meta=dict(CLASSES=classes, PALETTE=palette))
```

<img width="416" alt="스크린샷 2021-12-14 오후 2 23 18" src="https://user-images.githubusercontent.com/76269316/145938167-94a28f97-b6ee-417a-95e2-67ef6d17051b.png">

설정한 iteration마다 이렇게 실행 결과가 나오게 됩니다.

<br>

### 8. Inference

다음과 같이 이미지를 inference 해볼 수 있습니다.

```python
checkpoint_file = '/content/drive/MyDrive/SIA/checkpoints/iter_28000.pth'  #학습된 모델

# checkpoint 저장된 model 파일을 이용하여 모델을 생성, 이때 Config는 위에서 update된 config 사용. 
model_ckpt = init_segmentor(cfg, checkpoint_file, device='cuda:0')  #cuda gpu 사용

img = mmcv.imread('/content/drive/MyDrive/SIA/images/BLD01552_PS3_K3A_NIA0373.png')
result = inference_segmentor(model_ckpt, img)
show_result_pyplot(model_ckpt, img, result, palette)
```

<img src="https://user-images.githubusercontent.com/76269316/145939512-3189bb3b-59b5-4947-99ee-3f685a121a7a.png" alt="image" style="zoom: 67%;" />

