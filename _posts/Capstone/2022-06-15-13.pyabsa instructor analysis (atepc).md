---
title:  "[Capstone] pyABSA 프레임워크 분석 - Instructor (ATEPC)"
excerpt: "pyABSA 프레임워크 분석"
toc: true
toc_label: "pyABSA 프레임워크 분석 - Instructor (ATEPC)"
toc_sticky: true
published: true

categories:
  - Capstone
tags:
  - PyTorch
  - pyABSA
last_modified_at: 2022-06-25

---

> 이전 포스팅에서 이어지는 내용입니다. [pyABSA 프레임워크 분석 - ABSADatasetList](https://seominseok4834.github.io/capstone/11.pyabsa-dataset-item-analysis/)
>
> 모든 코드는 [PyABSA github](https://github.com/yangheng95/PyABSA)에서 가져왔습니다.

<br>

아래 코드는 데모 코드로 다음과 같은 순서로 실행됩니다.

```python
from pyabsa.functional import ATEPCModelList
from pyabsa.functional import Trainer, ATEPCTrainer
from pyabsa.functional import ABSADatasetList
from pyabsa.functional import ATEPCConfigManager

atepc_config = ATEPCConfigManager.get_atepc_config_english()  # 1 환경변수 설정

atepc_config.pretrained_bert = 'microsoft/deberta-v3-base'
atepc_config.model = ATEPCModelList.FAST_LCF_ATEPC
dataset_path = ABSADatasetList.Restaurant14  # 2 데이터셋 설정
# or your local dataset: dataset_path = 'your local dataset path'

# 모델 로드 및 훈련
aspect_extractor = ATEPCTrainer(config=atepc_config,
                                dataset=dataset_path,
                                from_checkpoint='',  # set checkpoint to train on the checkpoint.
                                checkpoint_save_mode=1,
                                auto_device=True
                                ).load_trained_model()
```

먼저 ATEPCConfigManager라는 클래스의 get_atepce_config_english 메소드를 통해 configuration 변수를 가져온 뒤(#1), ABSADatasetList 클래스의 멤버 변수 Restaurant14 데이터셋의 경로를 가져온 뒤(#2), ATEPCTrainer 클래스를 통해 모델을 생성, 훈련을 진행합니다.(#3)

이번 포스팅에서는 #3 부분에 대해서 다룹니다.

<br>

✔ configuration에 저장된 값들을 불러와 사용하는 부분이 많습니다. 어떤 값이 들어가는지 궁금하다면 아래 configuration을 참고해주세요.

```python
ATEPCConfigManager(args={'model': <class 'pyabsa.core.atepc.models.fast_lcf_atepc.FAST_LCF_ATEPC'>, 'optimizer': 'adamw', 'learning_rate': 2e-05, 'pretrained_bert': 'microsoft/deberta-v3-base', 'cache_dataset': True, 'warmup_step': -1, 'use_bert_spc': False, 'show_metric': False, 'max_seq_len': 80, 'SRD': 3, 'use_syntax_based_SRD': False, 'lcf': 'cdw', 'window': 'lr', 'dropout': 0.5, 'l2reg': 1e-05, 'num_epoch': 10, 'batch_size': 16, 'initializer': 'xavier_uniform_', 'seed': [52], 'polarities_dim': 3, 'log_step': 50, 'patience': 99999, 'gradient_accumulation_steps': 1, 'dynamic_truncate': True, 'srd_alignment': True, 'evaluate_begin': 0, 'hidden_dim': 768, 'embed_dim': 768, 'ABSADatasetsVersion': '2022.06.10', 'dataset_name': 'Restaurant14', 'dataset_file': {'train': ['integrated_datasets/atepc_datasets/110.SemEval/114.restaurant14/Restaurants_Train.xml.seg.atepc'], 'test': ['integrated_datasets/atepc_datasets/110.SemEval/114.restaurant14/Restaurants_Test_Gold.xml.seg.atepc'], 'valid': []}, 'device': device(type='cuda', index=0), 'device_name': 'NVIDIA GeForce RTX 2080 Ti', 'auto_device': True, 'model_name': 'fast_lcf_atepc', 'PyABSAVersion': '1.14.8', 'TransformersVersion': '4.18.0', 'TorchVersion': '1.10.2+cu102+cuda10.2', 'MV': <metric_visualizer.metric_visualizer.MetricVisualizer object at 0x7fe8bef1aee0>, 'save_mode': 1, 'model_path_to_save': 'checkpoints', 'sep_indices': 2, 'spacy_model': 'en_core_web_sm', 'IOB_label_to_index': {'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}, 'index_to_label': {0: 'Negative', 1: 'Neutral', 2: 'Positive'}, 'label_to_index': {'Negative': 0, 'Neutral': 1, 'Positive': 2}, 'index_to_IOB_label': {1: 'B-ASP', 2: 'I-ASP', 3: 'O', 4: '[CLS]', 5: '[SEP]'}, 'label_list': ['B-ASP', 'I-ASP', 'O', '[CLS]', '[SEP]'], 'num_labels': 6, 'max_test_metrics': {'max_apc_test_acc': 87.12, 'max_apc_test_f1': 80.93, 'max_ate_test_f1': 88.81}, 'metrics_of_this_checkpoint': {'apc_acc': 86.23, 'apc_f1': 79.56, 'ate_f1': 87.69}}, args_call_count={'model': 5, 'optimizer': 2, 'learning_rate': 1, 'pretrained_bert': 4, 'cache_dataset': 1, 'warmup_step': 2261, 'use_bert_spc': 14860, 'show_metric': 0, 'max_seq_len': 50176, 'SRD': 9444, 'use_syntax_based_SRD': 4722, 'lcf': 17146, 'window': 0, 'dropout': 1, 'l2reg': 2, 'num_epoch': 2, 'batch_size': 5, 'initializer': 0, 'seed': 7, 'polarities_dim': 91, 'log_step': 2261, 'patience': 27, 'gradient_accumulation_steps': 3, 'dynamic_truncate': 9444, 'srd_alignment': 0, 'evaluate_begin': 91, 'hidden_dim': 6, 'embed_dim': 0, 'ABSADatasetsVersion': 0, 'dataset_name': 30, 'dataset_file': 95, 'device': 77072, 'device_name': 0, 'auto_device': 14861, 'model_name': 4855, 'PyABSAVersion': 0, 'TransformersVersion': 0, 'TorchVersion': 0, 'MV': 5, 'save_mode': 53, 'model_path_to_save': 55, 'sep_indices': 136660, 'spacy_model': 3, 'IOB_label_to_index': 1, 'index_to_label': 2, 'label_to_index': 0, 'index_to_IOB_label': 0, 'label_list': 2135162, 'num_labels': 3, 'max_test_metrics': 629, 'metrics_of_this_checkpoint': 270})
```

<br>

✔ 프레임워크다보니까 대부분의 함수가 모듈화 돼 있어서 코드가 왔다갔다 합니다. 설명하던 함수로 돌아올 경우 ❗️를 사용해서 표시하도록 하겠습니다.

<br>

### Trainer

Trainer 클래스는 [PyABSA/pyabsa/functional/trainer/trainer.py](https://github.com/yangheng95/PyABSA/blob/release/pyabsa/functional/trainer/trainer.py#L59)에 정의돼 있습니다.

원래 이번 포스팅에서 Trainer 클래스에 대해 정리하려고 했는데, Trainer에서 train4atepc 메소드를 호출하면 Instructor 클래스가 생성되고 훈련까지 진행됩니다. 그래서 이번 포스팅에서는 실제 모델 생성 및 훈련을 담당하는 Instructor 클래스가 주된 내용이 될 것 같습니다.

<br>

데모 코드에서 생성한 ATEPCTrainer 클래스는 Trainer 클래스를 상속 받은 클래스입니다.

```python
class APCTrainer(Trainer):
    pass


class ATEPCTrainer(Trainer):
    pass


class TCTrainer(Trainer):
    pass


class AOTCTrainer(Trainer):
    pass
```

cf) [pass, continue, break 차이점 알아보기](https://chancoding.tistory.com/7)

<br>

Trainer 클래스의 생성자를 먼저 살펴보겠습니다.

전체 코드가 너무 기니까 line by line으로 보겠습니다.

**trainer.py**

59 ~ 68 line

```python
class Trainer:
    def __init__(self,
                 config: ConfigManager = None,
                 dataset=None,
                 from_checkpoint: str = None,
                 checkpoint_save_mode: int = 0,
                 auto_device=True,
                 path_to_save=None,
                 load_aug=False
                 ):
```

먼저 함수 인자입니다. 데모 코드에서와 동일하게 ConfigManager, ABSADatasetList, checkpoint, auto_device 등을 넘겨받습니다.

```python
aspect_extractor = ATEPCTrainer(config=atepc_config,
                                dataset=dataset_path,
                                from_checkpoint='',  # set checkpoint to train on the checkpoint.
                                checkpoint_save_mode=1,
                                auto_device=True
                                ).load_trained_model()
```

<br>

**Trainer \_\_init__**

84 ~ 86 line

```python
if not torch.cuda.device_count() > 1 and auto_device == 'allcuda':
  print('Cuda count <= 1, reset auto_device=True')
  auto_device = True
```

다음 라인에서는 GPU를 확인합니다. 사용할 수 있는 GPU가 없고, auto_device가 allcuda로 설정돼 있으면, auto_device를 True로 변경합니다.

+torch.cuda.device_count()는 사용가능한 GPU 개수를 반환합니다.

<br>

**Trainer \_\_init__**

87 ~ 91 line

```python
if 'hidden_dim' not in config.args or 'embed_dim' not in config.args:
  pretrain_config = AutoConfig.from_pretrained(config.pretrained_bert)  # Hugging Face의 configuration 파일 로드
  config.hidden_dim = pretrain_config.hidden_size  # Hugging Face Bert Model의 hidden dim
  config.embed_dim = pretrain_config.hidden_size
  config.ABSADatasetsVersion = query_local_version()
```

87 ~ 91 line에서는 hidden_dim과 embed_dim이 configuration에 정의돼 있지 않는 경우,

데모 코드에서 pertained_bert로 'microsoft/deberta-v3-base'를 사용했는데 해당 Hugging Face의 configuration 파일을 불러와 configuration의 hidden_dim과 embed_dim으로 저장합니다.

```python
atepc_config.pretrained_bert = 'microsoft/deberta-v3-base'
```

<br>

query_local_version 메소드는 [PyABSA/pyabsa/utils/file_utils.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/utils/file_utils.py#L310)에 정의돼 있습니다.

**query_local_version**

310 ~ 317 line

```python
def query_local_version():
    try:
        fin = open(find_cwd_file(['__init__.py', 'integrated_datasets']))
        local_version = fin.read().split('\'')[-2]
        fin.close()
    except:
        return None
    return local_version
```

find_cwd_file은 [findfile](https://github.com/yangheng95/findfile) 프레임워크의 메소드로, 파일 경로를 리턴합니다. 리턴 받은 경로의 파일을 읽은 뒤, version을 가져와 configuration에 저장합니다.

+findfile 프레임워크는 추후 따로 정리하도록 하겠습니다.

<br>

다시 [Trainer 클래스의 생성자](https://github.com/yangheng95/PyABSA/blob/release/pyabsa/functional/trainer/trainer.py#L92)로 돌아와서❗️

**Trainer \_\_init__**

92 ~ 95 line

```python
if isinstance(config, APCConfigManager):
  self.train_func = train4apc
  self.model_class = SentimentClassifier
  self.task = 'apc'

elif isinstance(config, ATEPCConfigManager):  # 현재 config는 ATEPCConfigManager
  self.train_func = train4atepc
  self.model_class = AspectExtractor
  self.task = 'atepc'

elif isinstance(config, TCConfigManager):
  self.train_func = train4tc
  self.model_class = TextClassifier
  self.task = 'classification'

elif isinstance(config, AOTCConfigManager):
  self.train_func = train4ao_tc
  self.model_class = AOTCTextClassifier
  self.task = 'ao_tc'
```

config가 어떤 태스크에 대한 config 클래스인지 체크합니다. 저희는 ATEPC 태스크를 진행 할 예정이기 때문에 elif isinstance(config, ATEPCConfigManager) 부분이 실행됩니다.

<br>

train_func에 train4atepc가 들어가게 되는데, train4atepc를 이해하기 위해선 데코레이터를 알아야 합니다.

데코레이터 함수에 대한 자세한 설명은 다음 세 개의 글을 참고해주세요.

> [파이썬 - 퍼스트클래스 함수 (First Class Function)](https://schoolofweb.net/blog/posts/%ed%8c%8c%ec%9d%b4%ec%8d%ac-%ed%8d%bc%ec%8a%a4%ed%8a%b8%ed%81%b4%eb%9e%98%ec%8a%a4-%ed%95%a8%ec%88%98-first-class-function/)
>
> [파이썬 - 클로저 (Closure)](https://schoolofweb.net/blog/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%81%B4%EB%A1%9C%EC%A0%80-closure/)
>
> [파이썬 - 데코레이터 (Decorator)](https://schoolofweb.net/blog/posts/%ed%8c%8c%ec%9d%b4%ec%8d%ac-%eb%8d%b0%ec%bd%94%eb%a0%88%ec%9d%b4%ed%84%b0-decorator/)

<br>

train4atepc 메소드는 [PyABSA/pyabsa/core/atepc/training/atepc_trainer.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/training/atepc_trainer.py#L460)에 정의돼 있는데, @retry라는 심볼이 붙어있습니다.

**train4atepc**

460 ~ 465 line

```python
@retry
def train4atepc(opt, from_checkpoint_path, logger):
    # in case of handling ConnectionError exception
    trainer = Instructor(opt, logger)
    resume_from_checkpoint(trainer, from_checkpoint_path)

    return trainer.run()
```

이는 [PyABSA/pyabsa/utils/pyabsa_utils.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/utils/pyabsa_utils.py#L155)에 정의된 retry 데코레이터 함수의 인자로 넘겨 실행하는 구문입니다.

**retry**

155 ~ 177 line

```python
def retry(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        count = 5
        while count:

            try:
                return f(*args, **kwargs)
            except (
                TransformerConnectionError,
                requests.exceptions.RequestException,
                requests.exceptions.ConnectionError,
                requests.exceptions.HTTPError,
                requests.exceptions.ConnectTimeout,
                requests.exceptions.ProxyError,
                requests.exceptions.SSLError,
                requests.exceptions.BaseHTTPError,
            ) as e:
                print('Training Exception: {}, will retry later'.format(e))
                time.sleep(60)
                count -= 1

    return decorated
```

retry 함수는 단순히 인자로 받은 메소드를 실행하는 역할을 합니다. exception 발생시 5번까지 재실행 합니다.

<br>

다시 [train4atepc](https://github.com/yangheng95/PyABSA/blob/4320db7dd0cfa0053a48169e59ff44403482eaf1/pyabsa/core/atepc/training/atepc_trainer.py#L460) 메소드로 돌아오면❗️

```python
@retry
def train4atepc(opt, from_checkpoint_path, logger):
    # in case of handling ConnectionError exception
    trainer = Instructor(opt, logger)
    resume_from_checkpoint(trainer, from_checkpoint_path)

    return trainer.run()
```

Instruct 클래스 인스턴스를 생성한 뒤 run 메소드까지 실행하기 때문에 Instructor 클래스의 ]생성자부터 run 메소드까지 이어서 설명하도록 하겠습니다.

추가적으로 trainer의 파라미터로 들어가는 opt와 logger는 ConfigManager와 파이썬 기본 내장 모듈인 logging입니다. (trainer.py 163 ~ 167 line)

```python
if self.checkpoint_save_mode:
  model_path.append(self.train_func(self.config, self.from_checkpoint, self.logger))
else:
  # always return the last trained model if dont save trained model
  model = self.model_class(model_arg=self.train_func(self.config, self.from_checkpoint, self.logger))
```

<br><br>

이번 포스팅에서 집중적으로 다룰 Instructor 클래스입니다.

### Instructor

Instructor 클래스는 [PyABSA/pyabsa/core/atepc/training/atepc_trainer.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/training/atepc_trainer.py#L40)에 정의돼 있습니다.

<script src="https://gist.github.com/seominseok4834/1981c64fe691a228a0149db6638e2c6d.js"></script>

<br>

코드가 길어서 생성자 부분을 라인별로 나눠서보도록 하겠습니다.

**Instructor \_\_init__**

42 ~ 65 line

```python
def __init__(self, opt, logger):
  self.warmup_scheduler = None
  self.lr_scheduler = None
  self.opt = opt
  self.logger = logger

  self.train_dataloader = None
  self.test_dataloader = None
  # if opt.use_bert_spc:
  #     self.logger.info('Warning: The use_bert_spc is disabled for extracting aspect,'
  #                      ' reset use_bert_spc=False and go on... ')
  #     opt.use_bert_spc = False
  import warnings
  warnings.filterwarnings('ignore')
  if self.opt.gradient_accumulation_steps < 1:
    raise ValueError("Invalid gradient_accumulation_steps parameter: {}, should be >= 1".format(
      self.opt.gradient_accumulation_steps))

    self.opt.batch_size = self.opt.batch_size // self.opt.gradient_accumulation_steps

    random.seed(self.opt.seed)
    np.random.seed(self.opt.seed)
    torch.manual_seed(self.opt.seed)
    torch.cuda.manual_seed(self.opt.seed)
```

클래스 내에서 사용할 멤버 변수를 정의합니다.

gradient accumulation은 미니 배치를 통해 gradient를 n-step 동안 gloabl gradients에 누적시킨 후, 한 번에 업데이트하는 방법입니다. cf) [Tensorflow Gradient Accumulation 간단 구현](https://hwiyong.tistory.com/419)

이를 위해 batch size를 gradient accumulation_steps로 나눈 몫을 batch size로 재정의해주었고, 훈련 결과 재현을 위해 시드를 고정했습니다.

<br>

**Instructor \_\_init__**

67 ~ 77 line

```python
if self.opt.model_path_to_save and not os.path.exists(self.opt.model_path_to_save):  # 1
  os.makedirs(self.opt.model_path_to_save)

  try:  # 2
    self.tokenizer = AutoTokenizer.from_pretrained(self.opt.pretrained_bert, do_lower_case='uncased' in self.opt.pretrained_bert)
    bert_base_model = AutoModel.from_pretrained(self.opt.pretrained_bert)
    self.opt.sep_indices = self.tokenizer.sep_token_id

  except ValueError as e:
    print('Init pretrained model failed, exception: {}'.format(e))
    raise TransformerConnectionError()
```

#1: 모델을 저장할 경로가 존재하는지 확인하고, 없다면 디렉토리를 생성합니다.

#2: try 구문으로 토크나이저와 pretrained 모델을 불러옵니다.

<br>

**Instructor \_\_init__**

79 line

```python
processor = ATEPCProcessor(self.tokenizer)
```

ATEPCProcessor 클래스의 인스턴스를 생성합니다. ✅ *이 부분은 이후 포스팅에서 설명하도록 하겠습니다.*

<br>

**Instructor \_\_init__**

81 ~ 83 line

```python
config_str = re.sub(r'<.*?>', '', str(sorted([str(self.opt.args[k]) for k in self.opt.args if k != 'seed'])))  # 1
hash_tag = sha256(config_str.encode()).hexdigest()  # 2
cache_path = '{}.{}.dataset.{}.cache'.format(self.opt.model_name, self.opt.dataset_name, hash_tag)  # 3

if os.path.exists(cache_path):  # cache_path가 존재하는 경우
  print('Loading dataset cache:', cache_path)
  with open(cache_path, mode='rb') as f:
    if self.opt.dataset_file['test']:  # 4
      self.train_data, self.test_data, opt = pickle.load(f)

    else:  # 5
      self.train_data, opt = pickle.load(f)
      # reset output dim according to dataset labels
      self.opt.polarities_dim = opt.polarities_dim
```

#1: configuration에 저장된 key 값들을 순회하면서 value 값을 string 형태로 정렬해서 저장합니다. (re.sub 메소드를 사용해서 <> 제거)

#2: 저장한 configuration 값들을 SHA256으로 암호화합니다.

#3: cache path를 생성해 해당 파일이 존재하는 경우 읽어들입니다.

#4: 만약 test 데이터셋이 존재하면, pickle.load 메소드로 train 데이터와 test 데이터를 읽어들입니다.

#5: test 데이터셋이 존재하지 않는 경우 train_data와 opt를 읽어들이고, 감성극성의 차원을 멤버변수인 opt에 저장합니다.

<br>

**Instructor \_\_init__**

96 ~ 136 line

```python
else:  # cache_path가 존재하지 않는 경우
  self.train_examples = processor.get_train_examples(self.opt.dataset_file['train'], 'train')  # 1
  train_features = convert_examples_to_features(self.train_examples, self.opt.max_seq_len, self.tokenizer, self.opt)
  self.opt.label_list = sorted(list(self.opt.IOB_label_to_index.keys()))
  self.opt.num_labels = len(self.opt.label_list) + 1
  all_spc_input_ids = torch.tensor([f.input_ids_spc for f in train_features], dtype=torch.long)
  all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)
  all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)
  all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)
  all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)
  all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)
  all_polarities = torch.tensor([f.polarity for f in train_features], dtype=torch.long)
  lcf_cdm_vec = torch.tensor([f.lcf_cdm_vec for f in train_features], dtype=torch.float32)
  lcf_cdw_vec = torch.tensor([f.lcf_cdw_vec for f in train_features], dtype=torch.float32)

  self.train_data = TensorDataset(all_spc_input_ids, all_segment_ids, all_input_mask, all_label_ids,
                                  all_polarities, all_valid_ids, all_lmask_ids, lcf_cdm_vec, lcf_cdw_vec)

  if self.opt.dataset_file['test']:
    self.test_examples = processor.get_test_examples(self.opt.dataset_file['test'], 'test')
    test_features = convert_examples_to_features(self.test_examples, self.opt.max_seq_len,
                                                 self.tokenizer, self.opt)
    all_spc_input_ids = torch.tensor([f.input_ids_spc for f in test_features], dtype=torch.long)
    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)
    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)
    all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)
    all_polarities = torch.tensor([f.polarity for f in test_features], dtype=torch.long)
    all_valid_ids = torch.tensor([f.valid_ids for f in test_features], dtype=torch.long)
    all_lmask_ids = torch.tensor([f.label_mask for f in test_features], dtype=torch.long)
    lcf_cdm_vec = torch.tensor([f.lcf_cdm_vec for f in test_features], dtype=torch.float32)
    lcf_cdw_vec = torch.tensor([f.lcf_cdw_vec for f in test_features], dtype=torch.float32)
    self.test_data = TensorDataset(all_spc_input_ids, all_segment_ids, all_input_mask, all_label_ids, all_polarities, all_valid_ids, all_lmask_ids, lcf_cdm_vec, lcf_cdw_vec)

    if self.opt.cache_dataset and not os.path.exists(cache_path):
      print('Caching dataset... please remove cached dataset if change model or dataset')
      with open(cache_path, mode='wb') as f:
        if self.opt.dataset_file['test']:
          pickle.dump((self.train_data, self.test_data, self.opt), f)
        else:
          pickle.dump((self.train_data, self.opt), f)
```

#1: 만약 cache path가 존재하지 않는다면, 79 line에서 생성한 ATEPCProcessor 클래스의 get_train_examples 메소드를 실행합니다.

opt.dataset_file에는 다음과 같이 데이터셋의 경로가 들어있습니다.

```python
'dataset_file': {'train': ['integrated_datasets/atepc_datasets/110.SemEval/114.restaurant14/Restaurants_Train.xml.seg.atepc'], 'test': ['integrated_datasets/atepc_datasets/110.SemEval/114.restaurant14/Restaurants_Test_Gold.xml.seg.atepc'], 'valid': []}
```

<br>

get_train_examples 메소드는 [PyABSA/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L185)의 ATEPCProcessor 클래스의 멤버 함수로 정의돼 있습니다.

이후 등장하는 메소드들(get_train_examples ➡️ \_read_tsv ➡️ readfile ➡️ \_create_examples) 모두 같은 파일에 정의돼 있습니다.

<script src="https://gist.github.com/seominseok4834/1568fc0853c2a82d15ba6da6ab5c6e81.js"></script>

<br>

실행 순서대로 먼저  get_train_examples 메소드를 살펴보겠습니다.

**get_train_examples**

185 ~ 188 line

```python
def get_train_examples(self, data_dir, set_tag):
  """See base class."""
  return self._create_examples(
    self._read_tsv(data_dir), set_tag)
```

\_read_tsv 메소드로 데이터를 읽어와 \_create_examples 메소드로 examples를 생성하는 것 같습니다.

<br>

**\_read_tsv**

168 ~ 174 line

```python
@classmethod
def _read_tsv(cls, input_file, quotechar=None):
  """Reads a tab separated value file."""
  data = []
  for file in input_file:
    data += readfile(file)
    return data
```

\_read_tsv는 ATEPCProcessor 클래스의 멤버 함수로, 인자로 넘겨받은 input_file을 순회하면서 readfile 메소드로 이를 읽어들여 data에 추가합니다.

<br>

**readfile**

이어서 readfile 메소드가 어떻게 동작하는지 살펴보겠습니다. 코드가 길어 라인별로 살펴보겠습니다.

67 ~ 91 line

```python
def readfile(filename):
    '''
    read file
    '''
    f = open(filename, encoding='utf8')  # 1
    data = []
    sentence = []
    tag = []
    polarity = []
    for line in f:  # 2
        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == "\n":  # 3
            if len(sentence) > 0:
                data.append((sentence, tag, polarity))
                sentence = []
                tag = []
                polarity = []
            continue
        splits = line.strip().split(' ')  # 4
        if len(splits) != 3:  # 5
            print('warning! ignore detected error line(s) in input file:{}'.format(line))
            break
        sentence.append(splits[0])
        tag.append(splits[-2])
        polarity.append(splits[-1])
        Labels.add(splits[-2])
```

인자로 넘겨받은 파일을 오픈해(#1), 한 라인씩 읽습니다.(#2)

현재 읽고 있는 데이터는 아래처럼 문장별로 분리돼 있습니다.

```xml
The O -999
bread B-ASP Positive
is O -999
top O -999
notch O -999
as O -999
well O -999
. O -999

I O -999
have O -999
to O -999
say O -999
they O -999
have O -999
one O -999
of O -999
the O -999
fastest O -999
delivery B-ASP Positive
times I-ASP Positive
in O -999
the O -999
city O -999
. O -999
```

#3: 만약 읽어들인 라인의 길이가 0이거나, \-DOCSTART, \\n인 경우 data에 이전에 읽어들인 것들을 추가합니다.

#4: 위 조건이 아닌 경우 라인별로 읽어 공백을 기준으로 split합니다. (split 했을 때 3개가 아니라면 에러 메시지를 출력하고 종료합니다. #5)

예를 들어 첫번째 문장 "The bread is top notch as well."의 경우 sentence, tag, polarity가 아래와 같이되게 됩니다.

```python
sentence = ['The', 'bread', 'is', 'top', 'notch', 'as', 'well', '.']
tag = ['O', 'B-ASP', 'O', 'O', 'O', 'O', 'O', 'O']
polarity = ['-999', 'Positive', '-999', '-999', '-999', '-999', '-999', '-999']
```

<br>

Labels는 15 line에 정의한 빈 집합입니다. Set은 중복을 허용하지 않기 때문에 B-ASP, I-OSP, O 세 개의 값만 저장되게 됩니다.

```python
Labels = set()
```

<br>

**read_file**

93 ~ 107 line

```python
prepared_data = []
for s, t, p in data:  # 1

  if len(s) > 0:  # 2
    # prepare the atepc dataset, refer to https://github.com/yangheng95/PyABSA/issues/78
    polarity_padding = [str(SENTIMENT_PADDING)] * len(t)

    if len(Labels) > 3:  # 3
      # for more IOB labels support, but can not split cases in some praticular conditions, e.g., (B,I,E,O)
      for p_idx in range(len(p) - 1):  # 4
        if (p[p_idx] != p[p_idx + 1] and p[p_idx] != str(SENTIMENT_PADDING) and p[p_idx + 1] != str(SENTIMENT_PADDING)) \
        or (p[p_idx] != str(SENTIMENT_PADDING) and p[p_idx + 1] == str(SENTIMENT_PADDING)):  # 5
          _p = p[:p_idx + 1] + polarity_padding[p_idx + 1:]
          p = polarity_padding[:p_idx + 1] + p[p_idx + 1:]
          prepared_data.append((s, t, _p))
```

#1: 이후 저장한 data를 순회하며 sentence, tag, polarity를 꺼냅니다.

#2: sentence의 길이가 0보다 크면 SENTIMENT_PADDING으로 정의한 -999를 string으로 바꾼 뒤, tag의 개수만큼 곱해줍니다.

+SENTIMENT_PADDING은 [PyABSA/pyabsa/utils/pyabsa_utils.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/utils/pyabsa_utils.py#L24)에 정의돼 있습니다.

위에서 예시로 든 "The bread is top notch as well."의 경우 ['-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999']가 polarity_padding이 되게 됩니다.

#3: Labels 값이 3개보다 큰 경우(IOB annotation 태그가 세 개 이상인 경우), polarity에 저장된 개수보다 하나 적게 루프를 순회합니다.(#4)

#5: 조건이 조금 복잡한데 ① i번째 인덱스, i+1번째 인덱스의 감성이 같지 않고, i번째 인덱스와 i+1번째 인덱스의 감성이 패딩값(-999)가 아니거나 ② i번째 인덱스와 i+1번째 인덱스의 감성이 패딩값(-999)이 아닌 경우, 두 조건 중 하나를 만족하면 실행됩니다.

\_p는 p[0번 인덱스 ~ i번째 인덱스] + polarity_padding[i+1번째 인덱스 ~ 끝], p에는 polarity_padding[0번 인덱스 ~ i번째 인덱스] + p[i+1q번째 인덱스 ~ 끝] 으로 슬라이싱한 다음, prepared_data에 추가합니다.

*아래 IOB annotation이 세 개인 데이터의 예시를 보면 감이 잡힐겁니다.*

<br>

108 ~ 121 line

```python
            else:  # 1
                for t_idx in range(1, len(t)):  # 2
                    # for 3 IOB label (B, I, O)
                    if p[t_idx - 1] != str(SENTIMENT_PADDING) and split_aspect(t[t_idx - 1], t[t_idx]):  # 3
                        _p = p[:t_idx] + polarity_padding[t_idx:]
                        p = polarity_padding[:t_idx] + p[t_idx:]
                        prepared_data.append((s, t, _p))

                    if p[t_idx] != str(SENTIMENT_PADDING) and t_idx == len(t) - 1 and split_aspect(t[t_idx]):   # 4
                        _p = p[:t_idx + 1] + polarity_padding[t_idx + 1:]
                        p = polarity_padding[:t_idx + 1] + p[t_idx + 1:]
                        prepared_data.append((s, t, _p))

    return prepared_data
```

Labels(IOB Annotation 태그)가 세 개 이하라면, #1의 else문이 실행됩니다.

#2: Label이 네 개 이상인 경우와 동일하게 tag의 길이보다 하나 적게 순회합니다.

#3을 보기전 split_aspect 메소드를 먼저 보겠습니다.

<br>

**split_aspect**

124 ~ 149 line

```python
def split_aspect(tag1, tag2=None):
    if tag1 == 'B-ASP' and tag2 == 'B-ASP':  # 문장에서 두 개의 감성이 연달아 나오는 경우
        return True
    if tag1 == 'B-ASP' and tag2 == 'O':  # 문장에서 감성으로 태그된 부분이 끝난 경우 (다음에 감성이 없는 단어가 나옴)
        return True
    elif tag1 == 'I-ASP' and tag2 == 'O':  # 문장에서 감성으로 태그된 부분이 끝난 경우 (다음에 감성이 없는 단어가 나옴)
        return True
    elif tag1 == 'I-ASP' and tag2 == 'B-ASP':  # 문장에서 두 개의 감성이 연달아 나오는 경우
        return True
    elif (tag1 == 'B-ASP' or tag1 == 'I-ASP') and not tag2:  # 문장에서 감성으로 태그된 부분이 끝난 경우 (문장이 아예 끝남)
        return True
    elif tag1 == 'O' and tag2 == 'I-ASP':
        # warnings.warn('Invalid annotation! Found I-ASP without B-ASP')
        return False
    elif tag1 == 'O' and tag2 == 'O':
        return False
    elif tag1 == 'O' and tag2 == 'B-ASP':
        return False
    elif tag1 == 'O' and not tag2:
        return False
    elif tag1 == 'B-ASP' and tag2 == 'I-ASP':
        return False
    elif tag1 == 'I-ASP' and tag2 == 'I-ASP':
        return False
    else:
        return False
```

split_aspect 메소드는 문장에서 감성을 분리하기 위한 메소드입니다.

① 문장에서 감성으로 태그된 부분이 끝난 경우 (다음에 감성이 없는 단어가 나오거나 문장이 아예 끝난 경우), ②문장에서 감성이 연달아 낭는 경우에만 True를 리턴합니다.

아래 예시에서는 10번째 인덱스에서 True를 리턴합니다.

```xml
Try O -999  <!-- 0 -->
the O -999  <!-- 1 -->
rose B-ASP -999  <!-- 2 -->
roll I-ASP -999  <!-- 3 -->
- O -999  <!-- 4 -->
LRB O -999  <!-- 5 -->
- O -999  <!-- 6 -->
not O -999  <!-- 7 -->
on O -999  <!-- 8 -->
menu B-ASP Neutral  <!-- 9 -->
- O -999  <!-- 10: True -->
RRB O -999  <!-- 11 -->
- O -999  <!-- 12 -->
. O -999  <!-- 13 -->
```

<br>

다시 [read_file](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L108) 메소드로 돌아와서❗️

108 ~ 121 line

```python
            else:  # 1
                for t_idx in range(1, len(t)):  # 2
                    # for 3 IOB label (B, I, O)
                    if p[t_idx - 1] != str(SENTIMENT_PADDING) and split_aspect(t[t_idx - 1], t[t_idx]):  # 3
                        _p = p[:t_idx] + polarity_padding[t_idx:]
                        p = polarity_padding[:t_idx] + p[t_idx:]
                        prepared_data.append((s, t, _p))

                    if p[t_idx] != str(SENTIMENT_PADDING) and t_idx == len(t) - 1 and split_aspect(t[t_idx]):   # 4
                        _p = p[:t_idx + 1] + polarity_padding[t_idx + 1:]
                        p = polarity_padding[:t_idx + 1] + p[t_idx + 1:]
                        prepared_data.append((s, t, _p))

    return prepared_data
```

위에 예시에서 10번째 인덱스라고 가정하면 #3의 조건을 만족해 아래 코드가 실행되고,

\_p와 p에 다음과 같이 저장되게 됩니다.

```python
_p = ['-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', 'Neutral', '-999', '-999', '-999', '-999']
p = ['-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999']
```

<br>

#4는 감성이 라벨링된 단어가 문장의 맨 끝에 등장할 때만 실행되는데, 아래 예시에서는

```xml
Desserts B-ASP -999  <!-- 0 -->
are O -999  <!-- 1 -->
almost O -999  <!-- 2 -->
incredible O -999  <!-- 3 -->
: O -999  <!-- 4 -->
my O -999  <!-- 5 -->
personal O -999  <!-- 6 -->
favorite O -999  <!-- 7 -->
is O -999  <!-- 8 -->
their O -999  <!-- 9 -->
Tart B-ASP Positive  <!-- 10 -->
of I-ASP Positive  <!-- 11 -->
the I-ASP Positive  <!-- 12 -->
Day I-ASP Positive  <!-- 13 -->
```

13번째 인덱스에서 #4가 실행되고, \_p와 p에 다음과 같이 저장되게 됩니다.

```python
_p = ['-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', 'Positive', 'Positive', 'Positive', 'Positive']
p = ['-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999']
```

이렇게 변경한 \_p를 prepared_data로 추가하고 최종적으로 readfile 메소드는 prepared_data를 리턴합니다.

prepared_data는 아래처럼 문장별로 튜플 형태로 묶여있는 리스트이다.

```python
[(['Desserts',
   'are',
   'almost',
   'incredible',
   ':',
   'my',
   'personal',
   'favorite',
   'is',
   'their',
   'Tart',
   'of',
   'the',
   'Day'],
  ['B-ASP',
   'O',
   'O',
   'O',
   'O',
   'O',
   'O',
   'O',
   'O',
   'O',
   'B-ASP',
   'I-ASP',
   'I-ASP',
   'I-ASP'],
  ['-999',
   '-999',
   '-999',
   '-999',
   '-999',
   '-999',
   '-999',
   '-999',
   '-999',
   '-999',
   'Positive',
   'Positive',
   'Positive',
   'Positive'])]
```

<br>

🧑‍🏫 지금까지 내용 정리

[get_train_examples](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L185)

```python
    def get_train_examples(self, data_dir, set_tag):
        """See base class."""
        return self._create_examples(
            self._read_tsv(data_dir), set_tag)
```

Instructor 클래스 생성자에서 get_train_examples 메소드 호출 ➡️ \_read_tsv 메소드에서 readfile 메소드를 호출 ➡️ 리턴받은 prepared_data를 파라미터로 \_create_examples 메소드 호출

<br>

이제 \_craete_examples 메소드를 살펴보겠습니다 ..

<br>

**_create_examples**

198 ~ 218 line

```python
def _create_examples(self, lines, set_type):
  examples = []

  for i, (sentence, tag, polarity) in enumerate(lines):  # 1
    aspect = []
    aspect_tag = []
    aspect_polarity = SENTIMENT_PADDING
    for w, t, p in zip(sentence, tag, polarity):  # 2
      if str(p) != str(SENTIMENT_PADDING):
        aspect.append(w)
        aspect_tag.append(t)
        aspect_polarity = p

        guid = "%s-%s" % (set_type, i)
        text_a = sentence
        text_b = aspect

        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, IOB_label=tag,
                                     aspect_label=aspect_tag, polarity=aspect_polarity))
        
	return examples
```

#1: 앞서 readfile 메소드로 저장한 prepare_dataset을 인자로 받아, enumerate로 순회합니다.

#2: w=sentence, t=tag, p=polarity로 p가 패딩값(-999)이 아닌 감성일 경우에만 aspect, aspect_tag, aspect를 저장한 뒤, 이를 InputExample이라는 클래스로 만들어 examples 리스트에 append한 뒤 examples를 리턴한다.

<br>

**InputExample \_\_init__**

18 ~ 38 line

```python
class InputExample(object):
    """A single training_tutorials/test example for simple sequence classification."""

    def __init__(self, guid, text_a, text_b=None, IOB_label=None, aspect_label=None, polarity=None):
        """Constructs a InputExample.
        Args:
            guid: Unique id for the example.
            text_a: string. The untokenized text of the first sequence. For single
            sequence core, only this sequence must be specified.
            text_b: (Optional) string. The untokenized text of the second sequence.
            Only must be specified for sequence pair core.
            label: (Optional) string. The label of the example. This should be
            specified for train and dev examples, but not for test examples.
        """
        self.guid = guid
        self.text_a = text_a
        self.text_b = text_b
        self.IOB_label = IOB_label
        self.aspect_label = aspect_label
        self.polarity = polarity
```

InputExample 클래스는 구조가 간단하다. 그냥 인자로 넘겨받은 모든 값을 멤버 변수로 저장한다.

<br>

위에서 사용한 예시의 문장을 InputExample 클래스로 변환하면 다음과 같이 된다.

```python
{'guid': 'train-0', 'text_a': ['Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Tart', 'of', 'the', 'Day'], 'text_b': ['Tart', 'of', 'the', 'Day'], 'IOB_label': ['B-ASP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP'], 'aspect_label': ['B-ASP', 'I-ASP', 'I-ASP', 'I-ASP'], 'polarity': 'Positive'}
```

<br><br>

드디어 다시 [Instructor 생성자](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/training/atepc_trainer.py#L96) 부분으로 가면❗️

**Instructor \_\_init__**

96 ~ 136 line

```python
else:  # cache_path가 존재하지 않는 경우
  self.train_examples = processor.get_train_examples(self.opt.dataset_file['train'], 'train')  # 1
  train_features = convert_examples_to_features(self.train_examples, self.opt.max_seq_len, self.tokenizer, self.opt)  # 2
  self.opt.label_list = sorted(list(self.opt.IOB_label_to_index.keys()))
  self.opt.num_labels = len(self.opt.label_list) + 1
  all_spc_input_ids = torch.tensor([f.input_ids_spc for f in train_features], dtype=torch.long)
  all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)
  all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)
  all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)
  all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)
  all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)
  all_polarities = torch.tensor([f.polarity for f in train_features], dtype=torch.long)
  lcf_cdm_vec = torch.tensor([f.lcf_cdm_vec for f in train_features], dtype=torch.float32)
  lcf_cdw_vec = torch.tensor([f.lcf_cdw_vec for f in train_features], dtype=torch.float32)

  self.train_data = TensorDataset(all_spc_input_ids, all_segment_ids, all_input_mask, all_label_ids,
                                  all_polarities, all_valid_ids, all_lmask_ids, lcf_cdm_vec, lcf_cdw_vec)

  if self.opt.dataset_file['test']:
    self.test_examples = processor.get_test_examples(self.opt.dataset_file['test'], 'test')
    test_features = convert_examples_to_features(self.test_examples, self.opt.max_seq_len,
                                                 self.tokenizer, self.opt)
    all_spc_input_ids = torch.tensor([f.input_ids_spc for f in test_features], dtype=torch.long)
    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)
    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)
    all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)
    all_polarities = torch.tensor([f.polarity for f in test_features], dtype=torch.long)
    all_valid_ids = torch.tensor([f.valid_ids for f in test_features], dtype=torch.long)
    all_lmask_ids = torch.tensor([f.label_mask for f in test_features], dtype=torch.long)
    lcf_cdm_vec = torch.tensor([f.lcf_cdm_vec for f in test_features], dtype=torch.float32)
    lcf_cdw_vec = torch.tensor([f.lcf_cdw_vec for f in test_features], dtype=torch.float32)
    self.test_data = TensorDataset(all_spc_input_ids, all_segment_ids, all_input_mask, all_label_ids, all_polarities, all_valid_ids, all_lmask_ids, lcf_cdm_vec, lcf_cdw_vec)

    if self.opt.cache_dataset and not os.path.exists(cache_path):
      print('Caching dataset... please remove cached dataset if change model or dataset')
      with open(cache_path, mode='wb') as f:
        if self.opt.dataset_file['test']:
          pickle.dump((self.train_data, self.test_data, self.opt), f)
        else:
          pickle.dump((self.train_data, self.opt), f)
```

이제 #1 코드의 train_examples에 어떤 값들이 들어있는지 알 수 있습니다.

#2: 앞에서 만든 examples를 다시 인자로 넘겨 feature로 변경합니다.. 😢

<br>

**convert_examples_to_features**

convert_examples_to_features 메소드는 [PyABSA/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L221)에 정의돼 있습니다.

코드가 길어서 라인별로 살펴보겠습니다.

221 ~ 224 line

```python
def convert_examples_to_features(examples, max_seq_len, tokenizer, opt=None):
    """Loads a data file into a list of `InputBatch`s."""

    configure_spacy_model(opt)
```

시작부터 다른 함수 호출 ..

<br>

configure_spacy_model은 [PyABSA/pyabsa/core/apc/dataset_utils/apc_utils.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/apc/dataset_utils/apc_utils.py#L351)에 정의돼 있습니다.

**configure_spacy_model**

351 ~ 365 line

```python
def configure_spacy_model(opt):
    if not hasattr(opt, 'spacy_model'):  # 1
        opt.spacy_model = 'en_core_web_sm'
    global nlp  # nlp를 전역 변수로 선언
    try:
        nlp = spacy.load(opt.spacy_model)  # 2
    except:
        print('Can not load {} from spacy, try to download it in order to parse syntax tree:'.format(opt.spacy_model),
              termcolor.colored('\npython -m spacy download {}'.format(opt.spacy_model), 'green'))
        try:
            os.system('python -m spacy download {}'.format(opt.spacy_model))  # 3
            nlp = spacy.load(opt.spacy_model)
        except:
            raise RuntimeError('Download failed, you can download {} manually.'.format(opt.spacy_model))
    return nlp
```

#1: configuration(opt)에 spacy_model이 정의돼 있지 않다면 'en_core_web_sm'을 사용합니다. ✔️**한국어 적용시 ko_core_news_sm로 변경**

#2: space.load 메소드를 사용해 해당 모델을 로드합니다. 만약 에러가 발생하면 에러 메시지를 출력하고 os.system 메소드로 시스템 명령어를 호출해 해당 모델을 다운받습니다.(#3)

이후 해당 spacy 모델을 리턴합니다.

<br>

다시 [convert_examples_to_features](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L226)로 돌아와서 ❗️

**convert_examples_to_features**

226 ~ 228 line

```python
bos_token = tokenizer.bos_token  # 1 (CLS)
eos_token = tokenizer.eos_token  # 2 (SEP)
label_map = {label: i for i, label in enumerate(sorted(list(Labels) + [tokenizer.bos_token, tokenizer.eos_token]), 1)}  # 3
opt.IOB_label_to_index = label_map  # 4
```

Begin of Sequence 토큰(#1), End of Sequence 토큰(#2)을 정의하고,  BOS 토큰과 EOS 토큰 그리고 Label들을 합쳐 {토큰: 인덱스} 형태로 변환합니다.(#3)

cf) Label은 IOB annotation tag를 저장한 집합으로, B-ASP, I-ASP, O가 저장돼 있습니다.

이를 configuration에 IOB_label_to_index로 저장합니다.(#4)

```python
'IOB_label_to_index': {'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}
```

🔼 configuration에 저장된 label_map

+BERT에 대해 잘 모르시는 분은 [BERT 설명하기](https://hwiyong.tistory.com/392) 포스팅을 참고해주세요.

<br>

**convert_examples_to_features**

230 ~ 231 line

```python
features = []
polarities_set = set()
```

features와 polarities_set을 생성한 뒤,

232 ~ 255 line

```python
for (ex_index, example) in enumerate(tqdm.tqdm(examples, postfix='convert examples to features')):  # 1
  text_tokens = example.text_a[:]
  aspect_tokens = example.text_b[:]
  IOB_label = example.IOB_label
  aspect_label = example.aspect_label
  polarity = example.polarity
  if polarity != SENTIMENT_PADDING or int(polarity) != SENTIMENT_PADDING:  # bad case handle in Chinese atepc_datasets
    polarities_set.add(polarity)  # ignore samples without polarities
    tokens = []
    labels = []
    valid = []
    label_mask = []
    enum_tokens = [bos_token] + text_tokens + [eos_token] + aspect_tokens + [eos_token]
    IOB_label = [bos_token] + IOB_label + [eos_token] + aspect_label + [eos_token]

    aspect = ' '.join(example.text_b)
    try:
      text_left, _, text_right = [s.strip() for s in ' '.join(example.text_a).partition(aspect)]
      except:
        continue
        text_raw = text_left + ' ' + aspect + ' ' + text_right

        if validate_example(text_raw, aspect, polarity):
          continue
```

examples를 순회합니다.(#1) tqdm으로 프로세스바를 표시합니다.

<br>

examples에 있는 InputExample 클래스의 멤버 변수를 상기시켜보겠습니다.

다음과 같은 데이터가 있다고 할 때,

```xml
Desserts B-ASP -999  <!-- 0 -->
are O -999  <!-- 1 -->
almost O -999  <!-- 2 -->
incredible O -999  <!-- 3 -->
: O -999  <!-- 4 -->
my O -999  <!-- 5 -->
personal O -999  <!-- 6 -->
favorite O -999  <!-- 7 -->
is O -999  <!-- 8 -->
their O -999  <!-- 9 -->
Tart B-ASP Positive  <!-- 10 -->
of I-ASP Positive  <!-- 11 -->
the I-ASP Positive  <!-- 12 -->
Day I-ASP Positive  <!-- 13 -->
```

<br>

InputExample 클래스는 라인 별로 다음 멤버변수를 갖습니다.

```python
class InputExample(object):
    """A single training_tutorials/test example for simple sequence classification."""

    def __init__(self, guid, text_a, text_b=None, IOB_label=None, aspect_label=None, polarity=None):
        self.guid = guid  # train-0 : 'set-type(train) - index 번호'
        self.text_a = text_a  # ['Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Tart', 'of', 'the', 'Day'] : 문장 전체
        self.text_b = text_b  # ['Tart', 'of', 'the', 'Day'] : Aspect Term
        self.IOB_label = IOB_label  # ['B-ASP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP'] : IOB annotation 태그
        self.aspect_label = aspect_label  # ['B-ASP', 'I-ASP', 'I-ASP', 'I-ASP'] : Aspect Term의 IOB annotation 태그
        self.polarity = polarity  # 'Positive' : Aspect Term의 감성
```

<br>

다시 [convert_examples_to_features](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L232) 메소드로 돌아와서❗️

**convert_examples_to_features**

232 ~ 255 line

```python
for (ex_index, example) in enumerate(tqdm.tqdm(examples, postfix='convert examples to features')):  # 1
        text_tokens = example.text_a[:]  # 2
        aspect_tokens = example.text_b[:]  # 3
        IOB_label = example.IOB_label  # 4
        aspect_label = example.aspect_label  # 5
        polarity = example.polarity  # 6
        if polarity != SENTIMENT_PADDING or int(polarity) != SENTIMENT_PADDING:  # bad case handle in Chinese atepc_datasets  # 7
            polarities_set.add(polarity)  # ignore samples without polarities
        tokens = []  # 8
        labels = []
        valid = []
        label_mask = []
        enum_tokens = [bos_token] + text_tokens + [eos_token] + aspect_tokens + [eos_token]  # 9
        IOB_label = [bos_token] + IOB_label + [eos_token] + aspect_label + [eos_token]  # 10

        aspect = ' '.join(example.text_b)  # 11
        try:
            text_left, _, text_right = [s.strip() for s in ' '.join(example.text_a).partition(aspect)]  # 12
        except:
            continue
        text_raw = text_left + ' ' + aspect + ' ' + text_right  # 13

        if validate_example(text_raw, aspect, polarity):  # 14
            continue
```

text_token에는 문장 전체를 단어 단위로 나누어 놓은 text_a가 들어가고(#2),  aspect_token에는 aspect term을 단어 단위로 나눈 text_b가 들어갑니다.(#3)

IOB_label에는 문장 전체의 IOB annotation 태그가(#4), aspect_label에는 aspect term의 IOB annotation 태그가 저장됩니다.(#5)

마지막으로 polarity는 aspect term의 감성으로 초기화됩니다.(#6)

```python
text_tokens = ['Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Tart', 'of', 'the', 'Day']
aspect_tokens = ['Tart', 'of', 'the', 'Day']
IOB_label = ['B-ASP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP']
aspect_label = ['B-ASP', 'I-ASP', 'I-ASP', 'I-ASP']
polarity = 'Positive'
```

#7: polarity가 패딩값(-999)가 아닌 경우 polarities_set에 polarity를 넣습니다. 이 때, polarity_set은 231 line에서 빈 집합으로 선언했기 때문에 중복을 허용하지 않아, 감성이 종류별로 하나씩만 담기게 됩니다.

#8: token, label, valid, label_mask를 담기 위한 리스트를 생성하고,

#9: enum_token은 문장의 가장 첫번째에 삽입하는 CLS 토큰과 문장을 구별할 때 사용하는 SEP 토큰을 사용하여 다음과 같이 정의합니다.

```python
enum_tokens = ['CLS'] + text_tokens + ['SEP'] +aspect_tokens + ['SEP']
# ['CLS', 'Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Tart', 'of', 'the', 'Day', 'SEP', 'Tart', 'of', 'the', 'Day', 'SEP']
```

#10: IOB_label도 enum_tokens와 동일하게 CLS 토큰과 SEP 토을 사용해 다음과 같이 정의합니다.

```python
IOB_label = ['CLS'] + IOB_label + ['SEP'] + aspect_label + ['SEP']  # 10
# ['CLS', 'B-ASP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP', 'SEP', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP', 'SEP']
```

<br>

```python
example.text_b  # ['Tart', 'of', 'the', 'Day']
```

#11: aspect term을 join 메소드를 통해 하나의 문자열로 만듧니다. Tart of the Day

<br>

#12 라인을 풀어쓰면 다음과 같습니다.

```python
temp = []

for s in ' '.join(example.text_a).partition(aspect):
    temp.append(s.strip())
```

['Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Tart', 'of', 'the', 'Day']를 join으로 합쳐 

Desserts are almost incredible : my personal favorite is their Tart of the Day로 만듧니다.

이후 partition 메소드를 사용해 튜플 형태로 ('Desserts are almost incredible : my personal favorite is their ', 'Tart of the Day', '') 리턴한 뒤, 리스트로 감싸 리스트 형태로 ['Desserts are almost incredible : my personal favorite is their', 'Tart of the Day', ''] 만든 뒤 text_left, _, text_right로 받은 것입니다.

<br>

#13: 이를 다시 합쳐 원본 텍스트로 만듧니다.

<br>

#14: 이후 valid_example 메소드를 호출해 example이 유효한지 검사합니다.

valid_example 메소드는 [PyABSA/pyabsa/utils/pyabsa_utils.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/utils/pyabsa_utils.py#L44)에 정의돼 있습니다.

**valid_example**

44 ~ 65 line

```python
def validate_example(text: str, aspect: str, polarity: str):
    if len(text) < len(aspect):  # 1
        raise ValueError(colored('AspectLengthExceedTextError -> <aspect: {}> is longer than <text: {}>, <polarity: {}>'.format(aspect, text, polarity), 'red'))

    if aspect.strip().lower() not in text.strip().lower():  # 2
        raise ValueError(colored('AspectNotInTextError -> <aspect: {}> is not in <text: {}>>'.format(aspect, text), 'yellow'))

    warning = False

    if len(aspect.split(' ')) > 10:  # 3
        print(colored('AspectTooLongWarning -> <aspect: {}> is too long, <text: {}>, <polarity: {}>'.format(aspect, text, polarity), 'yellow'))
        warning = True

    if len(polarity.split(' ')) > 3:  # 4
        print(colored('LabelTooLongWarning -> <polarity: {}> is too long, <text: {}>, <aspect: {}>'.format(polarity, text, aspect), 'yellow'))
        warning = True

    if text.strip() == aspect.strip():  # 5
        print(colored('AspectEqualsTextWarning -> <aspect: {}> equals <text: {}>, <polarity: {}>'.format(aspect, text, polarity), 'yellow'))
        warning = True

    return warning
```

#1: aspect term의 길이가 텍스트 길이보다 큰 지 확인합니다. aspect term이 텍스트보다 길 경우 ValueError를 발생시킵니다.

#2: aspect가 텍스트 내에 존재하는지 확인합니다. 존재하지 않을 경우 마찬가지로 ValueError를 발생시킵니다.

#3: aspect가 11개 이상의 단어로 구성돼 있는지 확인합니다. 이 때는 에러는 아니고 경고 메시지만 출력합니다.

#4: polarity가 4개 이상의 단어로 구성돼 있는지 확인합니다. 이 때도 경고 메시지만 출력합니다.

#5: 문장 전체가 aspect로 라벨링돼 있는지 확인합니다. 문장 전체가 aspect일 경우 경고 메시지를 출력합니다.

#1과 #2의 경우에만 warning이 False이기 때문에 두 케이스만 제외하면 모두 이어서 동작하게 됩니다.

<br>

다시 [convert_examples_to_features](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L257) 메소드로 돌아오겠습니다❗️

**convert_examples_to_features**

257 ~ 271 line

```python
prepared_inputs = prepare_input_for_atepc(opt, tokenizer, text_left, text_right, aspect)  # 1
lcf_cdm_vec = prepared_inputs['lcf_cdm_vec']
lcf_cdw_vec = prepared_inputs['lcf_cdw_vec']

for i, word in enumerate(enum_tokens):
  token = tokenizer.tokenize(word)
  tokens.extend(token)
  cur_iob = IOB_label[i]
  for m in range(len(token)):
    if m == 0:
      label_mask.append(1)
      labels.append(cur_iob)
      valid.append(1)
    else:
      valid.append(0)
```

#1: prepare_input_for_atepc 메소드를 실행합니다 ..

<br>

prepare_input_for_atepc 메소드는 [PyABSA/pyabsa/core/atepc/dataset_utils/atepc_utils.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/atepc_utils.py#L75)에 정의돼 있습니다.

코드가 길어서 라인별로 천천히 살펴보겠습니다.

**prepare_input_for_atepc**

75 ~ 87 line

```python
def prepare_input_for_atepc(opt, tokenizer, text_left, text_right, aspect):
    if hasattr(opt, 'dynamic_truncate') and opt.dynamic_truncate:  # 1
        _max_seq_len = opt.max_seq_len - len(aspect.split())  # 2
        text_left = text_left.split(' ')  # 3
        text_right = text_right.split(' ')
        if _max_seq_len < len(text_left) + len(text_right):  # 4
            cut_len = len(text_left) + len(text_right) - _max_seq_len  # 5
            if len(text_left) > len(text_right):  # 6
                text_left = text_left[cut_len:]
            else:  # 7
                text_right = text_right[:len(text_right) - cut_len]
        text_left = ' '.join(text_left)  # 8
        text_right = ' '.join(text_right)=
```

#1: configuration에 dynamic_truncate 속성이 정의돼 있고, True인지 확인합니다. (맨 위 configuration을 보면 True로 설정돼 있는 것을 확인할 수 있습니다.)

#2: configuration에 저장된 max_seq_len에서 aspect 단어 개수를 구한 뒤 빼줍니다.

#3: 앞서 249 line에서 생성한 text_left, text_right를 다시 스플릿합니다.

```python
text_left, _, text_right = [s.strip() for s in ' '.join(example.text_a).partition(aspect)]  # 249 line
```

ex) "Desserts are almost incredible : my personal favorite is their Tart of the Day"에서 Tart of the Day가 aspect였다면

```python
text_left = 'Desserts are almost incredible : my personal favorite is their'
text_right = ''
```

가 되게 되는데 이를 다시 split해서 단어 단위로 쪼갭니다.

```python
text_left = ['Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their']
text_right = ['']
```

<br>

#4: 이후 \_max\_seq\_len보다 길이가 길다면, 얼만큼 잘라야하는지를 cut_len로 저장합니다.(#5)

#6: 만약 aspect term 왼쪽의 텍스트가 더 길다면 text_left의 앞쪽 부분을 자르고, 오른쪽 텍스트가 더 길다면 text_right의 뒷쪽 부분을 자릅니다.(#7)

#8: 이후 join 메소드로 다시 문자열로 변환합니다.

<br>

**prepare_input_for_atepc**

89 ~ 93 line

```python
bos_token = tokenizer.bos_token if tokenizer.bos_token else '[CLS]'  # 1
eos_token = tokenizer.eos_token if tokenizer.eos_token else '[SEP]'

text_raw = text_left + ' ' + aspect + ' ' + text_right  # 2
text_spc = bos_token + ' ' + text_raw + ' ' + eos_token + ' ' + aspect + ' ' + eos_token  # 3
```

#1: bos_token과 eos_token을 정의한 뒤,

#2: 원본 텍스트를 복구합니다.

#3: 앞서 봤던것처럼 문장의 가장 첫번째에 삽입하는 CLS 토큰과 문장을 구별할 때 사용하는 SEP 토큰을 사용하여 text_spc를 다음과 같이 정의합니다.

**'[CLS] Desserts are almost incredible : my personal favorite is their Tart of the Day [SEP] Tart of the Day [SEP]'**

<br>

**prepare_input_for_atepc**

95 ~ 103 line

```python
text_bert_tokens = tokenizer.tokenize(text_spc)  # 1
text_raw_bert_tokens = tokenizer.tokenize(bos_token + ' ' + text_raw + ' ' + eos_token)  # 2
aspect_bert_tokens = tokenizer.tokenize(aspect)  # 3

text_bert_indices = tokenizer.convert_tokens_to_ids(text_bert_tokens)  # 4
text_raw_bert_indices = tokenizer.convert_tokens_to_ids(text_raw_bert_tokens)
aspect_bert_indices = tokenizer.convert_tokens_to_ids(aspect_bert_tokens)

aspect_begin = len(tokenizer.tokenize(bos_token + ' ' + text_left))  # 5
```

#1: 앞서 만든 text_spc를 bert tokenizer를 사용해 토크나이징하고,

```python
text_bert_tokens = ['[CLS]', 'Des', '##ser', '##ts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Ta', '##rt', 'of', 'the', 'Day', '[SEP]', 'Ta', '##rt', 'of', 'the', 'Day', '[SEP]']
```

#2: 원본 텍스트에도 bos_token과 eos_token을 추가해 토크나이징 합니다.

```python
text_raw_bert_tokens = ['[CLS]', 'Des', '##ser', '##ts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Ta', '##rt', 'of', 'the', 'Day', '[SEP]']
```

#3: 마찬가지로 aspect도 토크나이징 해줍니다.

```python
aspect_bert_tokens = ['Ta', '##rt', 'of', 'the', 'Day']
```

#4: 이후 앞서 토크나이징한 것들을 id로 변환한 뒤, 저장합니다.

```python
text_bert_indices = [101, 14177, 6906, 2145, 1132, 1593, 10965, 131, 1139, 2357, 5095, 1110, 1147, 22515, 3740, 1104, 1103, 2295, 102, 22515, 3740, 1104, 1103, 2295, 102]

text_raw_bert_indices = [101, 14177, 6906, 2145, 1132, 1593, 10965, 131, 1139, 2357, 5095, 1110, 1147, 22515, 3740, 1104, 1103, 2295, 102]

aspect_bert_indices = [22515, 3740, 1104, 1103, 2295]
```

#5: aspect term의 시작 위치를 계산해 aspect_begin으로 저장합니다.

<br>

**prepare_input_for_atepc**

105 ~ 114 line

```python
if 'lcfs' in opt.model_name or opt.use_syntax_based_SRD:
  syntactical_dist, _ = get_syntax_distance(text_raw, aspect, tokenizer, opt)
else:
  syntactical_dist = None

  lcf_cdm_vec = get_lca_ids_and_cdm_vec(opt, text_bert_indices, aspect_bert_indices,
                                        aspect_begin, syntactical_dist)

  lcf_cdw_vec = get_cdw_vec(opt, text_bert_indices, aspect_bert_indices,
                            aspect_begin, syntactical_dist)
```

![image](https://user-images.githubusercontent.com/76269316/175448613-732b80ad-c038-4295-9d06-21791e19b045.png)

✅ *이 부분은 모델의 Local Context Focus Layer(LCF)에서 사용하는 CDM/CDW 벡터를 계산하는 부분인데, 추후에 설명하도록 하겠습니다.*

<br>

**prepare_input_for_atepc**

116 ~ 127 line

```python
    inputs = {
        'text_raw': text_raw,
        'text_spc': text_spc,
        'aspect': aspect,
        'text_bert_indices': text_bert_indices,
        'text_raw_bert_indices': text_raw_bert_indices,
        'aspect_bert_indices': aspect_bert_indices,
        'lcf_cdm_vec': lcf_cdm_vec,
        'lcf_cdw_vec': lcf_cdw_vec,
    }

    return inputs
```

```python
text_raw = 'Desserts are almost incredible : my personal favorite is their Tart of the Day'
test_spc = '[CLS] Desserts are almost incredible : my personal favorite is their Tart of the Day [SEP] Tart of the Day [SEP]'
aspect = 'Tart of the Day'
text_bert_indices = [101, 14177, 6906, 2145, 1132, 1593, 10965, 131, 1139, 2357, 5095, 1110, 1147, 22515, 3740, 1104, 1103, 2295, 102, 22515, 3740, 1104, 1103, 2295, 102]
text_raw_bert_indices = [101, 14177, 6906, 2145, 1132, 1593, 10965, 131, 1139, 2357, 5095, 1110, 1147, 22515, 3740, 1104, 1103, 2295, 102]
aspect_bert_indices = [22515, 3740, 1104, 1103, 2295]
lcf_cdm_vec = ?
lcf_cdw_vec = ?
```

이후 계산한 값들을 묶어 inputs로 만든 뒤 리턴합니다.

<br>

다시 [convert_examples_to_features](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L257) 메소드로 돌아오겠습니다❗️

**convert_examples_to_features**

257 ~ 271 line

```python
prepared_inputs = prepare_input_for_atepc(opt, tokenizer, text_left, text_right, aspect)
lcf_cdm_vec = prepared_inputs['lcf_cdm_vec']  # 1
lcf_cdw_vec = prepared_inputs['lcf_cdw_vec']

for i, word in enumerate(enum_tokens):  # 2
  token = tokenizer.tokenize(word)  # 3
  tokens.extend(token)  # 4
  cur_iob = IOB_label[i]  # 5
  for m in range(len(token)):  # 6
    if m == 0:
      label_mask.append(1)
      labels.append(cur_iob)
      valid.append(1)
    else:
      valid.append(0)
```

#1: 리턴받은 inputs에서 cdm 벡터와 cdw 벡터를 꺼낸 뒤, 

[convert_examples_to_features 244 line](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L244)에 정의한 enum_token을 순회합니다.(#2)

```pyhon
enum_tokens = [bos_token] + text_tokens + [eos_token] + aspect_tokens + [eos_token]
```

```python
enum_tokens = ['[CLS]', 'Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Tart', 'of', 'the', 'Day', '[SEP]', 'Tart', 'of', 'the', 'Day', '[SEP]']
```

#3:  각 단어를 토크나이저로 토큰화합니다. 이후 tokens라는 리스트에 extend합니다.(#4)

cf) [list append()와 extend() 차이점](https://m.blog.naver.com/wideeyed/221541104629): list.extend(iterable)는 리스트 끝에 **가장 바깥쪽 iterable의 모든 항목을 넣습니다.**

<br>

#6은 enum_tokens의 Desserts까지가 어떻게 동작하는지 확인해보겠습니다.

+tokens, labels, valid, label_mask는 [convert_examples_to_features 240 ~ 243 line](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L240)에서 생성한 빈 리스트입니다.

<br>

```python
IOB_label = ['B-ASP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP']
```

<br>

**1. i = 0, word=[CLS]**

```python
token = tokenizer.tokenize(word)  # token = '[CLS]'
tokens.extend(token)  # tokens = ['[CLS]']
cur_iob = IOB_label[i]  # cur_iob = 'B-ASP'
for m in range(len(token)):  # for m in range(1):
  if m == 0:
    label_mask.append(1)  # label_mask = [1]
    labels.append(cur_iob)  # labels = ['B-ASP']
    valid.append(1)  # valid = [1]
  else:
    valid.append(0)
```

for loop의 경우 range가 1이기 때문에 if m==0만 실행되게 됩니다.

<br>

**2. i = 1, word=Desserts**

```python
token = tokenizer.tokenize(word)  # token = 'Desserts'
tokens.extend(token)  # tokens = ['[CLS]', 'Des', '##ser', '##ts']
cur_iob = IOB_label[i]  # cur_iob = 'O'
for m in range(len(token)):  # for m in range(3):
  if m == 0:
    label_mask.append(1)
    labels.append(cur_iob)  # labels = ['B-ASP']
    valid.append(1)  # valid = [1]
  else:
    valid.append(0)
```

이번에는 range가 3이기 때문에 3번 실행되게 됩니다.

1. m = 0일때
   ```python
   label_mask.append(1)  # label_mask = [1, 1]
   labels.append(cur_iob)  # labels = ['B-ASP']
   valid.append(1)  # valid = [1, 1]
   ```

2. m = 1일때
   ```python
   valid.append(0)  # valid = [1, 1, 0]
   ```

3. m = 2일때
   ```python
   valid.append(0)  # valid = [1, 1, 0, 0]
   ```

<br>

convert_exampls_to_features 메소드 257 ~ 271 line을 실행하면 문장별로 label_mask에는 문장을 구성하는 단어의 개수만큼 1로 가득찬 리스트가 생성되고, labels에는 IOB_label과 동일한 값을 갖는 리스트가, valid에는 토큰화한 단어의 개수만큼 1, 0으로 구성된 리스트가 생성되게 됩니다.

<br>

**convert_examples_to_features**

272 ~ 274 line

```python
tokens = tokens[0:min(len(tokens), max_seq_len - 2)]
labels = labels[0:min(len(labels), max_seq_len - 2)]
valid = valid[0:min(len(valid), max_seq_len - 2)]
```

이후 생성한 tokens, labels, valid 리스트를 max_seq_len과 비교하여 max_seq_len을 초과하지 않는다면 그대로 저장합니다.

<br>

**convert_examples_to_features**

278 ~ 283 line

```python
segment_ids = [0] * max_seq_len  # simply set segment_ids to all zeros
label_ids = []

for i, token in enumerate(tokens):
  if len(labels) > i:
    label_ids.append(label_map[labels[i]])
```

다음 라인에서는 segment_ids를 0으로 초기화된 리스트를 생성한 뒤, toekns를 순회하며 i가 labels의 길이보다 커지면 label_ids에 label_map[labels[i]]를 추가합니다.

label_map은 [convert_examples_to_features 228 line](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L228)에서 정의한 dictionary로,

```python
label_map = {'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}
```

다음 값이 저장돼 있습니다.

<br>

**convert_examples_to_features**

285 ~ 287 line

```python
input_ids_spc = tokenizer.convert_tokens_to_ids(tokens)  # 1
input_mask = [1] * len(input_ids_spc)  # 2
label_mask = [1] * len(label_ids)
```

#1: 입력 문장을 토큰화한 tokens 리스트를 id 값으로 변경합니다.

#2: input_mask는 input_ids_spc 길이만큼 1을, label_mask에는 label_ids의 길이만큼 1을 채운 리스트를 만듧니다.

<br>

**convert_examples_to_features**

288 ~ 303 line

```python
while len(input_ids_spc) < max_seq_len:  # 1
  input_ids_spc.append(0)
  input_mask.append(0)
  label_ids.append(0)
  label_mask.append(0)
  while len(valid) < max_seq_len:  # 2
    valid.append(1)
while len(label_ids) < max_seq_len:  # 3
  label_ids.append(0)
  label_mask.append(0)
assert len(input_ids_spc) == max_seq_len  # 4
assert len(input_mask) == max_seq_len
assert len(segment_ids) == max_seq_len
assert len(label_ids) == max_seq_len
assert len(valid) == max_seq_len
assert len(label_mask) == max_seq_len
```

#1: 만약 input_ids_spc가 max_seq_len 길이보다 작다면 길이만큼 0을 추가해줍니다. (#2: valid에는 1 추가)

#3: label_ids도 마찬가지로 길이가 짧다면 0을 추가해서 맞춰줍니다.

#4: 모든 리스트의 길이가 max_seq_len와 동일한지 확인합니다.

<br>

**convert_examples_to_features**

305 ~ 316 line

```python
features.append(
  InputFeatures(input_ids_spc=input_ids_spc,
                input_mask=input_mask,
                segment_ids=segment_ids,
                label_id=label_ids,
                polarity=polarity,
                valid_ids=valid,
                label_mask=label_mask,
                tokens=example.text_a,
                lcf_cdm_vec=lcf_cdm_vec,
                lcf_cdw_vec=lcf_cdw_vec)
)
```

이후 해당 값들을 [InputFeatres 클래스](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L41) 인스턴스로 생성한 뒤, features 리스트에 append 해줍니다.

[모든 examples](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L232)에 대해 위 과정을 반복한 뒤, 

317 ~ 321 line

```python
check_and_fix_labels(polarities_set, 'polarity', features, opt)
check_and_fix_IOB_labels(label_map, opt)
opt.polarities_dim = len(polarities_set)

return features
```

polarities_set과 label_map을 확인한 뒤, polarities_dim을 저장하고 features 리스트를 리턴합니다.

<br>

다시 [Instructor 생성자](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/training/atepc_trainer.py#L98)로 돌아오겠습니다❗️

**Instructor \_\_init__**

96 ~ 112 line

```python
else:
  self.train_examples = processor.get_train_examples(self.opt.dataset_file['train'], 'train')  # 1
  train_features = convert_examples_to_features(self.train_examples, self.opt.max_seq_len, self.tokenizer, self.opt)  # 2
  self.opt.label_list = sorted(list(self.opt.IOB_label_to_index.keys()))  # 3
  self.opt.num_labels = len(self.opt.label_list) + 1  # 4
  all_spc_input_ids = torch.tensor([f.input_ids_spc for f in train_features], dtype=torch.long)  # 5
  all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)
  all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)
  all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)
  all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)
  all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)
  all_polarities = torch.tensor([f.polarity for f in train_features], dtype=torch.long)
  lcf_cdm_vec = torch.tensor([f.lcf_cdm_vec for f in train_features], dtype=torch.float32)
  lcf_cdw_vec = torch.tensor([f.lcf_cdw_vec for f in train_features], dtype=torch.float32)
  
  self.train_data = TensorDataset(all_spc_input_ids, all_segment_ids, all_input_mask, all_label_ids, all_polarities, all_valid_ids, all_lmask_ids, lcf_cdm_vec, lcf_cdw_vec)  # 6
```

방금까지가 #2의 내용이고, 이후 #3에서는 IOB_label_to_index의 key 값(B-ASP, I-ASP, O, CLS, SEP)를 리스트로 만든 뒤 정렬해

```python
IOB_label_to_index: {'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}
```

label_list로 저장합니다.

```python
'label_list': ['B-ASP', 'I-ASP', 'O', '[CLS]'
```

<br>

#4: label_list의 길이 + 1을 num_labels로 저장하고, convert_examples_to_features에서 생성한 InputFeatres 인스턴스 리스트를 순회하며 tensor 타입으로 변환합니다.(#5)

#6: 이후 이를 TensorDataset으로 저장합니다.

<br>

**Instructor \_\_init__**

114 ~ 128 line

```python
if self.opt.dataset_file['test']:
  self.test_examples = processor.get_test_examples(self.opt.dataset_file['test'], 'test')
  test_features = convert_examples_to_features(self.test_examples, self.opt.max_seq_len,
                                               self.tokenizer, self.opt)
  all_spc_input_ids = torch.tensor([f.input_ids_spc for f in test_features], dtype=torch.long)
  all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)
  all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)
  all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)
  all_polarities = torch.tensor([f.polarity for f in test_features], dtype=torch.long)
  all_valid_ids = torch.tensor([f.valid_ids for f in test_features], dtype=torch.long)
  all_lmask_ids = torch.tensor([f.label_mask for f in test_features], dtype=torch.long)
  lcf_cdm_vec = torch.tensor([f.lcf_cdm_vec for f in test_features], dtype=torch.float32)
  lcf_cdw_vec = torch.tensor([f.lcf_cdw_vec for f in test_features], dtype=torch.float32)
  self.test_data = TensorDataset(all_spc_input_ids, all_segment_ids, all_input_mask, all_label_ids, all_polarities, all_valid_ids, all_lmask_ids, lcf_cdm_vec, lcf_cdw_vec)
```

만약 테스트 데이터셋이 존재한다면 훈련 데이터와 동일하게 convert_examples_to_features 메소드로 InputFeatres 인스턴스 리스트를 생성한 뒤 TensorDataset으로 저장합니다.

<br>

**Instructor \_\_init__**

130 ~ 136 line

```python
if self.opt.cache_dataset and not os.path.exists(cache_path):
  print('Caching dataset... please remove cached dataset if change model or dataset')
  with open(cache_path, mode='wb') as f:
    if self.opt.dataset_file['test']:
      pickle.dump((self.train_data, self.test_data, self.opt), f)
    else:
      pickle.dump((self.train_data, self.opt), f)
```

다음 라인에서는 opt.cache_dataset과 cache_path가 존재하는지 확인한 뒤, (opt.cache_dataset은 존재하는데 cache_path가 존재하지 않을 경우 실행)

훈련 데이터셋과 테스트 데이터셋(테스트 데이터셋은 존재할 때만, 자료형은 TensorDataset) pickle dump로 저장합니다.

<br>

**Instructor \_\_init__**

139 ~ 145 line

```python
for key in opt.args:  # 1
  if key not in self.opt.args:
    self.opt.args[key] = opt.args[key]
    self.opt.args_call_count[key] = opt.args_call_count[key]

bert_base_model.config.num_labels = self.opt.num_labels  # 2
self.opt.label_list = opt.label_list  # 3
```

#1: configuration(opt.args)에는 저장돼 있는데, Instructor 멤버 변수 opt에는 저장돼 있지 않은 경우, key-value와 call_count를 저장해주고,

#2: [Instructor \_\_init__ 72 line](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/training/atepc_trainer.py#L72)에서 AutoModel.from_pretrained로 생성한 bert_base_model에 label 개수를 지정해줍니다.

```python
bert_base_model = AutoModel.from_pretrained(self.opt.pretrained_bert)
```

#3: 멤버 변수 opt에도 label_list를 저장해줍니다.

<br>

**Instructor \_\_init__**

147 ~ 154 line

```python
self.num_train_optimization_steps = int(len(self.train_data) / self.opt.batch_size / self.opt.gradient_accumulation_steps) * self.opt.num_epoch  # 1

train_sampler = RandomSampler(self.train_data)  # 2
self.train_dataloader = DataLoader(self.train_data, sampler=train_sampler, pin_memory=True, batch_size=self.opt.batch_size)  # 3
if self.opt.dataset_file['test']:  # 4
  test_sampler = SequentialSampler(self.test_data)
  self.test_dataloader = DataLoader(self.test_data, sampler=test_sampler, pin_memory=True, batch_size=self.opt.batch_size)  # 5
```

#1: 훈련시 사용할 optimize step을 지정해주고,

#2: 데이터 로드시 셔플을 위해 RandomSampler를 생성, 훈련용 데이터로더를 생성합니다.(#3)

#4: 테스트 데이터가 존재할 경우 테스트 데이터로더도 생성하고, 배치 사이즈를 지정합니다.((#5))

<br>

**Instructor \_\_init__**

156 ~ 165 line

```python
self.model = self.opt.model(bert_base_model, opt=self.opt)  # 1

param_optimizer = list(self.model.named_parameters())  # 2
no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']  # 3
self.optimizer_grouped_parameters = [  # 4
  {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
   'weight_decay': self.opt.l2reg},
  {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
   'weight_decay': 0}
]
```

#1: 실제 모델을 정의하고

#2: 모델이 갖고 있는 파라미터의 이름들을 리스트로 만들어 param_optimizer로 생성합니다.

#3: weight decay는 weight들의 값이 증가하는 것을 제한함으로써, 모델의 복잡도를 감소시킴으로써 제한하는 기법으로 이를 적용하지 않을 파라미터를 no_decay로 정의해놓은 것 같습니다. weight decay에 대한 설명은 아래 링크를 참고해주세요.

cf) [Regularization (Weight Decay)](https://deepapple.tistory.com/6)

#4: 파라미터를 순회하며 weight_decay를 적용하지 않을 파라미터와 적용할 파라미터를 dictionary로 나눠 optimizer_grouped_parameters로 저장합니다.

<br>

**Instructor \_\_init__**

167 ~ 171 line

```python
if self.opt.auto_device == 'allcuda':
  self.model.to(self.opt.device)
  self.model = torch.nn.parallel.DataParallel(self.model)
else:
  self.model.to(self.opt.device)
```

configuration의 auto_device에 따라 model을 device로 로드합니다.

<br>

**Instructor \_\_init__**

177 ~ 185 line

```python
if amp:  # 1
  self.model, self.optimizer = amp.initialize(self.model, self.optimizer, opt_level="O1")

self.opt.device = torch.device(self.opt.device)  # 2
if self.opt.device.type == 'cuda':
  self.logger.info("cuda memory allocated:{}".format(torch.cuda.memory_allocated(device=self.opt.device)))

print_args(self.opt, self.logger)  # 3
```

#1: amp(automatic mixed precision)는 클래스 위에 31 ~ 37 line에서 import한 라이브러리로, 학습 속도를 증가시켜주는 라이브러리입니다. 

cf) [Amp에 대해 알아보자 (Automatic Mixed Precision)](https://cvml.tistory.com/8)

```python
try:
    import apex.amp as amp

    # assert torch.version.__version__ < '1.10.0'
    print('Use FP16 via Apex!')
except Exception:
    amp = None
```

정상적으로 import 됐다면 #1 라인이 실행돼 amp.initialize가 실행되게 됩니다.

<br>

#2: torch.device로 장치를 불러옵니다. 만약 GPU를 사용한다면 다음 조건문을 통해 로그를 남깁니다.

#3: print_args 메소드를 통해 configuration과 call count를 로그로 남깁니다.

<br>

[PyABSA/pyabsa/utils/pyabsa_utils.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/utils/pyabsa_utils.py#L35)

```python
def print_args(config, logger=None, mode=0):
    args = [key for key in sorted(config.args.keys())]
    for arg in args:
        if logger:  # logger에 INFO로 추가
            logger.info('{0}:{1}\t-->\tCalling Count:{2}'.format(arg, config.args[arg], config.args_call_count[arg]))
        else:  # 프롬프트에 출력
            print('{0}:{1}\t-->\tCalling Count:{2}'.format(arg, config.args[arg], config.args_call_count[arg]))
```

![스크린샷 2022-06-26 오후 3 05 58](https://user-images.githubusercontent.com/76269316/175801751-a86561ea-80c9-4ec7-9884-9b832e13c197.png)

![스크린샷 2022-06-26 오후 3 06 45](https://user-images.githubusercontent.com/76269316/175801784-49b5b479-6797-4402-bb4d-5db9f03e888a.png)

여기까지가 Instructor 클래스의 생성자 메소드입니다.

<br>

다시 train4atepc 메소드로 돌아오❗️

**[train4atepc](https://github.com/yangheng95/PyABSA/blob/4320db7dd0cfa0053a48169e59ff44403482eaf1/pyabsa/core/atepc/training/atepc_trainer.py#L460)**

460 ~ 465 line

```python
@retry
def train4atepc(opt, from_checkpoint_path, logger):
    # in case of handling ConnectionError exception
    trainer = Instructor(opt, logger)  # 1
    resume_from_checkpoint(trainer, from_checkpoint_path)  # 2

    return trainer.run()  # 3
```

#1에서 Instructor를 생성하고, #3에서 run 메소드를 실행한 뒤 리턴합니다.

이 부분은 apc에서 정리한 [\_train\_and\_evaluate](https://seominseok4834.github.io/capstone/12.pyabsa-instructor-analysis-(apc)/#_train_and_evaluate) 메소드와 비슷하기 때문에 추후에 정리하도록 하겠습니다 .. <img width="130" alt="스크린샷 2022-06-26 오후 4 15 10" src="https://user-images.githubusercontent.com/76269316/175803675-6a2cc761-1fc2-4986-b63c-55a64799cd42.png">
