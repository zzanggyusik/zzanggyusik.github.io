---
title:  "[Capstone] pyABSA í”„ë ˆì„ì›Œí¬ ë¶„ì„ - Instructor (ATEPC)"
excerpt: "pyABSA í”„ë ˆì„ì›Œí¬ ë¶„ì„"
toc: true
toc_label: "pyABSA í”„ë ˆì„ì›Œí¬ ë¶„ì„ - Instructor (ATEPC)"
toc_sticky: true
published: true

categories:
  - Capstone
tags:
  - PyTorch
  - pyABSA
last_modified_at: 2022-06-25

---

> ì´ì „ í¬ìŠ¤íŒ…ì—ì„œ ì´ì–´ì§€ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤. [pyABSA í”„ë ˆì„ì›Œí¬ ë¶„ì„ - ABSADatasetList](https://seominseok4834.github.io/capstone/11.pyabsa-dataset-item-analysis/)
>
> ëª¨ë“  ì½”ë“œëŠ” [PyABSA github](https://github.com/yangheng95/PyABSA)ì—ì„œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.

<br>

ì•„ë˜ ì½”ë“œëŠ” ë°ëª¨ ì½”ë“œë¡œ ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.

```python
from pyabsa.functional import ATEPCModelList
from pyabsa.functional import Trainer, ATEPCTrainer
from pyabsa.functional import ABSADatasetList
from pyabsa.functional import ATEPCConfigManager

atepc_config = ATEPCConfigManager.get_atepc_config_english()  # 1 í™˜ê²½ë³€ìˆ˜ ì„¤ì •

atepc_config.pretrained_bert = 'microsoft/deberta-v3-base'
atepc_config.model = ATEPCModelList.FAST_LCF_ATEPC
dataset_path = ABSADatasetList.Restaurant14  # 2 ë°ì´í„°ì…‹ ì„¤ì •
# or your local dataset: dataset_path = 'your local dataset path'

# ëª¨ë¸ ë¡œë“œ ë° í›ˆë ¨
aspect_extractor = ATEPCTrainer(config=atepc_config,
                                dataset=dataset_path,
                                from_checkpoint='',  # set checkpoint to train on the checkpoint.
                                checkpoint_save_mode=1,
                                auto_device=True
                                ).load_trained_model()
```

ë¨¼ì € ATEPCConfigManagerë¼ëŠ” í´ë˜ìŠ¤ì˜ get_atepce_config_english ë©”ì†Œë“œë¥¼ í†µí•´ configuration ë³€ìˆ˜ë¥¼ ê°€ì ¸ì˜¨ ë’¤(#1), ABSADatasetList í´ë˜ìŠ¤ì˜ ë©¤ë²„ ë³€ìˆ˜ Restaurant14 ë°ì´í„°ì…‹ì˜ ê²½ë¡œë¥¼ ê°€ì ¸ì˜¨ ë’¤(#2), ATEPCTrainer í´ë˜ìŠ¤ë¥¼ í†µí•´ ëª¨ë¸ì„ ìƒì„±, í›ˆë ¨ì„ ì§„í–‰í•©ë‹ˆë‹¤.(#3)

ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” #3 ë¶€ë¶„ì— ëŒ€í•´ì„œ ë‹¤ë£¹ë‹ˆë‹¤.

<br>

âœ” configurationì— ì €ì¥ëœ ê°’ë“¤ì„ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•˜ëŠ” ë¶€ë¶„ì´ ë§ìŠµë‹ˆë‹¤. ì–´ë–¤ ê°’ì´ ë“¤ì–´ê°€ëŠ”ì§€ ê¶ê¸ˆí•˜ë‹¤ë©´ ì•„ë˜ configurationì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.

```python
ATEPCConfigManager(args={'model': <class 'pyabsa.core.atepc.models.fast_lcf_atepc.FAST_LCF_ATEPC'>, 'optimizer': 'adamw', 'learning_rate': 2e-05, 'pretrained_bert': 'microsoft/deberta-v3-base', 'cache_dataset': True, 'warmup_step': -1, 'use_bert_spc': False, 'show_metric': False, 'max_seq_len': 80, 'SRD': 3, 'use_syntax_based_SRD': False, 'lcf': 'cdw', 'window': 'lr', 'dropout': 0.5, 'l2reg': 1e-05, 'num_epoch': 10, 'batch_size': 16, 'initializer': 'xavier_uniform_', 'seed': [52], 'polarities_dim': 3, 'log_step': 50, 'patience': 99999, 'gradient_accumulation_steps': 1, 'dynamic_truncate': True, 'srd_alignment': True, 'evaluate_begin': 0, 'hidden_dim': 768, 'embed_dim': 768, 'ABSADatasetsVersion': '2022.06.10', 'dataset_name': 'Restaurant14', 'dataset_file': {'train': ['integrated_datasets/atepc_datasets/110.SemEval/114.restaurant14/Restaurants_Train.xml.seg.atepc'], 'test': ['integrated_datasets/atepc_datasets/110.SemEval/114.restaurant14/Restaurants_Test_Gold.xml.seg.atepc'], 'valid': []}, 'device': device(type='cuda', index=0), 'device_name': 'NVIDIA GeForce RTX 2080 Ti', 'auto_device': True, 'model_name': 'fast_lcf_atepc', 'PyABSAVersion': '1.14.8', 'TransformersVersion': '4.18.0', 'TorchVersion': '1.10.2+cu102+cuda10.2', 'MV': <metric_visualizer.metric_visualizer.MetricVisualizer object at 0x7fe8bef1aee0>, 'save_mode': 1, 'model_path_to_save': 'checkpoints', 'sep_indices': 2, 'spacy_model': 'en_core_web_sm', 'IOB_label_to_index': {'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}, 'index_to_label': {0: 'Negative', 1: 'Neutral', 2: 'Positive'}, 'label_to_index': {'Negative': 0, 'Neutral': 1, 'Positive': 2}, 'index_to_IOB_label': {1: 'B-ASP', 2: 'I-ASP', 3: 'O', 4: '[CLS]', 5: '[SEP]'}, 'label_list': ['B-ASP', 'I-ASP', 'O', '[CLS]', '[SEP]'], 'num_labels': 6, 'max_test_metrics': {'max_apc_test_acc': 87.12, 'max_apc_test_f1': 80.93, 'max_ate_test_f1': 88.81}, 'metrics_of_this_checkpoint': {'apc_acc': 86.23, 'apc_f1': 79.56, 'ate_f1': 87.69}}, args_call_count={'model': 5, 'optimizer': 2, 'learning_rate': 1, 'pretrained_bert': 4, 'cache_dataset': 1, 'warmup_step': 2261, 'use_bert_spc': 14860, 'show_metric': 0, 'max_seq_len': 50176, 'SRD': 9444, 'use_syntax_based_SRD': 4722, 'lcf': 17146, 'window': 0, 'dropout': 1, 'l2reg': 2, 'num_epoch': 2, 'batch_size': 5, 'initializer': 0, 'seed': 7, 'polarities_dim': 91, 'log_step': 2261, 'patience': 27, 'gradient_accumulation_steps': 3, 'dynamic_truncate': 9444, 'srd_alignment': 0, 'evaluate_begin': 91, 'hidden_dim': 6, 'embed_dim': 0, 'ABSADatasetsVersion': 0, 'dataset_name': 30, 'dataset_file': 95, 'device': 77072, 'device_name': 0, 'auto_device': 14861, 'model_name': 4855, 'PyABSAVersion': 0, 'TransformersVersion': 0, 'TorchVersion': 0, 'MV': 5, 'save_mode': 53, 'model_path_to_save': 55, 'sep_indices': 136660, 'spacy_model': 3, 'IOB_label_to_index': 1, 'index_to_label': 2, 'label_to_index': 0, 'index_to_IOB_label': 0, 'label_list': 2135162, 'num_labels': 3, 'max_test_metrics': 629, 'metrics_of_this_checkpoint': 270})
```

<br>

âœ” í”„ë ˆì„ì›Œí¬ë‹¤ë³´ë‹ˆê¹Œ ëŒ€ë¶€ë¶„ì˜ í•¨ìˆ˜ê°€ ëª¨ë“ˆí™” ë¼ ìˆì–´ì„œ ì½”ë“œê°€ ì™”ë‹¤ê°”ë‹¤ í•©ë‹ˆë‹¤. ì„¤ëª…í•˜ë˜ í•¨ìˆ˜ë¡œ ëŒì•„ì˜¬ ê²½ìš° â—ï¸ë¥¼ ì‚¬ìš©í•´ì„œ í‘œì‹œí•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

<br>

### Trainer

Trainer í´ë˜ìŠ¤ëŠ” [PyABSA/pyabsa/functional/trainer/trainer.py](https://github.com/yangheng95/PyABSA/blob/release/pyabsa/functional/trainer/trainer.py#L59)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

ì›ë˜ ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œ Trainer í´ë˜ìŠ¤ì— ëŒ€í•´ ì •ë¦¬í•˜ë ¤ê³  í–ˆëŠ”ë°, Trainerì—ì„œ train4atepc ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ë©´ Instructor í´ë˜ìŠ¤ê°€ ìƒì„±ë˜ê³  í›ˆë ¨ê¹Œì§€ ì§„í–‰ë©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì‹¤ì œ ëª¨ë¸ ìƒì„± ë° í›ˆë ¨ì„ ë‹´ë‹¹í•˜ëŠ” Instructor í´ë˜ìŠ¤ê°€ ì£¼ëœ ë‚´ìš©ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

<br>

ë°ëª¨ ì½”ë“œì—ì„œ ìƒì„±í•œ ATEPCTrainer í´ë˜ìŠ¤ëŠ” Trainer í´ë˜ìŠ¤ë¥¼ ìƒì† ë°›ì€ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
class APCTrainer(Trainer):
    pass


class ATEPCTrainer(Trainer):
    pass


class TCTrainer(Trainer):
    pass


class AOTCTrainer(Trainer):
    pass
```

cf) [pass, continue, break ì°¨ì´ì  ì•Œì•„ë³´ê¸°](https://chancoding.tistory.com/7)

<br>

Trainer í´ë˜ìŠ¤ì˜ ìƒì„±ìë¥¼ ë¨¼ì € ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

ì „ì²´ ì½”ë“œê°€ ë„ˆë¬´ ê¸°ë‹ˆê¹Œ line by lineìœ¼ë¡œ ë³´ê² ìŠµë‹ˆë‹¤.

**trainer.py**

59 ~ 68 line

```python
class Trainer:
    def __init__(self,
                 config: ConfigManager = None,
                 dataset=None,
                 from_checkpoint: str = None,
                 checkpoint_save_mode: int = 0,
                 auto_device=True,
                 path_to_save=None,
                 load_aug=False
                 ):
```

ë¨¼ì € í•¨ìˆ˜ ì¸ìì…ë‹ˆë‹¤. ë°ëª¨ ì½”ë“œì—ì„œì™€ ë™ì¼í•˜ê²Œ ConfigManager, ABSADatasetList, checkpoint, auto_device ë“±ì„ ë„˜ê²¨ë°›ìŠµë‹ˆë‹¤.

```python
aspect_extractor = ATEPCTrainer(config=atepc_config,
                                dataset=dataset_path,
                                from_checkpoint='',  # set checkpoint to train on the checkpoint.
                                checkpoint_save_mode=1,
                                auto_device=True
                                ).load_trained_model()
```

<br>

**Trainer \_\_init__**

84 ~ 86 line

```python
if not torch.cuda.device_count() > 1 and auto_device == 'allcuda':
  print('Cuda count <= 1, reset auto_device=True')
  auto_device = True
```

ë‹¤ìŒ ë¼ì¸ì—ì„œëŠ” GPUë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” GPUê°€ ì—†ê³ , auto_deviceê°€ allcudaë¡œ ì„¤ì •ë¼ ìˆìœ¼ë©´, auto_deviceë¥¼ Trueë¡œ ë³€ê²½í•©ë‹ˆë‹¤.

+torch.cuda.device_count()ëŠ” ì‚¬ìš©ê°€ëŠ¥í•œ GPU ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

<br>

**Trainer \_\_init__**

87 ~ 91 line

```python
if 'hidden_dim' not in config.args or 'embed_dim' not in config.args:
  pretrain_config = AutoConfig.from_pretrained(config.pretrained_bert)  # Hugging Faceì˜ configuration íŒŒì¼ ë¡œë“œ
  config.hidden_dim = pretrain_config.hidden_size  # Hugging Face Bert Modelì˜ hidden dim
  config.embed_dim = pretrain_config.hidden_size
  config.ABSADatasetsVersion = query_local_version()
```

87 ~ 91 lineì—ì„œëŠ” hidden_dimê³¼ embed_dimì´ configurationì— ì •ì˜ë¼ ìˆì§€ ì•ŠëŠ” ê²½ìš°,

ë°ëª¨ ì½”ë“œì—ì„œ pertained_bertë¡œ 'microsoft/deberta-v3-base'ë¥¼ ì‚¬ìš©í–ˆëŠ”ë° í•´ë‹¹ Hugging Faceì˜ configuration íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ configurationì˜ hidden_dimê³¼ embed_dimìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

```python
atepc_config.pretrained_bert = 'microsoft/deberta-v3-base'
```

<br>

query_local_version ë©”ì†Œë“œëŠ” [PyABSA/pyabsa/utils/file_utils.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/utils/file_utils.py#L310)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

**query_local_version**

310 ~ 317 line

```python
def query_local_version():
    try:
        fin = open(find_cwd_file(['__init__.py', 'integrated_datasets']))
        local_version = fin.read().split('\'')[-2]
        fin.close()
    except:
        return None
    return local_version
```

find_cwd_fileì€ [findfile](https://github.com/yangheng95/findfile) í”„ë ˆì„ì›Œí¬ì˜ ë©”ì†Œë“œë¡œ, íŒŒì¼ ê²½ë¡œë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤. ë¦¬í„´ ë°›ì€ ê²½ë¡œì˜ íŒŒì¼ì„ ì½ì€ ë’¤, versionì„ ê°€ì ¸ì™€ configurationì— ì €ì¥í•©ë‹ˆë‹¤.

+findfile í”„ë ˆì„ì›Œí¬ëŠ” ì¶”í›„ ë”°ë¡œ ì •ë¦¬í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

<br>

ë‹¤ì‹œ [Trainer í´ë˜ìŠ¤ì˜ ìƒì„±ì](https://github.com/yangheng95/PyABSA/blob/release/pyabsa/functional/trainer/trainer.py#L92)ë¡œ ëŒì•„ì™€ì„œâ—ï¸

**Trainer \_\_init__**

92 ~ 95 line

```python
if isinstance(config, APCConfigManager):
  self.train_func = train4apc
  self.model_class = SentimentClassifier
  self.task = 'apc'

elif isinstance(config, ATEPCConfigManager):  # í˜„ì¬ configëŠ” ATEPCConfigManager
  self.train_func = train4atepc
  self.model_class = AspectExtractor
  self.task = 'atepc'

elif isinstance(config, TCConfigManager):
  self.train_func = train4tc
  self.model_class = TextClassifier
  self.task = 'classification'

elif isinstance(config, AOTCConfigManager):
  self.train_func = train4ao_tc
  self.model_class = AOTCTextClassifier
  self.task = 'ao_tc'
```

configê°€ ì–´ë–¤ íƒœìŠ¤í¬ì— ëŒ€í•œ config í´ë˜ìŠ¤ì¸ì§€ ì²´í¬í•©ë‹ˆë‹¤. ì €í¬ëŠ” ATEPC íƒœìŠ¤í¬ë¥¼ ì§„í–‰ í•  ì˜ˆì •ì´ê¸° ë•Œë¬¸ì— elif isinstance(config, ATEPCConfigManager) ë¶€ë¶„ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.

<br>

train_funcì— train4atepcê°€ ë“¤ì–´ê°€ê²Œ ë˜ëŠ”ë°, train4atepcë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„  ë°ì½”ë ˆì´í„°ë¥¼ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.

ë°ì½”ë ˆì´í„° í•¨ìˆ˜ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ë‹¤ìŒ ì„¸ ê°œì˜ ê¸€ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.

> [íŒŒì´ì¬ - í¼ìŠ¤íŠ¸í´ë˜ìŠ¤ í•¨ìˆ˜ (First Class Function)](https://schoolofweb.net/blog/posts/%ed%8c%8c%ec%9d%b4%ec%8d%ac-%ed%8d%bc%ec%8a%a4%ed%8a%b8%ed%81%b4%eb%9e%98%ec%8a%a4-%ed%95%a8%ec%88%98-first-class-function/)
>
> [íŒŒì´ì¬ - í´ë¡œì € (Closure)](https://schoolofweb.net/blog/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%81%B4%EB%A1%9C%EC%A0%80-closure/)
>
> [íŒŒì´ì¬ - ë°ì½”ë ˆì´í„° (Decorator)](https://schoolofweb.net/blog/posts/%ed%8c%8c%ec%9d%b4%ec%8d%ac-%eb%8d%b0%ec%bd%94%eb%a0%88%ec%9d%b4%ed%84%b0-decorator/)

<br>

train4atepc ë©”ì†Œë“œëŠ” [PyABSA/pyabsa/core/atepc/training/atepc_trainer.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/training/atepc_trainer.py#L460)ì— ì •ì˜ë¼ ìˆëŠ”ë°, @retryë¼ëŠ” ì‹¬ë³¼ì´ ë¶™ì–´ìˆìŠµë‹ˆë‹¤.

**train4atepc**

460 ~ 465 line

```python
@retry
def train4atepc(opt, from_checkpoint_path, logger):
    # in case of handling ConnectionError exception
    trainer = Instructor(opt, logger)
    resume_from_checkpoint(trainer, from_checkpoint_path)

    return trainer.run()
```

ì´ëŠ” [PyABSA/pyabsa/utils/pyabsa_utils.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/utils/pyabsa_utils.py#L155)ì— ì •ì˜ëœ retry ë°ì½”ë ˆì´í„° í•¨ìˆ˜ì˜ ì¸ìë¡œ ë„˜ê²¨ ì‹¤í–‰í•˜ëŠ” êµ¬ë¬¸ì…ë‹ˆë‹¤.

**retry**

155 ~ 177 line

```python
def retry(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        count = 5
        while count:

            try:
                return f(*args, **kwargs)
            except (
                TransformerConnectionError,
                requests.exceptions.RequestException,
                requests.exceptions.ConnectionError,
                requests.exceptions.HTTPError,
                requests.exceptions.ConnectTimeout,
                requests.exceptions.ProxyError,
                requests.exceptions.SSLError,
                requests.exceptions.BaseHTTPError,
            ) as e:
                print('Training Exception: {}, will retry later'.format(e))
                time.sleep(60)
                count -= 1

    return decorated
```

retry í•¨ìˆ˜ëŠ” ë‹¨ìˆœíˆ ì¸ìë¡œ ë°›ì€ ë©”ì†Œë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. exception ë°œìƒì‹œ 5ë²ˆê¹Œì§€ ì¬ì‹¤í–‰ í•©ë‹ˆë‹¤.

<br>

ë‹¤ì‹œ [train4atepc](https://github.com/yangheng95/PyABSA/blob/4320db7dd0cfa0053a48169e59ff44403482eaf1/pyabsa/core/atepc/training/atepc_trainer.py#L460) ë©”ì†Œë“œë¡œ ëŒì•„ì˜¤ë©´â—ï¸

```python
@retry
def train4atepc(opt, from_checkpoint_path, logger):
    # in case of handling ConnectionError exception
    trainer = Instructor(opt, logger)
    resume_from_checkpoint(trainer, from_checkpoint_path)

    return trainer.run()
```

Instruct í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•œ ë’¤ run ë©”ì†Œë“œê¹Œì§€ ì‹¤í–‰í•˜ê¸° ë•Œë¬¸ì— Instructor í´ë˜ìŠ¤ì˜ ]ìƒì„±ìë¶€í„° run ë©”ì†Œë“œê¹Œì§€ ì´ì–´ì„œ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

ì¶”ê°€ì ìœ¼ë¡œ trainerì˜ íŒŒë¼ë¯¸í„°ë¡œ ë“¤ì–´ê°€ëŠ” optì™€ loggerëŠ” ConfigManagerì™€ íŒŒì´ì¬ ê¸°ë³¸ ë‚´ì¥ ëª¨ë“ˆì¸ loggingì…ë‹ˆë‹¤. (trainer.py 163 ~ 167 line)

```python
if self.checkpoint_save_mode:
  model_path.append(self.train_func(self.config, self.from_checkpoint, self.logger))
else:
  # always return the last trained model if dont save trained model
  model = self.model_class(model_arg=self.train_func(self.config, self.from_checkpoint, self.logger))
```

<br><br>

ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œ ì§‘ì¤‘ì ìœ¼ë¡œ ë‹¤ë£° Instructor í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

### Instructor

Instructor í´ë˜ìŠ¤ëŠ” [PyABSA/pyabsa/core/atepc/training/atepc_trainer.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/training/atepc_trainer.py#L40)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

<script src="https://gist.github.com/seominseok4834/1981c64fe691a228a0149db6638e2c6d.js"></script>

<br>

ì½”ë“œê°€ ê¸¸ì–´ì„œ ìƒì„±ì ë¶€ë¶„ì„ ë¼ì¸ë³„ë¡œ ë‚˜ëˆ ì„œë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

**Instructor \_\_init__**

42 ~ 65 line

```python
def __init__(self, opt, logger):
  self.warmup_scheduler = None
  self.lr_scheduler = None
  self.opt = opt
  self.logger = logger

  self.train_dataloader = None
  self.test_dataloader = None
  # if opt.use_bert_spc:
  #     self.logger.info('Warning: The use_bert_spc is disabled for extracting aspect,'
  #                      ' reset use_bert_spc=False and go on... ')
  #     opt.use_bert_spc = False
  import warnings
  warnings.filterwarnings('ignore')
  if self.opt.gradient_accumulation_steps < 1:
    raise ValueError("Invalid gradient_accumulation_steps parameter: {}, should be >= 1".format(
      self.opt.gradient_accumulation_steps))

    self.opt.batch_size = self.opt.batch_size // self.opt.gradient_accumulation_steps

    random.seed(self.opt.seed)
    np.random.seed(self.opt.seed)
    torch.manual_seed(self.opt.seed)
    torch.cuda.manual_seed(self.opt.seed)
```

í´ë˜ìŠ¤ ë‚´ì—ì„œ ì‚¬ìš©í•  ë©¤ë²„ ë³€ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

gradient accumulationì€ ë¯¸ë‹ˆ ë°°ì¹˜ë¥¼ í†µí•´ gradientë¥¼ n-step ë™ì•ˆ gloabl gradientsì— ëˆ„ì ì‹œí‚¨ í›„, í•œ ë²ˆì— ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. cf) [Tensorflow Gradient Accumulation ê°„ë‹¨ êµ¬í˜„](https://hwiyong.tistory.com/419)

ì´ë¥¼ ìœ„í•´ batch sizeë¥¼ gradient accumulation_stepsë¡œ ë‚˜ëˆˆ ëª«ì„ batch sizeë¡œ ì¬ì •ì˜í•´ì£¼ì—ˆê³ , í›ˆë ¨ ê²°ê³¼ ì¬í˜„ì„ ìœ„í•´ ì‹œë“œë¥¼ ê³ ì •í–ˆìŠµë‹ˆë‹¤.

<br>

**Instructor \_\_init__**

67 ~ 77 line

```python
if self.opt.model_path_to_save and not os.path.exists(self.opt.model_path_to_save):  # 1
  os.makedirs(self.opt.model_path_to_save)

  try:  # 2
    self.tokenizer = AutoTokenizer.from_pretrained(self.opt.pretrained_bert, do_lower_case='uncased' in self.opt.pretrained_bert)
    bert_base_model = AutoModel.from_pretrained(self.opt.pretrained_bert)
    self.opt.sep_indices = self.tokenizer.sep_token_id

  except ValueError as e:
    print('Init pretrained model failed, exception: {}'.format(e))
    raise TransformerConnectionError()
```

#1: ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ë‹¤ë©´ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

#2: try êµ¬ë¬¸ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ì™€ pretrained ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

<br>

**Instructor \_\_init__**

79 line

```python
processor = ATEPCProcessor(self.tokenizer)
```

ATEPCProcessor í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. âœ… *ì´ ë¶€ë¶„ì€ ì´í›„ í¬ìŠ¤íŒ…ì—ì„œ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.*

<br>

**Instructor \_\_init__**

81 ~ 83 line

```python
config_str = re.sub(r'<.*?>', '', str(sorted([str(self.opt.args[k]) for k in self.opt.args if k != 'seed'])))  # 1
hash_tag = sha256(config_str.encode()).hexdigest()  # 2
cache_path = '{}.{}.dataset.{}.cache'.format(self.opt.model_name, self.opt.dataset_name, hash_tag)  # 3

if os.path.exists(cache_path):  # cache_pathê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°
  print('Loading dataset cache:', cache_path)
  with open(cache_path, mode='rb') as f:
    if self.opt.dataset_file['test']:  # 4
      self.train_data, self.test_data, opt = pickle.load(f)

    else:  # 5
      self.train_data, opt = pickle.load(f)
      # reset output dim according to dataset labels
      self.opt.polarities_dim = opt.polarities_dim
```

#1: configurationì— ì €ì¥ëœ key ê°’ë“¤ì„ ìˆœíšŒí•˜ë©´ì„œ value ê°’ì„ string í˜•íƒœë¡œ ì •ë ¬í•´ì„œ ì €ì¥í•©ë‹ˆë‹¤. (re.sub ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ì„œ <> ì œê±°)

#2: ì €ì¥í•œ configuration ê°’ë“¤ì„ SHA256ìœ¼ë¡œ ì•”í˜¸í™”í•©ë‹ˆë‹¤.

#3: cache pathë¥¼ ìƒì„±í•´ í•´ë‹¹ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì½ì–´ë“¤ì…ë‹ˆë‹¤.

#4: ë§Œì•½ test ë°ì´í„°ì…‹ì´ ì¡´ì¬í•˜ë©´, pickle.load ë©”ì†Œë“œë¡œ train ë°ì´í„°ì™€ test ë°ì´í„°ë¥¼ ì½ì–´ë“¤ì…ë‹ˆë‹¤.

#5: test ë°ì´í„°ì…‹ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš° train_dataì™€ optë¥¼ ì½ì–´ë“¤ì´ê³ , ê°ì„±ê·¹ì„±ì˜ ì°¨ì›ì„ ë©¤ë²„ë³€ìˆ˜ì¸ optì— ì €ì¥í•©ë‹ˆë‹¤.

<br>

**Instructor \_\_init__**

96 ~ 136 line

```python
else:  # cache_pathê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
  self.train_examples = processor.get_train_examples(self.opt.dataset_file['train'], 'train')  # 1
  train_features = convert_examples_to_features(self.train_examples, self.opt.max_seq_len, self.tokenizer, self.opt)
  self.opt.label_list = sorted(list(self.opt.IOB_label_to_index.keys()))
  self.opt.num_labels = len(self.opt.label_list) + 1
  all_spc_input_ids = torch.tensor([f.input_ids_spc for f in train_features], dtype=torch.long)
  all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)
  all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)
  all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)
  all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)
  all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)
  all_polarities = torch.tensor([f.polarity for f in train_features], dtype=torch.long)
  lcf_cdm_vec = torch.tensor([f.lcf_cdm_vec for f in train_features], dtype=torch.float32)
  lcf_cdw_vec = torch.tensor([f.lcf_cdw_vec for f in train_features], dtype=torch.float32)

  self.train_data = TensorDataset(all_spc_input_ids, all_segment_ids, all_input_mask, all_label_ids,
                                  all_polarities, all_valid_ids, all_lmask_ids, lcf_cdm_vec, lcf_cdw_vec)

  if self.opt.dataset_file['test']:
    self.test_examples = processor.get_test_examples(self.opt.dataset_file['test'], 'test')
    test_features = convert_examples_to_features(self.test_examples, self.opt.max_seq_len,
                                                 self.tokenizer, self.opt)
    all_spc_input_ids = torch.tensor([f.input_ids_spc for f in test_features], dtype=torch.long)
    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)
    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)
    all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)
    all_polarities = torch.tensor([f.polarity for f in test_features], dtype=torch.long)
    all_valid_ids = torch.tensor([f.valid_ids for f in test_features], dtype=torch.long)
    all_lmask_ids = torch.tensor([f.label_mask for f in test_features], dtype=torch.long)
    lcf_cdm_vec = torch.tensor([f.lcf_cdm_vec for f in test_features], dtype=torch.float32)
    lcf_cdw_vec = torch.tensor([f.lcf_cdw_vec for f in test_features], dtype=torch.float32)
    self.test_data = TensorDataset(all_spc_input_ids, all_segment_ids, all_input_mask, all_label_ids, all_polarities, all_valid_ids, all_lmask_ids, lcf_cdm_vec, lcf_cdw_vec)

    if self.opt.cache_dataset and not os.path.exists(cache_path):
      print('Caching dataset... please remove cached dataset if change model or dataset')
      with open(cache_path, mode='wb') as f:
        if self.opt.dataset_file['test']:
          pickle.dump((self.train_data, self.test_data, self.opt), f)
        else:
          pickle.dump((self.train_data, self.opt), f)
```

#1: ë§Œì•½ cache pathê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, 79 lineì—ì„œ ìƒì„±í•œ ATEPCProcessor í´ë˜ìŠ¤ì˜ get_train_examples ë©”ì†Œë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

opt.dataset_fileì—ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë°ì´í„°ì…‹ì˜ ê²½ë¡œê°€ ë“¤ì–´ìˆìŠµë‹ˆë‹¤.

```python
'dataset_file': {'train': ['integrated_datasets/atepc_datasets/110.SemEval/114.restaurant14/Restaurants_Train.xml.seg.atepc'], 'test': ['integrated_datasets/atepc_datasets/110.SemEval/114.restaurant14/Restaurants_Test_Gold.xml.seg.atepc'], 'valid': []}
```

<br>

get_train_examples ë©”ì†Œë“œëŠ” [PyABSA/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L185)ì˜ ATEPCProcessor í´ë˜ìŠ¤ì˜ ë©¤ë²„ í•¨ìˆ˜ë¡œ ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

ì´í›„ ë“±ì¥í•˜ëŠ” ë©”ì†Œë“œë“¤(get_train_examples â¡ï¸ \_read_tsv â¡ï¸ readfile â¡ï¸ \_create_examples) ëª¨ë‘ ê°™ì€ íŒŒì¼ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

<script src="https://gist.github.com/seominseok4834/1568fc0853c2a82d15ba6da6ab5c6e81.js"></script>

<br>

ì‹¤í–‰ ìˆœì„œëŒ€ë¡œ ë¨¼ì €  get_train_examples ë©”ì†Œë“œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

**get_train_examples**

185 ~ 188 line

```python
def get_train_examples(self, data_dir, set_tag):
  """See base class."""
  return self._create_examples(
    self._read_tsv(data_dir), set_tag)
```

\_read_tsv ë©”ì†Œë“œë¡œ ë°ì´í„°ë¥¼ ì½ì–´ì™€ \_create_examples ë©”ì†Œë“œë¡œ examplesë¥¼ ìƒì„±í•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.

<br>

**\_read_tsv**

168 ~ 174 line

```python
@classmethod
def _read_tsv(cls, input_file, quotechar=None):
  """Reads a tab separated value file."""
  data = []
  for file in input_file:
    data += readfile(file)
    return data
```

\_read_tsvëŠ” ATEPCProcessor í´ë˜ìŠ¤ì˜ ë©¤ë²„ í•¨ìˆ˜ë¡œ, ì¸ìë¡œ ë„˜ê²¨ë°›ì€ input_fileì„ ìˆœíšŒí•˜ë©´ì„œ readfile ë©”ì†Œë“œë¡œ ì´ë¥¼ ì½ì–´ë“¤ì—¬ dataì— ì¶”ê°€í•©ë‹ˆë‹¤.

<br>

**readfile**

ì´ì–´ì„œ readfile ë©”ì†Œë“œê°€ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ì½”ë“œê°€ ê¸¸ì–´ ë¼ì¸ë³„ë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

67 ~ 91 line

```python
def readfile(filename):
    '''
    read file
    '''
    f = open(filename, encoding='utf8')  # 1
    data = []
    sentence = []
    tag = []
    polarity = []
    for line in f:  # 2
        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == "\n":  # 3
            if len(sentence) > 0:
                data.append((sentence, tag, polarity))
                sentence = []
                tag = []
                polarity = []
            continue
        splits = line.strip().split(' ')  # 4
        if len(splits) != 3:  # 5
            print('warning! ignore detected error line(s) in input file:{}'.format(line))
            break
        sentence.append(splits[0])
        tag.append(splits[-2])
        polarity.append(splits[-1])
        Labels.add(splits[-2])
```

ì¸ìë¡œ ë„˜ê²¨ë°›ì€ íŒŒì¼ì„ ì˜¤í”ˆí•´(#1), í•œ ë¼ì¸ì”© ì½ìŠµë‹ˆë‹¤.(#2)

í˜„ì¬ ì½ê³  ìˆëŠ” ë°ì´í„°ëŠ” ì•„ë˜ì²˜ëŸ¼ ë¬¸ì¥ë³„ë¡œ ë¶„ë¦¬ë¼ ìˆìŠµë‹ˆë‹¤.

```xml
The O -999
bread B-ASP Positive
is O -999
top O -999
notch O -999
as O -999
well O -999
. O -999

I O -999
have O -999
to O -999
say O -999
they O -999
have O -999
one O -999
of O -999
the O -999
fastest O -999
delivery B-ASP Positive
times I-ASP Positive
in O -999
the O -999
city O -999
. O -999
```

#3: ë§Œì•½ ì½ì–´ë“¤ì¸ ë¼ì¸ì˜ ê¸¸ì´ê°€ 0ì´ê±°ë‚˜, \-DOCSTART, \\nì¸ ê²½ìš° dataì— ì´ì „ì— ì½ì–´ë“¤ì¸ ê²ƒë“¤ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

#4: ìœ„ ì¡°ê±´ì´ ì•„ë‹Œ ê²½ìš° ë¼ì¸ë³„ë¡œ ì½ì–´ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ splití•©ë‹ˆë‹¤. (split í–ˆì„ ë•Œ 3ê°œê°€ ì•„ë‹ˆë¼ë©´ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  ì¢…ë£Œí•©ë‹ˆë‹¤. #5)

ì˜ˆë¥¼ ë“¤ì–´ ì²«ë²ˆì§¸ ë¬¸ì¥ "The bread is top notch as well."ì˜ ê²½ìš° sentence, tag, polarityê°€ ì•„ë˜ì™€ ê°™ì´ë˜ê²Œ ë©ë‹ˆë‹¤.

```python
sentence = ['The', 'bread', 'is', 'top', 'notch', 'as', 'well', '.']
tag = ['O', 'B-ASP', 'O', 'O', 'O', 'O', 'O', 'O']
polarity = ['-999', 'Positive', '-999', '-999', '-999', '-999', '-999', '-999']
```

<br>

LabelsëŠ” 15 lineì— ì •ì˜í•œ ë¹ˆ ì§‘í•©ì…ë‹ˆë‹¤. Setì€ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— B-ASP, I-OSP, O ì„¸ ê°œì˜ ê°’ë§Œ ì €ì¥ë˜ê²Œ ë©ë‹ˆë‹¤.

```python
Labels = set()
```

<br>

**read_file**

93 ~ 107 line

```python
prepared_data = []
for s, t, p in data:  # 1

  if len(s) > 0:  # 2
    # prepare the atepc dataset, refer to https://github.com/yangheng95/PyABSA/issues/78
    polarity_padding = [str(SENTIMENT_PADDING)] * len(t)

    if len(Labels) > 3:  # 3
      # for more IOB labels support, but can not split cases in some praticular conditions, e.g., (B,I,E,O)
      for p_idx in range(len(p) - 1):  # 4
        if (p[p_idx] != p[p_idx + 1] and p[p_idx] != str(SENTIMENT_PADDING) and p[p_idx + 1] != str(SENTIMENT_PADDING)) \
        or (p[p_idx] != str(SENTIMENT_PADDING) and p[p_idx + 1] == str(SENTIMENT_PADDING)):  # 5
          _p = p[:p_idx + 1] + polarity_padding[p_idx + 1:]
          p = polarity_padding[:p_idx + 1] + p[p_idx + 1:]
          prepared_data.append((s, t, _p))
```

#1: ì´í›„ ì €ì¥í•œ dataë¥¼ ìˆœíšŒí•˜ë©° sentence, tag, polarityë¥¼ êº¼ëƒ…ë‹ˆë‹¤.

#2: sentenceì˜ ê¸¸ì´ê°€ 0ë³´ë‹¤ í¬ë©´ SENTIMENT_PADDINGìœ¼ë¡œ ì •ì˜í•œ -999ë¥¼ stringìœ¼ë¡œ ë°”ê¾¼ ë’¤, tagì˜ ê°œìˆ˜ë§Œí¼ ê³±í•´ì¤ë‹ˆë‹¤.

+SENTIMENT_PADDINGì€ [PyABSA/pyabsa/utils/pyabsa_utils.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/utils/pyabsa_utils.py#L24)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

ìœ„ì—ì„œ ì˜ˆì‹œë¡œ ë“  "The bread is top notch as well."ì˜ ê²½ìš° ['-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999']ê°€ polarity_paddingì´ ë˜ê²Œ ë©ë‹ˆë‹¤.

#3: Labels ê°’ì´ 3ê°œë³´ë‹¤ í° ê²½ìš°(IOB annotation íƒœê·¸ê°€ ì„¸ ê°œ ì´ìƒì¸ ê²½ìš°), polarityì— ì €ì¥ëœ ê°œìˆ˜ë³´ë‹¤ í•˜ë‚˜ ì ê²Œ ë£¨í”„ë¥¼ ìˆœíšŒí•©ë‹ˆë‹¤.(#4)

#5: ì¡°ê±´ì´ ì¡°ê¸ˆ ë³µì¡í•œë° â‘  ië²ˆì§¸ ì¸ë±ìŠ¤, i+1ë²ˆì§¸ ì¸ë±ìŠ¤ì˜ ê°ì„±ì´ ê°™ì§€ ì•Šê³ , ië²ˆì§¸ ì¸ë±ìŠ¤ì™€ i+1ë²ˆì§¸ ì¸ë±ìŠ¤ì˜ ê°ì„±ì´ íŒ¨ë”©ê°’(-999)ê°€ ì•„ë‹ˆê±°ë‚˜ â‘¡ ië²ˆì§¸ ì¸ë±ìŠ¤ì™€ i+1ë²ˆì§¸ ì¸ë±ìŠ¤ì˜ ê°ì„±ì´ íŒ¨ë”©ê°’(-999)ì´ ì•„ë‹Œ ê²½ìš°, ë‘ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¥¼ ë§Œì¡±í•˜ë©´ ì‹¤í–‰ë©ë‹ˆë‹¤.

\_pëŠ” p[0ë²ˆ ì¸ë±ìŠ¤ ~ ië²ˆì§¸ ì¸ë±ìŠ¤] + polarity_padding[i+1ë²ˆì§¸ ì¸ë±ìŠ¤ ~ ë], pì—ëŠ” polarity_padding[0ë²ˆ ì¸ë±ìŠ¤ ~ ië²ˆì§¸ ì¸ë±ìŠ¤] + p[i+1që²ˆì§¸ ì¸ë±ìŠ¤ ~ ë] ìœ¼ë¡œ ìŠ¬ë¼ì´ì‹±í•œ ë‹¤ìŒ, prepared_dataì— ì¶”ê°€í•©ë‹ˆë‹¤.

*ì•„ë˜ IOB annotationì´ ì„¸ ê°œì¸ ë°ì´í„°ì˜ ì˜ˆì‹œë¥¼ ë³´ë©´ ê°ì´ ì¡íê²ë‹ˆë‹¤.*

<br>

108 ~ 121 line

```python
            else:  # 1
                for t_idx in range(1, len(t)):  # 2
                    # for 3 IOB label (B, I, O)
                    if p[t_idx - 1] != str(SENTIMENT_PADDING) and split_aspect(t[t_idx - 1], t[t_idx]):  # 3
                        _p = p[:t_idx] + polarity_padding[t_idx:]
                        p = polarity_padding[:t_idx] + p[t_idx:]
                        prepared_data.append((s, t, _p))

                    if p[t_idx] != str(SENTIMENT_PADDING) and t_idx == len(t) - 1 and split_aspect(t[t_idx]):   # 4
                        _p = p[:t_idx + 1] + polarity_padding[t_idx + 1:]
                        p = polarity_padding[:t_idx + 1] + p[t_idx + 1:]
                        prepared_data.append((s, t, _p))

    return prepared_data
```

Labels(IOB Annotation íƒœê·¸)ê°€ ì„¸ ê°œ ì´í•˜ë¼ë©´, #1ì˜ elseë¬¸ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.

#2: Labelì´ ë„¤ ê°œ ì´ìƒì¸ ê²½ìš°ì™€ ë™ì¼í•˜ê²Œ tagì˜ ê¸¸ì´ë³´ë‹¤ í•˜ë‚˜ ì ê²Œ ìˆœíšŒí•©ë‹ˆë‹¤.

#3ì„ ë³´ê¸°ì „ split_aspect ë©”ì†Œë“œë¥¼ ë¨¼ì € ë³´ê² ìŠµë‹ˆë‹¤.

<br>

**split_aspect**

124 ~ 149 line

```python
def split_aspect(tag1, tag2=None):
    if tag1 == 'B-ASP' and tag2 == 'B-ASP':  # ë¬¸ì¥ì—ì„œ ë‘ ê°œì˜ ê°ì„±ì´ ì—°ë‹¬ì•„ ë‚˜ì˜¤ëŠ” ê²½ìš°
        return True
    if tag1 == 'B-ASP' and tag2 == 'O':  # ë¬¸ì¥ì—ì„œ ê°ì„±ìœ¼ë¡œ íƒœê·¸ëœ ë¶€ë¶„ì´ ëë‚œ ê²½ìš° (ë‹¤ìŒì— ê°ì„±ì´ ì—†ëŠ” ë‹¨ì–´ê°€ ë‚˜ì˜´)
        return True
    elif tag1 == 'I-ASP' and tag2 == 'O':  # ë¬¸ì¥ì—ì„œ ê°ì„±ìœ¼ë¡œ íƒœê·¸ëœ ë¶€ë¶„ì´ ëë‚œ ê²½ìš° (ë‹¤ìŒì— ê°ì„±ì´ ì—†ëŠ” ë‹¨ì–´ê°€ ë‚˜ì˜´)
        return True
    elif tag1 == 'I-ASP' and tag2 == 'B-ASP':  # ë¬¸ì¥ì—ì„œ ë‘ ê°œì˜ ê°ì„±ì´ ì—°ë‹¬ì•„ ë‚˜ì˜¤ëŠ” ê²½ìš°
        return True
    elif (tag1 == 'B-ASP' or tag1 == 'I-ASP') and not tag2:  # ë¬¸ì¥ì—ì„œ ê°ì„±ìœ¼ë¡œ íƒœê·¸ëœ ë¶€ë¶„ì´ ëë‚œ ê²½ìš° (ë¬¸ì¥ì´ ì•„ì˜ˆ ëë‚¨)
        return True
    elif tag1 == 'O' and tag2 == 'I-ASP':
        # warnings.warn('Invalid annotation! Found I-ASP without B-ASP')
        return False
    elif tag1 == 'O' and tag2 == 'O':
        return False
    elif tag1 == 'O' and tag2 == 'B-ASP':
        return False
    elif tag1 == 'O' and not tag2:
        return False
    elif tag1 == 'B-ASP' and tag2 == 'I-ASP':
        return False
    elif tag1 == 'I-ASP' and tag2 == 'I-ASP':
        return False
    else:
        return False
```

split_aspect ë©”ì†Œë“œëŠ” ë¬¸ì¥ì—ì„œ ê°ì„±ì„ ë¶„ë¦¬í•˜ê¸° ìœ„í•œ ë©”ì†Œë“œì…ë‹ˆë‹¤.

â‘  ë¬¸ì¥ì—ì„œ ê°ì„±ìœ¼ë¡œ íƒœê·¸ëœ ë¶€ë¶„ì´ ëë‚œ ê²½ìš° (ë‹¤ìŒì— ê°ì„±ì´ ì—†ëŠ” ë‹¨ì–´ê°€ ë‚˜ì˜¤ê±°ë‚˜ ë¬¸ì¥ì´ ì•„ì˜ˆ ëë‚œ ê²½ìš°), â‘¡ë¬¸ì¥ì—ì„œ ê°ì„±ì´ ì—°ë‹¬ì•„ ë‚­ëŠ” ê²½ìš°ì—ë§Œ Trueë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.

ì•„ë˜ ì˜ˆì‹œì—ì„œëŠ” 10ë²ˆì§¸ ì¸ë±ìŠ¤ì—ì„œ Trueë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.

```xml
Try O -999  <!-- 0 -->
the O -999  <!-- 1 -->
rose B-ASP -999  <!-- 2 -->
roll I-ASP -999  <!-- 3 -->
- O -999  <!-- 4 -->
LRB O -999  <!-- 5 -->
- O -999  <!-- 6 -->
not O -999  <!-- 7 -->
on O -999  <!-- 8 -->
menu B-ASP Neutral  <!-- 9 -->
- O -999  <!-- 10: True -->
RRB O -999  <!-- 11 -->
- O -999  <!-- 12 -->
. O -999  <!-- 13 -->
```

<br>

ë‹¤ì‹œ [read_file](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L108) ë©”ì†Œë“œë¡œ ëŒì•„ì™€ì„œâ—ï¸

108 ~ 121 line

```python
            else:  # 1
                for t_idx in range(1, len(t)):  # 2
                    # for 3 IOB label (B, I, O)
                    if p[t_idx - 1] != str(SENTIMENT_PADDING) and split_aspect(t[t_idx - 1], t[t_idx]):  # 3
                        _p = p[:t_idx] + polarity_padding[t_idx:]
                        p = polarity_padding[:t_idx] + p[t_idx:]
                        prepared_data.append((s, t, _p))

                    if p[t_idx] != str(SENTIMENT_PADDING) and t_idx == len(t) - 1 and split_aspect(t[t_idx]):   # 4
                        _p = p[:t_idx + 1] + polarity_padding[t_idx + 1:]
                        p = polarity_padding[:t_idx + 1] + p[t_idx + 1:]
                        prepared_data.append((s, t, _p))

    return prepared_data
```

ìœ„ì— ì˜ˆì‹œì—ì„œ 10ë²ˆì§¸ ì¸ë±ìŠ¤ë¼ê³  ê°€ì •í•˜ë©´ #3ì˜ ì¡°ê±´ì„ ë§Œì¡±í•´ ì•„ë˜ ì½”ë“œê°€ ì‹¤í–‰ë˜ê³ ,

\_pì™€ pì— ë‹¤ìŒê³¼ ê°™ì´ ì €ì¥ë˜ê²Œ ë©ë‹ˆë‹¤.

```python
_p = ['-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', 'Neutral', '-999', '-999', '-999', '-999']
p = ['-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999']
```

<br>

#4ëŠ” ê°ì„±ì´ ë¼ë²¨ë§ëœ ë‹¨ì–´ê°€ ë¬¸ì¥ì˜ ë§¨ ëì— ë“±ì¥í•  ë•Œë§Œ ì‹¤í–‰ë˜ëŠ”ë°, ì•„ë˜ ì˜ˆì‹œì—ì„œëŠ”

```xml
Desserts B-ASP -999  <!-- 0 -->
are O -999  <!-- 1 -->
almost O -999  <!-- 2 -->
incredible O -999  <!-- 3 -->
: O -999  <!-- 4 -->
my O -999  <!-- 5 -->
personal O -999  <!-- 6 -->
favorite O -999  <!-- 7 -->
is O -999  <!-- 8 -->
their O -999  <!-- 9 -->
Tart B-ASP Positive  <!-- 10 -->
of I-ASP Positive  <!-- 11 -->
the I-ASP Positive  <!-- 12 -->
Day I-ASP Positive  <!-- 13 -->
```

13ë²ˆì§¸ ì¸ë±ìŠ¤ì—ì„œ #4ê°€ ì‹¤í–‰ë˜ê³ , \_pì™€ pì— ë‹¤ìŒê³¼ ê°™ì´ ì €ì¥ë˜ê²Œ ë©ë‹ˆë‹¤.

```python
_p = ['-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', 'Positive', 'Positive', 'Positive', 'Positive']
p = ['-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999', '-999']
```

ì´ë ‡ê²Œ ë³€ê²½í•œ \_pë¥¼ prepared_dataë¡œ ì¶”ê°€í•˜ê³  ìµœì¢…ì ìœ¼ë¡œ readfile ë©”ì†Œë“œëŠ” prepared_dataë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.

prepared_dataëŠ” ì•„ë˜ì²˜ëŸ¼ ë¬¸ì¥ë³„ë¡œ íŠœí”Œ í˜•íƒœë¡œ ë¬¶ì—¬ìˆëŠ” ë¦¬ìŠ¤íŠ¸ì´ë‹¤.

```python
[(['Desserts',
   'are',
   'almost',
   'incredible',
   ':',
   'my',
   'personal',
   'favorite',
   'is',
   'their',
   'Tart',
   'of',
   'the',
   'Day'],
  ['B-ASP',
   'O',
   'O',
   'O',
   'O',
   'O',
   'O',
   'O',
   'O',
   'O',
   'B-ASP',
   'I-ASP',
   'I-ASP',
   'I-ASP'],
  ['-999',
   '-999',
   '-999',
   '-999',
   '-999',
   '-999',
   '-999',
   '-999',
   '-999',
   '-999',
   'Positive',
   'Positive',
   'Positive',
   'Positive'])]
```

<br>

ğŸ§‘â€ğŸ« ì§€ê¸ˆê¹Œì§€ ë‚´ìš© ì •ë¦¬

[get_train_examples](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L185)

```python
    def get_train_examples(self, data_dir, set_tag):
        """See base class."""
        return self._create_examples(
            self._read_tsv(data_dir), set_tag)
```

Instructor í´ë˜ìŠ¤ ìƒì„±ìì—ì„œ get_train_examples ë©”ì†Œë“œ í˜¸ì¶œ â¡ï¸ \_read_tsv ë©”ì†Œë“œì—ì„œ readfile ë©”ì†Œë“œë¥¼ í˜¸ì¶œ â¡ï¸ ë¦¬í„´ë°›ì€ prepared_dataë¥¼ íŒŒë¼ë¯¸í„°ë¡œ \_create_examples ë©”ì†Œë“œ í˜¸ì¶œ

<br>

ì´ì œ \_craete_examples ë©”ì†Œë“œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤ ..

<br>

**_create_examples**

198 ~ 218 line

```python
def _create_examples(self, lines, set_type):
  examples = []

  for i, (sentence, tag, polarity) in enumerate(lines):  # 1
    aspect = []
    aspect_tag = []
    aspect_polarity = SENTIMENT_PADDING
    for w, t, p in zip(sentence, tag, polarity):  # 2
      if str(p) != str(SENTIMENT_PADDING):
        aspect.append(w)
        aspect_tag.append(t)
        aspect_polarity = p

        guid = "%s-%s" % (set_type, i)
        text_a = sentence
        text_b = aspect

        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, IOB_label=tag,
                                     aspect_label=aspect_tag, polarity=aspect_polarity))
        
	return examples
```

#1: ì•ì„œ readfile ë©”ì†Œë“œë¡œ ì €ì¥í•œ prepare_datasetì„ ì¸ìë¡œ ë°›ì•„, enumerateë¡œ ìˆœíšŒí•©ë‹ˆë‹¤.

#2: w=sentence, t=tag, p=polarityë¡œ pê°€ íŒ¨ë”©ê°’(-999)ì´ ì•„ë‹Œ ê°ì„±ì¼ ê²½ìš°ì—ë§Œ aspect, aspect_tag, aspectë¥¼ ì €ì¥í•œ ë’¤, ì´ë¥¼ InputExampleì´ë¼ëŠ” í´ë˜ìŠ¤ë¡œ ë§Œë“¤ì–´ examples ë¦¬ìŠ¤íŠ¸ì— appendí•œ ë’¤ examplesë¥¼ ë¦¬í„´í•œë‹¤.

<br>

**InputExample \_\_init__**

18 ~ 38 line

```python
class InputExample(object):
    """A single training_tutorials/test example for simple sequence classification."""

    def __init__(self, guid, text_a, text_b=None, IOB_label=None, aspect_label=None, polarity=None):
        """Constructs a InputExample.
        Args:
            guid: Unique id for the example.
            text_a: string. The untokenized text of the first sequence. For single
            sequence core, only this sequence must be specified.
            text_b: (Optional) string. The untokenized text of the second sequence.
            Only must be specified for sequence pair core.
            label: (Optional) string. The label of the example. This should be
            specified for train and dev examples, but not for test examples.
        """
        self.guid = guid
        self.text_a = text_a
        self.text_b = text_b
        self.IOB_label = IOB_label
        self.aspect_label = aspect_label
        self.polarity = polarity
```

InputExample í´ë˜ìŠ¤ëŠ” êµ¬ì¡°ê°€ ê°„ë‹¨í•˜ë‹¤. ê·¸ëƒ¥ ì¸ìë¡œ ë„˜ê²¨ë°›ì€ ëª¨ë“  ê°’ì„ ë©¤ë²„ ë³€ìˆ˜ë¡œ ì €ì¥í•œë‹¤.

<br>

ìœ„ì—ì„œ ì‚¬ìš©í•œ ì˜ˆì‹œì˜ ë¬¸ì¥ì„ InputExample í´ë˜ìŠ¤ë¡œ ë³€í™˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ëœë‹¤.

```python
{'guid': 'train-0', 'text_a': ['Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Tart', 'of', 'the', 'Day'], 'text_b': ['Tart', 'of', 'the', 'Day'], 'IOB_label': ['B-ASP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP'], 'aspect_label': ['B-ASP', 'I-ASP', 'I-ASP', 'I-ASP'], 'polarity': 'Positive'}
```

<br><br>

ë“œë””ì–´ ë‹¤ì‹œ [Instructor ìƒì„±ì](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/training/atepc_trainer.py#L96) ë¶€ë¶„ìœ¼ë¡œ ê°€ë©´â—ï¸

**Instructor \_\_init__**

96 ~ 136 line

```python
else:  # cache_pathê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
  self.train_examples = processor.get_train_examples(self.opt.dataset_file['train'], 'train')  # 1
  train_features = convert_examples_to_features(self.train_examples, self.opt.max_seq_len, self.tokenizer, self.opt)  # 2
  self.opt.label_list = sorted(list(self.opt.IOB_label_to_index.keys()))
  self.opt.num_labels = len(self.opt.label_list) + 1
  all_spc_input_ids = torch.tensor([f.input_ids_spc for f in train_features], dtype=torch.long)
  all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)
  all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)
  all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)
  all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)
  all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)
  all_polarities = torch.tensor([f.polarity for f in train_features], dtype=torch.long)
  lcf_cdm_vec = torch.tensor([f.lcf_cdm_vec for f in train_features], dtype=torch.float32)
  lcf_cdw_vec = torch.tensor([f.lcf_cdw_vec for f in train_features], dtype=torch.float32)

  self.train_data = TensorDataset(all_spc_input_ids, all_segment_ids, all_input_mask, all_label_ids,
                                  all_polarities, all_valid_ids, all_lmask_ids, lcf_cdm_vec, lcf_cdw_vec)

  if self.opt.dataset_file['test']:
    self.test_examples = processor.get_test_examples(self.opt.dataset_file['test'], 'test')
    test_features = convert_examples_to_features(self.test_examples, self.opt.max_seq_len,
                                                 self.tokenizer, self.opt)
    all_spc_input_ids = torch.tensor([f.input_ids_spc for f in test_features], dtype=torch.long)
    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)
    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)
    all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)
    all_polarities = torch.tensor([f.polarity for f in test_features], dtype=torch.long)
    all_valid_ids = torch.tensor([f.valid_ids for f in test_features], dtype=torch.long)
    all_lmask_ids = torch.tensor([f.label_mask for f in test_features], dtype=torch.long)
    lcf_cdm_vec = torch.tensor([f.lcf_cdm_vec for f in test_features], dtype=torch.float32)
    lcf_cdw_vec = torch.tensor([f.lcf_cdw_vec for f in test_features], dtype=torch.float32)
    self.test_data = TensorDataset(all_spc_input_ids, all_segment_ids, all_input_mask, all_label_ids, all_polarities, all_valid_ids, all_lmask_ids, lcf_cdm_vec, lcf_cdw_vec)

    if self.opt.cache_dataset and not os.path.exists(cache_path):
      print('Caching dataset... please remove cached dataset if change model or dataset')
      with open(cache_path, mode='wb') as f:
        if self.opt.dataset_file['test']:
          pickle.dump((self.train_data, self.test_data, self.opt), f)
        else:
          pickle.dump((self.train_data, self.opt), f)
```

ì´ì œ #1 ì½”ë“œì˜ train_examplesì— ì–´ë–¤ ê°’ë“¤ì´ ë“¤ì–´ìˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#2: ì•ì—ì„œ ë§Œë“  examplesë¥¼ ë‹¤ì‹œ ì¸ìë¡œ ë„˜ê²¨ featureë¡œ ë³€ê²½í•©ë‹ˆë‹¤.. ğŸ˜¢

<br>

**convert_examples_to_features**

convert_examples_to_features ë©”ì†Œë“œëŠ” [PyABSA/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L221)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

ì½”ë“œê°€ ê¸¸ì–´ì„œ ë¼ì¸ë³„ë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

221 ~ 224 line

```python
def convert_examples_to_features(examples, max_seq_len, tokenizer, opt=None):
    """Loads a data file into a list of `InputBatch`s."""

    configure_spacy_model(opt)
```

ì‹œì‘ë¶€í„° ë‹¤ë¥¸ í•¨ìˆ˜ í˜¸ì¶œ ..

<br>

configure_spacy_modelì€ [PyABSA/pyabsa/core/apc/dataset_utils/apc_utils.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/apc/dataset_utils/apc_utils.py#L351)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

**configure_spacy_model**

351 ~ 365 line

```python
def configure_spacy_model(opt):
    if not hasattr(opt, 'spacy_model'):  # 1
        opt.spacy_model = 'en_core_web_sm'
    global nlp  # nlpë¥¼ ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸
    try:
        nlp = spacy.load(opt.spacy_model)  # 2
    except:
        print('Can not load {} from spacy, try to download it in order to parse syntax tree:'.format(opt.spacy_model),
              termcolor.colored('\npython -m spacy download {}'.format(opt.spacy_model), 'green'))
        try:
            os.system('python -m spacy download {}'.format(opt.spacy_model))  # 3
            nlp = spacy.load(opt.spacy_model)
        except:
            raise RuntimeError('Download failed, you can download {} manually.'.format(opt.spacy_model))
    return nlp
```

#1: configuration(opt)ì— spacy_modelì´ ì •ì˜ë¼ ìˆì§€ ì•Šë‹¤ë©´ 'en_core_web_sm'ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. âœ”ï¸**í•œêµ­ì–´ ì ìš©ì‹œ ko_core_news_smë¡œ ë³€ê²½**

#2: space.load ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ í•´ë‹¹ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. ë§Œì•½ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ê³  os.system ë©”ì†Œë“œë¡œ ì‹œìŠ¤í…œ ëª…ë ¹ì–´ë¥¼ í˜¸ì¶œí•´ í•´ë‹¹ ëª¨ë¸ì„ ë‹¤ìš´ë°›ìŠµë‹ˆë‹¤.(#3)

ì´í›„ í•´ë‹¹ spacy ëª¨ë¸ì„ ë¦¬í„´í•©ë‹ˆë‹¤.

<br>

ë‹¤ì‹œ [convert_examples_to_features](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L226)ë¡œ ëŒì•„ì™€ì„œ â—ï¸

**convert_examples_to_features**

226 ~ 228 line

```python
bos_token = tokenizer.bos_token  # 1 (CLS)
eos_token = tokenizer.eos_token  # 2 (SEP)
label_map = {label: i for i, label in enumerate(sorted(list(Labels) + [tokenizer.bos_token, tokenizer.eos_token]), 1)}  # 3
opt.IOB_label_to_index = label_map  # 4
```

Begin of Sequence í† í°(#1), End of Sequence í† í°(#2)ì„ ì •ì˜í•˜ê³ ,  BOS í† í°ê³¼ EOS í† í° ê·¸ë¦¬ê³  Labelë“¤ì„ í•©ì³ {í† í°: ì¸ë±ìŠ¤} í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.(#3)

cf) Labelì€ IOB annotation tagë¥¼ ì €ì¥í•œ ì§‘í•©ìœ¼ë¡œ, B-ASP, I-ASP, Oê°€ ì €ì¥ë¼ ìˆìŠµë‹ˆë‹¤.

ì´ë¥¼ configurationì— IOB_label_to_indexë¡œ ì €ì¥í•©ë‹ˆë‹¤.(#4)

```python
'IOB_label_to_index': {'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}
```

ğŸ”¼ configurationì— ì €ì¥ëœ label_map

+BERTì— ëŒ€í•´ ì˜ ëª¨ë¥´ì‹œëŠ” ë¶„ì€ [BERT ì„¤ëª…í•˜ê¸°](https://hwiyong.tistory.com/392) í¬ìŠ¤íŒ…ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.

<br>

**convert_examples_to_features**

230 ~ 231 line

```python
features = []
polarities_set = set()
```

featuresì™€ polarities_setì„ ìƒì„±í•œ ë’¤,

232 ~ 255 line

```python
for (ex_index, example) in enumerate(tqdm.tqdm(examples, postfix='convert examples to features')):  # 1
  text_tokens = example.text_a[:]
  aspect_tokens = example.text_b[:]
  IOB_label = example.IOB_label
  aspect_label = example.aspect_label
  polarity = example.polarity
  if polarity != SENTIMENT_PADDING or int(polarity) != SENTIMENT_PADDING:  # bad case handle in Chinese atepc_datasets
    polarities_set.add(polarity)  # ignore samples without polarities
    tokens = []
    labels = []
    valid = []
    label_mask = []
    enum_tokens = [bos_token] + text_tokens + [eos_token] + aspect_tokens + [eos_token]
    IOB_label = [bos_token] + IOB_label + [eos_token] + aspect_label + [eos_token]

    aspect = ' '.join(example.text_b)
    try:
      text_left, _, text_right = [s.strip() for s in ' '.join(example.text_a).partition(aspect)]
      except:
        continue
        text_raw = text_left + ' ' + aspect + ' ' + text_right

        if validate_example(text_raw, aspect, polarity):
          continue
```

examplesë¥¼ ìˆœíšŒí•©ë‹ˆë‹¤.(#1) tqdmìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ë°”ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.

<br>

examplesì— ìˆëŠ” InputExample í´ë˜ìŠ¤ì˜ ë©¤ë²„ ë³€ìˆ˜ë¥¼ ìƒê¸°ì‹œì¼œë³´ê² ìŠµë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ê°€ ìˆë‹¤ê³  í•  ë•Œ,

```xml
Desserts B-ASP -999  <!-- 0 -->
are O -999  <!-- 1 -->
almost O -999  <!-- 2 -->
incredible O -999  <!-- 3 -->
: O -999  <!-- 4 -->
my O -999  <!-- 5 -->
personal O -999  <!-- 6 -->
favorite O -999  <!-- 7 -->
is O -999  <!-- 8 -->
their O -999  <!-- 9 -->
Tart B-ASP Positive  <!-- 10 -->
of I-ASP Positive  <!-- 11 -->
the I-ASP Positive  <!-- 12 -->
Day I-ASP Positive  <!-- 13 -->
```

<br>

InputExample í´ë˜ìŠ¤ëŠ” ë¼ì¸ ë³„ë¡œ ë‹¤ìŒ ë©¤ë²„ë³€ìˆ˜ë¥¼ ê°–ìŠµë‹ˆë‹¤.

```python
class InputExample(object):
    """A single training_tutorials/test example for simple sequence classification."""

    def __init__(self, guid, text_a, text_b=None, IOB_label=None, aspect_label=None, polarity=None):
        self.guid = guid  # train-0 : 'set-type(train) - index ë²ˆí˜¸'
        self.text_a = text_a  # ['Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Tart', 'of', 'the', 'Day'] : ë¬¸ì¥ ì „ì²´
        self.text_b = text_b  # ['Tart', 'of', 'the', 'Day'] : Aspect Term
        self.IOB_label = IOB_label  # ['B-ASP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP'] : IOB annotation íƒœê·¸
        self.aspect_label = aspect_label  # ['B-ASP', 'I-ASP', 'I-ASP', 'I-ASP'] : Aspect Termì˜ IOB annotation íƒœê·¸
        self.polarity = polarity  # 'Positive' : Aspect Termì˜ ê°ì„±
```

<br>

ë‹¤ì‹œ [convert_examples_to_features](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L232) ë©”ì†Œë“œë¡œ ëŒì•„ì™€ì„œâ—ï¸

**convert_examples_to_features**

232 ~ 255 line

```python
for (ex_index, example) in enumerate(tqdm.tqdm(examples, postfix='convert examples to features')):  # 1
        text_tokens = example.text_a[:]  # 2
        aspect_tokens = example.text_b[:]  # 3
        IOB_label = example.IOB_label  # 4
        aspect_label = example.aspect_label  # 5
        polarity = example.polarity  # 6
        if polarity != SENTIMENT_PADDING or int(polarity) != SENTIMENT_PADDING:  # bad case handle in Chinese atepc_datasets  # 7
            polarities_set.add(polarity)  # ignore samples without polarities
        tokens = []  # 8
        labels = []
        valid = []
        label_mask = []
        enum_tokens = [bos_token] + text_tokens + [eos_token] + aspect_tokens + [eos_token]  # 9
        IOB_label = [bos_token] + IOB_label + [eos_token] + aspect_label + [eos_token]  # 10

        aspect = ' '.join(example.text_b)  # 11
        try:
            text_left, _, text_right = [s.strip() for s in ' '.join(example.text_a).partition(aspect)]  # 12
        except:
            continue
        text_raw = text_left + ' ' + aspect + ' ' + text_right  # 13

        if validate_example(text_raw, aspect, polarity):  # 14
            continue
```

text_tokenì—ëŠ” ë¬¸ì¥ ì „ì²´ë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë†“ì€ text_aê°€ ë“¤ì–´ê°€ê³ (#2),  aspect_tokenì—ëŠ” aspect termì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ text_bê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤.(#3)

IOB_labelì—ëŠ” ë¬¸ì¥ ì „ì²´ì˜ IOB annotation íƒœê·¸ê°€(#4), aspect_labelì—ëŠ” aspect termì˜ IOB annotation íƒœê·¸ê°€ ì €ì¥ë©ë‹ˆë‹¤.(#5)

ë§ˆì§€ë§‰ìœ¼ë¡œ polarityëŠ” aspect termì˜ ê°ì„±ìœ¼ë¡œ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.(#6)

```python
text_tokens = ['Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Tart', 'of', 'the', 'Day']
aspect_tokens = ['Tart', 'of', 'the', 'Day']
IOB_label = ['B-ASP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP']
aspect_label = ['B-ASP', 'I-ASP', 'I-ASP', 'I-ASP']
polarity = 'Positive'
```

#7: polarityê°€ íŒ¨ë”©ê°’(-999)ê°€ ì•„ë‹Œ ê²½ìš° polarities_setì— polarityë¥¼ ë„£ìŠµë‹ˆë‹¤. ì´ ë•Œ, polarity_setì€ 231 lineì—ì„œ ë¹ˆ ì§‘í•©ìœ¼ë¡œ ì„ ì–¸í–ˆê¸° ë•Œë¬¸ì— ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•Šì•„, ê°ì„±ì´ ì¢…ë¥˜ë³„ë¡œ í•˜ë‚˜ì”©ë§Œ ë‹´ê¸°ê²Œ ë©ë‹ˆë‹¤.

#8: token, label, valid, label_maskë¥¼ ë‹´ê¸° ìœ„í•œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ ,

#9: enum_tokenì€ ë¬¸ì¥ì˜ ê°€ì¥ ì²«ë²ˆì§¸ì— ì‚½ì…í•˜ëŠ” CLS í† í°ê³¼ ë¬¸ì¥ì„ êµ¬ë³„í•  ë•Œ ì‚¬ìš©í•˜ëŠ” SEP í† í°ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•©ë‹ˆë‹¤.

```python
enum_tokens = ['CLS'] + text_tokens + ['SEP'] +aspect_tokens + ['SEP']
# ['CLS', 'Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Tart', 'of', 'the', 'Day', 'SEP', 'Tart', 'of', 'the', 'Day', 'SEP']
```

#10: IOB_labelë„ enum_tokensì™€ ë™ì¼í•˜ê²Œ CLS í† í°ê³¼ SEP í† ì„ ì‚¬ìš©í•´ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•©ë‹ˆë‹¤.

```python
IOB_label = ['CLS'] + IOB_label + ['SEP'] + aspect_label + ['SEP']  # 10
# ['CLS', 'B-ASP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP', 'SEP', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP', 'SEP']
```

<br>

```python
example.text_b  # ['Tart', 'of', 'the', 'Day']
```

#11: aspect termì„ join ë©”ì†Œë“œë¥¼ í†µí•´ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë§Œë“§ë‹ˆë‹¤. Tart of the Day

<br>

#12 ë¼ì¸ì„ í’€ì–´ì“°ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
temp = []

for s in ' '.join(example.text_a).partition(aspect):
    temp.append(s.strip())
```

['Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Tart', 'of', 'the', 'Day']ë¥¼ joinìœ¼ë¡œ í•©ì³ 

Desserts are almost incredible : my personal favorite is their Tart of the Dayë¡œ ë§Œë“§ë‹ˆë‹¤.

ì´í›„ partition ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ íŠœí”Œ í˜•íƒœë¡œ ('Desserts are almost incredible : my personal favorite is their ', 'Tart of the Day', '') ë¦¬í„´í•œ ë’¤, ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ['Desserts are almost incredible : my personal favorite is their', 'Tart of the Day', ''] ë§Œë“  ë’¤ text_left, _, text_rightë¡œ ë°›ì€ ê²ƒì…ë‹ˆë‹¤.

<br>

#13: ì´ë¥¼ ë‹¤ì‹œ í•©ì³ ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ë§Œë“§ë‹ˆë‹¤.

<br>

#14: ì´í›„ valid_example ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•´ exampleì´ ìœ íš¨í•œì§€ ê²€ì‚¬í•©ë‹ˆë‹¤.

valid_example ë©”ì†Œë“œëŠ” [PyABSA/pyabsa/utils/pyabsa_utils.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/utils/pyabsa_utils.py#L44)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

**valid_example**

44 ~ 65 line

```python
def validate_example(text: str, aspect: str, polarity: str):
    if len(text) < len(aspect):  # 1
        raise ValueError(colored('AspectLengthExceedTextError -> <aspect: {}> is longer than <text: {}>, <polarity: {}>'.format(aspect, text, polarity), 'red'))

    if aspect.strip().lower() not in text.strip().lower():  # 2
        raise ValueError(colored('AspectNotInTextError -> <aspect: {}> is not in <text: {}>>'.format(aspect, text), 'yellow'))

    warning = False

    if len(aspect.split(' ')) > 10:  # 3
        print(colored('AspectTooLongWarning -> <aspect: {}> is too long, <text: {}>, <polarity: {}>'.format(aspect, text, polarity), 'yellow'))
        warning = True

    if len(polarity.split(' ')) > 3:  # 4
        print(colored('LabelTooLongWarning -> <polarity: {}> is too long, <text: {}>, <aspect: {}>'.format(polarity, text, aspect), 'yellow'))
        warning = True

    if text.strip() == aspect.strip():  # 5
        print(colored('AspectEqualsTextWarning -> <aspect: {}> equals <text: {}>, <polarity: {}>'.format(aspect, text, polarity), 'yellow'))
        warning = True

    return warning
```

#1: aspect termì˜ ê¸¸ì´ê°€ í…ìŠ¤íŠ¸ ê¸¸ì´ë³´ë‹¤ í° ì§€ í™•ì¸í•©ë‹ˆë‹¤. aspect termì´ í…ìŠ¤íŠ¸ë³´ë‹¤ ê¸¸ ê²½ìš° ValueErrorë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.

#2: aspectê°€ í…ìŠ¤íŠ¸ ë‚´ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° ë§ˆì°¬ê°€ì§€ë¡œ ValueErrorë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.

#3: aspectê°€ 11ê°œ ì´ìƒì˜ ë‹¨ì–´ë¡œ êµ¬ì„±ë¼ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì´ ë•ŒëŠ” ì—ëŸ¬ëŠ” ì•„ë‹ˆê³  ê²½ê³  ë©”ì‹œì§€ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.

#4: polarityê°€ 4ê°œ ì´ìƒì˜ ë‹¨ì–´ë¡œ êµ¬ì„±ë¼ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì´ ë•Œë„ ê²½ê³  ë©”ì‹œì§€ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.

#5: ë¬¸ì¥ ì „ì²´ê°€ aspectë¡œ ë¼ë²¨ë§ë¼ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ë¬¸ì¥ ì „ì²´ê°€ aspectì¼ ê²½ìš° ê²½ê³  ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

#1ê³¼ #2ì˜ ê²½ìš°ì—ë§Œ warningì´ Falseì´ê¸° ë•Œë¬¸ì— ë‘ ì¼€ì´ìŠ¤ë§Œ ì œì™¸í•˜ë©´ ëª¨ë‘ ì´ì–´ì„œ ë™ì‘í•˜ê²Œ ë©ë‹ˆë‹¤.

<br>

ë‹¤ì‹œ [convert_examples_to_features](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L257) ë©”ì†Œë“œë¡œ ëŒì•„ì˜¤ê² ìŠµë‹ˆë‹¤â—ï¸

**convert_examples_to_features**

257 ~ 271 line

```python
prepared_inputs = prepare_input_for_atepc(opt, tokenizer, text_left, text_right, aspect)  # 1
lcf_cdm_vec = prepared_inputs['lcf_cdm_vec']
lcf_cdw_vec = prepared_inputs['lcf_cdw_vec']

for i, word in enumerate(enum_tokens):
  token = tokenizer.tokenize(word)
  tokens.extend(token)
  cur_iob = IOB_label[i]
  for m in range(len(token)):
    if m == 0:
      label_mask.append(1)
      labels.append(cur_iob)
      valid.append(1)
    else:
      valid.append(0)
```

#1: prepare_input_for_atepc ë©”ì†Œë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤ ..

<br>

prepare_input_for_atepc ë©”ì†Œë“œëŠ” [PyABSA/pyabsa/core/atepc/dataset_utils/atepc_utils.py](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/atepc_utils.py#L75)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

ì½”ë“œê°€ ê¸¸ì–´ì„œ ë¼ì¸ë³„ë¡œ ì²œì²œíˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

**prepare_input_for_atepc**

75 ~ 87 line

```python
def prepare_input_for_atepc(opt, tokenizer, text_left, text_right, aspect):
    if hasattr(opt, 'dynamic_truncate') and opt.dynamic_truncate:  # 1
        _max_seq_len = opt.max_seq_len - len(aspect.split())  # 2
        text_left = text_left.split(' ')  # 3
        text_right = text_right.split(' ')
        if _max_seq_len < len(text_left) + len(text_right):  # 4
            cut_len = len(text_left) + len(text_right) - _max_seq_len  # 5
            if len(text_left) > len(text_right):  # 6
                text_left = text_left[cut_len:]
            else:  # 7
                text_right = text_right[:len(text_right) - cut_len]
        text_left = ' '.join(text_left)  # 8
        text_right = ' '.join(text_right)=
```

#1: configurationì— dynamic_truncate ì†ì„±ì´ ì •ì˜ë¼ ìˆê³ , Trueì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤. (ë§¨ ìœ„ configurationì„ ë³´ë©´ Trueë¡œ ì„¤ì •ë¼ ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)

#2: configurationì— ì €ì¥ëœ max_seq_lenì—ì„œ aspect ë‹¨ì–´ ê°œìˆ˜ë¥¼ êµ¬í•œ ë’¤ ë¹¼ì¤ë‹ˆë‹¤.

#3: ì•ì„œ 249 lineì—ì„œ ìƒì„±í•œ text_left, text_rightë¥¼ ë‹¤ì‹œ ìŠ¤í”Œë¦¿í•©ë‹ˆë‹¤.

```python
text_left, _, text_right = [s.strip() for s in ' '.join(example.text_a).partition(aspect)]  # 249 line
```

ex) "Desserts are almost incredible : my personal favorite is their Tart of the Day"ì—ì„œ Tart of the Dayê°€ aspectì˜€ë‹¤ë©´

```python
text_left = 'Desserts are almost incredible : my personal favorite is their'
text_right = ''
```

ê°€ ë˜ê²Œ ë˜ëŠ”ë° ì´ë¥¼ ë‹¤ì‹œ splití•´ì„œ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìª¼ê°­ë‹ˆë‹¤.

```python
text_left = ['Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their']
text_right = ['']
```

<br>

#4: ì´í›„ \_max\_seq\_lenë³´ë‹¤ ê¸¸ì´ê°€ ê¸¸ë‹¤ë©´, ì–¼ë§Œí¼ ì˜ë¼ì•¼í•˜ëŠ”ì§€ë¥¼ cut_lenë¡œ ì €ì¥í•©ë‹ˆë‹¤.(#5)

#6: ë§Œì•½ aspect term ì™¼ìª½ì˜ í…ìŠ¤íŠ¸ê°€ ë” ê¸¸ë‹¤ë©´ text_leftì˜ ì•ìª½ ë¶€ë¶„ì„ ìë¥´ê³ , ì˜¤ë¥¸ìª½ í…ìŠ¤íŠ¸ê°€ ë” ê¸¸ë‹¤ë©´ text_rightì˜ ë’·ìª½ ë¶€ë¶„ì„ ìë¦…ë‹ˆë‹¤.(#7)

#8: ì´í›„ join ë©”ì†Œë“œë¡œ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

<br>

**prepare_input_for_atepc**

89 ~ 93 line

```python
bos_token = tokenizer.bos_token if tokenizer.bos_token else '[CLS]'  # 1
eos_token = tokenizer.eos_token if tokenizer.eos_token else '[SEP]'

text_raw = text_left + ' ' + aspect + ' ' + text_right  # 2
text_spc = bos_token + ' ' + text_raw + ' ' + eos_token + ' ' + aspect + ' ' + eos_token  # 3
```

#1: bos_tokenê³¼ eos_tokenì„ ì •ì˜í•œ ë’¤,

#2: ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë³µêµ¬í•©ë‹ˆë‹¤.

#3: ì•ì„œ ë´¤ë˜ê²ƒì²˜ëŸ¼ ë¬¸ì¥ì˜ ê°€ì¥ ì²«ë²ˆì§¸ì— ì‚½ì…í•˜ëŠ” CLS í† í°ê³¼ ë¬¸ì¥ì„ êµ¬ë³„í•  ë•Œ ì‚¬ìš©í•˜ëŠ” SEP í† í°ì„ ì‚¬ìš©í•˜ì—¬ text_spcë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•©ë‹ˆë‹¤.

**'[CLS] Desserts are almost incredible : my personal favorite is their Tart of the Day [SEP] Tart of the Day [SEP]'**

<br>

**prepare_input_for_atepc**

95 ~ 103 line

```python
text_bert_tokens = tokenizer.tokenize(text_spc)  # 1
text_raw_bert_tokens = tokenizer.tokenize(bos_token + ' ' + text_raw + ' ' + eos_token)  # 2
aspect_bert_tokens = tokenizer.tokenize(aspect)  # 3

text_bert_indices = tokenizer.convert_tokens_to_ids(text_bert_tokens)  # 4
text_raw_bert_indices = tokenizer.convert_tokens_to_ids(text_raw_bert_tokens)
aspect_bert_indices = tokenizer.convert_tokens_to_ids(aspect_bert_tokens)

aspect_begin = len(tokenizer.tokenize(bos_token + ' ' + text_left))  # 5
```

#1: ì•ì„œ ë§Œë“  text_spcë¥¼ bert tokenizerë¥¼ ì‚¬ìš©í•´ í† í¬ë‚˜ì´ì§•í•˜ê³ ,

```python
text_bert_tokens = ['[CLS]', 'Des', '##ser', '##ts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Ta', '##rt', 'of', 'the', 'Day', '[SEP]', 'Ta', '##rt', 'of', 'the', 'Day', '[SEP]']
```

#2: ì›ë³¸ í…ìŠ¤íŠ¸ì—ë„ bos_tokenê³¼ eos_tokenì„ ì¶”ê°€í•´ í† í¬ë‚˜ì´ì§• í•©ë‹ˆë‹¤.

```python
text_raw_bert_tokens = ['[CLS]', 'Des', '##ser', '##ts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Ta', '##rt', 'of', 'the', 'Day', '[SEP]']
```

#3: ë§ˆì°¬ê°€ì§€ë¡œ aspectë„ í† í¬ë‚˜ì´ì§• í•´ì¤ë‹ˆë‹¤.

```python
aspect_bert_tokens = ['Ta', '##rt', 'of', 'the', 'Day']
```

#4: ì´í›„ ì•ì„œ í† í¬ë‚˜ì´ì§•í•œ ê²ƒë“¤ì„ idë¡œ ë³€í™˜í•œ ë’¤, ì €ì¥í•©ë‹ˆë‹¤.

```python
text_bert_indices = [101, 14177, 6906, 2145, 1132, 1593, 10965, 131, 1139, 2357, 5095, 1110, 1147, 22515, 3740, 1104, 1103, 2295, 102, 22515, 3740, 1104, 1103, 2295, 102]

text_raw_bert_indices = [101, 14177, 6906, 2145, 1132, 1593, 10965, 131, 1139, 2357, 5095, 1110, 1147, 22515, 3740, 1104, 1103, 2295, 102]

aspect_bert_indices = [22515, 3740, 1104, 1103, 2295]
```

#5: aspect termì˜ ì‹œì‘ ìœ„ì¹˜ë¥¼ ê³„ì‚°í•´ aspect_beginìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

<br>

**prepare_input_for_atepc**

105 ~ 114 line

```python
if 'lcfs' in opt.model_name or opt.use_syntax_based_SRD:
  syntactical_dist, _ = get_syntax_distance(text_raw, aspect, tokenizer, opt)
else:
  syntactical_dist = None

  lcf_cdm_vec = get_lca_ids_and_cdm_vec(opt, text_bert_indices, aspect_bert_indices,
                                        aspect_begin, syntactical_dist)

  lcf_cdw_vec = get_cdw_vec(opt, text_bert_indices, aspect_bert_indices,
                            aspect_begin, syntactical_dist)
```

![image](https://user-images.githubusercontent.com/76269316/175448613-732b80ad-c038-4295-9d06-21791e19b045.png)

âœ… *ì´ ë¶€ë¶„ì€ ëª¨ë¸ì˜ Local Context Focus Layer(LCF)ì—ì„œ ì‚¬ìš©í•˜ëŠ” CDM/CDW ë²¡í„°ë¥¼ ê³„ì‚°í•˜ëŠ” ë¶€ë¶„ì¸ë°, ì¶”í›„ì— ì„¤ëª…í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.*

<br>

**prepare_input_for_atepc**

116 ~ 127 line

```python
    inputs = {
        'text_raw': text_raw,
        'text_spc': text_spc,
        'aspect': aspect,
        'text_bert_indices': text_bert_indices,
        'text_raw_bert_indices': text_raw_bert_indices,
        'aspect_bert_indices': aspect_bert_indices,
        'lcf_cdm_vec': lcf_cdm_vec,
        'lcf_cdw_vec': lcf_cdw_vec,
    }

    return inputs
```

```python
text_raw = 'Desserts are almost incredible : my personal favorite is their Tart of the Day'
test_spc = '[CLS] Desserts are almost incredible : my personal favorite is their Tart of the Day [SEP] Tart of the Day [SEP]'
aspect = 'Tart of the Day'
text_bert_indices = [101, 14177, 6906, 2145, 1132, 1593, 10965, 131, 1139, 2357, 5095, 1110, 1147, 22515, 3740, 1104, 1103, 2295, 102, 22515, 3740, 1104, 1103, 2295, 102]
text_raw_bert_indices = [101, 14177, 6906, 2145, 1132, 1593, 10965, 131, 1139, 2357, 5095, 1110, 1147, 22515, 3740, 1104, 1103, 2295, 102]
aspect_bert_indices = [22515, 3740, 1104, 1103, 2295]
lcf_cdm_vec = ?
lcf_cdw_vec = ?
```

ì´í›„ ê³„ì‚°í•œ ê°’ë“¤ì„ ë¬¶ì–´ inputsë¡œ ë§Œë“  ë’¤ ë¦¬í„´í•©ë‹ˆë‹¤.

<br>

ë‹¤ì‹œ [convert_examples_to_features](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L257) ë©”ì†Œë“œë¡œ ëŒì•„ì˜¤ê² ìŠµë‹ˆë‹¤â—ï¸

**convert_examples_to_features**

257 ~ 271 line

```python
prepared_inputs = prepare_input_for_atepc(opt, tokenizer, text_left, text_right, aspect)
lcf_cdm_vec = prepared_inputs['lcf_cdm_vec']  # 1
lcf_cdw_vec = prepared_inputs['lcf_cdw_vec']

for i, word in enumerate(enum_tokens):  # 2
  token = tokenizer.tokenize(word)  # 3
  tokens.extend(token)  # 4
  cur_iob = IOB_label[i]  # 5
  for m in range(len(token)):  # 6
    if m == 0:
      label_mask.append(1)
      labels.append(cur_iob)
      valid.append(1)
    else:
      valid.append(0)
```

#1: ë¦¬í„´ë°›ì€ inputsì—ì„œ cdm ë²¡í„°ì™€ cdw ë²¡í„°ë¥¼ êº¼ë‚¸ ë’¤, 

[convert_examples_to_features 244 line](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L244)ì— ì •ì˜í•œ enum_tokenì„ ìˆœíšŒí•©ë‹ˆë‹¤.(#2)

```pyhon
enum_tokens = [bos_token] + text_tokens + [eos_token] + aspect_tokens + [eos_token]
```

```python
enum_tokens = ['[CLS]', 'Desserts', 'are', 'almost', 'incredible', ':', 'my', 'personal', 'favorite', 'is', 'their', 'Tart', 'of', 'the', 'Day', '[SEP]', 'Tart', 'of', 'the', 'Day', '[SEP]']
```

#3:  ê° ë‹¨ì–´ë¥¼ í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”í•©ë‹ˆë‹¤. ì´í›„ tokensë¼ëŠ” ë¦¬ìŠ¤íŠ¸ì— extendí•©ë‹ˆë‹¤.(#4)

cf) [list append()ì™€ extend() ì°¨ì´ì ](https://m.blog.naver.com/wideeyed/221541104629): list.extend(iterable)ëŠ” ë¦¬ìŠ¤íŠ¸ ëì— **ê°€ì¥ ë°”ê¹¥ìª½ iterableì˜ ëª¨ë“  í•­ëª©ì„ ë„£ìŠµë‹ˆë‹¤.**

<br>

#6ì€ enum_tokensì˜ Dessertsê¹Œì§€ê°€ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.

+tokens, labels, valid, label_maskëŠ” [convert_examples_to_features 240 ~ 243 line](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L240)ì—ì„œ ìƒì„±í•œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

<br>

```python
IOB_label = ['B-ASP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ASP', 'I-ASP', 'I-ASP', 'I-ASP']
```

<br>

**1. i = 0, word=[CLS]**

```python
token = tokenizer.tokenize(word)  # token = '[CLS]'
tokens.extend(token)  # tokens = ['[CLS]']
cur_iob = IOB_label[i]  # cur_iob = 'B-ASP'
for m in range(len(token)):  # for m in range(1):
  if m == 0:
    label_mask.append(1)  # label_mask = [1]
    labels.append(cur_iob)  # labels = ['B-ASP']
    valid.append(1)  # valid = [1]
  else:
    valid.append(0)
```

for loopì˜ ê²½ìš° rangeê°€ 1ì´ê¸° ë•Œë¬¸ì— if m==0ë§Œ ì‹¤í–‰ë˜ê²Œ ë©ë‹ˆë‹¤.

<br>

**2. i = 1, word=Desserts**

```python
token = tokenizer.tokenize(word)  # token = 'Desserts'
tokens.extend(token)  # tokens = ['[CLS]', 'Des', '##ser', '##ts']
cur_iob = IOB_label[i]  # cur_iob = 'O'
for m in range(len(token)):  # for m in range(3):
  if m == 0:
    label_mask.append(1)
    labels.append(cur_iob)  # labels = ['B-ASP']
    valid.append(1)  # valid = [1]
  else:
    valid.append(0)
```

ì´ë²ˆì—ëŠ” rangeê°€ 3ì´ê¸° ë•Œë¬¸ì— 3ë²ˆ ì‹¤í–‰ë˜ê²Œ ë©ë‹ˆë‹¤.

1. m = 0ì¼ë•Œ
   ```python
   label_mask.append(1)  # label_mask = [1, 1]
   labels.append(cur_iob)  # labels = ['B-ASP']
   valid.append(1)  # valid = [1, 1]
   ```

2. m = 1ì¼ë•Œ
   ```python
   valid.append(0)  # valid = [1, 1, 0]
   ```

3. m = 2ì¼ë•Œ
   ```python
   valid.append(0)  # valid = [1, 1, 0, 0]
   ```

<br>

convert_exampls_to_features ë©”ì†Œë“œ 257 ~ 271 lineì„ ì‹¤í–‰í•˜ë©´ ë¬¸ì¥ë³„ë¡œ label_maskì—ëŠ” ë¬¸ì¥ì„ êµ¬ì„±í•˜ëŠ” ë‹¨ì–´ì˜ ê°œìˆ˜ë§Œí¼ 1ë¡œ ê°€ë“ì°¬ ë¦¬ìŠ¤íŠ¸ê°€ ìƒì„±ë˜ê³ , labelsì—ëŠ” IOB_labelê³¼ ë™ì¼í•œ ê°’ì„ ê°–ëŠ” ë¦¬ìŠ¤íŠ¸ê°€, validì—ëŠ” í† í°í™”í•œ ë‹¨ì–´ì˜ ê°œìˆ˜ë§Œí¼ 1, 0ìœ¼ë¡œ êµ¬ì„±ëœ ë¦¬ìŠ¤íŠ¸ê°€ ìƒì„±ë˜ê²Œ ë©ë‹ˆë‹¤.

<br>

**convert_examples_to_features**

272 ~ 274 line

```python
tokens = tokens[0:min(len(tokens), max_seq_len - 2)]
labels = labels[0:min(len(labels), max_seq_len - 2)]
valid = valid[0:min(len(valid), max_seq_len - 2)]
```

ì´í›„ ìƒì„±í•œ tokens, labels, valid ë¦¬ìŠ¤íŠ¸ë¥¼ max_seq_lenê³¼ ë¹„êµí•˜ì—¬ max_seq_lenì„ ì´ˆê³¼í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ê·¸ëŒ€ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

<br>

**convert_examples_to_features**

278 ~ 283 line

```python
segment_ids = [0] * max_seq_len  # simply set segment_ids to all zeros
label_ids = []

for i, token in enumerate(tokens):
  if len(labels) > i:
    label_ids.append(label_map[labels[i]])
```

ë‹¤ìŒ ë¼ì¸ì—ì„œëŠ” segment_idsë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œ ë’¤, toeknsë¥¼ ìˆœíšŒí•˜ë©° iê°€ labelsì˜ ê¸¸ì´ë³´ë‹¤ ì»¤ì§€ë©´ label_idsì— label_map[labels[i]]ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

label_mapì€ [convert_examples_to_features 228 line](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L228)ì—ì„œ ì •ì˜í•œ dictionaryë¡œ,

```python
label_map = {'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}
```

ë‹¤ìŒ ê°’ì´ ì €ì¥ë¼ ìˆìŠµë‹ˆë‹¤.

<br>

**convert_examples_to_features**

285 ~ 287 line

```python
input_ids_spc = tokenizer.convert_tokens_to_ids(tokens)  # 1
input_mask = [1] * len(input_ids_spc)  # 2
label_mask = [1] * len(label_ids)
```

#1: ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”í•œ tokens ë¦¬ìŠ¤íŠ¸ë¥¼ id ê°’ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.

#2: input_maskëŠ” input_ids_spc ê¸¸ì´ë§Œí¼ 1ì„, label_maskì—ëŠ” label_idsì˜ ê¸¸ì´ë§Œí¼ 1ì„ ì±„ìš´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“§ë‹ˆë‹¤.

<br>

**convert_examples_to_features**

288 ~ 303 line

```python
while len(input_ids_spc) < max_seq_len:  # 1
  input_ids_spc.append(0)
  input_mask.append(0)
  label_ids.append(0)
  label_mask.append(0)
  while len(valid) < max_seq_len:  # 2
    valid.append(1)
while len(label_ids) < max_seq_len:  # 3
  label_ids.append(0)
  label_mask.append(0)
assert len(input_ids_spc) == max_seq_len  # 4
assert len(input_mask) == max_seq_len
assert len(segment_ids) == max_seq_len
assert len(label_ids) == max_seq_len
assert len(valid) == max_seq_len
assert len(label_mask) == max_seq_len
```

#1: ë§Œì•½ input_ids_spcê°€ max_seq_len ê¸¸ì´ë³´ë‹¤ ì‘ë‹¤ë©´ ê¸¸ì´ë§Œí¼ 0ì„ ì¶”ê°€í•´ì¤ë‹ˆë‹¤. (#2: validì—ëŠ” 1 ì¶”ê°€)

#3: label_idsë„ ë§ˆì°¬ê°€ì§€ë¡œ ê¸¸ì´ê°€ ì§§ë‹¤ë©´ 0ì„ ì¶”ê°€í•´ì„œ ë§ì¶°ì¤ë‹ˆë‹¤.

#4: ëª¨ë“  ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´ê°€ max_seq_lenì™€ ë™ì¼í•œì§€ í™•ì¸í•©ë‹ˆë‹¤.

<br>

**convert_examples_to_features**

305 ~ 316 line

```python
features.append(
  InputFeatures(input_ids_spc=input_ids_spc,
                input_mask=input_mask,
                segment_ids=segment_ids,
                label_id=label_ids,
                polarity=polarity,
                valid_ids=valid,
                label_mask=label_mask,
                tokens=example.text_a,
                lcf_cdm_vec=lcf_cdm_vec,
                lcf_cdw_vec=lcf_cdw_vec)
)
```

ì´í›„ í•´ë‹¹ ê°’ë“¤ì„ [InputFeatres í´ë˜ìŠ¤](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L41) ì¸ìŠ¤í„´ìŠ¤ë¡œ ìƒì„±í•œ ë’¤, features ë¦¬ìŠ¤íŠ¸ì— append í•´ì¤ë‹ˆë‹¤.

[ëª¨ë“  examples](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/dataset_utils/data_utils_for_training.py#L232)ì— ëŒ€í•´ ìœ„ ê³¼ì •ì„ ë°˜ë³µí•œ ë’¤, 

317 ~ 321 line

```python
check_and_fix_labels(polarities_set, 'polarity', features, opt)
check_and_fix_IOB_labels(label_map, opt)
opt.polarities_dim = len(polarities_set)

return features
```

polarities_setê³¼ label_mapì„ í™•ì¸í•œ ë’¤, polarities_dimì„ ì €ì¥í•˜ê³  features ë¦¬ìŠ¤íŠ¸ë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.

<br>

ë‹¤ì‹œ [Instructor ìƒì„±ì](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/training/atepc_trainer.py#L98)ë¡œ ëŒì•„ì˜¤ê² ìŠµë‹ˆë‹¤â—ï¸

**Instructor \_\_init__**

96 ~ 112 line

```python
else:
  self.train_examples = processor.get_train_examples(self.opt.dataset_file['train'], 'train')  # 1
  train_features = convert_examples_to_features(self.train_examples, self.opt.max_seq_len, self.tokenizer, self.opt)  # 2
  self.opt.label_list = sorted(list(self.opt.IOB_label_to_index.keys()))  # 3
  self.opt.num_labels = len(self.opt.label_list) + 1  # 4
  all_spc_input_ids = torch.tensor([f.input_ids_spc for f in train_features], dtype=torch.long)  # 5
  all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)
  all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)
  all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)
  all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)
  all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)
  all_polarities = torch.tensor([f.polarity for f in train_features], dtype=torch.long)
  lcf_cdm_vec = torch.tensor([f.lcf_cdm_vec for f in train_features], dtype=torch.float32)
  lcf_cdw_vec = torch.tensor([f.lcf_cdw_vec for f in train_features], dtype=torch.float32)
  
  self.train_data = TensorDataset(all_spc_input_ids, all_segment_ids, all_input_mask, all_label_ids, all_polarities, all_valid_ids, all_lmask_ids, lcf_cdm_vec, lcf_cdw_vec)  # 6
```

ë°©ê¸ˆê¹Œì§€ê°€ #2ì˜ ë‚´ìš©ì´ê³ , ì´í›„ #3ì—ì„œëŠ” IOB_label_to_indexì˜ key ê°’(B-ASP, I-ASP, O, CLS, SEP)ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“  ë’¤ ì •ë ¬í•´

```python
IOB_label_to_index: {'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}
```

label_listë¡œ ì €ì¥í•©ë‹ˆë‹¤.

```python
'label_list': ['B-ASP', 'I-ASP', 'O', '[CLS]'
```

<br>

#4: label_listì˜ ê¸¸ì´ + 1ì„ num_labelsë¡œ ì €ì¥í•˜ê³ , convert_examples_to_featuresì—ì„œ ìƒì„±í•œ InputFeatres ì¸ìŠ¤í„´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° tensor íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.(#5)

#6: ì´í›„ ì´ë¥¼ TensorDatasetìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

<br>

**Instructor \_\_init__**

114 ~ 128 line

```python
if self.opt.dataset_file['test']:
  self.test_examples = processor.get_test_examples(self.opt.dataset_file['test'], 'test')
  test_features = convert_examples_to_features(self.test_examples, self.opt.max_seq_len,
                                               self.tokenizer, self.opt)
  all_spc_input_ids = torch.tensor([f.input_ids_spc for f in test_features], dtype=torch.long)
  all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)
  all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)
  all_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)
  all_polarities = torch.tensor([f.polarity for f in test_features], dtype=torch.long)
  all_valid_ids = torch.tensor([f.valid_ids for f in test_features], dtype=torch.long)
  all_lmask_ids = torch.tensor([f.label_mask for f in test_features], dtype=torch.long)
  lcf_cdm_vec = torch.tensor([f.lcf_cdm_vec for f in test_features], dtype=torch.float32)
  lcf_cdw_vec = torch.tensor([f.lcf_cdw_vec for f in test_features], dtype=torch.float32)
  self.test_data = TensorDataset(all_spc_input_ids, all_segment_ids, all_input_mask, all_label_ids, all_polarities, all_valid_ids, all_lmask_ids, lcf_cdm_vec, lcf_cdw_vec)
```

ë§Œì•½ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì´ ì¡´ì¬í•œë‹¤ë©´ í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼í•˜ê²Œ convert_examples_to_features ë©”ì†Œë“œë¡œ InputFeatres ì¸ìŠ¤í„´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œ ë’¤ TensorDatasetìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

<br>

**Instructor \_\_init__**

130 ~ 136 line

```python
if self.opt.cache_dataset and not os.path.exists(cache_path):
  print('Caching dataset... please remove cached dataset if change model or dataset')
  with open(cache_path, mode='wb') as f:
    if self.opt.dataset_file['test']:
      pickle.dump((self.train_data, self.test_data, self.opt), f)
    else:
      pickle.dump((self.train_data, self.opt), f)
```

ë‹¤ìŒ ë¼ì¸ì—ì„œëŠ” opt.cache_datasetê³¼ cache_pathê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•œ ë’¤, (opt.cache_datasetì€ ì¡´ì¬í•˜ëŠ”ë° cache_pathê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° ì‹¤í–‰)

í›ˆë ¨ ë°ì´í„°ì…‹ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹(í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì€ ì¡´ì¬í•  ë•Œë§Œ, ìë£Œí˜•ì€ TensorDataset) pickle dumpë¡œ ì €ì¥í•©ë‹ˆë‹¤.

<br>

**Instructor \_\_init__**

139 ~ 145 line

```python
for key in opt.args:  # 1
  if key not in self.opt.args:
    self.opt.args[key] = opt.args[key]
    self.opt.args_call_count[key] = opt.args_call_count[key]

bert_base_model.config.num_labels = self.opt.num_labels  # 2
self.opt.label_list = opt.label_list  # 3
```

#1: configuration(opt.args)ì—ëŠ” ì €ì¥ë¼ ìˆëŠ”ë°, Instructor ë©¤ë²„ ë³€ìˆ˜ optì—ëŠ” ì €ì¥ë¼ ìˆì§€ ì•Šì€ ê²½ìš°, key-valueì™€ call_countë¥¼ ì €ì¥í•´ì£¼ê³ ,

#2: [Instructor \_\_init__ 72 line](https://github.com/yangheng95/PyABSA/blob/0eeb4788269a498d34c2aff942e03af78026617e/pyabsa/core/atepc/training/atepc_trainer.py#L72)ì—ì„œ AutoModel.from_pretrainedë¡œ ìƒì„±í•œ bert_base_modelì— label ê°œìˆ˜ë¥¼ ì§€ì •í•´ì¤ë‹ˆë‹¤.

```python
bert_base_model = AutoModel.from_pretrained(self.opt.pretrained_bert)
```

#3: ë©¤ë²„ ë³€ìˆ˜ optì—ë„ label_listë¥¼ ì €ì¥í•´ì¤ë‹ˆë‹¤.

<br>

**Instructor \_\_init__**

147 ~ 154 line

```python
self.num_train_optimization_steps = int(len(self.train_data) / self.opt.batch_size / self.opt.gradient_accumulation_steps) * self.opt.num_epoch  # 1

train_sampler = RandomSampler(self.train_data)  # 2
self.train_dataloader = DataLoader(self.train_data, sampler=train_sampler, pin_memory=True, batch_size=self.opt.batch_size)  # 3
if self.opt.dataset_file['test']:  # 4
  test_sampler = SequentialSampler(self.test_data)
  self.test_dataloader = DataLoader(self.test_data, sampler=test_sampler, pin_memory=True, batch_size=self.opt.batch_size)  # 5
```

#1: í›ˆë ¨ì‹œ ì‚¬ìš©í•  optimize stepì„ ì§€ì •í•´ì£¼ê³ ,

#2: ë°ì´í„° ë¡œë“œì‹œ ì…”í”Œì„ ìœ„í•´ RandomSamplerë¥¼ ìƒì„±, í›ˆë ¨ìš© ë°ì´í„°ë¡œë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.(#3)

#4: í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì¡´ì¬í•  ê²½ìš° í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë”ë„ ìƒì„±í•˜ê³ , ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì§€ì •í•©ë‹ˆë‹¤.((#5))

<br>

**Instructor \_\_init__**

156 ~ 165 line

```python
self.model = self.opt.model(bert_base_model, opt=self.opt)  # 1

param_optimizer = list(self.model.named_parameters())  # 2
no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']  # 3
self.optimizer_grouped_parameters = [  # 4
  {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
   'weight_decay': self.opt.l2reg},
  {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
   'weight_decay': 0}
]
```

#1: ì‹¤ì œ ëª¨ë¸ì„ ì •ì˜í•˜ê³ 

#2: ëª¨ë¸ì´ ê°–ê³  ìˆëŠ” íŒŒë¼ë¯¸í„°ì˜ ì´ë¦„ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ param_optimizerë¡œ ìƒì„±í•©ë‹ˆë‹¤.

#3: weight decayëŠ” weightë“¤ì˜ ê°’ì´ ì¦ê°€í•˜ëŠ” ê²ƒì„ ì œí•œí•¨ìœ¼ë¡œì¨, ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ê°ì†Œì‹œí‚´ìœ¼ë¡œì¨ ì œí•œí•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ ì´ë¥¼ ì ìš©í•˜ì§€ ì•Šì„ íŒŒë¼ë¯¸í„°ë¥¼ no_decayë¡œ ì •ì˜í•´ë†“ì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤. weight decayì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ ë§í¬ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.

cf) [Regularization (Weight Decay)](https://deepapple.tistory.com/6)

#4: íŒŒë¼ë¯¸í„°ë¥¼ ìˆœíšŒí•˜ë©° weight_decayë¥¼ ì ìš©í•˜ì§€ ì•Šì„ íŒŒë¼ë¯¸í„°ì™€ ì ìš©í•  íŒŒë¼ë¯¸í„°ë¥¼ dictionaryë¡œ ë‚˜ëˆ  optimizer_grouped_parametersë¡œ ì €ì¥í•©ë‹ˆë‹¤.

<br>

**Instructor \_\_init__**

167 ~ 171 line

```python
if self.opt.auto_device == 'allcuda':
  self.model.to(self.opt.device)
  self.model = torch.nn.parallel.DataParallel(self.model)
else:
  self.model.to(self.opt.device)
```

configurationì˜ auto_deviceì— ë”°ë¼ modelì„ deviceë¡œ ë¡œë“œí•©ë‹ˆë‹¤.

<br>

**Instructor \_\_init__**

177 ~ 185 line

```python
if amp:  # 1
  self.model, self.optimizer = amp.initialize(self.model, self.optimizer, opt_level="O1")

self.opt.device = torch.device(self.opt.device)  # 2
if self.opt.device.type == 'cuda':
  self.logger.info("cuda memory allocated:{}".format(torch.cuda.memory_allocated(device=self.opt.device)))

print_args(self.opt, self.logger)  # 3
```

#1: amp(automatic mixed precision)ëŠ” í´ë˜ìŠ¤ ìœ„ì— 31 ~ 37 lineì—ì„œ importí•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, í•™ìŠµ ì†ë„ë¥¼ ì¦ê°€ì‹œì¼œì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. 

cf) [Ampì— ëŒ€í•´ ì•Œì•„ë³´ì (Automatic Mixed Precision)](https://cvml.tistory.com/8)

```python
try:
    import apex.amp as amp

    # assert torch.version.__version__ < '1.10.0'
    print('Use FP16 via Apex!')
except Exception:
    amp = None
```

ì •ìƒì ìœ¼ë¡œ import ëë‹¤ë©´ #1 ë¼ì¸ì´ ì‹¤í–‰ë¼ amp.initializeê°€ ì‹¤í–‰ë˜ê²Œ ë©ë‹ˆë‹¤.

<br>

#2: torch.deviceë¡œ ì¥ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ë§Œì•½ GPUë¥¼ ì‚¬ìš©í•œë‹¤ë©´ ë‹¤ìŒ ì¡°ê±´ë¬¸ì„ í†µí•´ ë¡œê·¸ë¥¼ ë‚¨ê¹ë‹ˆë‹¤.

#3: print_args ë©”ì†Œë“œë¥¼ í†µí•´ configurationê³¼ call countë¥¼ ë¡œê·¸ë¡œ ë‚¨ê¹ë‹ˆë‹¤.

<br>

[PyABSA/pyabsa/utils/pyabsa_utils.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/utils/pyabsa_utils.py#L35)

```python
def print_args(config, logger=None, mode=0):
    args = [key for key in sorted(config.args.keys())]
    for arg in args:
        if logger:  # loggerì— INFOë¡œ ì¶”ê°€
            logger.info('{0}:{1}\t-->\tCalling Count:{2}'.format(arg, config.args[arg], config.args_call_count[arg]))
        else:  # í”„ë¡¬í”„íŠ¸ì— ì¶œë ¥
            print('{0}:{1}\t-->\tCalling Count:{2}'.format(arg, config.args[arg], config.args_call_count[arg]))
```

![ìŠ¤í¬ë¦°ìƒ· 2022-06-26 ì˜¤í›„ 3 05 58](https://user-images.githubusercontent.com/76269316/175801751-a86561ea-80c9-4ec7-9884-9b832e13c197.png)

![ìŠ¤í¬ë¦°ìƒ· 2022-06-26 ì˜¤í›„ 3 06 45](https://user-images.githubusercontent.com/76269316/175801784-49b5b479-6797-4402-bb4d-5db9f03e888a.png)

ì—¬ê¸°ê¹Œì§€ê°€ Instructor í´ë˜ìŠ¤ì˜ ìƒì„±ì ë©”ì†Œë“œì…ë‹ˆë‹¤.

<br>

ë‹¤ì‹œ train4atepc ë©”ì†Œë“œë¡œ ëŒì•„ì˜¤â—ï¸

**[train4atepc](https://github.com/yangheng95/PyABSA/blob/4320db7dd0cfa0053a48169e59ff44403482eaf1/pyabsa/core/atepc/training/atepc_trainer.py#L460)**

460 ~ 465 line

```python
@retry
def train4atepc(opt, from_checkpoint_path, logger):
    # in case of handling ConnectionError exception
    trainer = Instructor(opt, logger)  # 1
    resume_from_checkpoint(trainer, from_checkpoint_path)  # 2

    return trainer.run()  # 3
```

#1ì—ì„œ Instructorë¥¼ ìƒì„±í•˜ê³ , #3ì—ì„œ run ë©”ì†Œë“œë¥¼ ì‹¤í–‰í•œ ë’¤ ë¦¬í„´í•©ë‹ˆë‹¤.

ì´ ë¶€ë¶„ì€ apcì—ì„œ ì •ë¦¬í•œ [\_train\_and\_evaluate](https://seominseok4834.github.io/capstone/12.pyabsa-instructor-analysis-(apc)/#_train_and_evaluate) ë©”ì†Œë“œì™€ ë¹„ìŠ·í•˜ê¸° ë•Œë¬¸ì— ì¶”í›„ì— ì •ë¦¬í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤ .. <img width="130" alt="ìŠ¤í¬ë¦°ìƒ· 2022-06-26 ì˜¤í›„ 4 15 10" src="https://user-images.githubusercontent.com/76269316/175803675-6a2cc761-1fc2-4986-b63c-55a64799cd42.png">
