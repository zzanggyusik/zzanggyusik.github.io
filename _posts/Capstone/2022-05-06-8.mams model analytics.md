---
title:  "[Capstone] MAMS for ABSA 모델 분석"
excerpt: "MAMS for ABSA 코드 리뷰"
toc: true
toc_label: "Capstone"
toc_sticky: true
published: true

categories:
  - Capstone
tags:
  - PyTorch
last_modified_at: 2022-05-06

---

> [이전에 리뷰한 논문](https://seominseok4834.github.io/paper%20review/1.a-challenge-dataset-and-effective-models-for-aspect-based-sentiment-analysis/)에서 구현한 모델을 캡스톤 디자인 프로젝트에 적용하기 위해 모델 부분 코드를 분석한 내용입니다.
>
> 모든 코드는 [MAMS-for-ABSA github](https://github.com/siat-nlp/MAMS-for-ABSA)에서 가져왔습니다.

<br>

### 1. 모델 생성

##### 모델 생성자 함수

```python
class CapsuleNetwork(nn.Module):

    def __init__(self, embedding, hidden_size, capsule_size, dropout, num_categories):
        super(CapsuleNetwork, self).__init__()
        self.embedding = embedding
        embed_size = embedding.embedding_dim
        self.capsule_size = capsule_size
        self.aspect_transform = nn.Sequential(
            nn.Linear(embed_size, capsule_size),
            nn.Dropout(dropout)
        )
        self.sentence_transform = nn.Sequential(
            nn.Linear(hidden_size, capsule_size),
            nn.Dropout(dropout)
        )
        self.norm_attention = BilinearAttention(capsule_size, capsule_size)
        self.guide_capsule = nn.Parameter(
            torch.Tensor(num_categories, capsule_size)
        )
        self.guide_weight = nn.Parameter(
            torch.Tensor(capsule_size, capsule_size)
        )
        self.scale = nn.Parameter(torch.tensor(4.0))
        self.capsule_projection = nn.Linear(capsule_size, capsule_size * num_categories)
        self.dropout = dropout
        self.num_categories = num_categories
        self._reset_parameters()
```

[/src/aspect_term_model/capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/capsnet.py)

위 생성자 함수는 /train/train.py에서 make_model()이라는 메소드를 통해 호출되게 됩니다. (make_mode -> make_recurrent_capsule_network를 통해 최종 호출됨)

make_recurrent_capsule_network 메소드를 보면,

```python
def make_recurrent_capsule_network(config):
    embedding = make_embedding(config)
    base_path = os.path.join(config['base_path'])  # ./data/MAMS-ATSA
    log_path = os.path.join(base_path, 'log/log.yml')
    log = yaml.safe_load(open(log_path))
    config = config['aspect_category_model'][config['aspect_category_model']['type']]  # config['aspect_term_model']
    aspect_embedding = nn.Embedding(num_embeddings=8, embedding_dim=config['embed_size'])
    model = RecurrentCapsuleNetwork(
        embedding=embedding,
        aspect_embedding=aspect_embedding,
        num_layers=config['num_layers'],  # 2
        bidirectional=config['bidirectional'],  # True
        capsule_size=config['capsule_size'],  # 300
        dropout=config['dropout'],  # 0.5
        num_categories=log['num_categories']  # 3
    )
    model.load_sentiment(os.path.join(base_path, 'processed/sentiment_matrix.npy'))
    return model
```

[/train/make_aspect_category_model.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/make_aspect_category_model.py)

##### make_embedding()

첫번째로 호출되는 make_embedding은 train 데이터에 포함된 단어들을 glove.840B.300d에 있는 단어 벡터로 변환한 파일(glove.npy)을 torch.nn.modules.sparse.Embedding로 로드합니다.

<br>

##### [nn.Embedding()](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)

임베딩 사전의 크기가 8이고, 각 임베딩 벡터의 크기가 300인 룩업 테이블을 생성합니다.

초기화는 밑에 load_sentiment() 메소드를 통해 이루어집니다.

<br>

이후 configuration 파일([/config.yml](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/config.ymll)과 log 파일에서 layer 개수, bidirectional인지, capsule 크기, dropout 등을 가져와 모델을 생성합니다.

<br>

##### load_sentiment()

load_sentiment는 모델 내 구현된 함수로 다음과 같이 구현돼 있습니다.

```python
def load_sentiment(self, path):
  sentiment = np.load(path)
  e1 = np.mean(sentiment)
  d1 = np.std(sentiment)
  e2 = 0
  d2 = np.sqrt(2.0 / (sentiment.shape[0] + sentiment.shape[1]))
  sentiment = (sentiment - e1) / d1 * d2 + e2
  self.guide_capsule.data.copy_(torch.tensor(sentiment))
```

[/src/aspect_term_model/capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/capsnet.py)

전처리 과정([/data_process/data_process.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/data_process.py)에서 생성한 sentiment_matrix.npy 파일을 로드해 torch.Tensor형으로 바꾼뒤 nn.Parameter의 데이터로 맵핑합니다.

<br>

```python
def __init__(self, embedding, hidden_size, capsule_size, dropout, num_categories):
  super(CapsuleNetwork, self).__init__()
  							···
  self.guide_weight = nn.Parameter(  # nn.Parameter의 data로 sentiment_matrix를 copy
    torch.Tensor(capsule_size, capsule_size)
  )
```

<br>

➕ 이 때 sentiment_matrix에 평균을 빼고 표준편차로 나눠서 정규화를 해주는데 처음에 sentiment_matrix를 저장할때도 정규화를 해주는데 왜 로드해서도 다시 정규화를 거치는지 잘 모르겠습니다 ..

```python
def load_sentiment_matrix(glove_path, sentiment_path):
    sentiment_matrix = np.zeros((3, 300), dtype=np.float32)
    sd = json.load(open(sentiment_path, 'r', encoding='utf-8'))
    sd['positive'] = set(sd['positive'])
    sd['negative'] = set(sd['negative'])
    sd['neutral'] = set(sd['neutral'])
    with open(glove_path, 'r', encoding='utf-8') as f:
        for line in f:
            content = line.split(' ')
            word = content[0]
            vec = np.array(list(map(float, content[1:])))
            if word in sd['positive']:
                sentiment_matrix[0] += vec
            elif word in sd['negative']:
                sentiment_matrix[1] += vec
            elif word in sd['neutral']:
                sentiment_matrix[2] += vec
    sentiment_matrix -= sentiment_matrix.mean()
    sentiment_matrix = sentiment_matrix / sentiment_matrix.std() * np.sqrt(2.0 / (300.0 + 3.0))
    return sentiment_matrix
```

[/data_process/utils.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/utils.py)의 load_sentiment_matrix 함수를 통해 sentiment_matrix를 생성하고 저장

<br><br>

```python
class CapsuleNetwork(nn.Module):

    def __init__(self, embedding, hidden_size, capsule_size, dropout, num_categories):
        super(CapsuleNetwork, self).__init__()
        self.embedding = embedding
        embed_size = embedding.embedding_dim
        self.capsule_size = capsule_size
        self.aspect_transform = nn.Sequential(
            nn.Linear(embed_size, capsule_size),
            nn.Dropout(dropout)
        )
        self.sentence_transform = nn.Sequential(
            nn.Linear(hidden_size, capsule_size),
            nn.Dropout(dropout)
        )
        self.norm_attention = BilinearAttention(capsule_size, capsule_size)
        self.guide_capsule = nn.Parameter(
            torch.Tensor(num_categories, capsule_size)
        )
        self.guide_weight = nn.Parameter(
            torch.Tensor(capsule_size, capsule_size)
        )
        self.scale = nn.Parameter(torch.tensor(4.0))
        self.capsule_projection = nn.Linear(capsule_size, capsule_size * num_categories)
        self.dropout = dropout
        self.num_categories = num_categories
        self._reset_parameters()
```

[/src/aspect_term_model/capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/capsnet.py)

모델 생성시 넘겨지는 파라미터 값을 다시 한 번 정리하면 <class 'torch.nn.modules.sparse.Embedding'> type의 glove 임베딩이 들어있고 (emed_size는 300, pretrained된 glove 단어 임베딩이 300차원으로 돼 있으므로) capsue_size=2, dropout=0.5, num_categories=3 입니다.

저희는 CapsuleNetwork를 상속받은 RecurrentCapsuleNetwork를 사용하기 때문에 hidden_size가 다릅니다.

[/src/aspect_term_model/recurrent_capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/recurrent_capsnet.py)

8 ~ 15 line

```python
    def __init__(self, embedding, num_layers, bidirectional, capsule_size, dropout, num_categories):
        super(RecurrentCapsuleNetwork, self).__init__(
            embedding=embedding,
            hidden_size=embedding.embedding_dim * (2 if bidirectional else 1),
            capsule_size=capsule_size,
            dropout=dropout,
            num_categories=num_categories
        )
```

embed_size = 300, hidden_size = 600, capsule_size = 300, dropout = 0.5. num_categories = 3

<br>

### forward 함수

[/src/aspect_term_model/capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/recurrent_capsnet.py)

58 ~ 87 line

```python
def forward(self, sentence, aspect):
  # get lengths and masks
  sentence = sentence_clip(sentence)
  aspect = sentence_clip(aspect)
  sentence_mask = (sentence != PAD_INDEX)
  aspect_mask = (aspect != PAD_INDEX)
  # sentence_lens = sentence_mask.long().sum(dim=1, keepdim=True)
  aspect_lens = aspect_mask.long().sum(dim=1, keepdim=True)
  # embedding
  sentence = self.embedding(sentence)
  sentence = F.dropout(sentence, p=self.dropout, training=self.training)
  aspect = self.embedding(aspect)
  aspect = F.dropout(aspect, p=self.dropout, training=self.training)
  # aspect average pooling
  aspect = aspect.masked_fill(aspect_mask.unsqueeze(-1) == 0, 0)
  aspect = aspect.sum(dim=1, keepdim=False) / aspect_lens.float()
  # sentence encode layer
  sentence = self._sentence_encode(sentence, aspect)
  # primary capsule layer
  sentence = self.sentence_transform(sentence)
  primary_capsule = squash(sentence, dim=-1)
  # aspect capsule layer
  aspect = self.aspect_transform(aspect)
  aspect_capsule = squash(aspect, dim=-1)
  # aspect aware normalization
  norm_weight = self.norm_attention.get_attention_weights(aspect_capsule, primary_capsule, sentence_mask)
  # capsule guided routing
  category_capsule = self._capsule_guided_routing(primary_capsule, norm_weight)
  category_capsule_norm = torch.sqrt(torch.sum(category_capsule * category_capsule, dim=-1, keepdim=False))
  return category_capsule_norm
```

<br>

**capsnet.py**

60 ~ 61 line

```python
  sentence = sentence_clip(sentence)
  aspect = sentence_clip(aspect)
```

[src/module/utils/sentence_clip.py](https://github.com/siat-nlp/MAMS-for-ABSA/tree/master/src/module/utils)가 실행됩니다.

```python
from src.module.utils.constants import PAD_INDEX

def sentence_clip(sentence):
    mask = (sentence != PAD_INDEX)  # word2index로 변환한 context, aspect 값에서 PAD_INDEX가 아닌 부분을 TRUE로 변환
    sentence_lens = mask.long().sum(dim=1, keepdim=False)  # 행별로 True의 개수를 구함
    max_len = sentence_lens.max().item()  # 최대 True 개수
    return sentence[:, :max_len]
```

<br>

```python
mask = (sentence != PAD_INDEX)
```

먼저 이 부분이 실행되면, 이렇게 word2index로 변환된 context(left_part + 2 + right_part)를

<img width="675" alt="스크린샷 2022-05-03 오후 11 04 54" src="https://user-images.githubusercontent.com/76269316/166468291-02aa355d-8dcc-49fd-b0cf-5a9b6222cb6a.png">

PAD_INDEX(0)가 아닌 부분을 TRUE로 변환합니다.

<img width="654" alt="스크린샷 2022-05-03 오후 11 06 22" src="https://user-images.githubusercontent.com/76269316/166468548-145bced2-fb8c-4271-a9d4-ca6d59b3c524.png">

<br>

이후 TRUE인 부분을 모두 더하면 길이가 64인 1차원 텐서 자료형이 생성되는데, 이 값들은 context에 있는 단어 개수의 합(True 개수)입니다.

```python
sentence_lens = mask.long().sum(dim=1, keepdim=False)
```

<img width="669" alt="스크린샷 2022-05-03 오후 11 08 28" src="https://user-images.githubusercontent.com/76269316/166468949-2d1d46e2-a2f4-4ad4-a71b-c2e67141aee6.png">

그 다음 최대 True 개수, 단어가 가장 많이 나온 행만큼만 열을 사용하도록 슬라이싱 해서 리턴합니다.

```python
    max_len = sentence_lens.max().item()
    return sentence[:, :max_len]
```

<img width="33" alt="스크린샷 2022-05-03 오후 11 09 57" src="https://user-images.githubusercontent.com/76269316/166469230-bb25c57d-9b8c-44e6-8084-ec11c42accc6.png">

▶️ 원래 sentence는 64X76 (batch size X aspect max len)이였는데, [전처리 데이터 저장 부분 참고](https://seominseok4834.github.io/capstone/4.mams-preprocess-code-analytics/#%EC%A0%84%EC%B2%98%EB%A6%AC-%B%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%80%EC%9E%A5) 현재 배치 사이즈로 뽑아온 데이터 중 최대 개수를 구해 그만큼만 사용하겠다는 의미입니다.

원래 64X76이였는데 <img width="173" alt="스크린샷 2022-05-03 오후 11 12 46" src="https://user-images.githubusercontent.com/76269316/166469813-15de570d-80a7-47bc-bc11-ab76346dff81.png"> 64X74로 줄어듦 <img width="180" alt="스크린샷 2022-05-03 오후 11 13 15" src="https://user-images.githubusercontent.com/76269316/166469913-9a643bfb-4487-4e9f-a19f-4404ab7d5845.png">

<br>

aspect도 동일하게 사용하는 열만큼 사이즈를 줄여줇니다.

<br>

다시 모델의 forward 부분으로 돌아와서

**capsnet.py**

62 ~ 63 line

```python
        sentence_mask = (sentence != PAD_INDEX)
        aspect_mask = (aspect != PAD_INDEX)
```

리턴 받은 sentence, aspect에서 PAD_INDEX가 아닌 부분을 TRUE로 하는 마스크를 생성합니다.

<img width="663" alt="스크린샷 2022-05-03 오후 11 15 30" src="https://user-images.githubusercontent.com/76269316/166470348-d99f4aac-7fef-4cb3-8f93-fbf3b0a590d9.png">

<img width="652" alt="스크린샷 2022-05-03 오후 11 15 48" src="https://user-images.githubusercontent.com/76269316/166470401-487c1a82-79ca-4fe8-94fb-271e5fd7c496.png">

<br>

**capsnet.py**

65 line

앞서 sentence_clip에서 구한것처럼, 모든 aspect의 단어 개수를 구해 64X1로 만듧니다.

```python
aspect_lens = aspect_mask.long().sum(dim=1, keepdim=True)
```

<img width="118" alt="스크린샷 2022-05-03 오후 11 17 44" src="https://user-images.githubusercontent.com/76269316/166470774-10446057-7747-4ac2-949d-6166c3f1e7b6.png">

<br>

이전까지는 전처리 과정이였고, 이번 라인(67 line)부터는 논문의 모델 설명 부분과 함께 설명하도록 하겠습니다.

##### Embedding Layer

임베딩 레이어에서는 문장 S를 임베딩 E로 변환합니다.

**capsnet.py**

67 ~ 70 line

```python
        sentence = self.embedding(sentence)
        sentence = F.dropout(sentence, p=self.dropout, training=self.training)
        aspect = self.embedding(aspect)
        aspect = F.dropout(aspect, p=self.dropout, training=self.training)
```

<img width="600" alt="스크린샷 2022-05-03 오후 11 20 50" src="https://user-images.githubusercontent.com/76269316/166471323-0ba4f1d2-4f71-4ed7-94f9-ec9669bd5425.png">

sentence와 aspect에 embedding과 dropout을 적용하면 다음과 같이 preprocess 코드에서 생성했던 300차원의 단어 임베딩으로 변환됩니다. (sentence shape: 64X74X300, aspect shape: 64X23X300)

<br>

aspect의 경우 average pooling 과정을 거칩니다.

72 line

```python
        aspect = aspect.masked_fill(aspect_mask.unsqueeze(-1) == 0, 0)
```

<br>

```python
aspect_mask.unsqueeze(-1) == 0
```

앞서 62 ~ 63 line에서 만들었던 aspect_mask에

<img width="652" alt="스크린샷 2022-05-03 오후 11 15 48" src="https://user-images.githubusercontent.com/76269316/166470401-487c1a82-79ca-4fe8-94fb-271e5fd7c496.png">

차원 하나를 추가합니다. (64X49 -> 64X49X1)

이 때 0과 같으면 True, 같지 않으면 False로 값을 지정해줬기 때문에 다음과 같이 값이 반전돼서 들어가게 됩니다.

<img width="145" alt="스크린샷 2022-05-03 오후 11 33 29" src="https://user-images.githubusercontent.com/76269316/166473886-6b660a3e-5a19-456d-9506-5e41737eff27.png">

이 인덱스에서 True인 부분 즉, aspect가 없는 부분의 임베딩은 모두 0으로 채워지게 됩니다.

0번째 aspect의 실행결과를 보면, 0번 인덱스를 제외하고 모두 단어가 없는데, 0번째 단어는 임베딩 값이 정상적으로 들어가 있는 반면,

<img width="674" alt="스크린샷 2022-05-03 오후 11 41 18" src="https://user-images.githubusercontent.com/76269316/166475376-d3be8a16-866c-47ea-a284-e1b658caa81f.png">

첫번째 단어는 임베딩 값이 모두 0인걸 확인할 수 있습니다. (이후의 인덱스도 동일)

<img width="939" alt="스크린샷 2022-05-03 오후 11 43 43" src="https://user-images.githubusercontent.com/76269316/166475882-8cac49f8-adf8-4e1b-9848-4001391d4d2b.png">

<br>

**capsnet.py**

73 line

```python
        aspect = aspect.sum(dim=1, keepdim=False) / aspect_lens.float()
```

이후 해당 값들을 더한 뒤 각 행의 길이로 나눠 저장합니다.

<img width="181" alt="스크린샷 2022-05-03 오후 11 48 08" src="https://user-images.githubusercontent.com/76269316/166476790-07757a51-19f6-41ea-8eea-19c537baebfd.png">

<img width="580" alt="스크린샷 2022-05-03 오후 11 48 50" src="https://user-images.githubusercontent.com/76269316/166476928-f0ae0f06-8f74-4a85-8571-070d324f6342.png">

<br>

### Encoding Layer

sentence와 average pooling을 적용한 aspect를 conactenate해서 aspect aware sentence 임베딩 <img width="42" alt="스크린샷 2022-05-06 오후 3 00 54" src="https://user-images.githubusercontent.com/76269316/167075592-b42b3c1d-233d-436e-a300-34843eb26bf9.png">를 만듧니다.

이후 이 <img width="42" alt="스크린샷 2022-05-06 오후 3 00 54" src="https://user-images.githubusercontent.com/76269316/167075592-b42b3c1d-233d-436e-a300-34843eb26bf9.png"> 임베딩을 bidirectional GRU와 residual connection을 통해 contextualized representation H로 만듧니다.

이 과정은 아래 코드를 통해 구현돼 있습니다.

![스크린샷 2022-05-06 오후 3 03 36](https://user-images.githubusercontent.com/76269316/167075856-94c2e3f5-2422-453b-ad3d-48dca0de64ed.png)

**capsnet.py**

75 line

```python
        sentence = self._sentence_encode(sentence, aspect)
```

\_sentence\_encode 메소드는 부모 클래스가 아닌 자식 클래스(RecurrentCapsuleNetwork)에 구현돼 있습니다.

[/src/aspect_term_model/recurrent_capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/recurrent_capsnet.py)

```python
import torch
from torch import nn
import torch.nn.functional as F
from src.aspect_term_model.capsnet import CapsuleNetwork

class RecurrentCapsuleNetwork(CapsuleNetwork):

    def __init__(self, embedding, num_layers, bidirectional, capsule_size, dropout, num_categories):
        super(RecurrentCapsuleNetwork, self).__init__(
            embedding=embedding,
            hidden_size=embedding.embedding_dim * (2 if bidirectional else 1),
            capsule_size=capsule_size,
            dropout=dropout,
            num_categories=num_categories
        )
        embed_size = embedding.embedding_dim
        self.rnn = nn.GRU(
            input_size=embed_size * 2,
            hidden_size=embed_size,
            num_layers=num_layers,
            bidirectional=bidirectional,
            batch_first=True
        )
        self.bidirectional = bidirectional

    def _sentence_encode(self, sentence, aspect, mask=None):
        batch_size, time_step, embed_size = sentence.size()
        aspect_aware_sentence = torch.cat((
            sentence, aspect.unsqueeze(1).expand(batch_size, time_step, embed_size)
        ), dim=-1)
        output, _ = self.rnn(aspect_aware_sentence)
        if self.bidirectional:
            sentence = sentence.unsqueeze(-1).expand(batch_size, time_step, embed_size, 2)
            sentence = sentence.contiguous().view(batch_size, time_step, embed_size * 2)
        output = output + sentence
        output = F.dropout(output, p=self.dropout, training=self.training)
        return output
```

<br>

\_sentence\_encode 메소드를 보면,

```python
    def _sentence_encode(self, sentence, aspect, mask=None):
        batch_size, time_step, embed_size = sentence.size()
        aspect_aware_sentence = torch.cat((  # sentence와 average pooling을 적용한 aspect를 conactenate
            sentence, aspect.unsqueeze(1).expand(batch_size, time_step, embed_size)
        ), dim=-1)
        output, _ = self.rnn(aspect_aware_sentence)
        if self.bidirectional:
            sentence = sentence.unsqueeze(-1).expand(batch_size, time_step, embed_size, 2)
            sentence = sentence.contiguous().view(batch_size, time_step, embed_size * 2)
        output = output + sentence
        output = F.dropout(output, p=self.dropout, training=self.training)
        return output
```

<br>

```python
        batch_size, time_step, embed_size = sentence.size()
```

먼저 배치 사이즈, context에 포함된 단어 개수, 임베딩 크기를 batch_size, time_step, embed_size로 받습니다.

<img width="221" alt="스크린샷 2022-05-04 오전 9 16 37" src="https://user-images.githubusercontent.com/76269316/166605936-fd055421-3bf6-4e93-b756-2f8108886acf.png">

<br>

```python
        aspect_aware_sentence = torch.cat((
            sentence, aspect.unsqueeze(1).expand(batch_size, time_step, embed_size)
        ), dim=-1)
```

이후 aspect의 1번 열에 차원을 하나 추가한 뒤, 64X74X300으로 확장합니다.

그런 다음, sentence와 aspect를 합칩니다.

<img width="222" alt="스크린샷 2022-05-04 오후 2 18 51" src="https://user-images.githubusercontent.com/76269316/166626554-80ad90cd-f655-4994-a5a6-48c76c658cbc.png">

<br>

```python
output, _ = self.rnn(aspect_aware_sentence)
```

이후 위에서 생성한 GRU 레이어의 입력으로 넣어줍니다. (참고: [PyTorch 구현 코드로 살펴보는 GRU](https://deep-learning-study.tistory.com/691)])

[/src/aspect_term_model/recurrent_capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/recurrent_capsnet.py)

17 ~ 23 line

```python
        self.rnn = nn.GRU(
            input_size=embed_size * 2,  # 300 * 2 = 600
            hidden_size=embed_size,  # 300
            num_layers=num_layers,  # 2
            bidirectional=bidirectional,  # True
            batch_first=True
        )
```

_는 GRU 레이어의 출력으로 나오는 텐서의 크기이고, <img width="220" alt="스크린샷 2022-05-04 오후 2 29 08" src="https://user-images.githubusercontent.com/76269316/166627243-13b2d16c-3e3c-425f-a234-192127b7421b.png">

output으로는 64X74X600의 텐서가 저장됩니다.

<img width="585" alt="스크린샷 2022-05-04 오후 2 30 53" src="https://user-images.githubusercontent.com/76269316/166627370-0cfde8ea-e7cc-4dc3-a6ad-9e73fddbec67.png">

<br>

[/src/aspect_term_model/recurrent_capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/recurrent_capsnet.py)

32 ~ 34 line

```python
        if self.bidirectional:
            sentence = sentence.unsqueeze(-1).expand(batch_size, time_step, embed_size, 2)
```

bidirectional이 True이므로 마지막 열에 차원을 하나 추가하고 64X74X300X2로 확장합니다.

<img width="247" alt="스크린샷 2022-05-04 오후 2 33 31" src="https://user-images.githubusercontent.com/76269316/166627564-fae74853-8c41-4499-bae9-5366db36afea.png">

<br>

```python
sentence = sentence.contiguous().view(batch_size, time_step, embed_size * 2)
```

Contiguous 메소드로 메모리 주소를 공유하고 (같은 포인터를 사용해서 메모리를 할당하지 않고, 데이터를 변경하면 원본 데이터가 변경됨), view를 이용해서 크기를 변경한다.

<img width="221" alt="스크린샷 2022-05-04 오후 2 45 26" src="https://user-images.githubusercontent.com/76269316/166628450-05fb90de-7ba2-4926-b551-f97a0c319c2d.png">

참고: [PyTorch contiguous](https://f-future.tistory.com/entry/Pytorch-Contiguous), [PyTorch view](https://hichoe95.tistory.com/26)

<br>

35 line

```python
        output = output + sentence
        output = F.dropout(output, p=self.dropout, training=self.training)
```

<img width="524" alt="스크린샷 2022-05-04 오후 2 46 42" src="https://user-images.githubusercontent.com/76269316/166628534-ccabee39-4be7-44ff-a5bc-8a822e002932.png">

이후 sentence와 output 두 텐서를 합친뒤 dropout을 적용하여 리턴합니다.

<img width="217" alt="스크린샷 2022-05-04 오후 2 47 52" src="https://user-images.githubusercontent.com/76269316/166628616-920fab3f-0453-404c-aca8-009098f00193.png">

<br>

여기까지의 과정을 그림으로 표현하면 다음과 같습니다.

sentence와 aspect를 단어 임베딩 형태로 변환하고, aspect에 average pooling을 적용해 sentence와 concatenate 해줍니다.

이후 BiGRU를 적용한 뒤 Primary Capsule Layer로 보냅니다.

![스크린샷 2022-05-06 오후 3 06 02](https://user-images.githubusercontent.com/76269316/167076127-46cd43b8-fe8b-4edb-87b5-0d85e4b4a6ee.png)

<br>

### Primary Capsule Layer

primary capsule layer에서는 context와 average pooling을 거친 aspect에 선형변환과 squashing activation function을 적용해 primary capsule과 aspect capsule을 얻습니다.

<img src="https://user-images.githubusercontent.com/76269316/167077063-8de2ee11-9d8c-4962-94a3-b8a573377970.png" alt="스크린샷 2022-05-06 오후 3 14 07" style="zoom:67%;" />

이를 수식으로 표현하면 다음과 같습니다.

![스크린샷 2022-05-06 오후 3 10 03](https://user-images.githubusercontent.com/76269316/167076580-3a5f6d07-4d21-4828-8324-1e1d54690855.png)

이때 <img width="142" alt="스크린샷 2022-05-06 오후 3 11 16" src="https://user-images.githubusercontent.com/76269316/167076718-45d78ec4-2948-472a-8880-f9c5e4d3147a.png">는 learnable parameter입니다. (훈련하면서 업데이트되는 파라미터)

squash function은 다음과 같이 정의돼 있습니다.

![스크린샷 2022-05-06 오후 3 12 10](https://user-images.githubusercontent.com/76269316/167076819-3c4fbc87-cfc5-42a4-b5e2-d650b56bd9fc.png)

코드를 살펴보겠습니다.

**capsnet.py**

77 line

```python
sentence = self.sentence_transform(sentence)
```

\_sentence\_encode 메소드로부터 받은 output을 이번에는 28 ~ 31 line에 linear layer로 정의한 sentence_transform의 입력으로 넣어줍니다. (선형변환)

28 ~ 31 line

```python
        self.sentence_transform = nn.Sequential(
            nn.Linear(hidden_size, capsule_size),  # 600 X 300
            nn.Dropout(dropout)
        )
```

64X74X600 X 600X300 -> <img width="220" alt="스크린샷 2022-05-04 오후 3 12 32" src="https://user-images.githubusercontent.com/76269316/166630804-3892fa3a-65cb-4f7e-a863-6d82536e822d.png">

<br>

78 line

```python
        primary_capsule = squash(sentence, dim=-1)
```

64X74X300으로 변환된 sentence를 [/src/module/utils/squash.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/module/utils/squash.py)에 정의된 squash 메소드의 파라미터로 넣어줍니다.

<br>

 [/src/module/utils/squash.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/module/utils/squash.py)

```python
def squash(x, dim=-1):
    squared = torch.sum(x * x, dim=dim, keepdim=True)
    scale = torch.sqrt(squared) / (1.0 + squared)
    return scale * x
```

한 라인씩 살펴보겠습니다.

```python
squared = torch.sum(x * x, dim=dim, keepdim=True)
```

먼저 sentence 행렬을 제곱한 다음, 

```python
x * x
```

![스크린샷 2022-05-05 오후 1 34 47](https://user-images.githubusercontent.com/76269316/166864107-b6a02229-c7d5-4609-a95b-3143b2bb93fd.png)

<br>

torch.sum으로 더해줍니다. (같은 행에 있는 요소들을 더함)

```python
torch.sum(x * x, dim=dim, keepdim=True)
```

![스크린샷 2022-05-05 오후 2 00 35](https://user-images.githubusercontent.com/76269316/166865896-aa9c9acb-6870-46e0-9e8d-8feff4ec0dc8.png)

<br>

이후 scale을 계산한 뒤, sentence에 곱한 뒤 리턴합니다.

```python
scale = torch.sqrt(squared) / (1.0 + squared)
return scale * X
```

<br>

**capsnet.py**

80 ~ 81 line

aspect도 마찬가지로

```python
        aspect = self.aspect_transform(aspect)
        aspect_capsule = squash(aspect, dim=-1)
```

24 ~ 27 line에 정의한 aspect_transform layer를 거친 뒤(선형 변환), scale 값을 곱한 텐서를 aspect_capsule에 저장합니다.

```python
        self.aspect_transform = nn.Sequential(
            nn.Linear(embed_size, capsule_size),
            nn.Dropout(dropout)
        )
```

<br>

##### Aspect Aware Normalization

입력으로 들어오는 문장의 길이가 다양하기 때문에 상층 레이어로 보내진느 캡슐의 수가 문장마다 달라지게 됩니다.

매우 긴 문장의 경우 squash activation function을 포화시켜 모든 카테고리(감정극성)에 대해 높은 confidence로 예측하고 반대로 매우 짧은 문장의 경우 모든 카테고리에 대해 낮은 confidence로 예측하기 때문에 모델의 성능을 저하시키게 됩니다.

이를 완화하기 위해 저자는 aspect aware normalization을 적용해 중요한 primary 캡슐을 선택하고 primary capsule의 가중치 u를 normalize합니다.

primary capsule의 가중치 u는 다음과 같이 표현할 수 있습니다.

![스크린샷 2022-05-06 오후 3 19 59](https://user-images.githubusercontent.com/76269316/167077738-245ef29a-5c69-4fef-b90f-f296813a0c7c.png)

<img width="42" alt="스크린샷 2022-05-06 오후 3 21 15" src="https://user-images.githubusercontent.com/76269316/167077883-bf98dd56-5144-4b94-9b25-9ecf14b17164.png">은 learnable parameter입니다.

<br>

##### Capsule Guided Routing

원래 [동적 라우팅 메커니즘 (Sabour et al., 2017)](https://papers.nips.cc/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf)은 라우팅의 반복적인 절차로 인해 훈련과정이 비효율적입니다.

그리고 라우팅 프로세스를 가이드하는데 사용되는 상위 계층의 정보가 없으므로 동적 라우팅이 self-directed 프로세스처럼 동작하는데 저자는 감성 사전에 대한 prior knowledge를 이용해 효율적인 라우팅을 한다고 합니다.

![스크린샷 2022-05-06 오후 3 29 18](https://user-images.githubusercontent.com/76269316/167078821-17a3fc64-0c74-409a-b272-2c5ba8c1c6f2.png)

G는 감성 행렬을 나타내는데 C는 감성 카테고리의 수, d는 임베딩 차원입니다.

이 감성 행렬 G에 squash activation function을 적용해 sentiment capsule Z를 얻습니다.

![스크린샷 2022-05-06 오후 3 32 06](https://user-images.githubusercontent.com/76269316/167079092-7807becd-02a6-47e2-98f8-5da48e4282cf.png)

이후 primary capsule과 sentiment capsule의 유사도를 계산한 뒤 softmax를 적용해 routing weight w를 계산합니다.

![스크린샷 2022-05-06 오후 3 33 39](https://user-images.githubusercontent.com/76269316/167079269-b1c2f10d-86d8-4bc4-8719-49bba22415d1.png)

<img width="42" alt="스크린샷 2022-05-06 오후 3 21 15" src="https://user-images.githubusercontent.com/76269316/167077883-bf98dd56-5144-4b94-9b25-9ecf14b17164.png">은 learnable parameter입니다.

<br>

### Category Capsule Layer

위에서 구한 aspect aware normalization과 routing weight를 가지고 최종적으로 category capsule V를 계산할 수 있습니다.

수식은 다음과 같습니다.

![스크린샷 2022-05-06 오후 3 35 53](https://user-images.githubusercontent.com/76269316/167079550-f9c4c8a4-5280-4baf-950f-e5dc1d6c3545.png)

s는 learnable scale 파라미터입니다.

<br>
