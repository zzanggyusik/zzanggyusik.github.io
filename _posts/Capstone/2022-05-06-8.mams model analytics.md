---
title:  "[Capstone] MAMS for ABSA 모델 분석"
excerpt: "MAMS for ABSA 코드 리뷰"
toc: true
toc_label: "Capstone"
toc_sticky: true
published: true

categories:
  - Capstone
tags:
  - PyTorch
last_modified_at: 2022-05-06

---

> [이전에 리뷰한 논문](https://seominseok4834.github.io/paper%20review/1.a-challenge-dataset-and-effective-models-for-aspect-based-sentiment-analysis/)에서 구현한 모델을 캡스톤 디자인 프로젝트에 적용하기 위해 모델 부분 코드를 분석한 내용입니다.
>
> 모든 코드는 [MAMS-for-ABSA github](https://github.com/siat-nlp/MAMS-for-ABSA)에서 가져왔습니다.

<br>

### 모델 생성

##### 모델 생성자 함수

```python
class CapsuleNetwork(nn.Module):

    def __init__(self, embedding, hidden_size, capsule_size, dropout, num_categories):
        super(CapsuleNetwork, self).__init__()
        self.embedding = embedding
        embed_size = embedding.embedding_dim
        self.capsule_size = capsule_size
        self.aspect_transform = nn.Sequential(
            nn.Linear(embed_size, capsule_size),
            nn.Dropout(dropout)
        )
        self.sentence_transform = nn.Sequential(
            nn.Linear(hidden_size, capsule_size),
            nn.Dropout(dropout)
        )
        self.norm_attention = BilinearAttention(capsule_size, capsule_size)
        self.guide_capsule = nn.Parameter(
            torch.Tensor(num_categories, capsule_size)
        )
        self.guide_weight = nn.Parameter(
            torch.Tensor(capsule_size, capsule_size)
        )
        self.scale = nn.Parameter(torch.tensor(4.0))
        self.capsule_projection = nn.Linear(capsule_size, capsule_size * num_categories)
        self.dropout = dropout
        self.num_categories = num_categories
        self._reset_parameters()
```

[/src/aspect_term_model/capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/capsnet.py)

위 생성자 함수는 /train/train.py에서 make_model()이라는 메소드를 통해 호출되게 됩니다. (make_mode -> make_recurrent_capsule_network를 통해 최종 호출됨)

make_recurrent_capsule_network 메소드를 보면,

```python
def make_recurrent_capsule_network(config):
    embedding = make_embedding(config)
    base_path = os.path.join(config['base_path'])  # ./data/MAMS-ATSA
    log_path = os.path.join(base_path, 'log/log.yml')
    log = yaml.safe_load(open(log_path))
    config = config['aspect_category_model'][config['aspect_category_model']['type']]  # config['aspect_term_model']
    aspect_embedding = nn.Embedding(num_embeddings=8, embedding_dim=config['embed_size'])
    model = RecurrentCapsuleNetwork(
        embedding=embedding,
        aspect_embedding=aspect_embedding,
        num_layers=config['num_layers'],  # 2
        bidirectional=config['bidirectional'],  # True
        capsule_size=config['capsule_size'],  # 300
        dropout=config['dropout'],  # 0.5
        num_categories=log['num_categories']  # 3
    )
    model.load_sentiment(os.path.join(base_path, 'processed/sentiment_matrix.npy'))
    return model
```

[/train/make_aspect_category_model.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/make_aspect_category_model.py)

##### make_embedding()

첫번째로 호출되는 make_embedding은 train 데이터에 포함된 단어들을 glove.840B.300d에 있는 단어 벡터로 변환한 파일(glove.npy)을 torch.nn.modules.sparse.Embedding로 로드합니다.

<br>

##### [nn.Embedding()](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)

임베딩 사전의 크기가 8이고, 각 임베딩 벡터의 크기가 300인 룩업 테이블을 생성합니다.

초기화는 밑에 load_sentiment() 메소드를 통해 이루어집니다.

<br>

이후 configuration 파일([/config.yml](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/config.ymll)과 log 파일에서 layer 개수, bidirectional인지, capsule 크기, dropout 등을 가져와 모델을 생성합니다.

<br>

##### load_sentiment()

load_sentiment는 모델 내 구현된 함수로 다음과 같이 구현돼 있습니다.

```python
def load_sentiment(self, path):
  sentiment = np.load(path)
  e1 = np.mean(sentiment)
  d1 = np.std(sentiment)
  e2 = 0
  d2 = np.sqrt(2.0 / (sentiment.shape[0] + sentiment.shape[1]))
  sentiment = (sentiment - e1) / d1 * d2 + e2
  self.guide_capsule.data.copy_(torch.tensor(sentiment))
```

[/src/aspect_term_model/capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/capsnet.py)

전처리 과정([/data_process/data_process.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/data_process.py)에서 생성한 sentiment_matrix.npy 파일을 로드해 torch.Tensor형으로 바꾼뒤 nn.Parameter의 데이터로 맵핑합니다.

<br>

```python
def __init__(self, embedding, hidden_size, capsule_size, dropout, num_categories):
  super(CapsuleNetwork, self).__init__()
  							···
  self.guide_weight = nn.Parameter(  # nn.Parameter의 data로 sentiment_matrix를 copy
    torch.Tensor(capsule_size, capsule_size)
  )
```

<br>

➕ 이 때 sentiment_matrix에 평균을 빼고 표준편차로 나눠서 정규화를 해주는데 처음에 sentiment_matrix를 저장할때도 정규화를 해주는데 왜 로드해서도 다시 정규화를 거치는지 잘 모르겠습니다 ..

```python
def load_sentiment_matrix(glove_path, sentiment_path):
    sentiment_matrix = np.zeros((3, 300), dtype=np.float32)
    sd = json.load(open(sentiment_path, 'r', encoding='utf-8'))
    sd['positive'] = set(sd['positive'])
    sd['negative'] = set(sd['negative'])
    sd['neutral'] = set(sd['neutral'])
    with open(glove_path, 'r', encoding='utf-8') as f:
        for line in f:
            content = line.split(' ')
            word = content[0]
            vec = np.array(list(map(float, content[1:])))
            if word in sd['positive']:
                sentiment_matrix[0] += vec
            elif word in sd['negative']:
                sentiment_matrix[1] += vec
            elif word in sd['neutral']:
                sentiment_matrix[2] += vec
    sentiment_matrix -= sentiment_matrix.mean()
    sentiment_matrix = sentiment_matrix / sentiment_matrix.std() * np.sqrt(2.0 / (300.0 + 3.0))
    return sentiment_matrix
```

[/data_process/utils.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/utils.py)의 load_sentiment_matrix 함수를 통해 sentiment_matrix를 생성하고 저장

<br><br>

```python
class CapsuleNetwork(nn.Module):

    def __init__(self, embedding, hidden_size, capsule_size, dropout, num_categories):
        super(CapsuleNetwork, self).__init__()
        self.embedding = embedding
        embed_size = embedding.embedding_dim
        self.capsule_size = capsule_size
        self.aspect_transform = nn.Sequential(
            nn.Linear(embed_size, capsule_size),
            nn.Dropout(dropout)
        )
        self.sentence_transform = nn.Sequential(
            nn.Linear(hidden_size, capsule_size),
            nn.Dropout(dropout)
        )
        self.norm_attention = BilinearAttention(capsule_size, capsule_size)
        self.guide_capsule = nn.Parameter(
            torch.Tensor(num_categories, capsule_size)
        )
        self.guide_weight = nn.Parameter(
            torch.Tensor(capsule_size, capsule_size)
        )
        self.scale = nn.Parameter(torch.tensor(4.0))
        self.capsule_projection = nn.Linear(capsule_size, capsule_size * num_categories)
        self.dropout = dropout
        self.num_categories = num_categories
        self._reset_parameters()
```

[/src/aspect_term_model/capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/capsnet.py)

모델 생성시 넘겨지는 파라미터 값을 다시 한 번 정리하면 <class 'torch.nn.modules.sparse.Embedding'> type의 glove 임베딩이 들어있고 (emed_size는 300, pretrained된 glove 단어 임베딩이 300차원으로 돼 있으므로) capsue_size=2, dropout=0.5, num_categories=3 입니다.

저희는 CapsuleNetwork를 상속받은 RecurrentCapsuleNetwork를 사용하기 때문에 hidden_size가 다릅니다.

[/src/aspect_term_model/recurrent_capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/recurrent_capsnet.py)

8 ~ 15 line

```python
    def __init__(self, embedding, num_layers, bidirectional, capsule_size, dropout, num_categories):
        super(RecurrentCapsuleNetwork, self).__init__(
            embedding=embedding,
            hidden_size=embedding.embedding_dim * (2 if bidirectional else 1),
            capsule_size=capsule_size,
            dropout=dropout,
            num_categories=num_categories
        )
```

embed_size = 300, hidden_size = 600, capsule_size = 300, dropout = 0.5. num_categories = 3

<br>

### forward 함수

[/src/aspect_term_model/capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/recurrent_capsnet.py)

58 ~ 87 line

```python
def forward(self, sentence, aspect):
  # get lengths and masks
  sentence = sentence_clip(sentence)
  aspect = sentence_clip(aspect)
  sentence_mask = (sentence != PAD_INDEX)
  aspect_mask = (aspect != PAD_INDEX)
  # sentence_lens = sentence_mask.long().sum(dim=1, keepdim=True)
  aspect_lens = aspect_mask.long().sum(dim=1, keepdim=True)
  # embedding
  sentence = self.embedding(sentence)
  sentence = F.dropout(sentence, p=self.dropout, training=self.training)
  aspect = self.embedding(aspect)
  aspect = F.dropout(aspect, p=self.dropout, training=self.training)
  # aspect average pooling
  aspect = aspect.masked_fill(aspect_mask.unsqueeze(-1) == 0, 0)
  aspect = aspect.sum(dim=1, keepdim=False) / aspect_lens.float()
  # sentence encode layer
  sentence = self._sentence_encode(sentence, aspect)
  # primary capsule layer
  sentence = self.sentence_transform(sentence)
  primary_capsule = squash(sentence, dim=-1)
  # aspect capsule layer
  aspect = self.aspect_transform(aspect)
  aspect_capsule = squash(aspect, dim=-1)
  # aspect aware normalization
  norm_weight = self.norm_attention.get_attention_weights(aspect_capsule, primary_capsule, sentence_mask)
  # capsule guided routing
  category_capsule = self._capsule_guided_routing(primary_capsule, norm_weight)
  category_capsule_norm = torch.sqrt(torch.sum(category_capsule * category_capsule, dim=-1, keepdim=False))
  return category_capsule_norm
```

<br>

**capsnet.py**

60 ~ 61 line

```python
  sentence = sentence_clip(sentence)
  aspect = sentence_clip(aspect)
```

[src/module/utils/sentence_clip.py](https://github.com/siat-nlp/MAMS-for-ABSA/tree/master/src/module/utils)가 실행됩니다.

```python
from src.module.utils.constants import PAD_INDEX

def sentence_clip(sentence):
    mask = (sentence != PAD_INDEX)  # word2index로 변환한 context, aspect 값에서 PAD_INDEX가 아닌 부분을 TRUE로 변환
    sentence_lens = mask.long().sum(dim=1, keepdim=False)  # 행별로 True의 개수를 구함
    max_len = sentence_lens.max().item()  # 최대 True 개수
    return sentence[:, :max_len]
```

<br>

```python
mask = (sentence != PAD_INDEX)
```

먼저 이 부분이 실행되면, 이렇게 word2index로 변환된 context(left_part + 2 + right_part)를

<img width="675" alt="스크린샷 2022-05-03 오후 11 04 54" src="https://user-images.githubusercontent.com/76269316/166468291-02aa355d-8dcc-49fd-b0cf-5a9b6222cb6a.png">

PAD_INDEX(0)가 아닌 부분을 TRUE로 변환합니다.

<img width="654" alt="스크린샷 2022-05-03 오후 11 06 22" src="https://user-images.githubusercontent.com/76269316/166468548-145bced2-fb8c-4271-a9d4-ca6d59b3c524.png">

<br>

이후 TRUE인 부분을 모두 더하면 길이가 64인 1차원 텐서 자료형이 생성되는데, 이 값들은 context에 있는 단어 개수의 합(True 개수)입니다.

```python
sentence_lens = mask.long().sum(dim=1, keepdim=False)
```

<img width="669" alt="스크린샷 2022-05-03 오후 11 08 28" src="https://user-images.githubusercontent.com/76269316/166468949-2d1d46e2-a2f4-4ad4-a71b-c2e67141aee6.png">

그 다음 최대 True 개수, 단어가 가장 많이 나온 행만큼만 열을 사용하도록 슬라이싱 해서 리턴합니다.

```python
    max_len = sentence_lens.max().item()
    return sentence[:, :max_len]
```

<img width="33" alt="스크린샷 2022-05-03 오후 11 09 57" src="https://user-images.githubusercontent.com/76269316/166469230-bb25c57d-9b8c-44e6-8084-ec11c42accc6.png">

▶️ 원래 sentence는 64X76 (batch size X aspect max len)이였는데, [전처리 데이터 저장 부분 참고](https://seominseok4834.github.io/capstone/4.mams-preprocess-code-analytics/#%EC%A0%84%EC%B2%98%EB%A6%AC-%B%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%80%EC%9E%A5) 현재 배치 사이즈로 뽑아온 데이터 중 최대 개수를 구해 그만큼만 사용하겠다는 의미입니다.

원래 64X76이였는데 <img width="173" alt="스크린샷 2022-05-03 오후 11 12 46" src="https://user-images.githubusercontent.com/76269316/166469813-15de570d-80a7-47bc-bc11-ab76346dff81.png"> 64X74로 줄어듦 <img width="180" alt="스크린샷 2022-05-03 오후 11 13 15" src="https://user-images.githubusercontent.com/76269316/166469913-9a643bfb-4487-4e9f-a19f-4404ab7d5845.png">

<br>

aspect도 동일하게 사용하는 열만큼 사이즈를 줄여줇니다.

<br>

다시 모델의 forward 부분으로 돌아와서

**capsnet.py**

62 ~ 63 line

```python
        sentence_mask = (sentence != PAD_INDEX)
        aspect_mask = (aspect != PAD_INDEX)
```

리턴 받은 sentence, aspect에서 PAD_INDEX가 아닌 부분을 TRUE로 하는 마스크를 생성합니다.

<img width="663" alt="스크린샷 2022-05-03 오후 11 15 30" src="https://user-images.githubusercontent.com/76269316/166470348-d99f4aac-7fef-4cb3-8f93-fbf3b0a590d9.png">

<img width="652" alt="스크린샷 2022-05-03 오후 11 15 48" src="https://user-images.githubusercontent.com/76269316/166470401-487c1a82-79ca-4fe8-94fb-271e5fd7c496.png">

<br>

**capsnet.py**

65 line

앞서 sentence_clip에서 구한것처럼, 모든 aspect의 단어 개수를 구해 64X1로 만듧니다.

```python
aspect_lens = aspect_mask.long().sum(dim=1, keepdim=True)
```

<img width="118" alt="스크린샷 2022-05-03 오후 11 17 44" src="https://user-images.githubusercontent.com/76269316/166470774-10446057-7747-4ac2-949d-6166c3f1e7b6.png">

<br>

이전까지는 전처리 과정이였고, 이번 라인(67 line)부터는 논문의 모델 설명 부분과 함께 설명하도록 하겠습니다.

##### Embedding Layer

임베딩 레이어에서는 문장 S를 임베딩 E로 변환합니다.

**capsnet.py**

67 ~ 70 line

```python
        sentence = self.embedding(sentence)
        sentence = F.dropout(sentence, p=self.dropout, training=self.training)
        aspect = self.embedding(aspect)
        aspect = F.dropout(aspect, p=self.dropout, training=self.training)
```

<img width="600" alt="스크린샷 2022-05-03 오후 11 20 50" src="https://user-images.githubusercontent.com/76269316/166471323-0ba4f1d2-4f71-4ed7-94f9-ec9669bd5425.png">

sentence와 aspect에 embedding과 dropout을 적용하면 다음과 같이 preprocess 코드에서 생성했던 300차원의 단어 임베딩으로 변환됩니다. (sentence shape: 64X74X300, aspect shape: 64X23X300)

<br>

aspect의 경우 average pooling 과정을 거칩니다.

72 line

```python
        aspect = aspect.masked_fill(aspect_mask.unsqueeze(-1) == 0, 0)
```

<br>

```python
aspect_mask.unsqueeze(-1) == 0
```

앞서 62 ~ 63 line에서 만들었던 aspect_mask에

<img width="652" alt="스크린샷 2022-05-03 오후 11 15 48" src="https://user-images.githubusercontent.com/76269316/166470401-487c1a82-79ca-4fe8-94fb-271e5fd7c496.png">

차원 하나를 추가합니다. (64X49 -> 64X49X1)

이 때 0과 같으면 True, 같지 않으면 False로 값을 지정해줬기 때문에 다음과 같이 값이 반전돼서 들어가게 됩니다.

<img width="145" alt="스크린샷 2022-05-03 오후 11 33 29" src="https://user-images.githubusercontent.com/76269316/166473886-6b660a3e-5a19-456d-9506-5e41737eff27.png">

이 인덱스에서 True인 부분 즉, aspect가 없는 부분의 임베딩은 모두 0으로 채워지게 됩니다.

0번째 aspect의 실행결과를 보면, 0번 인덱스를 제외하고 모두 단어가 없는데, 0번째 단어는 임베딩 값이 정상적으로 들어가 있는 반면,

<img width="674" alt="스크린샷 2022-05-03 오후 11 41 18" src="https://user-images.githubusercontent.com/76269316/166475376-d3be8a16-866c-47ea-a284-e1b658caa81f.png">

첫번째 단어는 임베딩 값이 모두 0인걸 확인할 수 있습니다. (이후의 인덱스도 동일)

<img width="939" alt="스크린샷 2022-05-03 오후 11 43 43" src="https://user-images.githubusercontent.com/76269316/166475882-8cac49f8-adf8-4e1b-9848-4001391d4d2b.png">

<br>

**capsnet.py**

73 line

```python
        aspect = aspect.sum(dim=1, keepdim=False) / aspect_lens.float()
```

이후 해당 값들을 더한 뒤 각 행의 길이로 나눠 저장합니다.

<img width="181" alt="스크린샷 2022-05-03 오후 11 48 08" src="https://user-images.githubusercontent.com/76269316/166476790-07757a51-19f6-41ea-8eea-19c537baebfd.png">

<img width="580" alt="스크린샷 2022-05-03 오후 11 48 50" src="https://user-images.githubusercontent.com/76269316/166476928-f0ae0f06-8f74-4a85-8571-070d324f6342.png">

<br>

### Encoding Layer

sentence와 average pooling을 적용한 aspect를 conactenate해서 aspect aware sentence 임베딩 <img width="42" alt="스크린샷 2022-05-06 오후 3 00 54" src="https://user-images.githubusercontent.com/76269316/167075592-b42b3c1d-233d-436e-a300-34843eb26bf9.png">를 만듧니다.

이후 이 <img width="42" alt="스크린샷 2022-05-06 오후 3 00 54" src="https://user-images.githubusercontent.com/76269316/167075592-b42b3c1d-233d-436e-a300-34843eb26bf9.png"> 임베딩을 bidirectional GRU와 residual connection을 통해 contextualized representation H로 만듧니다.

이 과정은 아래 코드를 통해 구현돼 있습니다.

![스크린샷 2022-05-06 오후 3 03 36](https://user-images.githubusercontent.com/76269316/167075856-94c2e3f5-2422-453b-ad3d-48dca0de64ed.png)

**capsnet.py**

75 line

```python
        sentence = self._sentence_encode(sentence, aspect)
```

\_sentence\_encode 메소드는 부모 클래스가 아닌 자식 클래스(RecurrentCapsuleNetwork)에 구현돼 있습니다.

[/src/aspect_term_model/recurrent_capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/recurrent_capsnet.py)

```python
import torch
from torch import nn
import torch.nn.functional as F
from src.aspect_term_model.capsnet import CapsuleNetwork

class RecurrentCapsuleNetwork(CapsuleNetwork):

    def __init__(self, embedding, num_layers, bidirectional, capsule_size, dropout, num_categories):
        super(RecurrentCapsuleNetwork, self).__init__(
            embedding=embedding,
            hidden_size=embedding.embedding_dim * (2 if bidirectional else 1),
            capsule_size=capsule_size,
            dropout=dropout,
            num_categories=num_categories
        )
        embed_size = embedding.embedding_dim
        self.rnn = nn.GRU(
            input_size=embed_size * 2,
            hidden_size=embed_size,
            num_layers=num_layers,
            bidirectional=bidirectional,
            batch_first=True
        )
        self.bidirectional = bidirectional

    def _sentence_encode(self, sentence, aspect, mask=None):
        batch_size, time_step, embed_size = sentence.size()
        aspect_aware_sentence = torch.cat((
            sentence, aspect.unsqueeze(1).expand(batch_size, time_step, embed_size)
        ), dim=-1)
        output, _ = self.rnn(aspect_aware_sentence)
        if self.bidirectional:
            sentence = sentence.unsqueeze(-1).expand(batch_size, time_step, embed_size, 2)
            sentence = sentence.contiguous().view(batch_size, time_step, embed_size * 2)
        output = output + sentence
        output = F.dropout(output, p=self.dropout, training=self.training)
        return output
```

<br>

\_sentence\_encode 메소드를 보면,

```python
    def _sentence_encode(self, sentence, aspect, mask=None):
        batch_size, time_step, embed_size = sentence.size()
        aspect_aware_sentence = torch.cat((  # sentence와 average pooling을 적용한 aspect를 conactenate
            sentence, aspect.unsqueeze(1).expand(batch_size, time_step, embed_size)
        ), dim=-1)
        output, _ = self.rnn(aspect_aware_sentence)
        if self.bidirectional:
            sentence = sentence.unsqueeze(-1).expand(batch_size, time_step, embed_size, 2)
            sentence = sentence.contiguous().view(batch_size, time_step, embed_size * 2)
        output = output + sentence
        output = F.dropout(output, p=self.dropout, training=self.training)
        return output
```

<br>

```python
        batch_size, time_step, embed_size = sentence.size()
```

먼저 배치 사이즈, context에 포함된 단어 개수, 임베딩 크기를 batch_size, time_step, embed_size로 받습니다.

<img width="221" alt="스크린샷 2022-05-04 오전 9 16 37" src="https://user-images.githubusercontent.com/76269316/166605936-fd055421-3bf6-4e93-b756-2f8108886acf.png">

<br>

```python
        aspect_aware_sentence = torch.cat((
            sentence, aspect.unsqueeze(1).expand(batch_size, time_step, embed_size)
        ), dim=-1)
```

이후 aspect의 1번 열에 차원을 하나 추가한 뒤, 64X74X300으로 확장합니다.

그런 다음, sentence와 aspect를 합칩니다.

<img width="222" alt="스크린샷 2022-05-04 오후 2 18 51" src="https://user-images.githubusercontent.com/76269316/166626554-80ad90cd-f655-4994-a5a6-48c76c658cbc.png">

<br>

```python
output, _ = self.rnn(aspect_aware_sentence)
```

이후 위에서 생성한 GRU 레이어의 입력으로 넣어줍니다. (참고: [PyTorch 구현 코드로 살펴보는 GRU](https://deep-learning-study.tistory.com/691)])

[/src/aspect_term_model/recurrent_capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/recurrent_capsnet.py)

17 ~ 23 line

```python
        self.rnn = nn.GRU(
            input_size=embed_size * 2,  # 300 * 2 = 600
            hidden_size=embed_size,  # 300
            num_layers=num_layers,  # 2
            bidirectional=bidirectional,  # True
            batch_first=True
        )
```

_는 GRU 레이어의 출력으로 나오는 텐서의 크기이고, <img width="220" alt="스크린샷 2022-05-04 오후 2 29 08" src="https://user-images.githubusercontent.com/76269316/166627243-13b2d16c-3e3c-425f-a234-192127b7421b.png">

output으로는 64X74X600의 텐서가 저장됩니다.

<img width="585" alt="스크린샷 2022-05-04 오후 2 30 53" src="https://user-images.githubusercontent.com/76269316/166627370-0cfde8ea-e7cc-4dc3-a6ad-9e73fddbec67.png">

<br>

[/src/aspect_term_model/recurrent_capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/recurrent_capsnet.py)

32 ~ 34 line

```python
        if self.bidirectional:
            sentence = sentence.unsqueeze(-1).expand(batch_size, time_step, embed_size, 2)
```

bidirectional이 True이므로 마지막 열에 차원을 하나 추가하고 64X74X300X2로 확장합니다.

<img width="247" alt="스크린샷 2022-05-04 오후 2 33 31" src="https://user-images.githubusercontent.com/76269316/166627564-fae74853-8c41-4499-bae9-5366db36afea.png">

<br>

```python
sentence = sentence.contiguous().view(batch_size, time_step, embed_size * 2)
```

Contiguous 메소드로 메모리 주소를 공유하고 (같은 포인터를 사용해서 메모리를 할당하지 않고, 데이터를 변경하면 원본 데이터가 변경됨), view를 이용해서 크기를 변경한다.

<img width="221" alt="스크린샷 2022-05-04 오후 2 45 26" src="https://user-images.githubusercontent.com/76269316/166628450-05fb90de-7ba2-4926-b551-f97a0c319c2d.png">

참고: [PyTorch contiguous](https://f-future.tistory.com/entry/Pytorch-Contiguous), [PyTorch view](https://hichoe95.tistory.com/26)

<br>

35 line

```python
        output = output + sentence
        output = F.dropout(output, p=self.dropout, training=self.training)
```

<img width="524" alt="스크린샷 2022-05-04 오후 2 46 42" src="https://user-images.githubusercontent.com/76269316/166628534-ccabee39-4be7-44ff-a5bc-8a822e002932.png">

이후 sentence와 output 두 텐서를 합친뒤 dropout을 적용하여 리턴합니다.

<img width="217" alt="스크린샷 2022-05-04 오후 2 47 52" src="https://user-images.githubusercontent.com/76269316/166628616-920fab3f-0453-404c-aca8-009098f00193.png">

<br>

여기까지의 과정을 그림으로 표현하면 다음과 같습니다.

sentence와 aspect를 단어 임베딩 형태로 변환하고, aspect에 average pooling을 적용해 sentence와 concatenate 해줍니다.

이후 BiGRU를 적용한 뒤 Primary Capsule Layer로 보냅니다.

<img src="https://user-images.githubusercontent.com/76269316/167303928-4a204b8b-6a8d-4f7b-9969-76915d31e6cb.png" alt="image" style="zoom: 80%;" />

<br>

### Primary Capsule Layer

primary capsule layer에서는 context와 average pooling을 거친 aspect에 선형변환과 squashing activation function을 적용해 primary capsule과 aspect capsule을 얻습니다.

<img src="https://user-images.githubusercontent.com/76269316/167077063-8de2ee11-9d8c-4962-94a3-b8a573377970.png" alt="스크린샷 2022-05-06 오후 3 14 07" style="zoom:67%;" />

이를 수식으로 표현하면 다음과 같습니다.

![스크린샷 2022-05-06 오후 3 10 03](https://user-images.githubusercontent.com/76269316/167076580-3a5f6d07-4d21-4828-8324-1e1d54690855.png)

이때 <img width="142" alt="스크린샷 2022-05-06 오후 3 11 16" src="https://user-images.githubusercontent.com/76269316/167076718-45d78ec4-2948-472a-8880-f9c5e4d3147a.png">는 learnable parameter입니다. (훈련하면서 업데이트되는 파라미터)

squash function은 다음과 같이 정의돼 있습니다.

![스크린샷 2022-05-06 오후 3 12 10](https://user-images.githubusercontent.com/76269316/167076819-3c4fbc87-cfc5-42a4-b5e2-d650b56bd9fc.png)

코드를 살펴보겠습니다.

**capsnet.py**

77 line

```python
sentence = self.sentence_transform(sentence)
```

\_sentence\_encode 메소드로부터 받은 output을 이번에는 28 ~ 31 line에 linear layer로 정의한 sentence_transform의 입력으로 넣어줍니다. (선형변환)

28 ~ 31 line

```python
        self.sentence_transform = nn.Sequential(
            nn.Linear(hidden_size, capsule_size),  # 600 X 300
            nn.Dropout(dropout)
        )
```

64X74X600 X 600X300 -> <img width="220" alt="스크린샷 2022-05-04 오후 3 12 32" src="https://user-images.githubusercontent.com/76269316/166630804-3892fa3a-65cb-4f7e-a863-6d82536e822d.png">

<br>

78 line

```python
        primary_capsule = squash(sentence, dim=-1)
```

64X74X300으로 변환된 sentence를 [/src/module/utils/squash.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/module/utils/squash.py)에 정의된 squash 메소드의 파라미터로 넣어줍니다.

<br>

 [/src/module/utils/squash.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/module/utils/squash.py)

```python
def squash(x, dim=-1):
    squared = torch.sum(x * x, dim=dim, keepdim=True)
    scale = torch.sqrt(squared) / (1.0 + squared)
    return scale * x
```

한 라인씩 살펴보겠습니다.

```python
squared = torch.sum(x * x, dim=dim, keepdim=True)
```

먼저 sentence 행렬을 제곱한 다음, 

```python
x * x
```

![스크린샷 2022-05-05 오후 1 34 47](https://user-images.githubusercontent.com/76269316/166864107-b6a02229-c7d5-4609-a95b-3143b2bb93fd.png)

<br>

torch.sum으로 더해줍니다. (같은 행에 있는 요소들을 더함)

```python
torch.sum(x * x, dim=dim, keepdim=True)
```

![스크린샷 2022-05-05 오후 2 00 35](https://user-images.githubusercontent.com/76269316/166865896-aa9c9acb-6870-46e0-9e8d-8feff4ec0dc8.png)

<br>

이후 scale을 계산한 뒤, sentence에 곱한 뒤 리턴합니다.

```python
scale = torch.sqrt(squared) / (1.0 + squared)
return scale * X
```

<br>

**capsnet.py**

80 ~ 81 line

aspect도 마찬가지로

```python
        aspect = self.aspect_transform(aspect)
        aspect_capsule = squash(aspect, dim=-1)
```

24 ~ 27 line에 정의한 aspect_transform layer를 거친 뒤(선형 변환), scale 값을 곱한 텐서를 aspect_capsule에 저장합니다.

```python
        self.aspect_transform = nn.Sequential(
            nn.Linear(embed_size, capsule_size),
            nn.Dropout(dropout)
        )
```

<br>

##### Aspect Aware Normalization

입력으로 들어오는 문장의 길이가 다양하기 때문에 상층 레이어로 보내지는 캡슐의 수가 문장마다 달라지게 됩니다.

매우 긴 문장의 경우 squash activation function을 포화시켜 모든 카테고리(감정극성)에 대해 높은 confidence로 예측하고 반대로 매우 짧은 문장의 경우 모든 카테고리에 대해 낮은 confidence로 예측하기 때문에 모델의 성능을 저하시키게 됩니다.

이를 완화하기 위해 저자는 aspect aware normalization을 적용해 중요한 primary 캡슐을 선택하고 primary capsule의 가중치 u를 normalize합니다.

primary capsule의 가중치 u는 다음과 같이 표현할 수 있습니다.

![스크린샷 2022-05-06 오후 3 19 59](https://user-images.githubusercontent.com/76269316/167077738-245ef29a-5c69-4fef-b90f-f296813a0c7c.png)

<img width="42" alt="스크린샷 2022-05-06 오후 3 21 15" src="https://user-images.githubusercontent.com/76269316/167077883-bf98dd56-5144-4b94-9b25-9ecf14b17164.png">은 learnable parameter입니다.

![image](https://user-images.githubusercontent.com/76269316/167303906-97fe61d4-82b2-401d-81c5-f163b0981043.png)

<br>

**capsnet.py**

83 line

```python
        norm_weight = self.norm_attention.get_attention_weights(aspect_capsule, primary_capsule, sentence_mask)
```

norm_attention은 32 line에서 정의했습니다.

```python
        self.norm_attention = BilinearAttention(capsule_size, capsule_size)
```

BilinearAttention은 Attention class를 상속받은 클래스입니다.

<br>

 [/src/module/attention/attention.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/module/attention/attention.py)

```python
class Attention(nn.Module):
    """
    The base class of attention.
    """

    def __init__(self, dropout):
        super(Attention, self).__init__()
        self.dropout = dropout

    def forward(self, query, key, value, mask=None):
        """
        query: FloatTensor (batch_size, query_size) or FloatTensor (batch_size, num_queries, query_size)
        key: FloatTensor (batch_size, time_step, key_size)
        value: FloatTensor (batch_size, time_step, hidden_size)
        mask: ByteTensor (batch_size, time_step) or ByteTensor (batch_size, num_queries, time_step)
        """
        single_query = False
        if len(query.size()) == 2:
            query = query.unsqueeze(1)
            single_query = True
        if mask is not None:
            if len(mask.size()) == 2:
                mask = mask.unsqueeze(1)
            else:
                assert mask.size(1) == query.size(1)
        score = self._score(query, key) # FloatTensor (batch_size, num_queries, time_step)
        weights = self._weights_normalize(score, mask)
        weights = F.dropout(weights, p=self.dropout, training=self.training)
        output = weights.matmul(value)
        if single_query:
            output = output.squeeze(1)
        return output

    def _score(self, query, key):
        raise NotImplementedError('Attention score method is not implemented.')

    def _weights_normalize(self, score, mask):
        if not mask is None:
            score = score.masked_fill(mask == 0, -constants.INF)
        weights = F.softmax(score, dim=-1)
        return weights

    def get_attention_weights(self, query, key, mask=None):
        single_query = False
        if len(query.size()) == 2:
            query = query.unsqueeze(1)
            single_query = True
        if mask is not None:
            if len(mask.size()) == 2:
                mask = mask.unsqueeze(1)
            else:
                assert mask.size(1) == query.size(1)
        score = self._score(query, key)  # FloatTensor (batch_size, num_queries, time_step)
        weights = self._weights_normalize(score, mask)
        weights = F.dropout(weights, p=self.dropout, training=self.training)
        if single_query:
            weights = weights.squeeze(1)
        return weights
```

<br>

get_attention_weights 함수를 보면, query = aspect_capsule(64X300), key = primary_capsule(64X74X300), mask = sentence_mask (64X74)를 파라미터로 넘겨 받았습니다.

```python
    def get_attention_weights(self, query, key, mask=None):
        single_query = False
        if len(query.size()) == 2:
            query = query.unsqueeze(1)  # 64X1X300
            single_query = True
        if mask is not None:
            if len(mask.size()) == 2:
                mask = mask.unsqueeze(1)
            else:
                assert mask.size(1) == query.size(1)
        score = self._score(query, key)  # FloatTensor (batch_size, num_queries, time_step)
        weights = self._weights_normalize(score, mask)
        weights = F.dropout(weights, p=self.dropout, training=self.training)
        if single_query:
            weights = weights.squeeze(1)
        return weights
```

<br>

```python
    if len(query.size()) == 2:
        query = query.unsqueeze(1)
        single_query = True
```

현재 query의 사이즈가 64X300으로 길이가 2여서 query의 첫번째 열에 1차원을 추가합니다.

![스크린샷 2022-05-07 오전 9 53 36](https://user-images.githubusercontent.com/76269316/167231462-5c212dd6-e709-4227-a8c6-2f6dfe4c39b2.png)

<br>

```python
        if mask is not None:
            if len(mask.size()) == 2:
                mask = mask.unsqueeze(1)
```

마스크도 nan 값이 아니고, sizer가 64X74로 길이가 2여서 mask도 동일하게 1차원을 추가해줍니다.

![스크린샷 2022-05-07 오전 9 55 11](https://user-images.githubusercontent.com/76269316/167231520-f0d17241-7afd-4b40-a14c-52fb802028e5.png)

<br>

```python
        score = self._score(query, key)
```

이후 _score 메소드를 호출하는데, 이는 상속받은 BilinearAttention 클래스에 구현돼 있습니다.

[/src/module/attention/bilinear_attention.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/module/attention/bilinear_attention.py)

```python
class BilinearAttention(Attention):

    def __init__(self, query_size, key_size, dropout=0):
        super(BilinearAttention, self).__init__(dropout)
        self.weights = nn.Parameter(torch.FloatTensor(query_size, key_size))  # 300 X 300
        init.xavier_uniform_(self.weights)

    def _score(self, query, key):
        """
        query: FloatTensor (batch_size, num_queries, query_size)
        key: FloatTensor (batch_size, time_step, key_size)
        """
        score = query.matmul(self.weights).matmul(key.transpose(1, 2))
        return score
```

<br>

```python
        score = query.matmul(self.weights).matmul(key.transpose(1, 2))
```

query(aspect_capsule)는 64X1X300, weights는 300X300, key.transpose(primary_capsule)는 64X300X74이므로 차례대로 matrix multiplication을 해주면 64X1X74가 되게 됩니다.

64X1X300 X 300X300 -> 64X1X300

64X1X300 X 64X300X74 -> 64X1X74

➕mamul은 dot과 연산 방식이 조금 다릅니다. [행렬곱 함수 np.matmul 사용법, np.dot과의 차이](https://jimmy-ai.tistory.com/104)

<br>

다시 get_attention_weights 메소드로 돌아와서

```python
        score = self._score(query, key)
```

현재 스코어에는 primary_capsule, <img width="40" alt="스크린샷 2022-05-07 오전 10 52 42" src="https://user-images.githubusercontent.com/76269316/167233319-20fd606c-ca67-4c13-93da-d6fae557ee36.png">(learnable parameter), aspect_capsule을 곱한 값이 들어있습니다.  <img width="77" alt="스크린샷 2022-05-07 오전 10 52 19" src="https://user-images.githubusercontent.com/76269316/167233313-6e46d223-a1cf-434c-9f67-4b654e120d07.png">

<br>

```python
        weights = self._weights_normalize(score, mask)
```

이후 위에 정의된 \_weight\_normalize 메소드를 사용해 softmax를 적용합니다.

```python
    def _weights_normalize(self, score, mask):
        if not mask is None:
            score = score.masked_fill(mask == 0, -constants.INF)  # sentence에서 단어가 없는 부분은 -1e9으로 채움 (64X1X74)
        weights = F.softmax(score, dim=-1)
        return weights
```

![스크린샷 2022-05-06 오후 3 19 59](https://user-images.githubusercontent.com/76269316/167077738-245ef29a-5c69-4fef-b90f-f296813a0c7c.png)

<br>

```python
        weights = F.dropout(weights, p=self.dropout, training=self.training)
        if single_query:
            weights = weights.squeeze(1)  # 차원이 1인 차원을 제거
        return weights
```

이후 dropout을 적용하고, 차원이 1인 차원을 제거한뒤 weights를 리턴합니다.

이 weights가 primary capsule weight <img width="29" alt="스크린샷 2022-05-07 오전 10 53 29" src="https://user-images.githubusercontent.com/76269316/167233345-45ccd7ec-35f2-4c4c-8c94-f983d8358cdd.png" style="zoom:150%;" >가 되게됩니다. 

<br>

##### Capsule Guided Routing

원래 [동적 라우팅 메커니즘 (Sabour et al., 2017)](https://papers.nips.cc/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf)은 라우팅의 반복적인 절차로 인해 훈련과정이 비효율적입니다.

그리고 라우팅 프로세스를 가이드하는데 사용되는 상위 계층의 정보가 없으므로 동적 라우팅이 self-directed 프로세스처럼 동작하는데, 저자는 감성 사전에 대한 prior knowledge를 이용한 효율적인 라우팅 메커니즘을 설계했다고 합니다.

![스크린샷 2022-05-06 오후 3 29 18](https://user-images.githubusercontent.com/76269316/167078821-17a3fc64-0c74-409a-b272-2c5ba8c1c6f2.png)

G는 감성 행렬을 나타내는데 C는 감성 카테고리의 수, d는 임베딩 차원입니다.

이 감성 행렬 G에 squash activation function을 적용해 sentiment capsule Z를 얻습니다. (capsnet 81 line)

![스크린샷 2022-05-06 오후 3 32 06](https://user-images.githubusercontent.com/76269316/167079092-7807becd-02a6-47e2-98f8-5da48e4282cf.png)

이후 primary capsule과 sentiment capsule의 유사도를 계산한 뒤 softmax를 적용해 routing weight w를 계산합니다.

![스크린샷 2022-05-06 오후 3 33 39](https://user-images.githubusercontent.com/76269316/167079269-b1c2f10d-86d8-4bc4-8719-49bba22415d1.png)

<img width="40" alt="스크린샷 2022-05-07 오전 11 01 27" src="https://user-images.githubusercontent.com/76269316/167233544-00abd39a-54d8-4194-a1f7-3ae1946104be.png">은 learnable parameter로 코드에서 guide_weight입니다.

![image](https://user-images.githubusercontent.com/76269316/167304565-ce3da33e-cca1-4cb2-9acd-348cac7225c9.png)

<br>

**capsnet.py**

85 line

```python
        category_capsule = self._capsule_guided_routing(primary_capsule, norm_weight)
```

먼저 92 ~ 100 line에 정의된 \_capsule_guided_routing 메소드를 호출합니다.

```python
    def _capsule_guided_routing(self, primary_capsule, norm_weight):
        guide_capsule = squash(self.guide_capsule)
        guide_matrix = primary_capsule.matmul(self.guide_weight).matmul(guide_capsule.transpose(0, 1))
        guide_matrix = F.softmax(guide_matrix, dim=-1)
        guide_matrix = guide_matrix * norm_weight.unsqueeze(-1) * self.scale  # (batch_size, time_step, num_categories)
        category_capsule = guide_matrix.transpose(1, 2).matmul(primary_capsule)
        category_capsule = F.dropout(category_capsule, p=self.dropout, training=self.training)
        category_capsule = squash(category_capsule)
        return category_capsule
```

<br>

```python
        guide_capsule = squash(self.guide_capsule)
```

guide_capsule은 33 ~ 35 line에 정의돼 있습니다.

```python
    self.guide_capsule = nn.Parameter(
        torch.Tensor(num_categories, capsule_size)  # 3X300
    )
```

guide_capsule과 바로 다음 라인에서 등장하는 guide_weight는 처음에 모델이 생성될 때 \_reset\_parameters()라는 메소드가 호출되며 사비에르 균일분포로 초기화됩니다.

**capsnet.py**

45 ~ 47 line

```python
    def _reset_parameters(self):
        init.xavier_uniform_(self.guide_capsule)
        init.xavier_uniform_(self.guide_weight)
```

<br>

❗️또한 이때 주의할 점이, guide_capsule에는 [sentiment_dict.json](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data/sentiment_dict.json) 파일로 정의한 감성 단어들에 대한 정보가 담겨있습니다.

<br>

[make_recurrent_capsule_network()](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/make_aspect_term_model.py)

마지막 바로 위에 라인을 보면 load_sentiment라는 메소드를 호출하는데,

```python
def make_recurrent_capsule_network(config):
    embedding = make_embedding(config)
    base_path = os.path.join(config['base_path'])
    log_path = os.path.join(base_path, 'log/log.yml')
    log = yaml.safe_load(open(log_path))
    config = config['aspect_term_model'][config['aspect_term_model']['type']]
    model = RecurrentCapsuleNetwork(
        embedding=embedding,
        num_layers=config['num_layers'],
        capsule_size=config['capsule_size'],
        bidirectional=config['bidirectional'],
        dropout=config['dropout'],
        num_categories=log['num_categories']
    )
    model.load_sentiment(os.path.join(base_path, 'processed/sentiment_matrix.npy'))  # load_sentiment 메소드 호출
    return model
```

<br>

**caponst.py**

49 ~ 56 line

```python
    def load_sentiment(self, path):
        sentiment = np.load(path)  # 전처리 과정에서 생성한 sentiment_matirx.npy 파일 로드
        e1 = np.mean(sentiment)  # 평균
        d1 = np.std(sentiment)  # 표준편차
        e2 = 0
        d2 = np.sqrt(2.0 / (sentiment.shape[0] + sentiment.shape[1]))  # shape[0] = 3, shape[1] = 300
        sentiment = (sentiment - e1) / d1 * d2 + e2
        self.guide_capsule.data.copy_(torch.tensor(sentiment))
```

<img width="221" alt="스크린샷 2022-05-07 오후 1 26 02" src="https://user-images.githubusercontent.com/76269316/167238125-bd1014ba-80ac-4f44-a9da-3897d58989ce.png">

다음과 같이 정규화를 거친 sentiment matrix를 guid_capsule로 복사합니다.

<br>

다시 capsnet 코드로 돌아와서 

```python
        guide_capsule = squash(self.guide_capsule)
```

guide_capsule(sentiment matrix)에 squash 함수를 적용한뒤, ![스크린샷 2022-05-07 오후 1 27 33](https://user-images.githubusercontent.com/76269316/167238170-5c0be60b-bdd4-44b4-b9a4-945c9c42acfe.png)

<br>

```python
        guide_matrix = primary_capsule.matmul(self.guide_weight).matmul(guide_capsule.transpose(0, 1))  # 64X74X3
```

primary_capsule(64X74X300), guide_weight(300X300), guide_capsule.transpose(300X3)을 matrix multiplication을 적용해 유사도를 계산합니다. ![스크린샷 2022-05-07 오후 1 28 32](https://user-images.githubusercontent.com/76269316/167238201-2a14d305-da27-45f5-b8e9-847332324a69.png)

guide_weight도 36 ~ 38 line에 정의돼 있습니다.

```python
        self.guide_weight = nn.Parameter(
            torch.Tensor(capsule_size, capsule_size)
        )
```

64X74X300 X 300X300 -> 64X74X300

64X74X300 X 300X3 -> 64X74X3

<br>

95 line

```python
       guide_matrix = F.softmax(guide_matrix, dim=-1)
```

이후 소프트맥스를 적용해 routing weights w를 계산합니다.

![스크린샷 2022-05-07 오후 1 29 21](https://user-images.githubusercontent.com/76269316/167238224-ddc9f360-5c62-4694-8a1b-cf00017792a3.png)

<br>

<br>

### Category Capsule Layer

위에서 구한 aspect aware normalization과 routing weight를 가지고 최종적으로 category capsule V를 계산할 수 있습니다.

수식은 다음과 같습니다.

![스크린샷 2022-05-06 오후 3 35 53](https://user-images.githubusercontent.com/76269316/167079550-f9c4c8a4-5280-4baf-950f-e5dc1d6c3545.png)

s는 learnable scale 파라미터입니다.

<br>

위에 코드와 이어집니다.

**capsnet.py**

96 line

먼저 위에서 구한 guide_matri(routing weights <img src="https://user-images.githubusercontent.com/76269316/167238378-14dc4c32-699d-4eb6-b12d-55700810919f.png" alt="스크린샷 2022-05-07 오후 1 33 47" style="zoom:67%;" />와 primary capsule의 가중치 ![스크린샷 2022-05-07 오후 1 38 33](https://user-images.githubusercontent.com/76269316/167238505-16a994cb-8f93-4004-8f8b-bb80b7c094dd.png), scale 파라미터 s를 곱해줍니다.

```python
        guide_matrix = guide_matrix * norm_weight.unsqueeze(-1) * self.scale
```

guide_matrix(64X74X3), norm_weight.unsqueeze(64X74X1), scale(1) -> 64X74X3

<img width="111" alt="스크린샷 2022-05-07 오후 1 41 42" src="https://user-images.githubusercontent.com/76269316/167238613-e82dda01-76ce-43cd-8232-b1fb02935d0d.png">

*행렬 연산으로 시그마까지 한 번엔 처리*

<br>

capsnet.py

39 line

```python
        self.scale = nn.Parameter(torch.tensor(4.0))
```

<br>

**capsnet.py**

97 line

```python
        category_capsule = guide_matrix.transpose(1, 2).matmul(primary_capsule)
```

이후 위에서 계산한 식에 primary_capsule까지 곱해줍니다.

guide_matrix.transpose(64X3X74) X primary_capsule(64X74X300) ->64X3X300

<img src="https://user-images.githubusercontent.com/76269316/167238673-4360927b-9def-43cd-9e60-ddf426a3357a.png" alt="image" style="zoom:50%;" />

98 ~ 100 line

```python
        category_capsule = F.dropout(category_capsule, p=self.dropout, training=self.training)
        category_capsule = squash(category_capsule)  # 64X3X300
        return category_capsule
```

이후 dropout과 squash 함수를 적용해 최종 category capsule V를 리턴합니다.

![스크린샷 2022-05-07 오후 1 44 49](https://user-images.githubusercontent.com/76269316/167238702-267390a9-29ae-46a0-859a-b5622ca25b8c.png)

<br>

**capsnet.py**

86 ~ 87 line

```python
        category_capsule_norm = torch.sqrt(torch.sum(category_capsule * category_capsule, dim=-1, keepdim=False))
        return category_capsule_norm
```

리턴 받은 V에 다음 연산을 취해 64X3으로 만든 뒤 forward 계산을 종료합니다.

batch_size X 감성 category 개수(긍정, 부정. 중립)로 만들어 문장별 각 감성의 확률을 계산한 것입니다.
