---
title:  "[Capstone] MAMS for ABSA 전처리 코드 분석"
excerpt: "MAMS for ABSA 코드 리뷰"
toc: true
toc_label: "MAMS for ABSA 전처리 코드 분석"
toc_sticky: true
published: true

categories:
  - Capstone
tags:
  - PyTorch
last_modified_at: 2022-04-23

---

> [이전에 리뷰한 논문](https://seominseok4834.github.io/paper%20review/1.a-challenge-dataset-and-effective-models-for-aspect-based-sentiment-analysis/)에서 구현한 모델을 캡스톤 디자인 프로젝트에 적용하기 위해 전처리 부분 코드를 분석한 내용입니다.
>
> 모든 코드는 [MAMS-for-ABSA github](https://github.com/siat-nlp/MAMS-for-ABSA)에서 가져왔습니다.

<br>

### 전체 디렉토리 구조

<details>
<summary>전체 디렉토리 구조</summary>
<div markdown="1">

```bash
.
├── LICENSE
├── README.md
├── config.yml
├── data
│   ├── MAMS-ACSA
│   │   └── raw
│   │       ├── test.xml
│   │       ├── train.xml
│   │       └── val.xml
│   ├── MAMS-ATSA
│   │   ├── log
│   │   │   └── log.yml
│   │   ├── processed
│   │   │   ├── glove.npy
│   │   │   ├── index2word.pickle
│   │   │   ├── sentiment_matrix.npy
│   │   │   ├── test.npz
│   │   │   ├── train.npz
│   │   │   ├── val.npz
│   │   │   └── word2index.pickle
│   │   └── raw
│   │       ├── test.xml
│   │       ├── train.xml
│   │       └── val.xml
│   ├── glove.840B.300d.txt
│   └── sentiment_dict.json
├── data_process
│   ├── __pycache__
│   │   ├── data_process.cpython-39.pyc
│   │   ├── dataset.cpython-39.pyc
│   │   ├── utils.cpython-39.pyc
│   │   └── vocab.cpython-39.pyc
│   ├── data_process.py
│   ├── dataset.py
│   ├── utils.py
│   └── vocab.py
├── preprocess.py
├── src
│   ├── aspect_category_model
│   │   ├── __pycache__
│   │   │   ├── bert_capsnet.cpython-39.pyc
│   │   │   ├── capsnet.cpython-39.pyc
│   │   │   └── recurrent_capsnet.cpython-39.pyc
│   │   ├── bert_capsnet.py
│   │   ├── capsnet.py
│   │   └── recurrent_capsnet.py
│   ├── aspect_term_model
│   │   ├── __pycache__
│   │   │   ├── bert_capsnet.cpython-39.pyc
│   │   │   ├── capsnet.cpython-39.pyc
│   │   │   └── recurrent_capsnet.cpython-39.pyc
│   │   ├── bert_capsnet.py
│   │   ├── capsnet.py
│   │   └── recurrent_capsnet.py
│   └── module
│       ├── attention
│       │   ├── __pycache__
│       │   │   ├── attention.cpython-39.pyc
│       │   │   ├── bilinear_attention.cpython-39.pyc
│       │   │   ├── concat_attention.cpython-39.pyc
│       │   │   ├── dot_attention.cpython-39.pyc
│       │   │   ├── mlp_attention.cpython-39.pyc
│       │   │   ├── scaled_dot_attention.cpython-39.pyc
│       │   │   ├── tanh_bilinear_attention.cpython-39.pyc
│       │   │   └── tanh_concat_attention.cpython-39.pyc
│       │   ├── attention.py
│       │   ├── bilinear_attention.py
│       │   ├── concat_attention.py
│       │   ├── dot_attention.py
│       │   ├── mlp_attention.py
│       │   ├── multi_head_attention.py
│       │   ├── no_query_attention.py
│       │   ├── scaled_dot_attention.py
│       │   ├── tanh_bilinear_attention.py
│       │   └── tanh_concat_attention.py
│       └── utils
│           ├── __pycache__
│           │   ├── constants.cpython-39.pyc
│           │   ├── loss.cpython-39.pyc
│           │   ├── sentence_clip.cpython-39.pyc
│           │   └── squash.cpython-39.pyc
│           ├── constants.py
│           ├── loss.py
│           ├── sentence_clip.py
│           └── squash.py
├── test.py
├── train
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── __init__.cpython-39.pyc
│   │   ├── eval.cpython-39.pyc
│   │   ├── make_aspect_category_model.cpython-39.pyc
│   │   ├── make_aspect_term_model.cpython-39.pyc
│   │   ├── make_data.cpython-39.pyc
│   │   ├── make_optimizer.cpython-39.pyc
│   │   └── train.cpython-39.pyc
│   ├── eval.py
│   ├── make_aspect_category_model.py
│   ├── make_aspect_term_model.py
│   ├── make_data.py
│   ├── make_optimizer.py
│   ├── test.py
│   └── train.py
└── train.py
```

</div>
</details>

<br>

![스크린샷 2022-04-23 오전 11 36 13](https://user-images.githubusercontent.com/76269316/164867294-7942b1dc-8efd-4101-9a3a-f9021db304b0.png)

Quick Start 가이드처럼 /preprocess.py를 실행하면 configuration 파일(환경 설정 파일)을 로드한 뒤 모든 전처리 함수를 호출하는 /data_process/data_process.py를 실행합니다.

**[preprocess.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/preprocess.py)**

```python
import yaml
from data_process.data_process import data_process

config = yaml.safe_load(open('config.yml'))

data_process(config)
```

<br>

각 기능들이 캡슐화 돼 있어서 해당 코드를 찾기 위해 다른 파일로 가야하는 경우가 잦습니다.

모든 전처리 함수는 /data_process/data_process.py를 통해 호출되기 때문에 이를 **[/data_process/data_process.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/data_process.py)** 하늘색으로 표시하고 모든 코드의 라인 번호를 같이 표기했습니다. 

<br>

<span style="color:#2EBEF8">**/data_process/data_process.py**</span>

1 ~ 22 line

```python
import os
import numpy as np
import pickle
import yaml
from data_process.utils import *

def data_process(config):
  	# /config.yml 파일의 설정 로드
    mode = config['mode']  # term
    assert mode in ('term', 'category')  # mode가 term이나 category가 아니면 AssertError 발생
    base_path = config['base_path']  # /data/MAMS-ATSA
    # 데이터셋 경로 로드
    raw_train_path = os.path.join(base_path, 'raw/train.xml')  # /data/MAMS-ATSA/raw/train.xml
    raw_val_path = os.path.join(base_path, 'raw/val.xml')  # /data/MAMS-ATSA/raw.xml
    raw_test_path = os.path.join(base_path, 'raw/test.xml')  # /data/MAMS-ATSA/raw/test.xml
    lowercase = config['lowercase']  # True
    if mode == 'term':  # mode가 term으로 설정돼 있기 때문에 if문 실행 (category로 바꿀 경우 else문 실행)
    		# /data_process/utils.py에 있는 parse_sentence_term 메소드 실행
        train_data = parse_sentence_term(raw_train_path, lowercase=lowercase)
        val_data = parse_sentence_term(raw_val_path, lowercase=lowercase)
        test_data = parse_sentence_term(raw_test_path, lowercase=lowercase)
    else:
        train_data = parse_sentence_category(raw_train_path, lowercase=lowercase)
        val_data = parse_sentence_category(raw_val_path, lowercase=lowercase)
        test_data = parse_sentence_category(raw_test_path, lowercase=lowercase)
```

data_process 메소드를 통해 캡슐화 돼 있는 전처리 함수들이 모두 호출돼 전처리가 실행됩니다.

논문에서 aspect-category sentiment analysis(ACSA), aspect-term sentiment analysis(ATSA)을 각각 수행했기 때문에 이를 mode로 설정하여 term이면 ATSA를, category이면 ACSA를 수행하도록 했습니다.

이후 데이터셋 경로를 지정한 다음 ./data_process/utils.py에 있는 parse_sentence_term 메소드를 실행합니다.

제가 ATSA를 수행할 예정이므로 term을 기준으로 설명하겠습니다.

<br>

### split_char로 데이터셋 가공

**[/data_process/utils.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/utils.py)**

parse_sentence_term()

50 ~ 70 line

```python
from xml.etree.ElementTree import parse

def parse_sentence_term(path, lowercase=False):
    tree = parse(path)  # 데이터셋이 xml 파일로 돼 있기 때문에 ElementTree로 parsing
    sentences = tree.getroot()
    data = []
    split_char = '__split__'
    for sentence in sentences:
        text = sentence.find('text')
        if text is None:
            continue
        text = text.text
        if lowercase:  # lowercase가 True인 경우 리뷰 내용을 소문자로 변경함
            text = text.lower()
        aspectTerms = sentence.find('aspectTerms')
        if aspectTerms is None:
            continue
        for aspectTerm in aspectTerms:
            term = aspectTerm.get('term')
            if lowercase:
                term = term.lower()
            polarity = aspectTerm.get('polarity')
            start = aspectTerm.get('from')
            end = aspectTerm.get('to')
            piece = text + split_char + term + split_char + polarity + split_char + start + split_char + end
            data.append(piece)
    return data
```

parse_sentence_term 메소드는 xml 파일을 읽어 다음과 같이 변경합니다.(리뷰\_\_split\_\_\_\_split\_\_감정\_\_split\_\_시작\_\_split\_\_끝)

```xml
	<sentence>
		<text>The decor is not special at all but their food and amazing prices make up for it.</text>
		<aspectTerms>
			<aspectTerm from="4" polarity="negative" term="decor" to="9"/>
			<aspectTerm from="42" polarity="positive" term="food" to="46"/>
			<aspectTerm from="59" polarity="positive" term="prices" to="65"/>
		</aspectTerms>
	</sentence>
```

<center>⬇️ </center>

The decor is not special at all but their food and amazing prices make up for it.\_\_split\_\_decor\_\_split\_\_negative\_\_split\_\_4\_\_split\_\_9

The decor is not special at all but their food and amazing prices make up for it.\_\_split\_\_food\_\_split\_\_positive\_\_split\_\_42\_\_split\_\_59

The decor is not special at all but their food and amazing prices make up for it.\_\_split\_\_prices\_\_split\_\_positive\_\_split\_\_59\_\_split\_\_65

모든 데이터를 split_char로 구분한 뒤 리스트에 담아 리턴합니다.

<br>

<span style="color:#2EBEF8">**/data_process/data_process.py**</span>

23 ~ 26 line

다시 data_process.py로 돌아와서 남은 코드를 보겠습니다.

```python
    remove_list = ['conflict'] 
    train_data = category_filter(train_data, remove_list)
    val_data = category_filter(val_data, remove_list)
    test_data = category_filter(test_data, remove_list)
```

라벨링된 데이터셋에는 positive, negative, neutral, conflict 네 개의 감성이 있는데, 이 중 conflict를 제외한 감정만 사용하기 위해 conflict로 라벨링된 데이터를 제거합니다.

category_filter 메소드는 /data_proces/utils.py에 있습니다.

<br>

**[/data_process/utils.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/utils.py)**

category_filter()

72 ~ 78 line

```python
def category_filter(data, remove_list):
    remove_set = set(remove_list)
    filtered_data = []
    for text in data:
        if not text.split('__split__')[2] in remove_set:
            filtered_data.append(text)
    return filtered_data
```

**리뷰\_\_split\_\_\_\_split\_\_감정\_\_split\_\_시작\_\_split\_\_끝**으로 구성돼 있는 데이터를 split_char로 다시 나눈 뒤 2번째 인덱스, 감정이 conflict가 아닌 리뷰들만 다시 filtered_data에 append 해서 리턴합니다.

<br>

### word2index, index2word 생성

<span style="color:#2EBEF8">**/data_process/data_process.py**</span>

27 line

```python
    word2index, index2word = build_vocab(train_data, max_size=config['max_vocab_size'], min_freq=config['min_vocab_freq'])  # config['max_vocab_size']: None, config['min_vocab_freq']: 0
```

이번엔 /data_process/utils.py의 build_vocab 메소드를 호출합니다.

<br>

**[/data_process/utils.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/utils.py)**

build_vocab()

80 ~ 88 line

```python
from data_process.vocab import Vocab

def build_vocab(data, max_size, min_freq):  # max_size='None', min_freq=0
    if max_size == 'None':
        max_size = None
    vocab = Vocab()  # /data_process/vocab.py의 Vocab 인스턴스 생성
    for piece in data:
        text = piece.split('__split__')[0]
        text = tokenizer(text)  # tokenizing
        vocab.add_list(text)
    return vocab.get_vocab(max_size=max_size, min_freq=min_freq)
```

<br>

Vocab 클래스는 /data_process/vocab.py에 정의돼 있습니다.

**[/data_process/vocab.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/vocab.py)**

```python
import operator
from src.module.utils.constants import PAD, UNK, ASPECT

class Vocab(object):

    def __init__(self):
        self._count_dict = dict()
        self._predefined_list = [PAD, UNK, ASPECT]
```

vocab 인스턴스를 생성한 뒤 위에서 만든 데이터셋(리뷰\_\_split\_\_\_\_split\_\_감정\_\_split\_\_시작\_\_split\_\_끝)을 순회하며  text 부분을 tokenizing 합니다.

<br>

**[/data_process/utils.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/utils.py)**

tokenizer()

12 ~ 20 line

❗️수정해야 하는 부분: tokenizer를 한국어를 지원하는 걸로 바꿔야 한다.

```python
import spacy
import re

url = re.compile('(<url>.*</url>)')
spacy_en = spacy.load('en')

def check(x):
    return len(x) >= 1 and not x.isspace()  # 길이가 1 이상이고 공백이 아닌 문자열만 True
  
def tokenizer(text):
    tokens = [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]
    return list(filter(check, tokens))  # filter(조건 함수, 순회 가능한 데이터) -> 조건을 만족하는 데이터만 반환
```

tokenizing은 spaCy를 사용했습니다.

먼저 url = re.compile은 정규식 패턴을 찾는건 컴파일 과정을 거치기 때문에 패턴 매치를 반복적으로 사용할 경우 성능상의 부담이 있습니다.  [Gorio Learning re.compile](https://greeksharifa.github.io/%EC%A0%95%EA%B7%9C%ED%91%9C%ED%98%84%EC%8B%9D(re)/2018/08/04/regex-usage-05-intermediate/)

그래서 정규식 패턴을 반복적으로 사용할 경우 re.compile을 통해 미리 컴파일 해두고 저장해서 사용합니다.

url.sub('@URL@', text)은 text(review)에 \(\<url>.*\</url>)과 매치되는 문자를 @URL@로 변경합니다. 패턴 매치를 잘 모르시는 분들은 여기로 ▶️ [파이썬 정규표현식(re) 사용법](https://greeksharifa.github.io/%EC%A0%95%EA%B7%9C%ED%91%9C%ED%98%84%EC%8B%9D(re)/2018/07/20/regex-usage-01-basic/)

저자가 사용한 데이터셋에 \<url> 태그로 묶여있는 리뷰가 있는 경우 안에 텍스트 부분만 추출합니다.

![스크린샷 2022-04-23 오후 2 20 18](https://user-images.githubusercontent.com/76269316/164878939-7e597d39-4b6d-46ca-869d-032dc30ee28e.png)

이후 space tokenizer를 이용해 다음과 같이 토큰화합니다.

![스크린샷 2022-04-23 오후 2 19 55](https://user-images.githubusercontent.com/76269316/164878932-c7fb212f-d3dd-4d34-914e-e12a6ed21f21.png)

<br>

이후 토큰을 vocab을 _count_dict에 추가합니다.

**[/data_process/utils.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/utils.py)**

build_vocab()

```python
        vocab.add_list(text)
    return vocab.get_vocab(max_size=max_size, min_freq=min_freq)
```

<br>

**[/data_process/vocab.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/vocab.py)**

```python
import operator
from src.module.utils.constants import PAD, UNK, ASPECT  # -> /src/module/utils/constants.py에 저장돼 있음 

class Vocab(object):

    def __init__(self):
        self._count_dict = dict()
        self._predefined_list = [PAD, UNK, ASPECT]
        
		def add(self, word):
        if word in self._count_dict:  # _count_dict에 있는 문자는 개수를 1 증가
            self._count_dict[word] += 1
        else:  # _count_dict에 없는 문자 (처음 나온 문자)는 개수를 1로 설정
            self._count_dict[word] = 1

    def add_list(self, words):
        for word in words:
            self.add(word)
            
    def get_vocab(self, max_size=None, min_freq=0):
        sorted_words = sorted(self._count_dict.items(), key=operator.itemgetter(1), reverse=True)  # key=operator.itemgetter(1) -> 빈도수를 기준으로 정렬, reverse=True -> 내림차순 정렬
        word2index = {}
        for word in self._predefined_list:  # '<pad'>: 0, '<unk>': 1, '<aspect>': 2로 인덱스 설정
            word2index[word] = len(word2index)
        for word, freq in sorted_words:
            if word in word2index:  # word2index에 이미 저장돼 있는 경우 continue
                continue
            if (max_size is not None and len(word2index) >= max_size) or freq < min_freq:  # max_size를 초과한 경우 또는 단어의 등장 빈도가 min_freq 보다 낮은 경우 해당 단어의 인덱스를 1(UNK)로 부여
                word2index[word] = word2index[UNK]
            else:  # 그렇지 않은 경우 3번부터 인덱스 번호 부여
                word2index[word] = len(word2index)
        index2word = {}
        index2word[word2index[UNK]] = UNK  # index2word[1] = '<unk>' 저장
        for word, index in word2index.items():
            if index == word2index[UNK]:  # 인덱스가 1인 경우 continue (index2word에 저장 안함)
                continue
            else:
                index2word[index] = word
        return word2index, index2word
```

_count_dict는 단어를 key로, 빈도수를 value로 하는 dictionary가 되게 됩니다.

이후 get_vocb 메소드를 통해 word2index, index2word를 리턴합니다.

<br>

<span style="color:#2EBEF8">**/data_process/data_process.py**</span>

다시 28 line으로 돌아와서 /data/MAMS-ATSA/processed 디렉토리가 없는 경우 생성한 뒤  직전에 만들었던 word2index와 train/val/test 데이터를 numpy array zip 파일로 저장합니다.

```python
 if not os.path.exists(os.path.join(base_path, 'processed')):  # /data/MAMS-ATSA/processed 디렉토리가 없는 경우 생성
        os.makedirs(os.path.join(base_path, 'processed'))
    if mode == 'term':
        save_term_data(train_data, word2index, os.path.join(base_path, 'processed/train.npz'))
        save_term_data(val_data, word2index, os.path.join(base_path, 'processed/val.npz'))
        save_term_data(test_data, word2index, os.path.join(base_path, 'processed/test.npz'))
    else:
        save_category_data(train_data, word2index, os.path.join(base_path, 'processed/train.npz'))
        save_category_data(val_data, word2index, os.path.join(base_path, 'processed/val.npz'))
        save_category_data(test_data, word2index, os.path.join(base_path, 'processed/test.npz'))
```

<br>

### 전처리 데이터 저장

**[/data_process/utils.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/utils.py)**

save_term_data()

코드가 기니까 가공하는 부분이랑, 저장하는 부분을 나눠서 보겠습니다.

90 ~ 126 line

❗️수정해야 하는 부분: assert text[start: end] == term 우리는 term을 감상포인트로 라벨링 했기 때문에 이 부분을 주석처리 해야 한다. (모델 입력 부분에서도 변경해야 하는데 이는 훈련 코드 설명할 때 표기하겠습니다.)

```python
def save_term_data(data, word2index, path):
    dirname = os.path.dirname(path)
    if not os.path.exists(dirname):  # /data/MAMS-ATSA/processed가 있는지 확인
        os.makedirs(dirname)
    sentence = []
    aspect = []
    label = []
    context = []
    bert_token = []
    bert_segment = []
    td_left = []
    td_right = []
    f = lambda x: word2index[x] if x in word2index else word2index[UNK]  # word2index에 x(word)가 저장돼 있으면 인덱스를 리턴, 아니면 1 (unknown) 리턴
    g = lambda x: list(map(f, tokenizer(x)))  # 리뷰를 tokenizing한 다음 lambda 함수 f 실행
    d = {
        'positive': 0,
        'negative': 1,
        'neutral': 2,
        'conflict': 3
    }
    for piece in data:
        text, term, polarity, start, end = piece.split('__split__')  # split_char로 분할
        start, end = int(start), int(end)  # from 인덱스, to 인덱스를 int형으로 형변환
        assert text[start: end] == term  # 문자열 slicing을 통해 추출한 term과 같은지 확인
        sentence.append(g(text))  # 인덱스로 변환한 리스트
        aspect.append(g(term))  # 인덱스로 변환한 리스트
        label.append(d[polarity])  # 감성을 인덱스로 변환
        left_part = g(text[:start])  # term 바로 앞의 단어까지 slicing한 다음 index로 변환
        right_part = g(text[end:])  # term 맨 끝 단어를 포함해 리뷰 끝까지 slicing한 다음 index로 변환
        context.append(left_part + [ASPECT_INDEX] + right_part)  # left_part + 2 + right_part (ASPECT_INDEX는 /src/module/utils/constants.py에 저장돼 있음)
        bert_sentence = bert_tokenizer.tokenize(text)  # bert_tokenizer로 리뷰 tokenizing
        bert_aspect = bert_tokenizer.tokenize(term)  # bert_tokenizing으로 측면 tokenizing
        bert_token.append(bert_tokenizer.convert_tokens_to_ids(['[CLS]'] + bert_sentence + ['[SEP]'] + bert_aspect + ['[SEP]']))  # bert_tokenizer로 토큰화한 토큰들을 index화 (CLS, SEP는 각각 문장의 시작, 끝을 알리는 special token임)
        bert_segment.append([0] * (len(bert_sentence) + 2) + [1] * (len(bert_aspect) + 1))  # bert_sentence의 길이 * 2만큼 0으로 채우고, bert_aspect의 길이 + 1만큼 1로 채움
        td_left.append(g(text[:end]))  # 처음 ~ term까지 (['The', 'decor']) 
        td_right.append(g(text[start:])[::-1])  # term ~ 끝까지 (['decor', 'is', 'not' ···]) 저장하고 역순으로 뒤집음
        assert len(bert_token[-1]) == len(bert_segment[-1])
```

먼저 가공하는 부분을 보면, The decor is not special at all but their food and amazing prices make up for it.\_\_split\_\_decor\_\_split\_\_negative\_\_split\_\_4\_\_split\_\_9 이렇게 저장된 데이터를 split_char를 기준으로 스플릿합니다.

start와 end에 4와 9를 저장하고,

sentence에는 The decor is not special at all but their food and amazing prices make up for it를 tokenizing한 다음 index로 바꿔 저장합니다.

aspect에는 decor를 index로 변환해 넣어줍니다.

label은 긍정이면 0, 부정이면 1, 중립이면 2로 변환해주고,  left_part에는 term(**decor**) 앞에 있는 단어 The를 저장하고, right_part에는 term(**decor**) 뒤에 있는 단어들 is, not, special, ···, it, . 을 저장합니다.

이후 left_part + 2 + right_part 형태로 context로 저장하고 bert_tokenizer를 이용해 리뷰를 토큰화한 뒤 bert_sentence로 저장합니다.

bert_aspect도 마찬가지로 term을 토큰화한 뒤 저장합니다.

bert_segment에는 bert_sentence의 길이 * 2만큼 0으로 채우고, bert_aspect의 길이 + 1만큼 1로 채웁니다.

마지막으로 td_left는 처음 ~ term까지 즉, The, decor를 td_left로 td_right는 term ~ 끝까지 decor, is, not, ···, it, .을 td_right로 저장한 뒤 마지막으로 변환한 bert_token과 sentence의 길이가 같은지 확인합니다.

<br>

이후에는 각 리스트 요소들의 최대 길이를 찾아서 shape을 맞춰주기 위해 최대 길이만큼 확장합니다.

그런 다음, numpy array로 변환해 np.savez로 저장합니다.

127 ~ 151 line

```python
    # 각 리스트 요소들의 최대 길이를 찾음
  	max_length = lambda x: max([len(y) for y in x])
    sentence_max_len = max_length(sentence) 
    aspect_max_len = max_length(aspect)
    context_max_len = max_length(context)
    bert_max_len = max_length(bert_token)
    td_left_max_len = max_length(td_left)
    td_right_max_len = max_length(td_right)
    num = len(data)
    # shape을 맞춰주기 위해 모든 요소들의 길이를 확장
    for i in range(num):
        sentence[i].extend([0] * (sentence_max_len - len(sentence[i])))
        aspect[i].extend([0] * (aspect_max_len - len(aspect[i])))
        context[i].extend([0] * (context_max_len - len(context[i])))
        bert_token[i].extend([0] * (bert_max_len - len(bert_token[i])))
        bert_segment[i].extend([0] * (bert_max_len - len(bert_segment[i])))
        td_left[i].extend([0] * (td_left_max_len - len(td_left[i])))
        td_right[i].extend([0] * (td_right_max_len - len(td_right[i])))
    # numpy array로 변환
    sentence = np.asarray(sentence, dtype=np.int32)
    aspect = np.asarray(aspect, dtype=np.int32)
    label = np.asarray(label, dtype=np.int32)
    context = np.asarray(context, dtype=np.int32)
    bert_token = np.asarray(bert_token, dtype=np.int32)
    bert_segment = np.asarray(bert_segment, dtype=np.int32)
    td_left = np.asarray(td_left, dtype=np.int32)
    td_right = np.asarray(td_right, dtype=np.int32)
    # path에 numpy array들을 저장
    np.savez(path, sentence=sentence, aspect=aspect, label=label, context=context, bert_token=bert_token, bert_segment=bert_segment,
             td_left=td_left, td_right=td_right)
```

<br>

### glove 단어 임베딩 + 감성 사전

<span style="color:#2EBEF8">**/data_process/data_process.py**</span>

38 ~ 39 line

```python
    glove = load_glove(config['glove_path'], len(index2word), word2index)  # glove_path: ./data/glove.840B.300d.txt
    sentiment_matrix = load_sentiment_matrix(config['glove_path'], config['sentiment_path'])  # sentiment_path: ./data/sentiment_dict.json
```

<br>

스탠포드 연구팀에서 만든 300차원의 단어 임베딩을 로드해서 train 데이터셋에 있는 단어들로 만든 word2index의 key 값으로 있는 경우 해당 벡터를 저장하고, sentiment_dict.json 파일에 저장된 감성 단어들도 마찬가지로 glove 단어 임베딩에 있는 경우 해당 벡터를 sentiment_matrix에 저장해준다.



<p align = "center">
  <img src = "https://user-images.githubusercontent.com/76269316/164893523-e3e0c2ad-24ed-453a-b2d3-d81bf5157370.png">
</p>
<p align = "center">
  glove.840B.300d.txt
</p>

<br>

<p align = "center">
  <img src = "https://user-images.githubusercontent.com/76269316/164893860-d51feb11-ea3d-4138-a790-35b496f29a76.png">
</p>
<p align = "center">
  sentiment_dict.json
</p>

<br>

**[/data_process/utils.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/utils.py)**

load_glove()

238 ~ 248 line

```python
def load_glove(path, vocab_size, word2index):
    if not os.path.isfile(path):
        raise IOError('Not a file', path)
    glove = np.random.uniform(-0.01, 0.01, [vocab_size, 300])  # len(index2word), 300 크기의 균등 분포 생성
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            content = line.split(' ')
            if content[0] in word2index:  # content[0]은 단어, 단어 임베딩에 있는 단어가 train 데이터셋으로 만든 word2index에 key값으로 있는 경우 실행
                glove[word2index[content[0]]] = np.array(list(map(float, content[1:])))  # 단어 임베딩을 float type으로 변환한 뒤 numpy array로 저장
    glove[PAD_INDEX, :] = 0  # PAD_INDEX=0 (/src/module/utils/constants.py에 저장돼 있음)
    return glove
```

250 ~ 269 line

```python
def load_sentiment_matrix(glove_path, sentiment_path):
    sentiment_matrix = np.zeros((3, 300), dtype=np.float32)  # 감정이 3개라 3x300으로 만듦
    sd = json.load(open(sentiment_path, 'r', encoding='utf-8'))
    sd['positive'] = set(sd['positive'])
    sd['negative'] = set(sd['negative'])
    sd['neutral'] = set(sd['neutral'])
    with open(glove_path, 'r', encoding='utf-8') as f:
        for line in f:
            content = line.split(' ')
            word = content[0]  # content[0]은 단어
            vec = np.array(list(map(float, content[1:])))
            # 감성 단어가 glove 단어 임베딩에 존재하는 경우 해당 감성 행에 저장
            if word in sd['positive']:
                sentiment_matrix[0] += vec
            elif word in sd['negative']:
                sentiment_matrix[1] += vec
            elif word in sd['neutral']:
                sentiment_matrix[2] += vec
    # 정규화
    sentiment_matrix -= sentiment_matrix.mean()
    sentiment_matrix = sentiment_matrix / sentiment_matrix.std() * np.sqrt(2.0 / (300.0 + 3.0))
    return sentiment_matrix
```

<br>

### 단어 임베딩 파일 및 로그 파일 저장

<span style="color:#2EBEF8">**/data_process/data_process.py**</span>

40 ~ 58 line

이후 glove와 sentiment_matrix를 npy 파일로 저장한 뒤, word2index와 index2word를 pickle 파일로 저장합니다.

```python
    np.save(os.path.join(base_path, 'processed/glove.npy'), glove)
    np.save(os.path.join(base_path, 'processed/sentiment_matrix.npy'), sentiment_matrix)
    with open(os.path.join(base_path, 'processed/word2index.pickle'), 'wb') as handle:
        pickle.dump(word2index, handle)
    with open(os.path.join(base_path, 'processed/index2word.pickle'), 'wb') as handle:
        pickle.dump(index2word, handle)
    analyze = analyze_term if mode == 'term' else analyze_category
    log = {
        'vocab_size': len(index2word),
        'oov_size': len(word2index) - len(index2word),
        'train_data': analyze(train_data),
        'val_data': analyze(val_data),
        'test_data': analyze(test_data),
        'num_categories': 3
    }
    if not os.path.exists(os.path.join(base_path, 'log')):
        os.makedirs(os.path.join(base_path, 'log'))
    with open(os.path.join(base_path, 'log/log.yml'), 'w') as handle:
        yaml.safe_dump(log, handle, encoding='utf-8', allow_unicode=True, default_flow_style=False)
```

그런 다음 로그 파일을 /data/MAMS-ATSA/log 디렉토리에 저장한 뒤 전처리를 종료합니다.

<br>

만약 한글 데이터셋을 적용하고 싶다면 ❗️로 표시해 둔 코드를 수정해야 합니다.
