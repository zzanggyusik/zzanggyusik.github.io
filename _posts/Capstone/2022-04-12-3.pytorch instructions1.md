---
title:  "[Capstone] PyTorch 정리 1"
excerpt: "PyTorch 정리"
toc: true
toc_label: "PyTorch 정리"
toc_sticky: true
published: true

categories:
  - Capstone
tags:
  - PyTorch
last_modified_at: 2022-04-12

---

> 본 내용은 [PyTorch Quickstart Tutorial](https://tutorials.pytorch.kr/beginner/basics/quickstart_tutorial.html)을 참고하여 작성한 내용입니다.

<br>

파이토치에는 데이터 세트를 다루는 요소가 두 가지 있는데 바로 **DataLoader**, **Dataset**입니다.

### Dataset

Dataset는 이름처럼 피처와 레이블이 저장돼 있습니다.

아래 코드는 [이전에 리뷰한 논문](https://seominseok4834.github.io/paper%20review/1.a-challenge-dataset-and-effective-models-for-aspect-based-sentiment-analysis/)에서 정의한 ABSADataset인데, pytorch의 Dataset 클래스를 상속받아 구현한 클래스입니다.

이렇게 자신의 모델에서 사용할 데이터셋 인스턴스를 생성해주는 클래스는 pytorch의 Dataset 클래스를 상속하는 방식으로 구현해야 합니다.

생성자(또는 초기화 (\__init__)) 메소드를 보면 path의 npz(numpy zip file) 파일을 로드해서 멤버 변수 data로 저장한 것을 확인할 수 있습니다.

getitem, len 메소드는 밑에 DataLoader에서 설명하도록 하겠습니다.

**ABSADataset**

```python
class ABSADataset(Dataset):

    def __init__(self, path, input_list):
        super(ABSADataset, self).__init__()
        data = np.load(path)
        self.data = {}
        for key, value in data.items():
            self.data[key] = torch.tensor(value).long()
        self.len = self.data['label'].size(0)
        self.input_list = input_list

    def __getitem__(self, index):
        return_value = []
        for input in self.input_list:
            return_value.append(self.data[input][index])
        return_value.append(self.data['label'][index])
        return return_value

    def __len__(self):
        return self.len
```

[출처: MAMS-for-ABSA/data_process/dataset.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/dataset.py)

<br>

### DataLoader

모델을 훈련할 때 훈련 데이터를 전부 한꺼번에 넣을 경우, 모든 데이터에 대해 gradient를 계산하는 과정에서 많은 메모리를 사용하고 이로인해 학습 시간도 증가하기 때문에 배치 사이즈만큼 데이터를 뽑아서 사용하게 됩니다.

> 1. epoch: 모든 데이터에 대해서 forward와 backward pass를 진행한 횟수
> 2. batch_size: forward와 backward를 한 번에 얼만큼의 데이터씩 진행할 것인지 크기를 의미
> 3. iteration: batch_size만큼 몇 번 forward, backward를 진행했는지 횟수
>
> data size = batch_size * iteration

*epoch, batch_size, iteration에 관한 개념은 [jinyo의 뇌](https://wingnim.tistory.com/33) 참고*

수십만, 수백만개의 데이터세트에서 배치사이즈만큼의 데이터를 뽑기 위해 단순하게 foreach 방식으로 데이터를 꺼낼 수도 있지만, 이렇게 할 경우 데이터를 모집단(데이터 세트)을 잘 대표하는 표본 데이터를 뽑을 수 없게 됩니다.

이렇게 데이터를 배치 사이즈만큼 반복적으로 추출해야 할 때 사용하는게 DataLoader입니다.

**DataLoader**

```python
    train_loader = DataLoader(
        dataset=train_data,
        batch_size=config['batch_size'],
        shuffle=True,
        pin_memory=True
    )
    val_loader = DataLoader(
        dataset=val_data,
        batch_size=config['batch_size'],
        shuffle=False,
        pin_memory=True
    )
```

[출처: MAMS-for-ABSA/train/make_data.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/make_data.py)

DataLoader를 생성할 때 저렇게 데이터 세트와, 배치 사이즈, 셔플 여부(데이터 섞기)등을 인자로 주고 생성하면

```python
for i, data in enumerate(train_loader):
```

 이렇게 iterable 객체를 반환해줘서 간단하게 배치 사이즈만큼 데이터를 추출할 수 있습니다.

*DataLoader 파라미터에 대한 자세한 내용은 [Hello Subinium!](https://subinium.github.io/pytorch-dataloader/) 참고*

<br>

**ABSADataset**

Dataset를 상속받는 클래스는 getitem, len 메소드를 구현해야 하는데 getitem 메소드는 DataLoader에서 배치사이즈만큼 데이터를 꺼낼 때 어떤 값들을 리턴할지 결정하는 함수입니다.

아래 예제 코드에서는 context, aspect, label을 리턴하도록 작성됐습니다.

len 메소드는 데이터의 개수를 리턴하는 함수로 몇 번의 iteration을 돌아야하는지 결정할 때 사용됩니다.

```python
    def __getitem__(self, index):
        return_value = []
        for input in self.input_list:
            return_value.append(self.data[input][index])
        return_value.append(self.data['label'][index])
        return return_value

    def __len__(self):
        return self.len
```

[출처: MAMS-for-ABSA/data_process/dataset.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/dataset.py)

<br>

### 신경망 모델 만들기

```python
from torch import nn
```

torch.nn 네임스페이스는 파이토치의 모든 모듈의 상위 클래스로 신경망을 구성하는데 필요한 모든 구성 요소를 제공합니다.

##### 클래스 정의하기

파이토치에서 신경망 모델은 nn.Module을 상속받는 클래스를 정의해서 생성합니다.

\__init__ 메소드에서 신경망 계층(layer)들을 정의하고 forward 함수에서 신경망에 데이터를 어떻게 처리할지 구현합니다.

```python
class CapsuleNetwork(nn.Module):

    def __init__(self, embedding, hidden_size, capsule_size, dropout, num_categories):
        super(CapsuleNetwork, self).__init__()
        self.embedding = embedding
        embed_size = embedding.embedding_dim
        self.capsule_size = capsule_size
        self.aspect_transform = nn.Sequential(
            nn.Linear(embed_size, capsule_size),
            nn.Dropout(dropout)
        )
        self.sentence_transform = nn.Sequential(
            nn.Linear(hidden_size, capsule_size),
            nn.Dropout(dropout)
        )
        self.norm_attention = BilinearAttention(capsule_size, capsule_size)
        self.guide_capsule = nn.Parameter(
            torch.Tensor(num_categories, capsule_size)
        )
        self.guide_weight = nn.Parameter(
            torch.Tensor(capsule_size, capsule_size)
        )
        self.scale = nn.Parameter(torch.tensor(4.0))
        self.capsule_projection = nn.Linear(capsule_size, capsule_size * num_categories)
        self.dropout = dropout
        self.num_categories = num_categories
        self._reset_parameters()
        
        def forward(self, sentence, aspect):
        # get lengths and masks
        sentence = sentence_clip(sentence)
        aspect = sentence_clip(aspect)
        sentence_mask = (sentence != PAD_INDEX)
        aspect_mask = (aspect != PAD_INDEX)
        # sentence_lens = sentence_mask.long().sum(dim=1, keepdim=True)
        aspect_lens = aspect_mask.long().sum(dim=1, keepdim=True)
        # embedding
        sentence = self.embedding(sentence)
        sentence = F.dropout(sentence, p=self.dropout, training=self.training)
        aspect = self.embedding(aspect)
        aspect = F.dropout(aspect, p=self.dropout, training=self.training)
        # aspect average pooling
        aspect = aspect.masked_fill(aspect_mask.unsqueeze(-1) == 0, 0)
        aspect = aspect.sum(dim=1, keepdim=False) / aspect_lens.float()
        # sentence encode layer
        sentence = self._sentence_encode(sentence, aspect)
        # primary capsule layer
        sentence = self.sentence_transform(sentence)
        primary_capsule = squash(sentence, dim=-1)
        # aspect capsule layer
        aspect = self.aspect_transform(aspect)
        aspect_capsule = squash(aspect, dim=-1)
        # aspect aware normalization
        norm_weight = self.norm_attention.get_attention_weights(aspect_capsule, primary_capsule, sentence_mask)
        # capsule guided routing
        category_capsule = self._capsule_guided_routing(primary_capsule, norm_weight)
        category_capsule_norm = torch.sqrt(torch.sum(category_capsule * category_capsule, dim=-1, keepdim=False))
        return category_capsule_norm
```

[출처: MAMS-for-ABSA/src/aspect_term_model/capsnet.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/src/aspect_term_model/capsnet.py)

코드를 전부 이해하려고 하실 필요는 없고, 그냥 \__init__ 메소드에서 신경망 계층(layer)들을 정의하고 forward 함수를 통해 데이터를 계산한다는 것만 아시면 됩니다.

<br>

```python
for i, data in enumerate(train_loader):
  global_step += 1
  model.train()  # 모델을 train mode로 변경
  input0, input1, label = data  # aspect, term, label
  input0, input1, label = input0.cuda(), input1.cuda(), label.cuda()  # 데이터를 gpu로 미리 올림
  optimizer.zero_grad()  # gradient 계산을 위해 0으로 초기화
  logit = model(input0, input1)  # 모델에 입력 데이터를 전달
```

[출처: MAMS-for-ABSA/train/train.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/train.py)

위 코드처럼 입력 데이터를 전달받은 모델은 일부 백그라운드 연산과 함께 forward 함수를 실행합니다.

❗️ model.forward()를 직접 호출하면 안됨

<br>

```python
pred = logit.argmax(dim=1)
```

입력 데이터에 대한 연산이 끝나면 모델은 각 클래스에 대한 원시(raw) 예측값을 갖는 n차원 텐서를 반환합니다.

이 값을 softmax 혹은 위 코드처럼 argmax를 통과시켜 예측 클래스를 얻을 수 있습니다.

<br>

##### 모델 계층

레이어 별로 어떤 역할을 하는지 간단하게 알아보기 위해 파이토치 공식 문서에서 사용하는 FashionMNIST  데이터 세트를 기준으로 설명하겠습니다.

입력 데이터크기는 다음과 같습니다. (3은 배치 사이즈)

```python
input_image = torch.rand(3,28,28)
print(input_image.size())
```

![스크린샷 2022-04-12 오후 10 03 02](https://user-images.githubusercontent.com/76269316/162968521-0f6a0822-4f51-4018-a474-61fb020a959c.png)

<br>

1.[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)

n차원의 데이터를 평평하게 만드는 층입니다. 28x28의 2D 이미지를 784개의 픽셀값을 갖는 연속된 배열로 변환했습니다. (dim=0의 미니배치 차원은 그대로 유지됩니다)

```python
layer1 = nn.Linear(in_features=28*28, out_features=20)
hidden1 = layer1(flat_image)
print(hidden1.size())
```

![스크린샷 2022-04-12 오후 10 11 42](https://user-images.githubusercontent.com/76269316/162970087-093b644a-4f69-4e5c-943b-13d06e4fcf99.png)

<br>

2.[nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)

float 32 type의 입력 데이터에 대해  y=wx+b 형태의 선형 변환을 수행합니다. 자세한 내용은 [여기](https://blog.naver.com/PostView.naver?blogId=vail131&logNo=222408673876)

y=wx+b 형태이기 때문에 weight, bias를 통해 변환하고 역전파(back propagation)에서 해당 가중치들이 바뀌면서 학습됩니다.

```python
layer1 = nn.Linear(in_features=28*28, out_features=20)
hidden1 = layer1(flat_image)
print(hidden1.size())
```

![스크린샷 2022-04-12 오후 10 18 07](https://user-images.githubusercontent.com/76269316/162971316-a2445ec3-799c-4ff0-8dc6-61438fff6fcb.png)

<br>

3.[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)

활성 함수(activation function) 중 하나로, 그래프가 다음과 같습니다.

<img src="https://user-images.githubusercontent.com/76269316/162972713-59b3709f-a921-4ea2-a3cc-7fd76137917c.png" alt="image" style="zoom: 33%;" />

따라서 입력값이 음수이면 미분값이 0이 되게 됩니다. 즉 weight를 0으로 만듧니다.

이렇게 weight가 0이 되게 되면 어떤 식으로든 복구할 수 없게 돼, 학습이 진행됨에 따라 많은 weight들이 죽을 수 있습니다.

이를 방지하기 위해 leaky ReLU를 사용하기도 합니다.

```python
print(f"Before ReLU: {hidden1}\n\n")
hidden1 = nn.ReLU()(hidden1)
print(f"After ReLU: {hidden1}")
```

<p align = "center">
  <img src = "https://user-images.githubusercontent.com/76269316/162973521-fa16a308-18e5-4b50-aae7-a4a5100bca84.png">
</p>
<p align = "center">
  ReLU layer를 통과하기전 tensor값이 음수인 것들이 ReLU layer를 거친 후 0으로 바뀐 것을 확인할 수 있다.
</p>

<br>

4.[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)

소프트맥스는 입력값을 0에서 1 사이의 값으로 정규화한 뒤, 각 클래스에 대한 확률을 추정합니다. 이때 각 클래스의 예측 확률의 총합은 1이 됩니다.

```python
softmax = nn.Softmax(dim=1)
pred_probab = softmax(logits)
```

<br>

5.[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)

nn.Sequential은 모듈의 컨테이너로, 입력 데이터는 sequential에 정의된 모듈의 순서대로 전달됩니다.

신경망의 깊이가 깊어질수록 코드가 복잡해지는데 가독성을 높여줍니다. 특히 forward 메소드에서 순전파(forward propagation)를 구현할 때 layer 형태보다 가독성이 뛰어나게 코드를 작성할 수 있습니다.

[Sequantial을 사용했을 때와 사용하지 않았을 때의 코드 가독성 차이](https://dororongju.tistory.com/147)

```python
seq_modules = nn.Sequential(
    flatten,
    layer1,
    nn.ReLU(),
    nn.Linear(20, 10)
)
input_image = torch.rand(3,28,28)
logits = seq_modules(input_image)
```

<br>

6.[nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)

뉴런이라고 부르는 노드를 무작위로 껐다 키는 것을 dropout이라고 합니다.

<img src="https://user-images.githubusercontent.com/76269316/163109160-014b50f1-5651-47b3-8110-2d72c5ab83dc.png" alt="image" style="zoom: 50%;" />

원래는 왼쪽 그림처럼 모든 노드들이 켜져있는 상태로 훈련을 하며 weight 값을 조정하는데, 이렇게 학습할 경우 train 데이터 셋에 과적합 돼서 validation이나 test시 성능이 좋지 않을 수 있습니다.

이를 방지하기 위해 사용하는 것이 dropout입니다. 매번 다른 형태의 노드로 학습하기 때문에 여러 형태의 네트워크를 앙상블 하는 효과를 낼 수 있습니다. (이로 인해 일반화 성능 또한 높아집니다)

[PyTorch로 시작하는 딥러닝 기초 09-3 Dropout](https://wegonnamakeit.tistory.com/46)
