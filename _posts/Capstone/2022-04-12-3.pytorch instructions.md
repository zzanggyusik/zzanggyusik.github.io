---
title:  "[Capstone] PyTorch 정리"
excerpt: "PyTorch 정리"
toc: true
toc_label: "PyTorch 정리"
toc_sticky: true
published: true

categories:
  - Capstone
tags:
  - PyTorch
last_modified_at: 2022-04-30

---

> <img src="https://user-images.githubusercontent.com/76269316/164960694-ff35dd5a-bd3f-4c85-9fb8-4e8d49a1241b.png" alt="image" style="zoom:50%;" />
>
> [[PyTorch] 쉽고 빠르게 배우는 딥러닝](https://www.inflearn.com/course/PyTorch-%EB%94%A5%EB%9F%AC%EB%8B%9D/dashboard)를 수강하고 공부한 내용을 정리한 포스팅입니다.

<br>

### Tensor

어떠한 값을 표현하기 위한 미지수 값이 1개이면 scalar, 2개면 vector, vector가 여러개 있는 것을 matrix라고 하는데, matrix보다 한 단계 높은 수준의 정형 데이터에 대한 크기를 tensor라고 합니다.

기존까지 tensor를 다루던 프레임워크는 Numpy인데, 넘파이는 GPU를 이용한 병렬 연산이 안되기 때문에 GPU를 활용하기 위해 Tensor가 등장하게 됐습니다.

Pytorch의 tensor는 numpy array와 GPU를 지원하냐 마냐의 차이기 때문에 거의 동일한 기능을 지원하며, 변환도 쉽습니다.

<br><br>

### Autograd

Forward (모델 입력값으로 출력값을 계산하는 것) 값을 바탕으로 Backward(loss를 계산해서, loss의 gradient를 이용해 w 값을 업데이트하는 back propagation을 backward라고 합니다)를 실행하는 걸 코드 한 두줄로 처리할 수 있습니다.

<br>

##### Defining new autograd functions

torch.autograd.Function 서브클래스를 이용해서 custom autograd operator를 만들 수 있습니다.

<br><br>

### NN

NN은 neural network를 생성할 때 굉장히 쉽고, 효과적으로 설계할 수 있도록 하는 모듈입니다.

loss 값을 계산할 때 사용하는 loss function들도 subclass(nn.L1Loss, nn.MSELoss ··· 등)로 정의돼 있습니다.
<br>

##### Custom NN Modules

pytorch에서 제공하는 nn module 안에 구현돼 있는 것들을 쉽게 커스터마이징 할 수 있습니다.

<br><br>

### OPTIM

OPTIM은 optimization 알고리즘을 쉽게 구현할 수 있도록 한 모듈입니다.

backpropagation 과정에서 loss 값이 최소가 되는 방향으로 w를 업데이트하는데, 이 때 사용되는 다양한 알고리즘들이 있는데 이를 쉽게 이용할 수 있도록 pytorch에서 설계해놨습니다.

torch.optim.Adam, torch.optim.SGD, torch.optim.Adadelta, ···

<br>

##### Tensor Example

```python
import torch

dtype = torch.float  # tensor 자료형
device = torch.device("cpu")  # cpu를 사용해서 계산
# device = torch.device("cuda: 0")  # gpu를 사용해서 계산 (0은 gpu 번호, 사용하려는 gpu numbering을 넣으면 됨)

# N: batch size, D_in: input dimension (feature), H: hidden dimension, D_out: output dimension
N, D_in, H, D_out = 64, 1000, 100, 10

# 테스트를 위해 랜덤하게 데이터를 생성
x = torch.randn(N, D_in, device=device, dtype=dtype)  # torch.randn: 정규분포에서 난수를 무작위로 뽑음
y = torch.randn(N, D_out, device=device, dtype=dtype)

# 랜덤하게 weight 값 초기화
# w1 = (1000, 100), w2 = (100, 10)
w1 = torch.randn(D_in, H, device=device, dtype=dtype)  # (64, 1000) X (1000, 100) -> (64, 100)
w2 = torch.randn(H, D_out, device=device, dtype=dtype)  # (64, 100) X (100, 10) -> (64, 10)

learning_rate = 1e-6

for t in range(500):
  # Forward
  h = x.mm(w1)  # matrix multiplication (64, 1000) X (1000, 100) -> (64, 100)
  h_relu = h.clamp(min=0)  # relu 함수 적용 -> 비선형성
  y_pred = h_relu.mm(w2)
  
  # loss 값 계산
  loss = (y_pred - y).pow(2).sum().item()  # loss 값을 제곱한 다음 다 더함 64 -> 1, item()은 변수 값(scalar)을 가져옴
  if t % 100 == 99:  # 100번째 iteration마다 loss 값 출력
    print(t, loss)
    
  # Back propagation
  grad_y_pred = 2.0 * (y_pred - y)  # 위에서 정의한 loss function을 편미분한 결과 (64, 10)
  grad_w2 = h_relu.t().mm(grad_y_pred)  # hidden layer를 transpose한 다음 행렬곱 계산 (100, 64) X (64, 10) -> (100, 10)
  grad_h_relu = grad_y_pred.m(w2.t())  # (64, 10) X (10, 100) -> (64, 100)
  grad_h = grad_h_relu.clone()  # grad_h를 복사
  grad_h[h < 0] = 0  # relu 함수는 0보다 작은 값을 0으로 변환
  grad_w1 = x.t().mm(grad_h)
  
  # w1, w2 업데이트
  w1 -= learning_rate * grad_w1
  w2 -= learning_rate * grad_w2
```

<br>

##### Autograd Example

```python
import torch

dtype = torch.float  # tensor 자료형
device = torch.device("cpu")  # cpu를 사용해서 계산
# device = torch.device("cuda: 0")  # gpu를 사용해서 계산 (0은 gpu 번호, 사용하려는 gpu numbering을 넣으면 됨)

# N: batch size, D_in: input dimension (feature), H: hidden dimension, D_out: output dimension
N, D_in, H, D_out = 64, 1000, 100, 10

# 테스트를 위해 랜덤하게 데이터를 생성
x = torch.randn(N, D_in, device=device, dtype=dtype)  # torch.randn: 정규분포에서 난수를 무작위로 뽑음
y = torch.randn(N, D_out, device=device, dtype=dtype)

# 랜덤하게 weight 값 초기화
# w1 = (1000, 100), w2 = (100, 10)
# (64, 1000) X (1000, 100) -> (64, 100)
# (64, 100) X (100, 10) -> (64, 10)
w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)  # requires_grad=True로 설정하면 backpropagation이 자동으로 처리됨
w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)

learning_rate = 1e-6

for t in range(500):
  # Forward
 	y_pred = x.mm(w1).clamp(min=0).mm(w2)
  
  # loss 값 계산
  loss = (y_pred - y).pow(2).sum()
  if t % 100 == 99:  # 100번째 iteration마다 loss 값 출력
    print(t, loss)
    
  # Back propagation
  # requires_grad=True로 설정돼 있는 모든 tensor에 대해 gradient 계산
  loss.backward()
  
  # w1, w2 업데이트
  with torch.no_grad():  # gradient 값들을 pytorch 내부에서 갖고 있는데, 이 상태에서 weight를 가져오면 중첩돼서 업데이트가 잘 안될수도 있기 때문에 autograd를 끄고 gradient 값을 가져옴
    w1 -= learning_rate * w1.grad
    w2 -= learning_rate * w2.grad
    
    # gradient를 0으로 초기화
    w1.grad.zero_()
    w2.grad.zero_()
```

<br>

##### Defining new autograd function Example

[TORCH.AUTOGRAD.FUNCTION.FUNCTIONCTX.SAVE_FOR_BACKWARD](https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.save_for_backward.html)

```python
import torch

# torch.autograd.Function을 상속받아 정의 -> forward, backward 함수를 작성해야함
class MyReLU(torch.autograd.Function):
  @staticmethod  # 인스턴스 생성 없이 사용할 수 있는 정 메소드
  def forward(ctx, input):
   ctx.save_for_backward(input)  # ctx는 backward 계산을 위해 저장되는 임시적인 값
  return input.clam(min=0)  # input값을 받아 forward 값을 리턴

  @staticmethod
  def backward(ctx, grad_output):
    input, = ctx.saved_tensors  # forward에서 저장한 context 값을 가져옴
    grad_input = grad_output.clone()
    grad_input[input < 0] = 0
    return grad_input

dtype = torch.float  # tensor 자료형
device = torch.device("cpu")  # cpu를 사용해서 계산
# device = torch.device("cuda: 0")  # gpu를 사용해서 계산 (0은 gpu 번호, 사용하려는 gpu numbering을 넣으면 됨)

# N: batch size, D_in: input dimension (feature), H: hidden dimension, D_out: output dimension
N, D_in, H, D_out = 64, 1000, 100, 10

# 테스트를 위해 랜덤하게 데이터를 생성
x = torch.randn(N, D_in, device=device, dtype=dtype)  # torch.randn: 정규분포에서 난수를 무작위로 뽑음
y = torch.randn(N, D_out, device=device, dtype=dtype)

# 랜덤하게 weight 값 초기화
# w1 = (1000, 100), w2 = (100, 10)
# (64, 1000) X (1000, 100) -> (64, 100)
# (64, 100) X (100, 10) -> (64, 10)
w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)  # requires_grad=True로 설정하면 backpropagation이 자동으로 처리됨
w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)

learning_rate = 1e-6

for t in range(500):
  relu = MyReLU.apply
  
  # Forward
 	y_pred = relu(x.mm(w1)).mm(w2)
  
  # loss 값 계산
  loss = (y_pred - y).pow(2).sum()
  if t % 100 == 99:  # 100번째 iteration마다 loss 값 출력
    print(t, loss)
    
  # Back propagation
  # requires_grad=True로 설정돼 있는 모든 tensor에 대해 gradient 계산
  loss.backward()
  
  # w1, w2 업데이트
  with torch.no_grad():  # gradient 값들을 pytorch 내부에서 갖고 있는데, 이 상태에서 weight를 가져오면 중첩돼서 업데이트가 잘 안될수도 있기 때문에 autograd를 끄고 gradient 값을 가져옴
    w1 -= learning_rate * w1.grad
    w2 -= learning_rate * w2.grad
    
    # gradient를 0으로 초기화
    w1.grad.zero_()
    w2.grad.zero_()
```

<br>

##### NN Example

```python
import torch

N, D_in, H, D_out = 64, 1000, 100, 10

x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

# 이전까지는 w1, w2를 설정하고 matrix multiplication으로 일일이 forward 계산을 해줬는데 torch.nn.Sequential을 사용하면 순차적으로 계산해줌
model = torch.nn.Sequential(
	torch.nn.Linear(D_in, H),
  torch.nn.ReLU(),
  torch.nn.Linear(H, D_out),
)

loss_fn = torch.nn.MSELoss(reduction='sum')

leraning_rate = 1e-4
for t in range(500):
  # torch.nn.Sequential은 입력값을 넣어주면 자동으로 forward 계산을 함
  y_pred = model(x)
  
  loss = loss_fn(y_pred, y)
  if t % 100 == 99:
    print(t, loss.item())
  
  # 이전 iteration의 gradient 값이 남아있을 수 있기 때문에 0으로 초기화
  model.zero_grad()
  
  loss.backward()
  
  with torch.no_grad():
    for param in model.parameters():  # model의 모든 파라미터에 대해 gradient를 업데이트
      param -= learning_rate * param.grad
```

<br>

##### Optim Example

```python
import torch

N, D_in, H, D_out = 64, 1000, 100, 10

x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

# 이전까지는 w1, w2를 설정하고 matrix multiplication으로 일일이 forward 계산을 해줬는데 torch.nn.Sequential을 사용하면 순차적으로 계산해줌
model = torch.nn.Sequential(
	torch.nn.Linear(D_in, H),
  torch.nn.ReLU(),
  torch.nn.Linear(H, D_out),
)

loss_fn = torch.nn.MSELoss(reduction='sum')

# loss 값이 최저가 되도록 하는 w를 업데이트하는 여러 알고리즘들을 torch.optim으로 구현해놨음
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

leraning_rate = 1e-4
for t in range(500):
  # torch.nn.Sequential은 입력값을 넣어주면 자동으로 forward 계산을 함
  y_pred = model(x)
  
  loss = loss_fn(y_pred, y)
  if t % 100 == 99:
    print(t, loss.item())
  
  # 이전 iteration의 gradient 값이 남아있을 수 있기 때문에 0으로 초기화
  optimizer.zero_grad()
  
  loss.backward()
  
  # w1, w2 업데이트
  optimizer.step()
```

<br>

##### Custom nn Modules Example

```python
import torch

class TwoLayerNet(torch.nn.Module):
  def __init__(self, D_in, H, D_out):
    super(TwoLayerNet, self).__init__()
    self.linear1 = torch.nn.Linear(D_in, H)
    self.linear2 = torch.nn.Linear(H, D_out)
    
 # layer를 통과한 결과값(activation function)을 forward 함수에 넣어줌
 def forward(self, x):
  h_relu = self.linear1(x).clamp(min=0)
  y_pred = self.linear2(h_relu)
  return y_pred

N, D_in, H, D_out = 64, 1000, 100, 10

x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

model = TwoLayerNet(D_in, H, D_out)

criterion = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)

for t in range(500):
  # torch.nn.Sequential은 입력값을 넣어주면 자동으로 forward 계산을 함
  y_pred = model(x)
  
  loss = criterion(y_pred, y)
  if t % 100 == 99:
    print(t, loss.item())
  
  # 이전 iteration의 gradient 값이 남아있을 수 있기 때문에 0으로 초기화
  optimizer.zero_grad()
  loss.backward()

  # w1, w2 업데이트
  optimizer.step()
```

<br>

##### Control Flow, Weight Sharing Example

control flow에 사용되는 조건문, 반복문 등을 forward 함수 안에서도 사용 가능하다.

```python
import random
import torch

class DynamicNet(torch.nn.Module):
  def __init__(self, D_in, H, D_out):
    super(TwoLayerNet, self).__init__()
    self.input_linear = torch.nn.Linear(D_in, H)
    self.middle_linear = torch.nn.Linear(H, H)
    self.output_linear = torch.nn.Linear(H, D_out)
    
 def forward(self, x):
  h_relu = self.input_linear(x).clamp(min=0)
  for _ in range(random.randint(0, 3)):  # 0 <= _ <= 3 사이의 값을 랜덤하게 뽑아 나온만큼 반복 (Hidden Layer 계산을 몇 번 할지)
    h_relu = self.middle_linear(h_relu).clamp(min=0)
  y_pred = self.output_linear(h_relu)
  return y_pred

N, D_in, H, D_out = 64, 1000, 100, 10

x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

model = DynamicNet(D_in, H, D_out)

criterion = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)

for t in range(500):
  # torch.nn.Sequential은 입력값을 넣어주면 자동으로 forward 계산을 함
  y_pred = model(x)
  
  loss = criterion(y_pred, y)
  if t % 100 == 99:
    print(t, loss.item())
  
  # 이전 iteration의 gradient 값이 남아있을 수 있기 때문에 0으로 초기화
  optimizer.zero_grad()
  loss.backward()

  # w1, w2 업데이트
  optimizer.step()
```

