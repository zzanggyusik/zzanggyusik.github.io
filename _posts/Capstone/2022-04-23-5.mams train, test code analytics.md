---
title:  "[Capstone] MAMS for ABSA 훈련/테스트 코드 분석"
excerpt: "MAMS for ABSA 코드 리뷰"
toc: true
toc_label: "MAMS for ABSA 훈련/테스트 코드 분석"
toc_sticky: true
published: true

categories:
  - Capstone
tags:
  - PyTorch
last_modified_at: 2022-04-24

---

> [MAMS for ABSA 전처리 코드 분석](https://seominseok4834.github.io/capstone/4.mams-preprocess-code-analytics/)과 이어지는 내용입니다.
>
> 모든 코드는 [MAMS-for-ABSA github](https://github.com/siat-nlp/MAMS-for-ABSA)에서 가져왔습니다.

<br>

![스크린샷 2022-04-23 오후 9 56 57](https://user-images.githubusercontent.com/76269316/164895501-0d95b6b8-a667-49f3-9354-a908c25fba08.png)

전처리 코드와 동일하게 루트 디렉토리에 있는 train.py 파일을 실행하면 configuration 파일을 로드하고, /train/train.py가 실행되면서 훈련이 시작됩니다.

<br>

[/train.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train.py)

```python
import yaml
import os
from train.train import train

config = yaml.safe_load(open('config.yml'))
mode = config['mode']
os.environ["CUDA_VISIBLE_DEVICES"] = str(config['aspect_' + mode + '_model'][config['aspect_' + mode + '_model']['type']]['gpu'])
train(config)
```

<br>

훈련을 위해 필요한 모든 함수는 /train/train.py를 통해 호출되기 때문에 **[/train/train.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/train.py)**는 하늘색으로 표시해 구분하도록 하겠습니다.

또한 이전 포스팅에서 말씀드린 것처럼 제가 ATSA를 진행할 예정이기 때문에 이를 기준으로 설명하도록 하겠습니다.

<span style="color:#2EBEF8">**/train/train.py**</span>

16 ~ 23 line

```python
from train import make_aspect_term_model, make_aspect_category_model
from train.make_data import make_term_data, make_category_data

def train(config):
    mode = config['mode']  # term
    if mode == 'term':
        model = make_aspect_term_model.make_model(config)  # model 생성
        train_loader, val_loader = make_term_data(config)  # dataloader 생성
    else:
        model = make_aspect_category_model.make_model(config)
        train_loader, val_loader = make_category_data(config)
        												···
```

먼저 모델과 데이터로더를 생성합니다. configuration 파일이 ATSA 기준이기 때문에 config['mode']가 term으로 설정돼 있습니다.

<br>

### 모델 생성

**config.yml**

```yaml
aspect_term_model:
  type: recurrent_capsnet
  recurrent_capsnet:
    embed_size: 300
    dropout: 0.5
    num_layers: 2
    capsule_size: 300
    bidirectional: True
    optimizer: adam
    batch_size: 64
    learning_rate: 0.0003
    weight_decay: 0
    num_epoches: 20
    gpu: 0
```

[/train/make_aspect_term_model.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/make_aspect_term_model.py)

**make_model()**

10 ~ 17 line

먼저 모델을 생성하는 코드를 보면, 현재 configuration 파일의 type이 recurrent_capsnet이기 때문에 아래 35 ~ 50 line의 make_recurrent_capsule_network()를 호출합니다.

```python
def make_model(config):
    model_type = config['aspect_term_model']['type']  # recurrent_capsnet
    if model_type == 'recurrent_capsnet':
        return make_recurrent_capsule_network(config)
    elif model_type == 'bert_capsnet':
        return make_bert_capsule_network(config)
    else:
        raise ValueError('No Supporting.')
```

<br>

**make_recurrent_capsule_network()**

35 ~ 36 line

```python
def make_recurrent_capsule_network(config):
    embedding = make_embedding(config)
    							···
```

52~ 62 line에 있는 make_embedding 메소드를 호출해 전처리 과정에서 저장한 단어 임베딩을 로드합니다.

<br>

**make_embedding()**

52 ~ 62 line

```python
def make_embedding(config):
    base_path = os.path.join(config['base_path'])  # ./data/MAMS-ATSA
    log_path = os.path.join(base_path, 'log/log.yml')  # ./data/MAMS-ATSA/log/log.yml
    log = yaml.safe_load(open(log_path))  # 로그 파일 로드
    vocab_size = log['vocab_size']  # 7895
    config = config['aspect_term_model'][config['aspect_term_model']['type']]  # recurrent_capsnet
    embed_size = config['embed_size']  # 300
    embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)  # (7895, 300)
    glove = np.load(os.path.join(base_path, 'processed/glove.npy'))  # ./data/MAMS-ATSA/processed/glove.npy
    embedding.weight.data.copy_(torch.tensor(glove))  # 전처리 과정에서 저장한 glove.npy의 가중치를 copy
    return embedding
```

전처리 과정에서 저장한 glove.npy 파일을 로드한 뒤 nn.Embedding의 가중치로 copy한 뒤 리턴합니다.

<br>

**make_recurrent_capsule_network()**

```python
def make_recurrent_capsule_network(config):
    embedding = make_embedding(config)  
    base_path = os.path.join(config['base_path'])  # ./data/MAMS-ATSA
    log_path = os.path.join(base_path, 'log/log.yml')  # ./data/MAMS-ATSA/log/log.yml
    log = yaml.safe_load(open(log_path))
    config = config['aspect_term_model'][config['aspect_term_model']['type']]  # recurrent_capsnet
    model = RecurrentCapsuleNetwork(
        embedding=embedding,
        num_layers=config['num_layers'],  # 2
        capsule_size=config['capsule_size'],  # 300
        bidirectional=config['bidirectional'],  # True
        dropout=config['dropout'],  # 0.5
        num_categories=log['num_categories']  # 3
    )
    model.load_sentiment(os.path.join(base_path, 'processed/sentiment_matrix.npy'))
    return model
```

glove 단어 임베딩을 로드한 뒤, configuration 파일에 저장한 파라미터와 함께 모델 생성 인자로 넣어 모델을 생성합니다.

<br>

### 데이터세트 생성

[/train/make_data.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/make_data.py)

**make_term_data()**

10 ~ 15 line

```python
import os
from torch.utils.data import DataLoader
from data_process.dataset import ABSADataset

input_list = {
    'recurrent_capsnet': ['context', 'aspect'],
    'bert_capsnet': ['bert_token', 'bert_segment']
}

def make_term_data(config):
    base_path = config['base_path']  # ./data/MAMS-ATSA
    train_path = os.path.join(base_path, 'processed/train.npz')  # ./data/MAMS-ATSA/processed/train.npz
    val_path = os.path.join(base_path, 'processed/val.npz')  # ./data/MAMS-ATSA/processed/val.npz
    train_data = ABSADataset(train_path, input_list[config['aspect_term_model']['type']])  # config['aspect_term_model']['type']: recurrent_capsnet -> input_list['recurrent_capsnet']
    val_data = ABSADataset(val_path, input_list[config['aspect_term_model']['type']])
																				···
```

전처리 과정에서 생성한 train.npz, val.npz 파일을 PyTorch Dataset 클래스를 상속받아 구현한 ABSADataset의 파라미터로 넘겨 train_data와 val_data로 생성합니다.

<br>

[/data_process/dataset.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/dataset.py)

```python
import torch
from torch.utils.data import Dataset
import numpy as np

class ABSADataset(Dataset):

    def __init__(self, path, input_list):
        super(ABSADataset, self).__init__()
        data = np.load(path)  # ./data/MAMS-ATSA/processed/train.npz(val.npz)
        self.data = {}
        for key, value in data.items():
            self.data[key] = torch.tensor(value).long()  # tensor long type으로 변경
        self.len = self.data['label'].size(0)  # 전체 데이터의 개수
        self.input_list = input_list  # ['context', 'aspect']

    def __getitem__(self, index):
        return_value = []
        for input in self.input_list:
            return_value.append(self.data[input][index])
        return_value.append(self.data['label'][index])
        return return_value

    def __len__(self):
        return self.len
```

생성자(\_\_init\_\_)를 보면 numpy zip 파일을 로드한 다음 data.items()로 iteration 하는 것을 볼 수 있습니다.

key 값에는 전처리 과정에서 저장한 sentence, aspect, label, context. bert_token. bert_segment, td_left, td_right가 들어있고 value에는 해당하는 값들이 들어있습니다. [전처리 데이터 저장](https://seominseok4834.github.io/capstone/4.mams-preprocess-code-analytics/#%EC%A0%84%EC%B2%98%EB%A6%AC-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%80%EC%9E%A5)

input_list에는 ['context', 'aspect']가 담겨있습니다.

<br>

\_\_getitem\_\_과 \_\_len\_\_ 메소드는 훈련, 검증시 데이터를 batch size만큼 뽑아올 때 사용되는데, 

\_\_getitem\_\_ 메소드를 보면 input_list에 있는 key 값(context, aspect)들만 꺼내오는 것을 확인할 수 있습니다. (label도 같이)

<br>

### 데이터로더 생성

[/train/make_data.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/make_data.py)

16 ~ 29 line

```python
    config = config['aspect_term_model'][config['aspect_term_model']['type']]  # recurrent_capsnet
		# pytorch DataLoader 생성
  	train_loader = DataLoader(
        dataset=train_data,
        batch_size=config['batch_size'],  # 64
        shuffle=True,
        pin_memory=True  # 빠른 데이터 전송을 위한 pin_memory 고정
    )
    val_loader = DataLoader(
        dataset=val_data,
        batch_size=config['batch_size'],
        shuffle=False,
        pin_memory=True 
    )
    return train_loader, val_loader
```

앞서 만든 ABSADataset를 인자로 넘겨 PyTorch DataLoader 클래스를 생성합니다.

이렇게 batch_size를 파라미터로 지정해주면 훈련, 검증시 자동으로 배치 사이즈만큼 데이터를 뽑아주기 때문에 매우 편리합니다.

<br>

<span style="color:#2EBEF8">**/train/train.py**</span>

24 ~ 32 line

```python
    model = model.cuda()  # 모델 파라미터를 GPU로 로드
    base_path = config['base_path']  # ./data/MAMS-ATSA
    model_path = os.path.join(base_path, 'checkpoints/%s.pth' % config['aspect_' + mode + '_model']['type'])  # 모델 저장 경로
    if not os.path.exists(os.path.dirname(model_path)):  # 디렉토리가 없는 경우 생성
        os.makedirs(os.path.dirname(model_path))
    with open(os.path.join(base_path, 'processed/index2word.pickle'), 'rb') as handle:  # 전처리 과정에서 저장한 index2word.pickle 파일 로드
        index2word = pickle.load(handle)
    criterion = CapsuleLoss()  # loss function 생성 (/src/module/utils/loss.py)
    optimizer = make_optimizer(config, model)  # optimizer 생성
										···
```

이렇게 DataLoader까지 만들고 나면, 이제 모델을 훈련 시키기 위해 GPU로 모델 파라미터를 로드합니다.

그리고 훈련시 validation loss 값이 최소가 될 때마다 해당 모델을 저장하기 위해 checkpoints 디렉토리를 생성하고, 전처리 과정에서 저장한 index2word.pickle 파일을 로드합니다.

loss 값을 계산하기 위한 CapsuleLoss 함수도 생성하고, optimizer도 생성합니다.

<br>

**[make_optimizer.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/make_optimizer.py)**

```python
from torch import optim
import adabound

def make_optimizer(config, model):
    mode = config['mode']  # term
    config = config['aspect_' + mode + '_model'][config['aspect_' + mode + '_model']['type']]  # recurrent_capsnet
    lr = config['learning_rate']  # 0.0003
    weight_decay = config['weight_decay']  # 0
    opt = {  # adam 사용
        'sgd': optim.SGD,
        'adadelta': optim.Adadelta,
        'adam': optim.Adam,
        'adamax': optim.Adamax,
        'adagrad': optim.Adagrad,
        'asgd': optim.ASGD,
        'rmsprop': optim.RMSprop,
        'adabound': adabound.AdaBound
    }
    if 'momentum' in config:
        optimizer = opt[config['optimizer']](model.parameters(), lr=lr, weight_decay=weight_decay, momentum=config['momentum'])
    else:  # config 파일에 momentum이 없어서 else 실행
        optimizer = opt[config['optimizer']](model.parameters(), lr=lr, weight_decay=weight_decay)
    return optimizer
```

<br>

### 모델 훈련

<span style="color:#2EBEF8">**/train/train.py**</span>

33 ~ 36 line

```python
    # 훈련시 사용하기 위한 변수
  	max_val_accuracy = 0
    min_val_loss = 100
    global_step = 0
    config = config['aspect_' + mode + '_model'][config['aspect_' + mode + '_model']['type']]  # recurrent_capsnet
```

이제 훈련을 위한 모든 설정이 끝났습니다. config 파일에서 미리 설정한 epoch 수만큼 훈련하는 코드를 살펴보도록 하겠습니다.

<br>

37 ~ 76 line

```python
    for epoch in range(config['num_epoches']):  # 20 epoch
        total_loss = 0
        total_samples = 0
        correct_samples = 0
        start = time.time()
        for i, data in enumerate(train_loader):  # batch_size만큼 자동으로 데이터를 뽑아줌
            global_step += 1
            model.train()  # 모델을 훈련모드로 변경 (dropout 같은 훈련을 위한 설정을 활성화)
            input0, input1, label = data  # context, aspect, label (ABSADataset __getitem__에서 정의)
            input0, input1, label = input0.cuda(), input1.cuda(), label.cuda()  # 데이터를 gpu로 올림
            optimizer.zero_grad()  # gradient 계산을 위해 0으로 초기화
            logit = model(input0, input1)  # 모델에 context와 aspect를 입력 데이터로 전달 -> forward 함수가 실행됨
            loss = criterion(logit, label)  # 예측값과 실제값을 비교해 loss 계산
            batch_size = input0.size(0)
            total_loss += batch_size * loss.item()  # 전체 loss 계산
            total_samples += batch_size  # 훈련을 실행한 데이터 개수 카운트
            pred = logit.argmax(dim=1)  # 예측값에 argmax를 취해 감정 예측 (긍정 / 부정)
            correct_samples += (label == pred).long().sum().item()  # 맞은 개수를 카운트
            loss.backward()  # backward 진행
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)  # gradient clipping (gradient vanishing, exploding을 방지하기 위해 사용)
            optimizer.step()  # backpropagation에서 수집된 변화량으로 매개변수 조정
            if i % 10 == 0 and i > 0:  # 10번의 iteration마다 total_loss, total_accuracy 계산 및 검증
                train_loss = total_loss / total_samples
                train_accuracy = correct_samples / total_samples
                total_loss = 0
                total_samples = 0
                correct_samples = 0
                val_accuracy, val_loss = eval(model, val_loader, criterion)  # /train/eval.py 실행
                print('[epoch %2d] [step %3d] train_loss: %.4f train_acc: %.4f val_loss: %.4f val_acc: %.4f'
                      % (epoch, i, train_loss, train_accuracy, val_loss, val_accuracy))
                if val_accuracy > max_val_accuracy:
                    max_val_accuracy = val_accuracy
                    # torch.save(aspect_term_model.state_dict(), model_path)
                if val_loss < min_val_loss:  # validation loss가 최소가 될 때마다 모델 갱신
                    min_val_loss = val_loss
                    if epoch > 0:
                        torch.save(model.state_dict(), model_path)
        end = time.time()
        print('time: %.4fs' % (end - start))
    print('max_val_accuracy:', max_val_accuracy)
```

모델을 훈련하는 코드는 다음과 같습니다.

앞서 만든 dataloader를 통해 배치 사이즈만큼 데이터를 뽑고, 모델 입력으로 데이터들을 넣어준 다음 loss 값을 계산합니다.

그리고 argmax를 통해 해당 값들을 예측 클래스 값으로 변환해 맞춘 개수를 카운트합니다.

이걸 반복하다가 10번의 iteration마다 검증 데이터셋으로 모델을 평가합니다.

<br>

[/train/eval.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/eval.py)

```python
import torch

def eval(model, data_loader, criterion=None):
    total_samples = 0
    correct_samples = 0
    total_loss = 0
    model.eval()  # model을 훈련 모드로 변경
    with torch.no_grad():
        for data in data_loader:
            input0, input1, label = data  # context, aspect, label (ABSADataset __getitem__에서 정의)
            input0, input1, label = input0.cuda(), input1.cuda(), label.cuda()  # 데이터를 gpu로 로드
            logit = model(input0, input1)  # 모델에 context와 aspect를 입력 데이터로 전달 -> forward 함수가 실행됨
            loss = criterion(logit, label).item() if criterion is not None else 0  # loss 값 계산
            total_samples += input0.size(0)  # 전체 개수 카운트
            pred = logit.argmax(dim=1)  # 감정 예측 (긍정 / 부정)
            correct_samples += (label == pred).long().sum().item()  # 맞춘 개수 카운트
            total_loss += loss * input0.size(0)
    accuracy = correct_samples / total_samples  # 정확도 계산
    avg_loss = total_loss / total_samples  # 평균 loss값 계산
    if criterion is not None:
        return accuracy, avg_loss
    else:
        return accuracy
```

모델 평가는 eval 메소드를 호출해서 이루어지는데, 이 부분 코드도 훈련 과정과 동일합니다.

평가시에는 gradient를 계산할 필요가 없기 때문에 관련 부분이 빠졌습니다. **주의하셔야할 점은 훈련과 검증시 모델의 모드가 다르다는 것입니다.**

모델 훈련시에는 과적합 방지를 위해 dropout 같은 기능들이 활성화 됩니다. 이를 위해 모델을 훈련 모드로 설정(model.train())해야 하는데, 검증시에는 dropout을 사용할 필요가 없으니 다시 활성화 시켜주어야 합니다. 이를 pytorch에서는 model.eval()를 통해 간단하게 지원합니다.

[model.eval()의 의미](https://bluehorn07.github.io/2021/02/27/model-eval-and-train.html)

<br>

이렇게 훈련/검증 과정이 끝나면 수행 시간과 최대 정확도가 출력되면서 훈련 과정이 종료되게 됩니다.

### 모델 테스트

테스트 코드도 동일한데 검증시 사용한 eval 메소드를 통해 테스트를 진행합니다. (❗️이 때 dataloader는 test_loader입니다)

```python
test_accuracy = eval(model, test_loader)
```

<br>

[/train/test.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/test.py)

```python
import torch
import os
from train import make_aspect_term_model, make_aspect_category_model
from train.make_data import make_term_test_data, make_category_test_data
from train.eval import eval

def test(config):
    mode = config['mode']
    if mode == 'term':
        model = make_aspect_term_model.make_model(config)
    else:
        model = make_aspect_category_model.make_model(config)
    model = model.cuda()
    model_path = os.path.join(config['base_path'], 'checkpoints/%s.pth' % config['aspect_' + mode + '_model']['type'])
    model.load_state_dict(torch.load(model_path))
    if mode == 'term':
        test_loader = make_term_test_data(config)
    else:
        test_loader = make_category_test_data(config)
    test_accuracy = eval(model, test_loader)
    print('test:\taccuracy: %.4f' % (test_accuracy))
```
