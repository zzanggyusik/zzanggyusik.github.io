---
title:  "[Capstone] pyABSA í”„ë ˆì„ì›Œí¬ ë¶„ì„ - Instructor (APC)"
excerpt: "pyABSA í”„ë ˆì„ì›Œí¬ ë¶„ì„"
toc: true
toc_label: "pyABSA í”„ë ˆì„ì›Œí¬ ë¶„ì„ - Instructor (APC)"
toc_sticky: true
published: true

categories:
  - Capstone
tags:
  - PyTorch
  - pyABSA
last_modified_at: 2022-06-19

---

> ì´ì „ í¬ìŠ¤íŒ…ì—ì„œ ì´ì–´ì§€ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤. [pyABSA í”„ë ˆì„ì›Œí¬ ë¶„ì„ - ABSADatasetList](https://seominseok4834.github.io/capstone/11.pyabsa-dataset-item-analysis/)
>
> ëª¨ë“  ì½”ë“œëŠ” [PyABSA github](https://github.com/yangheng95/PyABSA)ì—ì„œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.

<br>

ì•„ë˜ ì½”ë“œëŠ” ë°ëª¨ ì½”ë“œë¡œ ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.

```python
from pyabsa.functional import APCModelList
from pyabsa.functional import Trainer, ATEPCTrainer
from pyabsa.functional import ABSADatasetList
from pyabsa.functional import APCConfigManager

apc_config = APCConfigManager.get_apc_config_english()  # 1 í™˜ê²½ë³€ìˆ˜ ì„¤ì •

apc_config.pretrained_bert = 'microsoft/deberta-v3-base'
apc_config.model = ATEPCModelList.FAST_LCF_ATEPC
dataset_path = ABSADatasetList.Restaurant14  # 2 ë°ì´í„°ì…‹ ì„¤ì •
# or your local dataset: dataset_path = 'your local dataset path'

# ëª¨ë¸ ë¡œë“œ ë° í›ˆë ¨
aspect_extractor = APCTrainer(config=apc_config,
                                dataset=dataset_path,
                                from_checkpoint='',  # set checkpoint to train on the checkpoint.
                                checkpoint_save_mode=1,
                                auto_device=True
                                ).load_trained_model()
```

ë¨¼ì € APConfigManagerë¼ëŠ” í´ë˜ìŠ¤ì˜ get_apc_config_english ë©”ì†Œë“œë¥¼ í†µí•´ configuration ë³€ìˆ˜ë¥¼ ê°€ì ¸ì˜¨ ë’¤(#1), ABSADatasetList í´ë˜ìŠ¤ì˜ ë©¤ë²„ ë³€ìˆ˜ Restaurant14 ë°ì´í„°ì…‹ì˜ ê²½ë¡œë¥¼ ê°€ì ¸ì˜¨ ë’¤(#2), APCTrainer í´ë˜ìŠ¤ë¥¼ í†µí•´ ëª¨ë¸ì„ ìƒì„±, í›ˆë ¨ì„ ì§„í–‰í•©ë‹ˆë‹¤.(#3)

ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” #3 ë¶€ë¶„ì— ëŒ€í•´ì„œ ë‹¤ë£¹ë‹ˆë‹¤.

<br>

â—ï¸configurationì— ì €ì¥ëœ ê°’ë“¤ì„ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•˜ëŠ” ë¶€ë¶„ì´ ë§ìŠµë‹ˆë‹¤. ì–´ë–¤ ê°’ì´ ë“¤ì–´ê°€ëŠ”ì§€ ê¶ê¸ˆí•˜ë‹¤ë©´ ì•„ë˜ configurationì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.

```python
ATEPCConfigManager(args={'model': <class 'pyabsa.core.atepc.models.fast_lcf_atepc.FAST_LCF_ATEPC'>, 'optimizer': 'adamw', 'learning_rate': 2e-05, 'pretrained_bert': 'microsoft/deberta-v3-base', 'cache_dataset': True, 'warmup_step': -1, 'use_bert_spc': False, 'show_metric': False, 'max_seq_len': 80, 'SRD': 3, 'use_syntax_based_SRD': False, 'lcf': 'cdw', 'window': 'lr', 'dropout': 0.5, 'l2reg': 1e-05, 'num_epoch': 10, 'batch_size': 16, 'initializer': 'xavier_uniform_', 'seed': [52], 'polarities_dim': 3, 'log_step': 50, 'patience': 99999, 'gradient_accumulation_steps': 1, 'dynamic_truncate': True, 'srd_alignment': True, 'evaluate_begin': 0, 'hidden_dim': 768, 'embed_dim': 768, 'ABSADatasetsVersion': '2022.06.10', 'dataset_name': 'Restaurant14', 'dataset_file': {'train': ['integrated_datasets/atepc_datasets/110.SemEval/114.restaurant14/Restaurants_Train.xml.seg.atepc'], 'test': ['integrated_datasets/atepc_datasets/110.SemEval/114.restaurant14/Restaurants_Test_Gold.xml.seg.atepc'], 'valid': []}, 'device': device(type='cuda', index=0), 'device_name': 'NVIDIA GeForce RTX 2080 Ti', 'auto_device': True, 'model_name': 'fast_lcf_atepc', 'PyABSAVersion': '1.14.8', 'TransformersVersion': '4.18.0', 'TorchVersion': '1.10.2+cu102+cuda10.2', 'MV': <metric_visualizer.metric_visualizer.MetricVisualizer object at 0x7fe8bef1aee0>, 'save_mode': 1, 'model_path_to_save': 'checkpoints', 'sep_indices': 2, 'spacy_model': 'en_core_web_sm', 'IOB_label_to_index': {'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}, 'index_to_label': {0: 'Negative', 1: 'Neutral', 2: 'Positive'}, 'label_to_index': {'Negative': 0, 'Neutral': 1, 'Positive': 2}, 'index_to_IOB_label': {1: 'B-ASP', 2: 'I-ASP', 3: 'O', 4: '[CLS]', 5: '[SEP]'}, 'label_list': ['B-ASP', 'I-ASP', 'O', '[CLS]', '[SEP]'], 'num_labels': 6, 'max_test_metrics': {'max_apc_test_acc': 87.12, 'max_apc_test_f1': 80.93, 'max_ate_test_f1': 88.81}, 'metrics_of_this_checkpoint': {'apc_acc': 86.23, 'apc_f1': 79.56, 'ate_f1': 87.69}}, args_call_count={'model': 5, 'optimizer': 2, 'learning_rate': 1, 'pretrained_bert': 4, 'cache_dataset': 1, 'warmup_step': 2261, 'use_bert_spc': 14860, 'show_metric': 0, 'max_seq_len': 50176, 'SRD': 9444, 'use_syntax_based_SRD': 4722, 'lcf': 17146, 'window': 0, 'dropout': 1, 'l2reg': 2, 'num_epoch': 2, 'batch_size': 5, 'initializer': 0, 'seed': 7, 'polarities_dim': 91, 'log_step': 2261, 'patience': 27, 'gradient_accumulation_steps': 3, 'dynamic_truncate': 9444, 'srd_alignment': 0, 'evaluate_begin': 91, 'hidden_dim': 6, 'embed_dim': 0, 'ABSADatasetsVersion': 0, 'dataset_name': 30, 'dataset_file': 95, 'device': 77072, 'device_name': 0, 'auto_device': 14861, 'model_name': 4855, 'PyABSAVersion': 0, 'TransformersVersion': 0, 'TorchVersion': 0, 'MV': 5, 'save_mode': 53, 'model_path_to_save': 55, 'sep_indices': 136660, 'spacy_model': 3, 'IOB_label_to_index': 1, 'index_to_label': 2, 'label_to_index': 0, 'index_to_IOB_label': 0, 'label_list': 2135162, 'num_labels': 3, 'max_test_metrics': 629, 'metrics_of_this_checkpoint': 270})
```

<br><br>

### Trainer

Trainer í´ë˜ìŠ¤ëŠ” [PyABSA/pyabsa/functional/trainer/trainer.py](https://github.com/yangheng95/PyABSA/blob/release/pyabsa/functional/trainer/trainer.py#L59)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

ì›ë˜ ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œ Trainer í´ë˜ìŠ¤ì— ëŒ€í•´ ì •ë¦¬í•˜ë ¤ê³  í–ˆëŠ”ë°, Trainerì—ì„œ train4apc ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ë©´ Instructor í´ë˜ìŠ¤ê°€ ìƒì„±ë˜ê³  í›ˆë ¨ê¹Œì§€ ì§„í–‰ë©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì‹¤ì œ ëª¨ë¸ ìƒì„± ë° í›ˆë ¨ì´ ì£¼ëœ ë‚´ìš©ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

<br>

ë°ëª¨ ì½”ë“œì—ì„œ ìƒì„±í•œ APCTrainer í´ë˜ìŠ¤ëŠ” Trainer í´ë˜ìŠ¤ë¥¼ ìƒì† ë°›ì€ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
class APCTrainer(Trainer):
    pass


class ATEPCTrainer(Trainer):
    pass


class TCTrainer(Trainer):
    pass


class AOTCTrainer(Trainer):
    pass
```

cf) [pass, continue, break ì°¨ì´ì  ì•Œì•„ë³´ê¸°](https://chancoding.tistory.com/7)

<br>

Trainer í´ë˜ìŠ¤ì˜ ìƒì„±ìë¥¼ ë¨¼ì € ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

ì „ì²´ ì½”ë“œê°€ ë„ˆë¬´ ê¸°ë‹ˆê¹Œ line by lineìœ¼ë¡œ ë³´ê² ìŠµë‹ˆë‹¤.

**trainer.py**

59 ~ 68 line

```python
class Trainer:
    def __init__(self,
                 config: ConfigManager = None,
                 dataset=None,
                 from_checkpoint: str = None,
                 checkpoint_save_mode: int = 0,
                 auto_device=True,
                 path_to_save=None,
                 load_aug=False
                 ):
```

ë¨¼ì € í•¨ìˆ˜ ì¸ìì…ë‹ˆë‹¤. ë°ëª¨ ì½”ë“œì—ì„œì™€ ë™ì¼í•˜ê²Œ ConfigManager, ABSADatasetList, checkpoint, auto_device ë“±ì„ ë„˜ê²¨ë°›ìŠµë‹ˆë‹¤.

```python
aspect_extractor = APCTrainer(config=apc_config,
                                dataset=dataset_path,
                                from_checkpoint='',  # set checkpoint to train on the checkpoint.
                                checkpoint_save_mode=1,
                                auto_device=True
                                ).load_trained_model()
```

<br>

**trainer.py**

84 ~ 86 line

```python
if not torch.cuda.device_count() > 1 and auto_device == 'allcuda':
  print('Cuda count <= 1, reset auto_device=True')
  auto_device = True
```

ë‹¤ìŒ ë¼ì¸ì—ì„œëŠ” GPUë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” GPUê°€ ì—†ê³ , auto_deviceê°€ allcudaë¡œ ì„¤ì •ë¼ ìˆìœ¼ë©´, auto_deviceë¥¼ Trueë¡œ ë³€ê²½í•©ë‹ˆë‹¤.

+torch.cuda.device_count()ëŠ” ì‚¬ìš©ê°€ëŠ¥í•œ GPU ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

<br>

**trainer.py**

87 ~ 91 line

```python
if 'hidden_dim' not in config.args or 'embed_dim' not in config.args:
  pretrain_config = AutoConfig.from_pretrained(config.pretrained_bert)  # Hugging Faceì˜ configuration íŒŒì¼ ë¡œë“œ
  config.hidden_dim = pretrain_config.hidden_size  # Hugging Face Bert Modelì˜ hidden dim
  config.embed_dim = pretrain_config.hidden_size
  config.ABSADatasetsVersion = query_local_version()
```

87 ~ 91 lineì—ì„œëŠ” hidden_dimê³¼ embed_dimì´ configurationì— ì •ì˜ë¼ ìˆì§€ ì•ŠëŠ” ê²½ìš°,

ë°ëª¨ ì½”ë“œì—ì„œ pertained_bertë¡œ 'microsoft/deberta-v3-base'ë¥¼ ì‚¬ìš©í–ˆëŠ”ë° í•´ë‹¹ Hugging Faceì˜ configuration íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ configurationì˜ hidden_dimê³¼ embed_dimìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

```python
atepc_config.pretrained_bert = 'microsoft/deberta-v3-base'
```

<br>

query_local_version ë©”ì†Œë“œëŠ” [PyABSA/pyabsa/utils/file_utils.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/utils/file_utils.py#L310)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

**file_utils.py**

310 ~ 317 line

```python
def query_local_version():
    try:
        fin = open(find_cwd_file(['__init__.py', 'integrated_datasets']))
        local_version = fin.read().split('\'')[-2]
        fin.close()
    except:
        return None
    return local_version
```

find_cwd_fileì€ [findfile](https://github.com/yangheng95/findfile) í”„ë ˆì„ì›Œí¬ì˜ ë©”ì†Œë“œë¡œ, íŒŒì¼ ê²½ë¡œë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤. ë¦¬í„´ ë°›ì€ ê²½ë¡œì˜ íŒŒì¼ì„ ì½ì€ ë’¤, versionì„ ê°€ì ¸ì™€ configurationì— ì €ì¥í•©ë‹ˆë‹¤.

+findfile í”„ë ˆì„ì›Œí¬ëŠ” ì¶”í›„ ë”°ë¡œ ì •ë¦¬í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

<br>

**trainer.py**

92 ~ 95 line

```python
if isinstance(config, APCConfigManager):
  self.train_func = train4apc
  self.model_class = SentimentClassifier
  self.task = 'apc'

elif isinstance(config, ATEPCConfigManager):  # í˜„ì¬ configëŠ” ATEPCConfigManager
  self.train_func = train4atepc
  self.model_class = AspectExtractor
  self.task = 'atepc'

elif isinstance(config, TCConfigManager):
  self.train_func = train4tc
  self.model_class = TextClassifier
  self.task = 'classification'

elif isinstance(config, AOTCConfigManager):
  self.train_func = train4ao_tc
  self.model_class = AOTCTextClassifier
  self.task = 'ao_tc'
```

configê°€ ì–´ë–¤ íƒœìŠ¤í¬ì— ëŒ€í•œ config í´ë˜ìŠ¤ì¸ì§€ ì²´í¬í•©ë‹ˆë‹¤. ì €í¬ëŠ” APC íƒœìŠ¤í¬ë¥¼ ì§„í–‰ í•  ì˜ˆì •ì´ê¸° ë•Œë¬¸ì— if isinstance(config, APCConfigManager) ë¶€ë¶„ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.

<br>

train_funcì— train4apcê°€ ë“¤ì–´ê°€ê²Œ ë˜ëŠ”ë°, train4apcë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„  ë°ì½”ë ˆì´í„°ë¥¼ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.

ë°ì½”ë ˆì´í„° í•¨ìˆ˜ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ë‹¤ìŒ ì„¸ ê°œì˜ ê¸€ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.

> [íŒŒì´ì¬ - í¼ìŠ¤íŠ¸í´ë˜ìŠ¤ í•¨ìˆ˜ (First Class Function)](https://schoolofweb.net/blog/posts/%ed%8c%8c%ec%9d%b4%ec%8d%ac-%ed%8d%bc%ec%8a%a4%ed%8a%b8%ed%81%b4%eb%9e%98%ec%8a%a4-%ed%95%a8%ec%88%98-first-class-function/)
>
> [íŒŒì´ì¬ - í´ë¡œì € (Closure)](https://schoolofweb.net/blog/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%81%B4%EB%A1%9C%EC%A0%80-closure/)
>
> [íŒŒì´ì¬ - ë°ì½”ë ˆì´í„° (Decorator)](https://schoolofweb.net/blog/posts/%ed%8c%8c%ec%9d%b4%ec%8d%ac-%eb%8d%b0%ec%bd%94%eb%a0%88%ec%9d%b4%ed%84%b0-decorator/)

<br>

train4apc ë©”ì†Œë“œëŠ” [PyABSA/pyabsa/core/apc/training/apc_trainer.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/core/apc/training/apc_trainer.py#L566)ì— ì •ì˜ë¼ ìˆëŠ”ë°, @retryë¼ëŠ” ì‹¬ë³¼ì´ ë¶™ì–´ìˆìŠµë‹ˆë‹¤.

**train4apc**

565 ~ 578 line

```python
@retry
def train4apc(opt, from_checkpoint_path, logger):
    random.seed(opt.seed)
    numpy.random.seed(opt.seed)
    torch.manual_seed(opt.seed)
    torch.cuda.manual_seed(opt.seed)

    opt.device = torch.device(opt.device)

    # in case of handling ConnectionError exception
    trainer = Instructor(opt, logger)
    resume_from_checkpoint(trainer, from_checkpoint_path)

    return trainer.run()
```

ì´ëŠ” [PyABSA/pyabsa/utils/pyabsa_utils.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/utils/pyabsa_utils.py#L155)ì— ì •ì˜ëœ retry ë°ì½”ë ˆì´í„° í•¨ìˆ˜ì˜ ì¸ìë¡œ ë„˜ê²¨ ì‹¤í–‰í•˜ëŠ” êµ¬ë¬¸ì…ë‹ˆë‹¤.

**retry**

155 ~ 177 line

```python
def retry(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        count = 5
        while count:

            try:
                return f(*args, **kwargs)
            except (
                TransformerConnectionError,
                requests.exceptions.RequestException,
                requests.exceptions.ConnectionError,
                requests.exceptions.HTTPError,
                requests.exceptions.ConnectTimeout,
                requests.exceptions.ProxyError,
                requests.exceptions.SSLError,
                requests.exceptions.BaseHTTPError,
            ) as e:
                print('Training Exception: {}, will retry later'.format(e))
                time.sleep(60)
                count -= 1

    return decorated
```

retry í•¨ìˆ˜ëŠ” ë‹¨ìˆœíˆ ì¸ìë¡œ ë°›ì€ ë©”ì†Œë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. exception ë°œìƒì‹œ 5ë²ˆê¹Œì§€ ì¬ì‹¤í–‰ í•©ë‹ˆë‹¤.

<br>

ë‹¤ì‹œ train4apc ë©”ì†Œë“œë¡œ ëŒì•„ì˜¤ë©´

```python
@retry
def train4apc(opt, from_checkpoint_path, logger):
    random.seed(opt.seed)
    numpy.random.seed(opt.seed)
    torch.manual_seed(opt.seed)
    torch.cuda.manual_seed(opt.seed)

    opt.device = torch.device(opt.device)

    # in case of handling ConnectionError exception
    trainer = Instructor(opt, logger)
    resume_from_checkpoint(trainer, from_checkpoint_path)

    return trainer.run()
```

ë¨¼ì € í›ˆë ¨ ê²°ê³¼ ì¬í˜„ì„ ìœ„í•´ ì‹œë“œë¥¼ ê³ ì •í•˜ê³ , ë””ë°”ì´ìŠ¤ë¥¼ ì„¤ì •í•œ ë’¤, Instructor í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë¦¬í„´ìœ¼ë¡œ run ë©”ì†Œë“œë¥¼ ì‹¤í–‰í•˜ê¸° ë•Œë¬¸ì— ìƒì„±ìë¶€í„° run ë©”ì†Œë“œê¹Œì§€ ì´ì–´ì„œ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

ì¶”ê°€ì ìœ¼ë¡œ trainerì˜ íŒŒë¼ë¯¸í„°ë¡œ ë“¤ì–´ê°€ëŠ” optì™€ loggerëŠ” ConfigManagerì™€ íŒŒì´ì¬ ê¸°ë³¸ ë‚´ì¥ ëª¨ë“ˆì¸ loggingì˜ loggerì…ë‹ˆë‹¤. (trainer.py 163 ~ 167 line)

```python
if self.checkpoint_save_mode:
  model_path.append(self.train_func(self.config, self.from_checkpoint, self.logger))
else:
  # always return the last trained model if dont save trained model
  model = self.model_class(model_arg=self.train_func(self.config, self.from_checkpoint, self.logger))
```

<br><br>

ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œ ì§‘ì¤‘ì ìœ¼ë¡œ ë‹¤ë£° Instructor í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

### Instructor

##### __init__

Instructor í´ë˜ìŠ¤ëŠ” [PyABSA/pyabsa/core/apc/training/apc_trainer.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/core/apc/training/apc_trainer.py#L39)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

ì½”ë“œê°€ ê¸¸ì–´ì„œ ìƒì„±ì ë¶€ë¶„ì„ ë¨¼ì € ë³´ê² ìŠµë‹ˆë‹¤. (39 ~ 60 line)

**apc_trainer.py**

```python
class Instructor:
    def __init__(self, opt, logger):
        self.logger = logger
        self.opt = opt  # optëŠ” configuration

        self.logger = logger

        self.model = APCEnsembler(self.opt)  # APCEnsembler í´ë˜ìŠ¤
        self.opt = self.model.opt  # â€»self.optê°€ modelì˜ optë¡œ ë°”ë€œ
        self.train_set = self.model.train_set
        self.test_set = self.model.test_set
        self.test_dataloader = self.model.test_dataloader
        self.val_dataloader = self.model.val_dataloader
        self.train_dataloader = self.model.train_dataloader
        self.tokenizer = self.model.tokenizer
        
        initializers = {
            'xavier_uniform_': torch.nn.init.xavier_uniform_,
            'xavier_normal_': torch.nn.init.xavier_normal_,
            'orthogonal_': torch.nn.init.orthogonal_,
        }
        self.initializer = initializers[self.opt.initializer]
```

ë©¤ë²„ ë³€ìˆ˜ë“¤ì„ ì •ì˜í•œ ë¶€ë¶„ì¸ë° APCEnsembler í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ modelë¡œ ì •ì˜í•œ ë’¤, APCEnsemblerì˜ ë©¤ë²„ ë³€ìˆ˜ë“¤ì„ ì €ì¥í•©ë‹ˆë‹¤.

âœš APCEnsemblerëŠ” [PyABSA/pyabsa/core/apc/models/ensembler.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/core/apc/models/ensembler.py#L40)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ ì½”ë“œëŠ” ì‹¤ì œ ëª¨ë¸ì„ ì •ì˜í•œ ë¶€ë¶„ìœ¼ë¡œ ì¶”í›„ì— ë”°ë¡œ ì •ë¦¬í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. (forward í•¨ìˆ˜ê°€ ì •ì˜ë¼ ìˆìŒ)

<br>

**apc_trainer.py**

63 ~ 67 line

```python
# use DataParallel for training if device count larger than 1
if self.opt.auto_device == 'allcuda':
  self.model.to(self.opt.device)
  self.model = torch.nn.parallel.DataParallel(self.model)
else:
  self.model.to(self.opt.device)
```

ì´í›„ ëª¨ë¸ì˜ gpu ì„¤ì •ì— ë”°ë¼ gpuì— ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.

ëª¨ë¸ì˜ auto_deviceê°€ allcudaë¡œ ë¼ ìˆìœ¼ë©´(ëª¨ë“  gpuë¥¼ ì‚¬ìš©), ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ DataParallelì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

<br>

69 ~ 76 line

```python
if hasattr(self.model.models[0], 'eta1') and hasattr(self.model.models[0], 'eta2'):  # 1
  if self.opt.eta == 0:  # 2
    torch.nn.init.uniform_(self.model.models[0].eta1)
    torch.nn.init.uniform_(self.model.models[0].eta2)

    eta1_id = id(self.model.models[0].eta1)  # 3
    eta2_id = id(self.model.models[0].eta2)

    base_params = filter(lambda p: id(p) != eta1_id and id(p) != eta2_id, self.model.models[0].parameters())  # 4
    self.opt.eta_lr = self.opt.learning_rate * 1000 if 'eta_lr' not in self.opt.args else self.opt.args['eta_lr']  # 5
```

#1: modelì—ëŠ” modelsë¼ëŠ” [ModuleList](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/core/apc/models/ensembler.py#L54)ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. êµ¬ê¸€ë§í•œ ë°”ì— ë”°ë¥´ë©´, ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì˜ êµ¬ì„± ìš”ì†Œë¥¼ ë‹´ìœ¼ë ¤ë©´ nn.Moduleì˜ íŒŒë¼ë¯¸í„°ë¡œ ë“±ë¡ë¼ì•¼ í•˜ëŠ”ë°, ì´ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ nn.ModuleListë¥¼ ì‚¬ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤. [Pytorch-ModuleList vs List](https://hongl.tistory.com/279)

ì‚¬ìš© ë°©ë²•ì€ ë¦¬ìŠ¤íŠ¸ì™€ ë™ì¼í•˜ê²Œ ì¸ë±ìŠ¤ë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤.

#1ì—ì„œ ì´ modelsë¼ëŠ” ModuleListê°€ eta1, eta2ë¼ëŠ” ì†ì„±ì„ ê°–ê³  ìˆëŠ”ì§€ í™•ì¸í•œë‹¤. etaëŠ” estimated time of arrivalì˜ ì•½ìë¡œ 1 epochì˜ ì˜ˆìƒ í•™ìŠµ ì‹œê°„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

eta1ê³¼ eta2ê°€ ì¡´ì¬í•˜ë©´ ëª¨ë¸ configurationì˜ etaê°€ 0ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.

0ì´ë¼ë©´ #2ê°€ ì‹¤í–‰ë¼ model.models[0].eta1ê³¼ model.models[0].eta2ë¥¼ 0ì—ì„œ 1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ê· ë“±(uniform)í•˜ê²Œ ì´ˆê¸°í™” ë©ë‹ˆë‹¤.

<p align = "center">
  <img src = "https://user-images.githubusercontent.com/76269316/174199111-dac3bc4d-2ff9-48a0-91d0-aacf369393d7.png">
</p>
<p align = "center">
  torch.nn.init.uniform_
</p>

ì´í›„ #3ì—ì„œ id ë©”ì†Œë“œë¥¼ í†µí•´ eta1ê³¼ eta2ê°€ ì €ì¥ëœ ê³ ìœ  ì£¼ì†Œ ê°’ì„ ì €ì¥í•œ ë’¤, #4ì—ì„œ model.models[0].parameters()ë¥¼ filter í•¨ìˆ˜ë¥¼ í†µí•´ eta1ê³¼ eta2ë¥¼ ì œê±°í•˜ê³  ì¶”ë ¤ëƒ…ë‹ˆë‹¤.

+[íŒŒì´ì¬ filter ë‚´ì¥ í•¨ìˆ˜ ì‚¬ìš©ë²•](https://www.daleseo.com/python-filter/)

#5ì—ì„œëŠ” model.optì— eta_lrì´ë¼ëŠ” ë³€ìˆ˜ê°€ ìˆë‹¤ë©´ ê·¸ê±¸ opt.eta_lrë¡œ ì €ì¥í•˜ê³  ê·¸ë ‡ì§€ ì•Šë‹¤ë©´, ëª¨ë¸ optì— ì €ì¥ëœ learning_rateë¥¼ ê°€ì ¸ì™€ 1,000ì„ ê³±í•œ ê°’ì„ opt.eta_lrë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

<br>

77 ~ 85 line

```python
self.optimizer = init_optimizer(self.opt.optimizer)(  # 6
  [
    {'params': base_params},
    {'params': self.model.models[0].eta1, 'lr': self.opt.eta_lr, 'weight_decay': self.opt.l2reg},
    {'params': self.model.models[0].eta2, 'lr': self.opt.eta_lr, 'weight_decay': self.opt.l2reg}
  ],
  lr=self.opt.learning_rate,
  weight_decay=self.opt.l2reg
)
```

ì´í›„ opt.optimizerë¥¼ ì¸ìë¡œ init_optimizer ë©”ì†Œë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

<br>

init_optimizerëŠ” [PyABSA/pyabsa/utils/pyabsa_utils.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/utils/pyabsa_utils.py#L227)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

**pyabsa_utils.py**

227 ~ 257 line

```python
def init_optimizer(optimizer):
    optimizers = {
        'adadelta': torch.optim.Adadelta,  # default lr=1.0
        'adagrad': torch.optim.Adagrad,  # default lr=0.01
        'adam': torch.optim.Adam,  # default lr=0.001
        'adamax': torch.optim.Adamax,  # default lr=0.002
        'asgd': torch.optim.ASGD,  # default lr=0.01
        'rmsprop': torch.optim.RMSprop,  # default lr=0.01
        'sgd': torch.optim.SGD,
        'adamw': torch.optim.AdamW,
        # 'radam': torch.optim.Adam if torch.version.__version__ <= '1.9.1' else torch.optim.RAdam,
        # 'nadam': torch.optim.Adam if torch.version.__version__ <= '1.9.1' else torch.optim.NAdam,
        # 'sparseadam': torch.optim.Adam if torch.version.__version__ <= '1.9.1' else torch.optim.SparseAdam,
        torch.optim.Adadelta: torch.optim.Adadelta,  # default lr=1.0
        torch.optim.Adagrad: torch.optim.Adagrad,  # default lr=0.01
        torch.optim.Adam: torch.optim.Adam,  # default lr=0.001
        torch.optim.Adamax: torch.optim.Adamax,  # default lr=0.002
        torch.optim.ASGD: torch.optim.ASGD,  # default lr=0.01
        torch.optim.RMSprop: torch.optim.RMSprop,  # default lr=0.01
        torch.optim.SGD: torch.optim.SGD,
        torch.optim.AdamW: torch.optim.AdamW,
        # torch.optim.RAdam: torch.optim.RAdam,
        # torch.optim.NAdam: torch.optim.NAdam,
        # torch.optim.SparseAdam: torch.optim.SparseAdam,
    }
    if optimizer in optimizers:  # 1
        return optimizers[optimizer]
    elif hasattr(torch.optim, optimizer.__name__):  # 2
        return optimizer
    else:
        raise KeyError('Unsupported optimizer: {}. Please use string or the optimizers in torch.optim as your optimizer'.format(optimizer))
```

init_optimizerëŠ” ì¸ìë¡œ ë„˜ê²¨ë°›ì€ optimizerë¥¼ ë¯¸ë¦¬ ì •ì˜í•´ë†“ì€ optimizersì—ì„œ ì°¾ì•„ ë°˜í™˜í•´ì£¼ëŠ” í•¨ìˆ˜ë¡œ, ì •ì˜í•´ë†“ì€ ë¦¬ìŠ¤íŠ¸ ì•ˆì— ìˆëŠ” ê²½ìš° í•´ë‹¹ ê°’ì„ ë¦¬í„´í•˜ê³  (#1) torch.optim.SparseAdamê³¼ ê°™ì´ ì •ì˜í•´ë†“ì§€ëŠ” ì•Šì•˜ì§€ë§Œ, torch.optimì— ì •ì˜ëœ ëª¨ë“ˆë“¤ì€ hasattrë¡œ í™•ì¸í•œ ë’¤, í•´ë‹¹ optimizerë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤.

+pytorchì˜ optimizerëŠ” \_\_name__ì´ë¼ëŠ” ì†ì„±ì´ ì¡´ì¬í•©ë‹ˆë‹¤. [Name of the optimizer being used](https://discuss.pytorch.org/t/name-of-the-optimizer-being-used/100341)

![ìŠ¤í¬ë¦°ìƒ· 2022-06-17 ì˜¤ì „ 10 08 24](https://user-images.githubusercontent.com/76269316/174202381-7f6276a1-3ea2-439b-b491-0c8616e9e723.png)

<br>

init_optimizerë¥¼ í†µí•´ pytorch optimizer ëª¨ë“ˆì„ ë¦¬í„´ë°›ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë‹´ê¸´ paramsì™€ lr, weight_decayë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë„˜ê²¨ optimizerë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

<br>

ë‹¤ì‹œ ìƒì„±ìë¡œ ëŒì•„ì™€ì„œ â—ï¸

**apc_trainer.py**

86 line

```python
else:  # 69 line if
  self.optimizer = init_optimizer(self.opt.optimizer)(
    self.model.parameters(),
    lr=self.opt.learning_rate,
    weight_decay=self.opt.l2reg
  )
```

ë‹¤ì‹œ apc_trainer ì½”ë“œë¡œ ëŒì•„ì™€ì„œ, í•´ë‹¹ elseë¬¸ì€ 69 lineì— ìˆëŠ” ifë¬¸ê³¼ ë§¤ì¹­ë˜ëŠ” elseë¬¸ìœ¼ë¡œ model.modelsì— eta1ê³¼ eta2ê°€ ì—†ëŠ” ê²½ìš° ì‹¤í–‰ë©ë‹ˆë‹¤.

eta1ê³¼ eta2ê°€ ì—†ëŠ” ê²½ìš° 77 ~ 85 lineê³¼ ë™ì¼í•˜ê²Œ optimizerë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ë•ŒëŠ” eta1ê³¼ eta2ê°€ ì—†ìœ¼ë¯€ë¡œ filter í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ë°”ë¡œ model.parametersë¡œ ë„˜ê²¨ì¤ë‹ˆë‹¤.

<br>

92 ~ 93 line

```python
self.train_dataloaders = []
self.val_dataloaders = []
```

ì´í›„ train_dataloaders, val_dataloadersë¥¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

<br>

95 ~ 96 line

```python
if amp:
  self.model, self.optimizer = amp.initialize(self.model, self.optimizer, opt_level="O1")
```

ampëŠ” í´ë˜ìŠ¤ ìœ„ì— importí•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, 30 ~ 36 lineì— ìˆìŠµë‹ˆë‹¤.

```python
try:
    import apex.amp as amp

    # assert torch.version.__version__ < '1.11.0'
    print('Use FP16 via Apex!')
except Exception:
    amp = None
```

ë§Œì•½ ì´ ë•Œ, ì •ìƒì ìœ¼ë¡œ import ëë‹¤ë©´ 95 lineì˜ if ë¬¸ì´ ì‹¤í–‰ë¼ amp.initializeê°€ ì‹¤í–‰ë  ê²ƒì…ë‹ˆë‹¤.

+ampëŠ” automatic mixed precisionì˜ ì•½ìë¡œ mixed precision trainingì„ í†µí•´ í•™ìŠµ ì†ë„ë¥¼ ì¦ê°€ì‹œì¼œì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. [Ampì— ëŒ€í•´ ì•Œì•„ë³´ì (Automatic Mixed Precision)](https://cvml.tistory.com/8)

<br>

98 ~ 101 line

```python
if os.path.exists('init_state_dict.bin'):  # 1
  os.remove('init_state_dict.bin')
if self.opt.cross_validate_fold > 0:  # 2
  torch.save(self.model.state_dict(), 'init_state_dict.bin')
```

ì´ì–´ì„œ ì‚´í´ë³´ë©´, #1ì—ì„œ í˜„ì¬ ê²½ë¡œì— init_state_dict.bin íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ ìˆì„ ê²½ìš° ì œê±°í•©ë‹ˆë‹¤.

#2ì—ì„œëŠ” optì˜ cross_validate_foldê°€ 0ë³´ë‹¤ í´ ê²½ìš° torch.save ë©”ì†Œë“œë¡œ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.

<br>

103 ~ 107 line

```python
self.opt.device = torch.device(self.opt.device)  # 1
if self.opt.device.type == 'cuda':
  self.logger.info("cuda memory allocated:{}".format(torch.cuda.memory_allocated(device=self.opt.device)))

print_args(self.opt, self.logger)  # 2
```

#1 ë””ë°”ì´ìŠ¤ë¥¼ ì„¤ì •í•˜ê³ , gpuë¥¼ ì‚¬ìš©í•  ê²½ìš° logë¥¼ ë‚¨ê¹ë‹ˆë‹¤.

#2 print_args ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ ë¡œê·¸ì™€ í”„ë¡¬í”„íŠ¸ì— ì¶œë ¥í•©ë‹ˆë‹¤.

<br>

[PyABSA/pyabsa/utils/pyabsa_utils.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/utils/pyabsa_utils.py#L35)

```python
def print_args(config, logger=None, mode=0):
    args = [key for key in sorted(config.args.keys())]
    for arg in args:
        if logger:  # loggerì— INFOë¡œ ì¶”ê°€
            logger.info('{0}:{1}\t-->\tCalling Count:{2}'.format(arg, config.args[arg], config.args_call_count[arg]))
        else:  # í”„ë¡¬í”„íŠ¸ì— ì¶œë ¥
            print('{0}:{1}\t-->\tCalling Count:{2}'.format(arg, config.args[arg], config.args_call_count[arg]))
```

![ìŠ¤í¬ë¦°ìƒ· 2022-06-26 ì˜¤í›„ 3 05 58](https://user-images.githubusercontent.com/76269316/175801751-a86561ea-80c9-4ec7-9884-9b832e13c197.png)

![ìŠ¤í¬ë¦°ìƒ· 2022-06-26 ì˜¤í›„ 3 06 45](https://user-images.githubusercontent.com/76269316/175801784-49b5b479-6797-4402-bb4d-5db9f03e888a.png)

ì—¬ê¸°ê¹Œì§€ê°€ Instructor í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ê³¼ì •ì´ê³ , train4apc ë©”ì†Œë“œì—ì„œ run ë©”ì†Œë“œì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¦¬í„´í•˜ê¸° ë•Œë¬¸ì— ê·¸ ë¶€ë¶„ì„ ì´ì–´ì„œ ë³´ê² ìŠµë‹ˆë‹¤.

<br>

##### run

**apc_trainer.py**

559 ~ 562 line

```python
def run(self):
  # Loss and Optimizer
  criterion = nn.CrossEntropyLoss()
  return self._train(criterion)
```

run ë©”ì†Œë“œëŠ” loss ê³„ì‚°ì„ ìœ„í•´ nn.CrossEntropyLoss ëª¨ë“ˆì„ ìƒì„±í•˜ê³  \_train ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.

<br>

##### _train

**apc_trainer.py**

155 ~ 156 line

```python
def _train(self, criterion):
  self.prepare_dataloader(self.train_set)
```

ë¨¼ì € prepare_dataloader ë©”ì†Œë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. ì½”ë“œê°€ ê¸¸ì–´ì„œ ì¡°ê±´ë³„ë¡œ ë‚˜ëˆ ì„œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

<br>

**prepare_dataloader**

127 ~ 130 line

```python
def prepare_dataloader(self, train_set):
  if self.train_dataloader and self.val_dataloader:  # 1
    self.val_dataloaders = [self.val_dataloader]
    self.train_dataloaders = [self.train_dataloader]
```

**train_dataloader, val_dataloaderê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° #1ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.**

train_dataloaderì™€ val_dataloaderëŠ” pytorch DataLoaderì¸ë°, ì´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ìŒ‰ë‹ˆë‹¤.

<br>

132 ~ 137 line

```python
elif self.opt.cross_validate_fold < 1:  # 2
  train_sampler = RandomSampler(self.train_set if not self.train_set else self.train_set)  # shuffleì„ ìœ„í•´ sampler ìƒì„±
  self.train_dataloaders.append(DataLoader(dataset=train_set,
                                           batch_size=self.opt.batch_size,
                                           sampler=train_sampler,
                                           pin_memory=True))
```

**ë§Œì•½ dataloaderê°€ ì¡´ì¬í•˜ì§€ ì•Šê³ , cross_validate_foldê°€ 0 ì´í•˜ì´ë©´ elif êµ¬ë¬¸(#2)ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.**

DataLoaderë¥¼ ìƒˆë¡œ ìƒì„±í•´ train_dataloadersì— ì¶”ê°€í•©ë‹ˆë‹¤. shuffleì„ í•˜ê¸° ìœ„í•´ RandomSamplerë¥¼ ìƒì„±í•´ dataloader íŒŒë¼ë¯¸í„°ë¡œ ë„˜ê²¨ì¤¬ìŠµë‹ˆë‹¤.

<br>

139 ~ 143 line

```python
else:  # 3
  split_dataset = train_set
  len_per_fold = len(split_dataset) // self.opt.cross_validate_fold + 1  # train_set (ì „ì²´ ê°œìˆ˜ / cross_validate_fold íšŸìˆ˜) + 1
  folds = random_split(split_dataset, tuple([len_per_fold] * (self.opt.cross_validate_fold - 1) + [
    len(split_dataset) - len_per_fold * (self.opt.cross_validate_fold - 1)]))  # 4 random_split ë©”ì†Œë“œë¡œ train/validation ë°ì´í„°ì…‹ì„ ë¶„í• 
```

**dataloaderê°€ ì¡´ì¬í•˜ì§€ ì•Šê³  cross_validate_foldê°€  2 ì´ìƒì¸ ê²½ìš°,  else êµ¬ë¬¸(#3)ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.**

train_setì˜ ì „ì²´ ê°œìˆ˜ë¥¼ cross_validate_fold íšŸìˆ˜ë¡œ ë‚˜ëˆˆ ëª«ì„ len_per_foldì— ì €ì¥í•©ë‹ˆë‹¤.

ì´í›„ #4ì—ì„œ random_split ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

ex) train_setì´ 500ê°œì´ê³ , cross_validate_foldê°€ 3ì¸ ê²½ìš°, len_per_foldì™€ foldsëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë©ë‹ˆë‹¤.

![ìŠ¤í¬ë¦°ìƒ· 2022-06-17 ì˜¤í›„ 9 55 20](https://user-images.githubusercontent.com/76269316/174302467-23b93502-439e-41ca-a001-127811f1f23a.png)

ì´ì–´ì„œ, 145 ~ 153 lineì„ ë³´ê² ìŠµë‹ˆë‹¤.

```python
for f_idx in range(self.opt.cross_validate_fold):
  train_set = ConcatDataset([x for i, x in enumerate(folds) if i != f_idx])  # 1
  val_set = folds[f_idx]  # 2
  train_sampler = RandomSampler(train_set if not train_set else train_set)  # 3
  val_sampler = SequentialSampler(val_set if not val_set else val_set)
  self.train_dataloaders.append(  # 4
    DataLoader(dataset=train_set, batch_size=self.opt.batch_size, sampler=train_sampler))
  self.val_dataloaders.append(
    DataLoader(dataset=val_set, batch_size=self.opt.batch_size, sampler=val_sampler))
```

ì´ ë¶€ë¶„ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” êµì°¨ ê²€ì¦ì— ëŒ€í•´ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤. ì œê°€ ì •ë¦¬í•´ë†“ì€ [í¬ìŠ¤íŒ…](https://seominseok4834.github.io/machine%20learning/2.scikit-learn-machine-learning-in-python/#%EA%B5%90%EC%B0%A8-%EA%B2%80%EC%A6%9D)ì„ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

#1: ìœ„ì—ì„œ ë‚˜ëˆˆ ë°ì´í„°ì…‹ì—ì„œ f_idxë²ˆì§¸ ë°ì´í„°ë¥¼ ì œì™¸í•œ ëª¨ë“  ë°ì´í„°ë¥¼ ConcatDatasetìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

#2: f_idxë²ˆì§¸ ë°ì´í„°ëŠ” validation ë°ì´í„°ì…‹ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤.

ì´ë¥¼ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

<img width="1512" alt="ìŠ¤í¬ë¦°ìƒ· 2022-06-17 ì˜¤í›„ 10 19 57" src="https://user-images.githubusercontent.com/76269316/174306266-63a3c5c2-1c2e-4a60-b7de-7d0c95917185.png" style="zoom:50%;" >

#3: shuffleì„ ìœ„í•´ train_sampler, val_samplerë¥¼ ìƒì„±í•©ë‹ˆë‹¤. *Â¿ ê·¸ëŸ°ë° ì™œ train_setì´ë‘ val_setì´ ë¹„ì–´ìˆì„ ë•Œ samplerë¥¼ ìƒì„±í•˜ëŠ” ê±¸ê¹Œìš”*

#4: DataLoaderë¥¼ ì •ì˜í•œ ë’¤, ê°ê° train_dataloaders, val_dataloadersì— ì¶”ê°€í•´ì¤ë‹ˆë‹¤.

ì—¬ê¸°ê¹Œì§€ê°€ prepare_dataloader ë©”ì†Œë“œì´ê³ , ë‹¤ì‹œ \_train ë©”ì†Œë“œë¡œ ëŒì•„ê°€ê² ìŠµë‹ˆë‹¤.

<br>

##### \_train

158 ~ 160 line

```python
if self.opt.warmup_step >= 0:
  self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=len(self.train_dataloaders[0]) * self.opt.num_epoch)
  self.warmup_scheduler = warmup.UntunedLinearWarmup(self.optimizer)
```

model optì˜ warumup_stepì´ 0 ì´ìƒì¸ ê²½ìš°, lr_schedulerëŠ” [cosine annealing scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#cosineannealinglr)ë¥¼ ì‚¬ìš©í•˜ê³ , warmup_schedulerëŠ” [pytorch_warmup](https://github.com/Tony-Y/pytorch_warmup)ì˜ UntunedLinearWarmupì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

cf) [Pytorch Learning Rate Scheduler ì •ë¦¬](https://gaussian37.github.io/dl-pytorch-lr_scheduler/)

<br>

162 ~ 165 line

```python
if len(self.val_dataloaders) > 1:
  return self._k_fold_train_and_evaluate(criterion)
else:
  return self._train_and_evaluate(criterion)
```

val_dataloadersì— 2ê°œ ì´ìƒì˜ dataloaderê°€ ë‹´ê²¨ ìˆëŠ” ê²½ìš° \_k_fold_train_and_evaluate ë©”ì†Œë“œë¥¼, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° \_train_and_evaluate ë©”ì†Œë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

ë¨¼ì € \_train_and_evaluate ë©”ì†Œë“œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤ .. ğŸ¥²

<br><br>

##### _train_and_evaluate

ì½”ë“œê°€ ì•„ì£¼ ê¸¸ê¸° ë•Œë¬¸ì— ë¼ì¸ë³„ë¡œ ì°¨ê·¼ì°¨ê·¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

167 ~ 181 line

```python
global_step = 0
max_fold_acc = 0
max_fold_f1 = 0
save_path = '{0}/{1}_{2}'.format(self.opt.model_path_to_save,
                                 self.opt.model_name,
                                 self.opt.dataset_name
                                )

self.opt.metrics_of_this_checkpoint = {'acc': 0, 'f1': 0}
self.opt.max_test_metrics = {'max_apc_test_acc': 0, 'max_apc_test_f1': 0}

Total_params = 0
Trainable_params = 0
NonTrainable_params = 0
```

í›ˆë ¨ì‹œ ì‚¬ìš©í•  ë³€ìˆ˜ë“¤ì„ ì •ì˜í•œ ë¶€ë¶„ì…ë‹ˆë‹¤.

<br>

183 ~ 189 line

```python
for param in self.model.parameters():
  mulValue = numpy.prod(param.size())  # 1
  Total_params += mulValue  # 2
  if param.requires_grad:  # 3
    Trainable_params += mulValue
  else:
    NonTrainable_params += mulValue
```

model.parametersë¡œ íŒŒë¼ë¯¸í„°ë“¤ì„ ìˆœíšŒí•˜ë©°, íŒŒë¼ë¯¸í„°ì˜ ê°œìˆ˜ë¥¼ êµ¬í•©ë‹ˆë‹¤.

#1ì—ì„œ numpy.prod ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ param.sizeë¡œ ë¦¬í„´ ë°›ì€ íŒŒë¼ë¯¸í„°ì˜ sizeë¥¼ ê³±í•´ì¤€ë‹¤.

cf) [numpy.prod](https://codetorial.net/numpy/functions/numpy_prod.html), [torch.nn.Module.parameters](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters)

ì´ë¥¼ ì „ì²´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” Total_paramsì— ë”í•´ì£¼ê³ (#2), íŒŒë¼ë¯¸í„°ì˜ requires_gradë¥¼ í™•ì¸í•´(#3) Trueì¸ ê²½ìš°(gradientë¥¼ ê³„ì‚°í•˜ëŠ” íŒŒë¼ë¯¸í„°ì¸ ê²½ìš°) Trainable_paramsì— ë”í•˜ê³ , ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” NonTrainable_paramsì— ë”í•©ë‹ˆë‹¤.

<br>

191 ~ 193 line

```python
patience = self.opt.patience + self.opt.evaluate_begin
if self.opt.log_step < 0:
  self.opt.log_step = len(self.train_dataloaders[0]) if self.opt.log_step < 0 else self.opt.log_step
```

patienceëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ metricì´ í–¥ìƒë˜ì§€ ì•Šì„ ë•Œ ëª‡ ì—í­ì„ ì°¸ì€ ë’¤ learning rateë¥¼ ì¤„ì¼ ê²ƒì¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.

cf) [PyTorchê°€ ì œê³µí•˜ëŠ” Learning rate scheduler ì •ë¦¬](https://sanghyu.tistory.com/113)

*ê·¸ëŸ°ë° patienceì— ì™œ evaluate_beginì„ ë”í• ê¹Œìš” Â¿ ..*

ì–´ì¨Œë“  patienceë¥¼ ì •ì˜í•œ ë‹¤ìŒ, log_stepì´ 0ë³´ë‹¤ ì‘ìœ¼ë©´ log_stepì„ train_dataloadersì˜ 0ë²ˆì§¸ ì¸ë±ìŠ¤ì— ìˆëŠ” í›ˆë ¨ ë°ì´í„°ì…‹ì˜ ê¸¸ì´ë¥¼ log_stepìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤.

<br>

195 ~ 203 line

```python
self.logger.info("***** Running training for Aspect Polarity Classification *****")
self.logger.info("Training set examples = %d", len(self.train_set))
if self.test_set:
  self.logger.info("Test set examples = %d", len(self.test_set))
  self.logger.info("Total params = %d, Trainable params = %d, Non-trainable params = %d", Total_params,
                   Trainable_params, NonTrainable_params)
  self.logger.info("Batch size = %d", self.opt.batch_size)
  self.logger.info("Num steps = %d", len(self.train_dataloaders[0]) // self.opt.batch_size * self.opt.num_epoch)
  postfix = ''
```

ì´í›„ í›ˆë ¨ë°ì´í„°, í…ŒìŠ¤íŠ¸ë°ì´í„° ê°œìˆ˜, ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë“±ì„ information logë¡œ ì €ì¥í•©ë‹ˆë‹¤.

postfixëŠ” tqdm í”„ë¡œì„¸ìŠ¤ë°”ì—ì„œ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•  ë•Œ ì‚¬ìš©í•  ë³€ìˆ˜ì…ë‹ˆë‹¤.

<br>

204 ~ 209 line

```python
for epoch in range(self.opt.num_epoch):
  patience -= 1
  iterator = tqdm(self.train_dataloaders[0], postfix='Epoch:{}'.format(epoch))
```

ëª¨ë¸ì— ì •ì˜ëœ ì—í­ ìˆ˜ë§Œí¼ ë°˜ë³µë˜ê³ , ì—í­ë§ˆë‹¤ patienceê°€ 1 ê°ì†Œí•©ë‹ˆë‹¤. (metricì´ í–¥ìƒë˜ë“ , ì•ˆë˜ë“  ì—í­ë§ˆë‹¤ patienceë¥¼ ì¤„ì´ê¸° ë•Œë¬¸ì— patienceì— evaluate_beginì„ ë”í•´ì¤¬ë‚˜ ?)

ì´í›„ train_dataloadersë¥¼ tqdmìœ¼ë¡œ ê°ìŒ‰ë‹ˆë‹¤. cf) [tqdm ì‚¬ìš©ë²•](https://skillmemory.tistory.com/entry/tqdm-%EC%82%AC%EC%9A%A9%EB%B2%95-python-%EC%A7%84%ED%96%89%EB%A5%A0-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8A%A4%EB%B0%94)

<br>

210 ~ 217 line

```python
for i_batch, sample_batched in enumerate(iterator):
  global_step += 1
  # switch model to training mode, clear gradient accumulators
  self.model.train()  # 1
  self.optimizer.zero_grad()  # 2
  inputs = {col: sample_batched[col].to(self.opt.device) for col in self.opt.inputs_cols}  # 3
  outputs = self.model(inputs)  # 4
  targets = sample_batched['polarity'].to(self.opt.device)  # 5
```

ì´ë²ˆ ë¼ì¸ë¶€í„°ê°€ ì‹¤ì œë¡œ gradientì™€ lossë¥¼ ê³„ì‚°í•´ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ì½”ë“œë¡œ, Iterator(train_dataloaders[0])ë¥¼ enumerateë¡œ ìˆœíšŒí•©ë‹ˆë‹¤.

#1: ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ë³€ê²½í•œ ë’¤, gradient ê°’ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™” í•©ë‹ˆë‹¤.(#2)

#3: sample_batched ë°ì´í„°ì˜ opt.inputs_cols ì—´ë“¤ì„ deviceë¡œ í• ë‹¹í•©ë‹ˆë‹¤. *Â¿ opt.inputs_colsê°€ ë­”ì§€ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤ ..*

#4: outputsëŠ” modelì— ì…ë ¥ ë°ì´í„°ë¥¼ ë„£ì—ˆì„ ë•Œì˜ ì˜ˆì¸¡ê°’ìœ¼ë¡œ, logitsì™€ loss ê°’ì´ dictionaryë¡œ ë‹´ê²¨ìˆìŠµë‹ˆë‹¤. (ì´ ë¶€ë¶„ì€ AOCEnsemblerë¥¼ ë‹¤ë£°ë•Œ ë‚˜ì˜µë‹ˆë‹¤) targetì€ ì‹¤ì œ ì •ë‹µ ë ˆì´ë¸”ì¸ë°, ë ˆì´ë¸”ë„ ì…ë ¥ ë°ì´í„°ì²˜ëŸ¼ deviceë¡œ í• ë‹¹í•©ë‹ˆë‹¤.(#5)

<br>

219 ~ 222 line

```python
sen_logits = outputs['logits']  # 1
loss = criterion(sen_logits, targets)  # 2
if isinstance(outputs, dict) and 'loss' in outputs:
  loss += torch.sum(outputs['loss'])
```

outputsì—ì„œ logitsì„ êº¼ë‚´ lossë¥¼ ê³„ì‚°í•œ ë’¤(#1 ~ #2), outputs ì•ˆì— ê³„ì‚°ëœ lossê°€ ìˆëŠ” ê²½ìš° ì´ë¥¼ ë”í•´ì¤ë‹ˆë‹¤.

<br>

224 ~ 225 line

```python
if self.opt.auto_device == 'allcuda':
  loss = loss.mean()
```

ì´í›„ ë§Œì•½ ëª¨ë“  gpuë¥¼ ì‚¬ìš©í•˜ê¸°ë¡œ í–ˆë‹¤ë©´, ì´ë¥¼ í‰ê· ëƒ…ë‹ˆë‹¤.

<br>

227 ~ 232 line

```python
if amp:  # 1
  with amp.scale_loss(loss, self.optimizer) as scaled_loss:
    scaled_loss.backward()
  else:
    loss.backward()
    self.optimizer.step()
```

#1: amp(automatic mixed precision)ê°€ ì„¤ì •ëœ ê²½ìš° amp.scale_lossë¡œ backwardë¥¼ ì§„í–‰í•˜ê³ , ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° pytorchë¥¼ ì‚¬ìš©í•´ backwardë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

<br>

234 ~ 236 line

```python
if self.opt.warmup_step >= 0:
  with self.warmup_scheduler.dampening():  # 1
    self.lr_scheduler.step()
```

warm upì€ [Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/abs/1812.01187)ì—ì„œ ë“±ì¥í•œ í•™ìŠµ ë°©ë²•ìœ¼ë¡œ, ëª¨ë“  íŒŒë¼ë¯¸í„°ëŠ” ì´ˆê¸°ì— ëœë¤ ê°’ìœ¼ë¡œ ì„¤ì •ë˜ëŠ”ë°, í•™ìŠµ ì´ˆê¸°ë¶€í„° í° learning rateë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµì˜ ë¶ˆì•ˆì •ì„±ì„ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ë”°ë¼ì„œ ì´ˆê¸°ì—ëŠ” ì‘ì€ í•™ìŠµë¥ ë¡œ ì‹œì‘í•´, í›ˆë ¨ì´ ì•ˆì •í™”ë˜ë©´ ì´ˆê¸° í•™ìŠµë¥ ë¡œ ëŒì•„ê°€ëŠ” ë°©ë²•ë¡ ì´ë‹¤.

![ìŠ¤í¬ë¦°ìƒ· 2022-06-18 ì˜¤í›„ 10 53 31](https://user-images.githubusercontent.com/76269316/174441623-1cf13f57-7baa-4dd6-869c-e7b989a8d018.png)

<br>

í•™ìŠµ ì†ë„ëŠ” warm up factorì— ì˜í•´ ì•½í™”(dampen)ë˜ëŠ”ë° ì´ ë¶€ë¶„ì„ ë‹´ë‹¹í•˜ëŠ” ì½”ë“œê°€ #1ì´ë‹¤.

<img src="https://user-images.githubusercontent.com/76269316/174441762-bad9871d-dd0b-40a5-8ed7-d652ac6c2973.png" alt="image" style="zoom: 33%;" />

<br>

239 ~ 244 line

```python
if global_step % self.opt.log_step == 0:  # 1
  if self.opt.dataset_file['test'] and epoch >= self.opt.evaluate_begin:  # 2
    if self.val_dataloaders:  # 3
      test_acc, f1 = self._evaluate_acc_f1(self.val_dataloaders[0])
    else:
      test_acc, f1 = self._evaluate_acc_f1(self.test_dataloader)
```

global_stepì„ log_stepìœ¼ë¡œ ë‚˜ëˆ´ì„ ë•Œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ëŠ” ê²½ìš°(#1) í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

#2: test ë°ì´í„°ì…‹ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—í­ì´ evaluate_beginë³´ë‹¤ í° ê²½ìš° í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

#3: ë§Œì•½ ê²€ì¦ ë°ì´í„°ì…‹ì´ ì¡´ì¬í•œë‹¤ë©´ ê²€ì¦ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ í‰ê°€í•˜ê³ , ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

<br>

##### _evaluate_acc_f1

**[apc_trainer.py](https://github.com/yangheng95/PyABSA/blob/4320db7dd0cfa0053a48169e59ff44403482eaf1/pyabsa/core/apc/training/apc_trainer.py#L509)**

525 ~ 557 line

```python
def _evaluate_acc_f1(self, test_dataloader):
    # switch model to evaluation mode
    self.model.eval()  # 1
    n_test_correct, n_test_total = 0, 0  # 2
    t_targets_all, t_outputs_all = None, None
    with torch.no_grad():  # 3
        for t_batch, t_sample_batched in enumerate(test_dataloader):  # 4
            # 5
            t_inputs = {col: t_sample_batched[col].to(self.opt.device) for col in self.opt.inputs_cols}

            t_targets = t_sample_batched['polarity'].to(self.opt.device)

            t_outputs = self.model(t_inputs)

            if isinstance(t_outputs, dict):  # 6
                sen_outputs = t_outputs['logits']
            else:
                sen_outputs = t_outputs

            n_test_correct += (torch.argmax(sen_outputs, -1) == t_targets).sum().item()  # 7
            n_test_total += len(sen_outputs)

            if t_targets_all is None:  # 8
                t_targets_all = t_targets
                t_outputs_all = sen_outputs
            else:
                t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)
                t_outputs_all = torch.cat((t_outputs_all, sen_outputs), dim=0)

    # 9
    test_acc = n_test_correct / n_test_total
    f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=list(range(self.opt.polarities_dim)), average='macro')
    return test_acc, f1
```

#1: ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•´, eval ëª¨ë“œë¡œ ë³€ê²½í•©ë‹ˆë‹¤.

#2: ì •í™•ë„ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ ë§ì¶˜ ê°œìˆ˜, ì „ì²´ í…ŒìŠ¤íŠ¸ ê°œìˆ˜ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

#3: no_grad ë©”ì†Œë“œë¡œ pytorchì˜ autograd ì—”ì§„ì„ ì¢…ë£Œ ì‹œí‚¨ë‹¤. cf) [Pytorchì—ì„œ no_grad()ì™€ eval()ì˜ ì •í™•í•œ ì°¨ì´ëŠ” ë¬´ì—‡ì¼ê¹Œ?](https://coffeedjimmy.github.io/pytorch/2019/11/05/pytorch_nograd_vs_train_eval/)

#4: ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œí¼ ë°ì´í„°ë¥¼ ë½‘ì•„ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.

#5: í›ˆë ¨ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ë°ì´í„°ì—ì„œ inputs_cols ì—´ë“¤ì„ ë½‘ì€ ë’¤, deviceë¡œ í• ë‹¹í•©ë‹ˆë‹¤. ë˜í•œ ì‹¤ì œ ì •ë‹µ ë ˆì´ë¸”(ê°ì„± ê·¹ì„±)ë„ deviceë¡œ í• ë‹¹í•˜ê³ , model ì…ë ¥ìœ¼ë¡œ t_inputsë¥¼ ë„£ì–´ outputsë¥¼ ë¦¬í„´ ë°›ìŠµë‹ˆë‹¤. outputsëŠ” modelì— ì…ë ¥ ë°ì´í„°ë¥¼ ë„£ì—ˆì„ ë•Œì˜ ì˜ˆì¸¡ê°’ìœ¼ë¡œ, logitsì™€ loss ê°’ì´ dictionaryë¡œ ë‹´ê²¨ìˆìŠµë‹ˆë‹¤.

#6: outputsê°€ dictionaryì¸ì§€ í™•ì¸í•˜ê³  logitsì„ sen_outputsë¡œ ì •ì˜í•©ë‹ˆë‹¤.

#7: torch.argmaxëŠ” ìµœëŒ€ê°’ì„ ê°–ëŠ” ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜í•˜ëŠ”ë°, ì •ë‹µ ë ˆì´ë¸”(t_targets)ê³¼ ë¹„êµí•´ ê°™ì€ ê²ƒë“¤ì„ sum ë©”ì†Œë“œë¡œ ë”í•´ ë§ì¶˜ ê°œìˆ˜ë¥¼ ê³„ì‚°í•œ ë’¤ ì´ë¥¼ n_test_correctì— ë”í•´ì¤ë‹ˆë‹¤. sen_outputsì˜ ê¸¸ì´ëŠ” n_test_totalë¡œ ì €ì¥í•©ë‹ˆë‹¤.

#8: f1 scoreë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ t_targetsì™€ sen_outputsë¥¼ t_targets_allê³¼ t_outputs_allì— ì €ì¥í•©ë‹ˆë‹¤.

#9: ë§ì¶˜ ê°œìˆ˜ / ì „ì²´ ê°œìˆ˜ë¡œ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ê³ , scikit-learnì˜ metrics.f1_scoreë¥¼ ì´ìš©í•´ f1 scoreë¥¼ ê³„ì‚°í•œ ë’¤ ê²°ê³¼ë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.

<br>

ë‹¤ì‹œ \_train_and_evaluate ë©”ì†Œë“œë¡œ ëŒì•„ì™€ì„œ,

##### _train_and_evaluate

244 ~ 251 line

```python
if global_step % self.opt.log_step == 0:
  if self.opt.dataset_file['test'] and epoch >= self.opt.evaluate_begin:
    if self.val_dataloaders:
      test_acc, f1 = self._evaluate_acc_f1(self.val_dataloaders[0])
    else:
      test_acc, f1 = self._evaluate_acc_f1(self.test_dataloader)

      # 1
      self.opt.metrics_of_this_checkpoint['acc'] = test_acc
      self.opt.metrics_of_this_checkpoint['f1'] = f1

      # 2
      sum_acc += test_acc
      sum_f1 += f1
```

\_evaluate_acc_f1 ë©”ì†Œë“œë¡œ ë¦¬í„´ë°›ì€ test_accì™€ f1ì„ optì— ì €ì¥í•œ ë’¤(#1) sum_accì™€ sum_f1ì— ê°ê° ë”í•´ì¤ë‹ˆë‹¤.(#2)

<br>

256 ~ 302 line

ì½”ë“œê°€ ì¢€ ê¸´ë°, í…ŒìŠ¤íŠ¸ ì •í™•ë„ì™€ f1 score ë‘˜ ì¤‘ í•˜ë‚˜ê°€ ì´ì „ ê¸°ë¡ë³´ë‹¤ ë†’ì€ ê²½ìš°(#1), ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°(#2)ë¡œ ë‚˜ëˆ ì„œ ë³´ê² ìŠµë‹ˆë‹¤.

```python
if test_acc > max_fold_acc or f1 > max_fold_f1:  # 1

    if test_acc > max_fold_acc:
        patience = self.opt.patience
        max_fold_acc = test_acc

    if f1 > max_fold_f1:
        max_fold_f1 = f1
        patience = self.opt.patience

    if self.opt.model_path_to_save:
        if not os.path.exists(self.opt.model_path_to_save):
            os.makedirs(self.opt.model_path_to_save)
        if save_path:
            try:
                shutil.rmtree(save_path)
                # logger.info('Remove sub-optimal trained model:', save_path)
            except:
                # logger.info('Can not remove sub-optimal trained model:', save_path)
                pass
        save_path = '{0}/{1}_{2}_acc_{3}_f1_{4}/'.format(self.opt.model_path_to_save,
                                                         self.opt.model_name,
                                                         self.opt.dataset_name,
                                                         round(test_acc * 100, 2),
                                                         round(f1 * 100, 2)
                                                         )

        if test_acc > self.opt.max_test_metrics['max_apc_test_acc']:
            self.opt.max_test_metrics['max_apc_test_acc'] = test_acc
        if f1 > self.opt.max_test_metrics['max_apc_test_f1']:
            self.opt.max_test_metrics['max_apc_test_f1'] = f1

        save_model(self.opt, self.model, self.tokenizer, save_path)

postfix = ('Epoch:{} | Loss:{:.4f} | Acc:{:.2f}(max:{:.2f}) |'
           ' F1:{:.2f}(max:{:.2f})'.format(epoch,
                                           loss.item(),
                                           test_acc * 100,
                                           max_fold_acc * 100,
                                           f1 * 100,
                                           max_fold_f1 * 100
                                           ))

else:  # 2
    if self.opt.save_mode and epoch >= self.opt.evaluate_begin:
        save_model(self.opt, self.model, self.tokenizer, save_path + '_{}/'.format(loss.item()))
    postfix = 'Epoch:{} | Loss: {} | No evaluation until epoch:{}'.format(epoch, round(loss.item(), 8), self.opt.evaluate_begin)
```

<br>

256 ~ 288 line

```python
if test_acc > max_fold_acc or f1 > max_fold_f1:  # 1
    # 2
    if test_acc > max_fold_acc:
        patience = self.opt.patience
        max_fold_acc = test_acc

    if f1 > max_fold_f1:
        max_fold_f1 = f1
        patience = self.opt.patience

    if self.opt.model_path_to_save:  # 3
        if not os.path.exists(self.opt.model_path_to_save):  # 4
            os.makedirs(self.opt.model_path_to_save)
        if save_path:
            try:  # 5
                shutil.rmtree(save_path)
                # logger.info('Remove sub-optimal trained model:', save_path)
            except:
                # logger.info('Can not remove sub-optimal trained model:', save_path)
                pass
        # 6
        save_path = '{0}/{1}_{2}_acc_{3}_f1_{4}/'.format(self.opt.model_path_to_save,
                                                         self.opt.model_name,
                                                         self.opt.dataset_name,
                                                         round(test_acc * 100, 2),
                                                         round(f1 * 100, 2))
        if test_acc > self.opt.max_test_metrics['max_apc_test_acc']:
            self.opt.max_test_metrics['max_apc_test_acc'] = test_acc
        if f1 > self.opt.max_test_metrics['max_apc_test_f1']:
            self.opt.max_test_metrics['max_apc_test_f1'] = f1

        # 8
        save_model(self.opt, self.model, self.tokenizer, save_path)

# 9
postfix = ('Epoch:{} | Loss:{:.4f} | Acc:{:.2f}(max:{:.2f}) |'
           ' F1:{:.2f}(max:{:.2f})'.format(epoch,
                                           loss.item(),
                                           test_acc * 100,
                                           max_fold_acc * 100,
                                           f1 * 100,
                                           max_fold_f1 * 100
                                           ))
```

#1: ì •í™•ë„ë‚˜ f1 scoreê°€ ì´ì „ ê¸°ë¡ë³´ë‹¤ ë†’ì€ ê²½ìš° í•´ë‹¹ ì¡°ê±´ë¬¸ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.

#2: ì •í™•ë„ì™€ f1 score ì¤‘ í–¥ìƒëœ í‰ê°€ ì§€ì˜ ìµœëŒ€ ìŠ¤ì½”ì–´ë¥¼ ê°±ì‹ í•˜ê³ , patienceë¥¼ ì¬ì„¤ì •í•©ë‹ˆë‹¤.

#3: ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ë§Œì•½ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ìƒˆë¡œ ìƒì„±í•œ ë’¤(#4) ì €ì¥ ê²½ë¡œì˜ í•˜ìœ„ ë””ë ‰í† ë¦¬ì™€ íŒŒì¼ì„ ëª¨ë‘ ì‚­ì œí•©ë‹ˆë‹¤(#5)

#6: ì €ì¥í•  ê²½ë¡œì™€ íŒŒì¼ëª…ì„ ì„¤ì •í•©ë‹ˆë‹¤.

#7: ì´ë²ˆì—ëŠ” optì— ì €ì¥ëœ ì •í™•ë„ì™€ f1 score ì¤‘ í–¥ìƒëœ í‰ê°€ ì§€ì˜ ìµœëŒ€ ìŠ¤ì½”ì–´ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.

#8: save_model ë©”ì†Œë“œë¡œ ëª¨ë¸ì„ ì €ì¥í•˜ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ tqdm í”„ë¡œì„¸ìŠ¤ë°”ì— í‘œì‹œí•  ë©”ì‹œì§€ë¥¼ postfixë¡œ ì„¤ì •í•©ë‹ˆë‹¤(#9)

<br>

299 ~ 307 line

```python
        else:  # ì •í™•ë„ë‚˜ f1 scoreê°€ í–¥ìƒë˜ì§€ ì•Šì€ ê²½ìš°
            if self.opt.save_mode and epoch >= self.opt.evaluate_begin:
                save_model(self.opt, self.model, self.tokenizer, save_path + '_{}/'.format(loss.item()))
            postfix = 'Epoch:{} | Loss: {} | No evaluation until epoch:{}'.format(epoch, round(loss.item(), 8), self.opt.evaluate_begin)

    # tqdm refresh
    iterator.postfix = postfix
    iterator.refresh()
if patience < 0:
    break
```

ì •í™•ë„ë‚˜ f1 scoreê°€ í–¥ìƒë˜ì§€ ì•Šì€ ê²½ìš° í•´ë‹¹ ì½”ë“œê°€ ì‹¤í–‰ë©ë‹ˆë‹¤. evaluate_beginë³´ë‹¤ ì—í­ì´ í° ê²½ìš° save_model ë©”ì†Œë“œë¥¼ í†µí•´ ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.

ë§ˆì°¬ê°€ì§€ë¡œ tqdm í”„ë¡œì„¸ìŠ¤ë°”ì— í‘œì‹œí•  ë©”ì‹œì§€ë¥¼ postfixë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ì´í›„ tqdm í”„ë¡œì„¸ìŠ¤ë°”ë¥¼ ë¦¬í”„ë ˆì‰¬ í•´ì¤ë‹ˆë‹¤.

ë§Œì•½ í›ˆë ¨ì„ ì§„í–‰í•˜ë‹¤ patienceê°€ 0ë³´ë‹¤ ì‘ì•„ì§€ë©´, í›ˆë ¨ì„ ì¤‘ì§€í•©ë‹ˆë‹¤.

<br>

ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” save_model ë©”ì†Œë“œë¥¼ ê°„ë‹¨í•˜ê²Œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

save_model ë©”ì†Œë“œëŠ” [PyABSA/pyabsa/utils/file_utils.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/utils/file_utils.py#L242)ì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

**file_utils.py**

242 ~ 290 line

```python
def save_model(opt, model, tokenizer, save_path):
    # Save a trained model, configuration and tokenizer
    if hasattr(model, 'module') or hasattr(model, 'core'):  # 1
        # print("save model from data-parallel!")
        model_to_save = model.module
    else:
        # print("save a single cuda model!")
        model_to_save = model
```

#1: modelì´ moduleì´ë‚˜ core ì†ì„±ì„ ê°–ê³  ìˆëŠ”ì§€ í™•ì¸í•œ ë’¤ model_to_saveì— ì €ì¥í•©ë‹ˆë‹¤.

configurationì— save_modeë¼ëŠ” ë³€ìˆ˜ê°€ ì €ì¥ë¼ ìˆëŠ”ë°, ì´ì— ë”°ë¼ save_model ë©”ì†Œë“œê°€ ë‹¤ë¥´ê²Œ ë™ì‘í•©ë‹ˆë‹¤. save_modeë³„ë¡œ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ë‚˜ëˆ ì„œ ë³´ê² ìŠµë‹ˆë‹¤.

<br>

**file_utils.py**

250 ~ 263 line

```python
if opt.save_mode == 1 or opt.save_mode == 2:
  if not os.path.exists(save_path):  # 1
    os.makedirs(save_path)
    # 2
    f_config = open(save_path + opt.model_name + '.config', mode='wb')
    f_tokenizer = open(save_path + opt.model_name + '.tokenizer', mode='wb')
    pickle.dump(opt, f_config)
    pickle.dump(tokenizer, f_tokenizer)
    f_config.close()
    f_tokenizer.close()
    # 3
    save_args(opt, save_path + opt.model_name + '.args.txt')
    if opt.save_mode == 1:  # 4
      torch.save(model_to_save.state_dict(), save_path + opt.model_name + '.state_dict')  # save the state dict
    elif opt.save_mode == 2:
      torch.save(model.cpu(), save_path + opt.model_name + '.model')  # save the state dict
```

ë¨¼ì € save_modeê°€ 1ì´ë‚˜ 2ì¸ ê²½ìš°ì…ë‹ˆë‹¤.

#1: ì €ì¥ ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ëŠ” ê²½ìš° ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

#2: configurationrê³¼ tokenizerë¥¼ pickle ëª¨ë“ˆë¡œ dumpí•©ë‹ˆë‹¤.

#3: save_args ë©”ì†Œë“œëŠ” [PyABSA/pyabsa/utils/pyabsa_utils.py](https://github.com/yangheng95/PyABSA/blob/42aac7857fd6fb781184433f68063581ed1c3cc4/pyabsa/utils/pyabsa_utils.py#L27)ì— ì •ì˜ëœ ë©”ì†Œë“œë¡œ txt íŒŒì¼ë¡œ configurationì— ì €ì¥ëœ ì¸ìë“¤ì„ ì €ì¥í•©ë‹ˆë‹¤.

```python
def save_args(config, save_path):
    f = open(os.path.join(save_path), mode='w', encoding='utf8')
    for arg in config.args:
        if config.args_call_count[arg]:
            f.write('{}: {}\n'.format(arg, config.args[arg]))
    f.close()
```

save_modeê°€ 1ì¸ ê²½ìš°(#4) state_dict í˜•íƒœë¡œ, save_modeê°€ 2ì¸ ê²½ìš°(#5)ì—ëŠ” model.cpuë¡œ ì €ì¥í•©ë‹ˆë‹¤.

<br>

**file_utils.py**

265 ~ 286 line

```python
  elif opt.save_mode == 3:
      # save the fine-tuned bert model
      # 1
      model_output_dir = save_path + 'fine-tuned-pretrained-model'
      if not os.path.exists(model_output_dir):
          os.makedirs(model_output_dir)

      # 2
      output_model_file = os.path.join(model_output_dir, 'pytorch_model.bin')
      output_config_file = os.path.join(model_output_dir, 'config.json')

      # 3
      if hasattr(model_to_save, 'bert4global'):
          model_to_save = model_to_save.bert4global
      elif hasattr(model_to_save, 'bert'):
          model_to_save = model_to_save.bert
      else:
          model_to_save = model_to_save
          # raise RuntimeError('No pretrained model found to save')

      torch.save(model_to_save.state_dict(), output_model_file)
      model_to_save.config.to_json_file(output_config_file)

      # 4
      if hasattr(tokenizer, 'tokenizer'):
          tokenizer.tokenizer.save_pretrained(model_output_dir)
      else:
          tokenizer.save_pretrained(model_output_dir)
```

ë‹¤ìŒì€ save_modeê°€ 3ì¸ ê²½ìš°ì…ë‹ˆë‹¤. fine-tuningí•œ ëª¨ë¸ì„ ì €ì¥í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.

#1: ì €ì¥ ê²½ë¡œë¥¼ fine-tuned-pretrained-modelë¡œ ë°”ê¾¸ê³ , ê²½ë¡œê°€ ì—†ëŠ” ê²½ìš° ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

#2: ëª¨ë¸ê³¼ configurationì„ ì €ì¥í•˜ê¸° ìœ„í•´ os.path.joinìœ¼ë¡œ ê²½ë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

#3: ëª¨ë¸ì´ bert4global, bert ì†ì„±ì„ ê°–ê³  ìˆëŠ” ê²½ìš°(ì € ì†ì„±ì´ ë­˜ê¹Œìš” Â¿) ëª¨ë¸ì„ ë³€ê²½í•´ì£¼ê³ , torch.saveë¡œ ì €ì¥í•©ë‹ˆë‹¤.

ë˜ config.to_json_file ë©”ì†Œë“œë¡œ configurationì„ json íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

#4: ë§ˆì§€ë§‰ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

<br>

**file_utils.py**

288 ~ 290 line

```python
  else:
      raise ValueError('Invalid save_mode: {}'.format(opt.save_mode))
  model.to(opt.device)
```

ë§Œì•½ save_modeê°€ 1, 2, 3ì´ ì•„ë‹Œ ê²½ìš° ValueErrorë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.

<br>

ë‹¤ì‹œ \_train_and_evaluate ë©”ì†Œë“œë¡œ ëŒì•„ì™€ì„œ â—ï¸

309 ~ 311 line

```python
if not self.val_dataloaders:
  self.opt.MV.add_metric('Max-Test-Acc w/o Valid Set', max_fold_acc * 100)
  self.opt.MV.add_metric('Max-Test-F1 w/o Valid Set', max_fold_f1 * 100)
```

ê²€ì¦ ë°ì´í„°ì…‹ì´ ì—†ëŠ” ê²½ìš° optì— MetricVisualizerë¥¼ Max-Testë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.

<br>

313 ~ 319 line

```python
if self.val_dataloaders:
  print('Loading best model: {} and evaluating on test set ...'.format(save_path))r
  self.reload_model(find_file(save_path, '.state_dict'))
  max_fold_acc, max_fold_f1 = self._evaluate_acc_f1(self.test_dataloader)

  self.opt.MV.add_metric('Max-Test-Acc', max_fold_acc * 100)
  self.opt.MV.add_metric('Max-Test-F1', max_fold_f1 * 100)
```

ê²€ì¦ ë°ì´í„°ì…‹ì´ ìˆëŠ” ê²½ìš°ì—ëŠ” reload_model ë©”ì†Œë“œë¡œ ëª¨ë¸ì„ ë¡œë“œí•œ ë’¤, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ í…ŒìŠ¤íŠ¸ í•œ ê²°ê³¼ë¥¼ MetricVisualizerë¡œ ì €ì¥í•©ë‹ˆë‹¤.

ì´ë ‡ê²Œ í•˜ëŠ” ì´ìœ ëŠ” 246 ~ 247 lineì—ì„œ ê²€ì¦ ë°ì´í„°ì…‹ì´ ìˆëŠ” ê²½ìš° ê²€ì¦ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

<br>

reload_model ë©”ì†Œë“œëŠ” Instructor í´ë˜ìŠ¤ ë©¤ë²„ í•¨ìˆ˜ë¡œ 120 ~ 125 lineì— ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤.

```python
def reload_model(self, ckpt='./init_state_dict.bin'):
  if os.path.exists(ckpt):
    if self.opt.auto_device == 'allcuda':
      self.model.module.load_state_dict(torch.load(ckpt))
    else:
      self.model.load_state_dict(torch.load(ckpt))
```

<br>

ë‹¤ì‹œ \_train_and_evaluate ë©”ì†Œë“œë¡œ ëŒì•„ì™€ì„œ â—ï¸

322 ~ 325 line

```python
self.logger.info(self.opt.MV.summary(no_print=True))

print('Training finished, we hope you can share your checkpoint with community, please see:',
      'https://github.com/yangheng95/PyABSA/blob/release/demos/documents/share-checkpoint.md')
```

logë¥¼ ê¸°ë¡í•˜ê³  í›ˆë ¨ì„ ë§ˆì³¤ë‹¤ê³  í”„ë¡¬í”„íŠ¸ì— ì¶œë ¥í•©ë‹ˆë‹¤.

<br>

327 ~ 347 line

```python
if self.val_dataloader or self.opt.save_mode:  # 1
  del self.train_dataloaders
  del self.test_dataloader
  del self.val_dataloaders
  del self.model
  cuda.empty_cache()
  time.sleep(3)
  return save_path
else:  # 2
  # direct return model if do not evaluate
  # if self.opt.model_path_to_save:
  #     save_path = '{0}/{1}/'.format(self.opt.model_path_to_save,
  #                                   self.opt.model_name
  #                                   )
  #     save_model(self.opt, self.model, self.tokenizer, save_path)
  del self.train_dataloaders
  del self.test_dataloader
  del self.val_dataloaders
  cuda.empty_cache()
  time.sleep(3)
  return self.model, self.opt, self.tokenizer
```

#1: ê²€ì¦ ë°ì´í„°ë¡œë” ë˜ëŠ” optì— save_modeê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë°ì´í„°ë¡œë”, ëª¨ë¸ì„ ì œê±°í•œ ë’¤ ì €ì¥ ê²½ë¡œë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.

#2: ë°ì´í„°ë¡œë”ë§Œ ì œê±°í•˜ê³ , gpu ìºì‹œë¥¼ ì •ë¦¬í•œ ë’¤ model, opt, tokenizerë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.

<br>

ğŸ§‘â€ğŸ«

##### í¬ìŠ¤íŒ… ë‚´ìš© ìš”ì•½

Trainer í´ë˜ìŠ¤ ìƒì„± â¡ï¸ train4apc ë©”ì†Œë“œ í˜¸ì¶œ â¡ï¸ Instructor í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± â¡ï¸ Instructor.run ë©”ì†Œë“œ ì‹¤í–‰ â¡ï¸ Instructor.\_train ë©”ì†Œë“œ ì‹¤í–‰ â¡ï¸  Instructor._k_fold_train_and_evaluate ë˜ëŠ” Instructor.\_train_and_evaluate ì‹¤í–‰ (ì´ ë¶€ë¶„ì—ì„œ gradient ê³„ì‚° ë° loss ê°’ ê³„ì‚°) â¡ï¸ self.model, self.opt, self.tokenizer ë¦¬í„´ â¡ï¸ model, opt, tokenizerë¥¼ ì¸ìë¡œ Trainer í´ë˜ìŠ¤ì—ì„œ Aspect Extractor í´ë˜ìŠ¤ë¥¼ ìƒì„±

<br>

##### Trainer, Instructor, APCEnsembler, AspectExtractor ì •ë¦¬

- Trainer: ëª¨ë¸ê³¼ ê´€ë ¨ëœ ë‹¤ì–‘í•œ í´ë˜ìŠ¤ë“¤(Instructor, AspectExtractor)ì„ ìƒì„±
- Instructor: ì‹¤ì œ ëª¨ë¸ ìƒì„±(APCEnsembler) ë° í›ˆë ¨
- APCEnsembler: ëª¨ë¸ ì •ì˜
- AspectExtractor: Aspect Term Extract & Sentiment Inference

<br>

ì•„ë¬´ë˜ë„ í”„ë ˆì„ì›Œí¬ë‹¤ë³´ë‹ˆê¹Œ ëŒ€ë¶€ë¶„ì˜ í•¨ìˆ˜ê°€ ëª¨ë“ˆí™” ë¼ ìˆì–´, ì™”ë‹¤ê°”ë‹¤ í•´ì„œ í—·ê°ˆë¦´ ê²ƒ ê°™ìŠµë‹ˆë‹¤ ğŸ˜¢

ì¶”í›„ ì‹œê°„ì´ ëœë‹¤ë©´ ë‹¤ì‹œ ì˜ ì •ë¦¬í•´ì„œ ì—…ë¡œë“œ í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤ ..
