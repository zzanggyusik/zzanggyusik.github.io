---
title:  "[Capstone] MAMS for ABSA 훈련 코드 분석"
excerpt: "MAMS for ABSA 코드 리뷰"
toc: true
toc_label: "MAMS for ABSA 훈련 코드 분석"
toc_sticky: true
published: true

categories:
  - Capstone
tags:
  - PyTorch
last_modified_at: 2022-04-23

---

> [MAMS for ABSA 전처리 코드 분석](https://seominseok4834.github.io/capstone/4.mams-preprocess-code-analytics/)과 이어지는 내용입니다.
>
> 모든 코드는 [MAMS-for-ABSA github](https://github.com/siat-nlp/MAMS-for-ABSA)에서 가져왔습니다.

<br>

![스크린샷 2022-04-23 오후 9 56 57](https://user-images.githubusercontent.com/76269316/164895501-0d95b6b8-a667-49f3-9354-a908c25fba08.png)

전처리 코드와 동일하게 루트 디렉토리에 있는 train.py 파일을 실행하면 configuration 파일을 로드하고, /train/train.py가 실행되면서 훈련이 시작됩니다.

<br>

[/train.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train.py)

```python
import yaml
import os
from train.train import train

config = yaml.safe_load(open('config.yml'))
mode = config['mode']
os.environ["CUDA_VISIBLE_DEVICES"] = str(config['aspect_' + mode + '_model'][config['aspect_' + mode + '_model']['type']]['gpu'])
train(config)
```

<br>

훈련을 위해 필요한 모든 함수는 /train/train.py를 통해 호출되기 때문에 **[/train/train.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/train.py)**는 하늘색으로 표시해 구분하도록 하겠습니다.

또한 이전 포스팅에서 말씀드린 것처럼 제가 ATSA를 진행할 예정이기 때문에 이를 기준으로 설명하도록 하겠습니다.

<span style="color:#2EBEF8">**/train/train.py**</span>

16 ~ 23 line

```python
from train import make_aspect_term_model, make_aspect_category_model
from train.make_data import make_term_data, make_category_data

def train(config):
    mode = config['mode']  # term
    if mode == 'term':
        model = make_aspect_term_model.make_model(config)  # model 생성
        train_loader, val_loader = make_term_data(config)  # dataloader 생성
    else:
        model = make_aspect_category_model.make_model(config)
        train_loader, val_loader = make_category_data(config)
        												···
```

먼저 모델과 데이터로더를 생성합니다. configuration 파일이 ATSA 기준이기 때문에 config['mode']가 term으로 설정돼 있습니다.

<br>

### 모델 생성

**config.yml**

```yaml
aspect_term_model:
  type: recurrent_capsnet
  recurrent_capsnet:
    embed_size: 300
    dropout: 0.5
    num_layers: 2
    capsule_size: 300
    bidirectional: True
    optimizer: adam
    batch_size: 64
    learning_rate: 0.0003
    weight_decay: 0
    num_epoches: 20
    gpu: 0
```

[/train/make_aspect_term_model.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/make_aspect_term_model.py)

**make_model()**

10 ~ 17 line

먼저 모델을 생성하는 코드를 보면, 현재 configuration 파일의 type이 recurrent_capsnet이기 때문에 아래 35 ~ 50 line의 make_recurrent_capsule_network()를 호출합니다.

```python
def make_model(config):
    model_type = config['aspect_term_model']['type']  # recurrent_capsnet
    if model_type == 'recurrent_capsnet':
        return make_recurrent_capsule_network(config)
    elif model_type == 'bert_capsnet':
        return make_bert_capsule_network(config)
    else:
        raise ValueError('No Supporting.')
```

<br>

**make_recurrent_capsule_network()**

35 ~ 36 line

```python
def make_recurrent_capsule_network(config):
    embedding = make_embedding(config)
    							···
```

52~ 62 line에 있는 make_embedding 메소드를 호출해 전처리 과정에서 저장한 단어 임베딩을 로드합니다.

<br>

**make_embedding()**

52 ~ 62 line

```python
def make_embedding(config):
    base_path = os.path.join(config['base_path'])  # ./data/MAMS-ATSA
    log_path = os.path.join(base_path, 'log/log.yml')  # ./data/MAMS-ATSA/log/log.yml
    log = yaml.safe_load(open(log_path))  # 로그 파일 로드
    vocab_size = log['vocab_size']  # 7895
    config = config['aspect_term_model'][config['aspect_term_model']['type']]  # recurrent_capsnet
    embed_size = config['embed_size']  # 300
    embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)  # (7895, 300)
    glove = np.load(os.path.join(base_path, 'processed/glove.npy'))  # ./data/MAMS-ATSA/processed/glove.npy
    embedding.weight.data.copy_(torch.tensor(glove))  # 전처리 과정에서 저장한 glove.npy의 가중치를 copy
    return embedding
```

전처리 과정에서 저장한 glove.npy 파일을 로드한 뒤 nn.Embedding의 가중치로 copy한 뒤 리턴합니다.

<br>

**make_recurrent_capsule_network()**

```python
def make_recurrent_capsule_network(config):
    embedding = make_embedding(config)  
    base_path = os.path.join(config['base_path'])  # ./data/MAMS-ATSA
    log_path = os.path.join(base_path, 'log/log.yml')  # ./data/MAMS-ATSA/log/log.yml
    log = yaml.safe_load(open(log_path))
    config = config['aspect_term_model'][config['aspect_term_model']['type']]  # recurrent_capsnet
    model = RecurrentCapsuleNetwork(
        embedding=embedding,
        num_layers=config['num_layers'],  # 2
        capsule_size=config['capsule_size'],  # 300
        bidirectional=config['bidirectional'],  # True
        dropout=config['dropout'],  # 0.5
        num_categories=log['num_categories']  # 3
    )
    model.load_sentiment(os.path.join(base_path, 'processed/sentiment_matrix.npy'))
    return model
```

glove 단어 임베딩을 로드한 뒤, configuration 파일에 저장한 파라미터와 함께 모델 생성 인자로 넣어 모델을 생성합니다.

<br>

### 데이터세트 생성

[/train/make_data.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/make_data.py)

**make_term_data()**

10 ~ 15 line

```python
import os
from torch.utils.data import DataLoader
from data_process.dataset import ABSADataset

input_list = {
    'recurrent_capsnet': ['context', 'aspect'],
    'bert_capsnet': ['bert_token', 'bert_segment']
}

def make_term_data(config):
    base_path = config['base_path']  # ./data/MAMS-ATSA
    train_path = os.path.join(base_path, 'processed/train.npz')  # ./data/MAMS-ATSA/processed/train.npz
    val_path = os.path.join(base_path, 'processed/val.npz')  # ./data/MAMS-ATSA/processed/val.npz
    train_data = ABSADataset(train_path, input_list[config['aspect_term_model']['type']])  # config['aspect_term_model']['type']: recurrent_capsnet -> input_list['recurrent_capsnet']
    val_data = ABSADataset(val_path, input_list[config['aspect_term_model']['type']])
																				···
```

전처리 과정에서 생성한 train.npz, val.npz 파일을 pytorch Dataset 클래스를 상속받아 구현한 ABSADataset의 파라미터로 넘겨 train_data와 val_data로 생성합니다.

<br>

[/data_process/dataset.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/data_process/dataset.py)

```python
import torch
from torch.utils.data import Dataset
import numpy as np

class ABSADataset(Dataset):

    def __init__(self, path, input_list):
        super(ABSADataset, self).__init__()
        data = np.load(path)  # ./data/MAMS-ATSA/processed/train.npz(val.npz)
        self.data = {}
        for key, value in data.items():
            self.data[key] = torch.tensor(value).long()  # tensor long type으로 변경
        self.len = self.data['label'].size(0)  # 전체 데이터의 개수
        self.input_list = input_list  # ['context', 'aspect']

    def __getitem__(self, index):
        return_value = []
        for input in self.input_list:
            return_value.append(self.data[input][index])
        return_value.append(self.data['label'][index])
        return return_value

    def __len__(self):
        return self.len
```

<br>

먼저 생성자를 보면 numpy zip 파일을 로드한 다음 data.items()로 iteration 하는 것을 볼 수 있습니다.

key 값에는 전처리 과정에서 저장한 sentence, aspect, label, context. bert_token. bert_segment, td_left, td_right가 들어있고 value에는 해당하는 값들이 들어있습니다. [전처리 데이터 저장](https://seominseok4834.github.io/capstone/4.mams-preprocess-code-analytics/#%EC%A0%84%EC%B2%98%EB%A6%AC-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%80%EC%9E%A5)

input_list에는 ['context', 'aspect']가 담겨있습니다.

<br>

\_\_getitem\_\_과 \_\_len\_\_ 메소드는 훈련, 검증시 데이터를 batch size만큼 뽑아올 때 사용되는데, 

\_\_getitem\_\_ 메소드를 보면 input_list에 있는 key 값(context, aspect)들만 꺼내오는 것을 확인할 수 있습니다. (label도 같이)

<br>

### 데이터로더 생성

[/train/make_data.py](https://github.com/siat-nlp/MAMS-for-ABSA/blob/master/train/make_data.py)

16 ~ 29 line

```python
    config = config['aspect_term_model'][config['aspect_term_model']['type']]  # recurrent_capsnet
		# pytorch DataLoader 생성
  	train_loader = DataLoader(
        dataset=train_data,
        batch_size=config['batch_size'],  # 64
        shuffle=True,
        pin_memory=True  # 빠른 데이터 전송을 위한 pin_memory 고정
    )
    val_loader = DataLoader(
        dataset=val_data,
        batch_size=config['batch_size'],
        shuffle=False,
        pin_memory=True 
    )
    return train_loader, val_loader
```

파이토치의 데이터로더를 생성한 뒤 리턴합니다.

<br>

<span style="color:#2EBEF8">**/train/train.py**</span>

24 ~ 36 line

```python
    model = model.cuda()  # 모델의 파라미터를 GPU로 로드
    base_path = config['base_path']  # ./data/MAMS-ATSA
    model_path = os.path.join(base_path, 'checkpoints/%s.pth' % config['aspect_' + mode + '_model']['type'])  # 모델 저장 경로
    if not os.path.exists(os.path.dirname(model_path)):  # 디렉토리가 없는 경우 생성
        os.makedirs(os.path.dirname(model_path))
    with open(os.path.join(base_path, 'processed/index2word.pickle'), 'rb') as handle:  # 전처리 과정에서 저장한 index2word.pickle 파일 로드
        index2word = pickle.load(handle)
    criterion = CapsuleLoss()  # loss function 생성 (/src/module/utils/loss.py)
    optimizer = make_optimizer(config, model)
    max_val_accuracy = 0
    min_val_loss = 100
    global_step = 0
    config = config['aspect_' + mode + '_model'][config['aspect_' + mode + '_model']['type']]
```

