---
title:  "6.Dimension Reduction"
excerpt: "차원 축소"
toc: true
toc_label: "Dimension Reduction"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - 차원 축소 
  - 파이썬 머신러닝 완벽 가이드
last_modified_at: 2021-08-14

---

>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.



### 1. 차원 축소(Dimension Reduction) 개요

차원 축소는 매우 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것입니다.

수백 개 이상의 피처로 구성된 데이터 세트의 경우 개별 피처 간에 상관관계가 높아 상대적으로 적은 차원에서 학습된 모델보다 예측 신뢰도가 떨어지게 됩니다.

<br>

다차원의 피처를 차원 축소해 피처 수를 줄이면 더 직관적으로(시각적으로 데이터를 압축해서 표현할 수 있음) 데이터를 해석할 수 있고, 학습 데이터의 크기가 줄어 학습에 필요한 처리 능력도 줄일 수 있습니다.

<br>
차원 축소는 **피처 선택(feature selection)**과 **피처 추출(feature extraction)**으로 나눌 수 있습니다.

피처(특성) 선택은 특정 피처에 종속성이 강한 불필요한 피처를 아예 제거해 데이터의 특징을 잘 나타내는 주요 피처만 선택하는 방식이고,

피처(특성) 추출은 기존 피처를 함축적으로 더 잘 설명할 수 있는 저차원의 중요 피처로 압축해 추출하는 방식입니다.

<br>

차원 축소 알고리즘 중 PCA, SVD, NMF에 대해 알아보겠습니다.

<br>

<br>

<br>

### PCA(Principal Component Analysis)

이번 챕터 설명이 부실해서 아래 강의를 참고해서 공부했습니다.

책을 보기 전에 강의를 먼저 보는 것을 추천드립니다.

<iframe width="1262" height="841" src="https://www.youtube.com/embed/FhQm2Tc8Kic?list=PLpIPLT0Pf7IoTxTCi2MEQ94MZnHaxrP0j" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<br>

PCA(주성분 분석)는 비지도 피처 추출(Unsupervised feature extraction) 방식으로 y값(label 값)을 이용하지 않고, X값(피처)들의 상관관계를 이용해 이를 대표하는 주성분(Principal Component)을 추출해 차원을 축소하는 기법입니다.

<br>

PCA는 다음 스텝으로 수행됩니다.

1. 입력 데이터 세트의 공분산 행렬을 생성합니다.
2. 공분산 행렬의 고유벡터와 고유값을 계산합니다.
3. 고유값이 가장 큰 순으로 K개(PCA 변환 차수만큼)만큼 고유벡터를 추출합니다.
4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환합니다.

<br>

공분산 행렬의 고유값, 고유벡터만으로도 변환할 수 있는 이유는 **<u>입력 데이터 공분산 행렬의 고유값</u>과 <u>주성분의 분산</u>이 같기 때문**입니다. (강의에 나옵니다)

<br>

<br>

붓꽃 데이터 세트에 PCA를 적용해 보겠습니다. 

먼저 sepal length와 sepal width를 X축, Y축으로 해 품종 데이터 분포를 시각화해 봤습니다. 

```python
from sklearn.datasets import load_iris
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

iris = load_iris()

#넘파이 데이터 세트를 판다스 DataFrame으로 변환
columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
irisDF = pd.DataFrame(iris.data, columns=columns)
irisDF['target'] = iris.target

#setosa 세모, versicolor 네모, virginica 동그라미로 표현
markers = ['^', 's', 'o']

#target 값은 setosa 0, versicolor 1, virginica 2. 각 target별로 다른 모양으로 표시
for i, marker in enumerate(markers):
    x_axis_data = irisDF[irisDF['target'] == i]['sepal_length']
    y_axis_data = irisDF[irisDF['target'] == i]['sepal_width']
    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])
    
plt.legend()
plt.xlabel('sepal length')
plt.ylabel('sepal width')
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/129441934-2dd17105-f126-4cb6-99ff-446068024447.png)

Setosa 품종의 경우 sepal width가 3.0보다 크고 sepal length가 6.0 이하인 곳에 일정하게 분포돼 있는 반면, versicolor와 virginica의 경우 sepal width, sepal length 조건만으로는 분류가 어려운 것을 확인할 수 있습니다.

<br>

PCA로 4개 속성을 2개로 압축한 다음 2개의 PCA 속성으로 붓꽃 데이터의 품종 분포를 다시 시각화해 보겠습니다.

PCA는 여러 속성 값을 연산해야 하므로 속성 스케일에 영향을 받습니다.

따라서 PCA로 압축하기 전에 각 속성값을 동일한 스케일로 변환하는 것이 필요합니다.

이를 위해 사이킷런 StandardScaler를 이용해 평균이 0, 분산이 1인 표준 정규 분포로 iris 데이터 세트 속성값들을 변환했습니다.

```python
from sklearn.preprocessing import StandardScaler

#Target 값을 제외한 모든 속성 값을 StandardScaler를 이용해 표준 정규 분포를 갖는 값들로 편환
iris_scaled = StandardScaler().fit_transform(irisDF.iloc[:, :-1])
```

주성분 분석은 비지도 피처 추출 방식이기 때문에 fit_transform을 적용할 때 target 값을 제외하고 스케일링 적용했습니다.

<br>

사이킷런 PCA 클래스를 사용해 스케일링 적용된 데이터 세트에 PCA를 적용해 4차원(4개 속성) 붓꽃 데이터를 2차원(2개의 PCA 속성) PCA 데이터로 변환했습니다. 

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)

#fit()과 transform()을 호출해 PCA 변환 데이터 반환
pca.fit(iris_scaled)
iris_pca = pca.transform(iris_scaled)
print(iris_pca.shape)
```

![image](https://user-images.githubusercontent.com/76269316/129442421-d5b1ada6-b486-4533-b5e1-1f6b7c83a296.png)

iris_pca가 변환된 PCA 데이터 세트 150x2 넘파이 행렬을 갖고 있는 것을 확인할 수 있습니다.

DataFrame으로 변환한 다음 2개의 PCA 속성을 각각 X축, Y축으로 해서 붓꽃 품종이 어떻게 분포되는지 확인해 보겠습니다.

```python
#PCA 변환된 데이터 컬럼 명을 각각 pca_component_1, pca_component_2로 명명
pca_columns = ['pca_component_1', 'pca_component_2']
irisDF_pca = pd.DataFrame(iris_pca, columns=pca_columns)
irisDF_pca['target'] = iris.target

#setosa 세모, versicolor 네모, virginica 동그라미로 표현
markers = ['^', 's', 'o']

#target 값은 setosa 0, versicolor 1, virginica 2. 각 target별로 다른 모양으로 표시
for i, marker in enumerate(markers):
    x_axis_data = irisDF[irisDF_pca['target'] == i]['sepal_length']
    y_axis_data = irisDF[irisDF_pca['target'] == i]['sepal_width']
    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])
    
plt.legend()
plt.xlabel('sepal length')
plt.ylabel('sepal width')
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/129442549-300978e7-3e9b-4f32-b0f9-6b8095af16e6.png)

PCA로 변환한 후에도 pca_component_1 축을 기반으로 setosa 품종은 명확하게 구분이 가능하지만 versicolor와 virginica는 pca_component_1 축을 기반으로 서로 겹치는 부분이 일부 존재하지만 비교적 잘 구분된 것을 확인할 수 있습니다.

<br>

PCA component 별로 원본 데이터의 변동성을 얼마나 반영하고 있는지 PCA 변환을 수행한 PCA 객체의 explained_variance_ratio_ 속성(전체 변동성에서 개별 PCA 컴포넌트별로 차지하는 변동성 비율)으로 확인할 수 있습니다.

```python
print(pca.explained_variance_ratio_)
```

![image](https://user-images.githubusercontent.com/76269316/129442601-e1f96aaf-5a1e-4716-98ce-53ab944fe07a.png)

첫 번째 PCA 변환 요소인 pca_component_1이 전체 변동성의 약 72.9%를 차지하며 pca_component_2가 약 22.8%를 차지하는 것을 확인할 수 있습니다.

이는 2개의 PCA 요소만으로도 원본 데이터의 변동성을 95%를 설명할 수 있다는 것을 의미합니다.

<br>

<br>

원본 붓꽃 데이터 세트와 PCA로 변환된 데이터 세트에 각각 분류를 적용해 결과를 비교해 보겠습니다.

Estimator는 RandomForestClassifier를 이용하고 cross_val_scor()로 3개의 교차 검증 세트로 정확도 결과를 비교하겠습니다.

<br>

**원본 붓꽃 데이터 세트**

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

rcf = RandomForestClassifier(random_state=156)
scores = cross_val_score(rcf, iris.data, iris.target, scoring='accuracy', cv=3)
print('원본 데이터 교차 검증 개별 정확도: ', scores)
print('원본 데이터 평균 정확도: ', np.mean(scores))
```

![image](https://user-images.githubusercontent.com/76269316/129442838-459e329d-7be1-4c41-a398-42f727697a25.png)

<br>

**PCA로 변환된 데이터 세트 **

```python
pca_X = irisDF_pca[['pca_component_1', 'pca_component_2']]
scores_pca = cross_val_score(rcf, pca_X, iris.target, scoring='accuracy', cv=3)
print('PCA 변환 데이터 교차 검증 개별 정확도: ', scores_pca)
print('PCA 변환 데이터 평균 정확도: ', np.mean(scores_pca))
```

![image](https://user-images.githubusercontent.com/76269316/129443006-1cc15d0a-b39d-4d76-bb4a-dcf5553f8c32.png)

PCA로 차원 축소시 기존 데이터의 정보가 유실되기 때문에 원본 데이터 세트 대비 예측 성능이 떨어질 수 밖에 없습니다.

그래도 4개의 속성이 2개로(속성 개수가 50% 감소) 감소한 것을 고려하면, 여전히 원본 데이터의 특성을 상단 부분 유지하고 있다는 것을 알 수 있습니다.

<br>

<br>

좀 더 많은 피처를 갖고 있는 [UCI Machine Learning Repository 신용카드 고객 데이터 세트(Credit Card Clients DataSet)](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)를 이용해 주성분 분석을 해보겠습니다.

 ![image](https://user-images.githubusercontent.com/76269316/129443424-c892314f-f16a-44e1-8108-1f4c22e5286d.png)

![image](https://user-images.githubusercontent.com/76269316/129443455-72e516df-572e-474e-a2b6-59bab1d52535.png)

DataFolder에서 default of credit card clients.xls 파일을 다운 받은 후 credit_card.xls로 변경해 사용했습니다.

<br>

엑셀 파일을 열면 다음과 같이 의미 없는 첫 번째 행(빨강색 네모)과 단순히 row를 식별하는데만 사용되는 id 컬럼(파란색 네모)이 존재하는데, 이들을 제거한 뒤 DataFrame으로 로딩했습니다.

![image](https://user-images.githubusercontent.com/76269316/129443554-0f597319-4f6f-4aaa-b4ed-6275b102ae3e.png)



```python
#header로 의미 없는 첫 행 제거, iloc로 id 컬럼 제거
import pandas as pd

df = pd.read_excel('credit_card.xls', header=1, sheet_name='Data').iloc[0:, 1:]
print(df.shape)
df.head()
```

![image](https://user-images.githubusercontent.com/76269316/129443628-7b8730d4-4f09-49c7-8dae-ba2338cabae4.png)

신용카드 데이터 세트에 30,000개의 레코드와 24개 속성이 있는데 이 중 '다음 달 연체 여부'를 의미하는 'default payment next month' 속성이 target 값(정상 납부 : 0, 연체 : 1)인데 컬럼명이 너무 길어서 변경하고,

PAY_0 컬럼 다음에 PAY_2 컬럼이 존재하므로 PAY_0 컬럼을 PAY_1으로 변경했습니다.

```python
df.rename(columns={'PAY_0':'PAY_1', 'default payment next month':'default'}, inplace=True)
y_target = df['default']
X_features = df.drop('default', axis=1)
```

<br>

23개 속성의 상관도를 DataFrame corr()를 이용해 구한뒤, Seaborn의 heatmap으로 시각화해 보겠습니다.

```python
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

corr = X_features.corr()
plt.figure(figsize=(14, 14))
sns.heatmap(corr, annot=True, fmt='.1g')  #annot: 각 셀의 값(상관계수) 표기 여부, fmt: 데이터 타입
```

![image](https://user-images.githubusercontent.com/76269316/129443867-539529e9-cf49-44f8-8192-3b4d00bfad74.png)

BILL_AMT1 ~ BILL_AMT6 6개 속성의 상관도가 대부분 0.9 이상으로 매우 높은 것을 확인할 수 있습니다.

이렇게 높은 상관도를 갖는 속성들은 소수의 PCA 요소만으로도 이 속성들의 변동성을 수용할 수 있습니다.

BILL_AMT1 ~ BILL_AMT6 6개 속성을 PCA 컴포넌트로 변환한 뒤 개별 컴포넌트의 변동성을 explained_variance_ratio_ 속성으로 확인해 보겠습니다.

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

#BILL_AMT1 ~ BILL_AMT6 6개 속성명 생성
cols_bill = ['BILL_AMT'+str(i) for i in range(1, 7)]
print('대상 속성명: ', cols_bill)

#2개의 PCA 속성을 갖는 PCA 객체 생성 후 explained_variance_ratio_ 계산을 위해 fit() 호출
scaler = StandardScaler()
df_cols_scaled = scaler.fit_transform(X_features[cols_bill])
pca = PCA(n_components=2)
pca.fit(df_cols_scaled)
print('PCA Component별 변동성: ', pca.explained_variance_ratio_)
```

![image](https://user-images.githubusercontent.com/76269316/129444024-6223e572-d00d-488a-8517-b75cfff5c135.png)

단 2개의 PCA 컴포넌트만으로도 6개 속성의 변동성을 약 95% 이상 설명할 수 있으며 특히 첫 번째 PCA 축으로 90%의 변동성을 수용할 수 있을 정도로 6개 속성의 상관도가 높다는 것을 확인할 수 있습니다.

<br>

<br>

붓꽃 데이터 세트와 마찬가지로, 원본 데이터 세트와 PCA로 변환된 데이터 세트에 각각 분류를 적용해 결과를 비교해 보겠습니다.

<br>

**원본 신용카드 데이터 세트**

```python
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

rcf = RandomForestClassifier(n_estimators=300, random_state=156)
scores = cross_val_score(rcf, X_features, y_target, scoring='accuracy', cv=3)

print('CV=3인 경우 개별 Fold 세트별 정확도: ', scores)
print('평균 정확도: {0:.4f}'.format(np.mean(scores)))
```

![image](https://user-images.githubusercontent.com/76269316/129444214-dbf962ca-0d82-4e06-b5c3-24cd42d63095.png)

<br>

**PCA로 변환된 데이터 세트**

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

#원본 데이터 세트에 먼저 StandardScaler 적용
scaler = StandardScaler()
df_scaled = scaler.fit_transform(X_features)

#6개 컴포넌트를 갖도록 PCA 변환을 수행하고 cross_val_score()로 분류 예측 수행
pca = PCA(n_components=6)
df_pca = pca.fit_transform(df_scaled)
scores_pca = cross_val_score(rcf, df_pca, y_target, scoring='accuracy', cv=3)

print('CV=3인 경우 PCA 변환된 개별 Fold 세트별 정확도: ', scores_pca)
print('PCA 변환 데이터 세트 평균 정확도: {0:.4f}'.format(np.mean(scores_pca)))
```

![image](https://user-images.githubusercontent.com/76269316/129444258-af05dbd6-fc30-412b-95c1-9a8d08d4f30b.png)

전체 23개 속성의 약 1/4 수준인 6개의 PCA 컴포넌트만으로도 원본 데이터를 기반으로한 분류 예측보다 약 1~2% 정도의 예측 성능 저하만 발생했습니다.

1~2%도 큰 성능 저하이지만 전체 속성의 1/4 정도만으로 이정도 성능을 예측한다는 것은 PCA가 그만큼 뛰어난 압축 능력을 보여주는 것이라고 할 수 있습니다.

<br>

<br>

<br>
