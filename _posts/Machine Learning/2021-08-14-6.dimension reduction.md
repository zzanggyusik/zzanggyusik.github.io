---
title:  "6.Dimension Reduction"
excerpt: "차원 축소"
toc: true
toc_label: "Dimension Reduction"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - 차원 축소 
  - 파이썬 머신러닝 완벽 가이드
last_modified_at: 2021-08-14
---

>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.



### 차원 축소(Dimension Reduction) 개요

차원 축소는 매우 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것입니다.

수백 개 이상의 피처로 구성된 데이터 세트의 경우 개별 피처 간에 상관관계가 높아 상대적으로 적은 차원에서 학습된 모델보다 예측 신뢰도가 떨어지게 됩니다.

<br>

다차원의 피처를 차원 축소해 피처 수를 줄이면 더 직관적으로(시각적으로 데이터를 압축해서 표현할 수 있음) 데이터를 해석할 수 있고, 학습 데이터의 크기가 줄어 학습에 필요한 처리 능력도 줄일 수 있습니다.

<br>
차원 축소는 **피처 선택(feature selection)**과 **피처 추출(feature extraction)**으로 나눌 수 있습니다.

피처(특성) 선택은 특정 피처에 종속성이 강한 불필요한 피처를 아예 제거해 데이터의 특징을 잘 나타내는 주요 피처만 선택하는 방식이고,

피처(특성) 추출은 기존 피처를 함축적으로 더 잘 설명할 수 있는 저차원의 중요 피처로 압축해 추출하는 방식입니다.

<br>

차원 축소 알고리즘 중 PCA, SVD, NMF에 대해 알아보겠습니다.

<br>

<br>

<br>

### PCA(Principal Component Analysis)

##### PCA 개요

이번 챕터 설명이 부실해서 아래 강의를 참고해서 공부했습니다.

책을 보기 전에 강의를 먼저 보는 것을 추천드립니다.

<iframe width="1262" height="841" src="https://www.youtube.com/embed/FhQm2Tc8Kic?list=PLpIPLT0Pf7IoTxTCi2MEQ94MZnHaxrP0j" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<br>

PCA(주성분 분석)는 비지도 피처 추출(Unsupervised feature extraction) 방식으로 y값(label 값)을 이용하지 않고, X값(피처)들의 상관관계를 이용해 이를 대표하는 주성분(Principal Component)을 추출해 차원을 축소하는 기법입니다.

<br>

PCA는 다음 스텝으로 수행됩니다.

1. 입력 데이터 세트의 공분산 행렬을 생성합니다.
2. 공분산 행렬의 고유벡터와 고유값을 계산합니다.
3. 고유값이 가장 큰 순으로 K개(PCA 변환 차수만큼)만큼 고유벡터를 추출합니다.
4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환합니다.

<br>

공분산 행렬의 고유값, 고유벡터만으로도 변환할 수 있는 이유는 **<u>입력 데이터 공분산 행렬의 고유값</u>과 <u>주성분의 분산</u>이 같기 때문**입니다. (강의에 나옵니다)

<br>

##### 붓꽃 데이터 세트에 PCA 적용하기

붓꽃 데이터 세트에 PCA를 적용해 보겠습니다. 

먼저 sepal length와 sepal width를 X축, Y축으로 해 품종 데이터 분포를 시각화해 봤습니다. 

```python
from sklearn.datasets import load_iris
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

iris = load_iris()

#넘파이 데이터 세트를 판다스 DataFrame으로 변환
columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
irisDF = pd.DataFrame(iris.data, columns=columns)
irisDF['target'] = iris.target

#setosa 세모, versicolor 네모, virginica 동그라미로 표현
markers = ['^', 's', 'o']

#target 값은 setosa 0, versicolor 1, virginica 2. 각 target별로 다른 모양으로 표시
for i, marker in enumerate(markers):
    x_axis_data = irisDF[irisDF['target'] == i]['sepal_length']
    y_axis_data = irisDF[irisDF['target'] == i]['sepal_width']
    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])
    
plt.legend()
plt.xlabel('sepal length')
plt.ylabel('sepal width')
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/129441934-2dd17105-f126-4cb6-99ff-446068024447.png)

Setosa 품종의 경우 sepal width가 3.0보다 크고 sepal length가 6.0 이하인 곳에 일정하게 분포돼 있는 반면, versicolor와 virginica의 경우 sepal width, sepal length 조건만으로는 분류가 어려운 것을 확인할 수 있습니다.

<br>

PCA로 4개 속성을 2개로 압축한 다음 2개의 PCA 속성으로 붓꽃 데이터의 품종 분포를 다시 시각화해 보겠습니다.

PCA는 여러 속성 값을 연산해야 하므로 속성 스케일에 영향을 받습니다.

따라서 PCA로 압축하기 전에 각 속성값을 동일한 스케일로 변환하는 것이 필요합니다.

이를 위해 사이킷런 StandardScaler를 이용해 평균이 0, 분산이 1인 표준 정규 분포로 iris 데이터 세트 속성값들을 변환했습니다.

```python
from sklearn.preprocessing import StandardScaler

#Target 값을 제외한 모든 속성 값을 StandardScaler를 이용해 표준 정규 분포를 갖는 값들로 편환
iris_scaled = StandardScaler().fit_transform(irisDF.iloc[:, :-1])
```

주성분 분석은 비지도 피처 추출 방식이기 때문에 fit_transform을 적용할 때 target 값을 제외하고 스케일링 적용했습니다.

<br>

사이킷런 PCA 클래스를 사용해 스케일링 적용된 데이터 세트에 PCA를 적용해 4차원(4개 속성) 붓꽃 데이터를 2차원(2개의 PCA 속성) PCA 데이터로 변환했습니다. 

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)

#fit()과 transform()을 호출해 PCA 변환 데이터 반환
pca.fit(iris_scaled)
iris_pca = pca.transform(iris_scaled)
print(iris_pca.shape)
```

![image](https://user-images.githubusercontent.com/76269316/129442421-d5b1ada6-b486-4533-b5e1-1f6b7c83a296.png)

iris_pca가 변환된 PCA 데이터 세트 150x2 넘파이 행렬을 갖고 있는 것을 확인할 수 있습니다.

DataFrame으로 변환한 다음 2개의 PCA 속성을 각각 X축, Y축으로 해서 붓꽃 품종이 어떻게 분포되는지 확인해 보겠습니다.

```python
#PCA 변환된 데이터 컬럼 명을 각각 pca_component_1, pca_component_2로 명명
pca_columns = ['pca_component_1', 'pca_component_2']
irisDF_pca = pd.DataFrame(iris_pca, columns=pca_columns)
irisDF_pca['target'] = iris.target

#setosa 세모, versicolor 네모, virginica 동그라미로 표현
markers = ['^', 's', 'o']

#target 값은 setosa 0, versicolor 1, virginica 2. 각 target별로 다른 모양으로 표시
for i, marker in enumerate(markers):
    x_axis_data = irisDF[irisDF_pca['target'] == i]['sepal_length']
    y_axis_data = irisDF[irisDF_pca['target'] == i]['sepal_width']
    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])
    
plt.legend()
plt.xlabel('pca_component_1')
plt.ylabel('pca_component_2')
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/129657754-31b63d75-89c4-4cea-8546-80ab5cc2f17d.png)

PCA로 변환한 후에도 pca_component_1 축을 기반으로 setosa 품종은 명확하게 구분이 가능하지만 versicolor와 virginica는 pca_component_1 축을 기반으로 서로 겹치는 부분이 일부 존재하는 것을 확인할 수 있습니다.

<br>

PCA component 별로 원본 데이터의 변동성을 얼마나 반영하고 있는지 PCA 변환을 수행한 PCA 객체의 explained_variance_ratio_ 속성(전체 변동성에서 개별 PCA 컴포넌트별로 차지하는 변동성 비율)으로 확인할 수 있습니다.

```python
print(pca.explained_variance_ratio_)
```

![image](https://user-images.githubusercontent.com/76269316/129442601-e1f96aaf-5a1e-4716-98ce-53ab944fe07a.png)

첫 번째 PCA 변환 요소인 pca_component_1이 전체 변동성의 약 72.9%를 차지하며 pca_component_2가 약 22.8%를 차지하는 것을 확인할 수 있습니다.

이는 2개의 PCA 요소만으로도 원본 데이터의 변동성을 95%를 설명할 수 있다는 것을 의미합니다.

<br>

<br>

원본 붓꽃 데이터 세트와 PCA로 변환된 데이터 세트에 각각 분류를 적용해 결과를 비교해 보겠습니다.

Estimator는 RandomForestClassifier를 이용하고 cross_val_scor()로 3개의 교차 검증 세트로 정확도 결과를 비교하겠습니다.

<br>

**원본 붓꽃 데이터 세트**

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

rcf = RandomForestClassifier(random_state=156)
scores = cross_val_score(rcf, iris.data, iris.target, scoring='accuracy', cv=3)
print('원본 데이터 교차 검증 개별 정확도: ', scores)
print('원본 데이터 평균 정확도: ', np.mean(scores))
```

![image](https://user-images.githubusercontent.com/76269316/129442838-459e329d-7be1-4c41-a398-42f727697a25.png)

<br>

**PCA로 변환된 데이터 세트**

```python
pca_X = irisDF_pca[['pca_component_1', 'pca_component_2']]
scores_pca = cross_val_score(rcf, pca_X, iris.target, scoring='accuracy', cv=3)
print('PCA 변환 데이터 교차 검증 개별 정확도: ', scores_pca)
print('PCA 변환 데이터 평균 정확도: ', np.mean(scores_pca))
```

![image](https://user-images.githubusercontent.com/76269316/129443006-1cc15d0a-b39d-4d76-bb4a-dcf5553f8c32.png)

PCA로 차원 축소시 기존 데이터의 정보가 유실되기 때문에 원본 데이터 세트 대비 예측 성능이 떨어질 수 밖에 없습니다.

그래도 4개의 속성이 2개로(속성 개수가 50% 감소) 감소한 것을 고려하면, 여전히 원본 데이터의 특성을 상단 부분 유지하고 있다는 것을 알 수 있습니다.

<br>

<br>

좀 더 많은 피처를 갖고 있는 [UCI Machine Learning Repository 신용카드 고객 데이터 세트(Credit Card Clients DataSet)](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)를 이용해 주성분 분석을 해보겠습니다.

 ![image](https://user-images.githubusercontent.com/76269316/129443424-c892314f-f16a-44e1-8108-1f4c22e5286d.png)

![image](https://user-images.githubusercontent.com/76269316/129443455-72e516df-572e-474e-a2b6-59bab1d52535.png)

DataFolder에서 default of credit card clients.xls 파일을 다운 받은 후 credit_card.xls로 변경해 사용했습니다.

<br>

엑셀 파일을 열면 다음과 같이 의미 없는 첫 번째 행(빨강색 네모)과 단순히 row를 식별하는데만 사용되는 id 컬럼(파란색 네모)이 존재하는데, 이들을 제거한 뒤 DataFrame으로 로딩했습니다.

![image](https://user-images.githubusercontent.com/76269316/129443554-0f597319-4f6f-4aaa-b4ed-6275b102ae3e.png)



```python
#header로 의미 없는 첫 행 제거, iloc로 id 컬럼 제거
import pandas as pd

df = pd.read_excel('credit_card.xls', header=1, sheet_name='Data').iloc[0:, 1:]
print(df.shape)
df.head()
```

![image](https://user-images.githubusercontent.com/76269316/129443628-7b8730d4-4f09-49c7-8dae-ba2338cabae4.png)

신용카드 데이터 세트에 30,000개의 레코드와 24개 속성이 있는데 이 중 '다음 달 연체 여부'를 의미하는 'default payment next month' 속성이 target 값(정상 납부 : 0, 연체 : 1)인데 컬럼명이 너무 길어서 변경하고,

PAY_0 컬럼 다음에 PAY_2 컬럼이 존재하므로 PAY_0 컬럼을 PAY_1으로 변경했습니다.

```python
df.rename(columns={'PAY_0':'PAY_1', 'default payment next month':'default'}, inplace=True)
y_target = df['default']
X_features = df.drop('default', axis=1)
```

<br>

23개 속성의 상관도를 DataFrame corr()를 이용해 구한뒤, Seaborn의 heatmap으로 시각화해 보겠습니다.

```python
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

corr = X_features.corr()
plt.figure(figsize=(14, 14))
sns.heatmap(corr, annot=True, fmt='.1g')  #annot: 각 셀의 값(상관계수) 표기 여부, fmt: 데이터 타입
```

![image](https://user-images.githubusercontent.com/76269316/129443867-539529e9-cf49-44f8-8192-3b4d00bfad74.png)

BILL_AMT1 ~ BILL_AMT6 6개 속성의 상관도가 대부분 0.9 이상으로 매우 높은 것을 확인할 수 있습니다.

이렇게 높은 상관도를 갖는 속성들은 소수의 PCA 요소만으로도 이 속성들의 변동성을 수용할 수 있습니다.

BILL_AMT1 ~ BILL_AMT6 6개 속성을 PCA 컴포넌트로 변환한 뒤 개별 컴포넌트의 변동성을 explained_variance_ratio_ 속성으로 확인해 보겠습니다.

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

#BILL_AMT1 ~ BILL_AMT6 6개 속성명 생성
cols_bill = ['BILL_AMT'+str(i) for i in range(1, 7)]
print('대상 속성명: ', cols_bill)

#2개의 PCA 속성을 갖는 PCA 객체 생성 후 explained_variance_ratio_ 계산을 위해 fit() 호출
scaler = StandardScaler()
df_cols_scaled = scaler.fit_transform(X_features[cols_bill])
pca = PCA(n_components=2)
pca.fit(df_cols_scaled)
print('PCA Component별 변동성: ', pca.explained_variance_ratio_)
```

![image](https://user-images.githubusercontent.com/76269316/129444024-6223e572-d00d-488a-8517-b75cfff5c135.png)

단 2개의 PCA 컴포넌트만으로도 6개 속성의 변동성을 약 95% 이상 설명할 수 있으며 특히 첫 번째 PCA 축으로 90%의 변동성을 수용할 수 있을 정도로 6개 속성의 상관도가 높다는 것을 확인할 수 있습니다.

<br>

<br>

붓꽃 데이터 세트와 마찬가지로, 원본 데이터 세트와 PCA로 변환된 데이터 세트에 각각 분류를 적용해 결과를 비교해 보겠습니다.

<br>

**원본 신용카드 데이터 세트**

```python
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

rcf = RandomForestClassifier(n_estimators=300, random_state=156)
scores = cross_val_score(rcf, X_features, y_target, scoring='accuracy', cv=3)

print('CV=3인 경우 개별 Fold 세트별 정확도: ', scores)
print('평균 정확도: {0:.4f}'.format(np.mean(scores)))
```

![image](https://user-images.githubusercontent.com/76269316/129444214-dbf962ca-0d82-4e06-b5c3-24cd42d63095.png)

<br>

**PCA로 변환된 데이터 세트**

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

#원본 데이터 세트에 먼저 StandardScaler 적용
scaler = StandardScaler()
df_scaled = scaler.fit_transform(X_features)

#6개 컴포넌트를 갖도록 PCA 변환을 수행하고 cross_val_score()로 분류 예측 수행
pca = PCA(n_components=6)
df_pca = pca.fit_transform(df_scaled)
scores_pca = cross_val_score(rcf, df_pca, y_target, scoring='accuracy', cv=3)

print('CV=3인 경우 PCA 변환된 개별 Fold 세트별 정확도: ', scores_pca)
print('PCA 변환 데이터 세트 평균 정확도: {0:.4f}'.format(np.mean(scores_pca)))
```

![image](https://user-images.githubusercontent.com/76269316/129444258-af05dbd6-fc30-412b-95c1-9a8d08d4f30b.png)

전체 23개 속성의 약 1/4 수준인 6개의 PCA 컴포넌트만으로도 원본 데이터를 기반으로한 분류 예측보다 약 1~2% 정도의 예측 성능 저하만 발생했습니다.

1~2%도 큰 성능 저하이지만 전체 속성의 1/4 정도만으로 이정도 성능을 예측한다는 것은 PCA가 그만큼 뛰어난 압축 능력을 보여주는 것이라고 할 수 있습니다. 

<br>

<br>

<br>

### LDA(Linear Discriminant Analysis)

##### LDA 개요

LDA(Linear Discriminant Analysis)는 선형 판별 분석법이라 불리며, PCA와 유사하게 입력 데이터 세트를 저차원 공간에 투영해 차원을 축소하는 기법이지만,

LDA는 지도학습의 분류(Classification)에서 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소한다는 점에서 다릅니다.

또한 LDA는 지도 피처 추출(Supervised feature extraction) 방식으로 y값(label 값)을 이용하여, X값(피처)들의 결합을 통해 새로운 변수를 추출하는 기법입니다.

<br>

PCA는 입력 데이터의 변동성의 가장 큰 축을 찾았지만, LDA는 입력 데이터의 결정 값 클래스를 최대한 분리할 수 있는 축을 찾습니다.

이를 위해 클래스 간 분산(between-class scatter)은 최대한 크게 가져가고, 클래스 내부 분산(within-class scatter)은 최대한 적게 가져가는 방식으로 차원을 축소합니다.

아래 그림은 좋은 클래스 분리를 위해 클래스 간 분산이 크고 클래스 내부 분산이 작은 것을 표현한 것입니다.



![image](https://user-images.githubusercontent.com/76269316/129447336-eb81d8a7-7714-47b1-a037-e288efe1fc0a.png)

<br>

LDA는 다음 스텝으로 수행됩니다.

1. 클래스 내부와 클래스 간 분산 행렬을 구합니다.
   이 두 개의 행렬은 입력 데이터의 결정 값 클래스 별로 개별 피처의 평균 벡터(mean vector)를 기반으로 구합니다.
2. 클래스 내부 분산 행렬을 ![image](https://user-images.githubusercontent.com/76269316/129447425-ae09d2be-b7fa-4514-bfe6-94e53668a34c.png), 클래스 간 분산 행렬을 ![image](https://user-images.githubusercontent.com/76269316/129447430-0d38bec3-078d-4ad2-8380-9fccbb2eda34.png)라고 할 때, 다음 식으로 두 행렬을 고유벡터로 분해할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/129447527-a9bd14cb-9fc2-4dd4-9d9f-d581cd2d1528.png)

3. 고유값이 가장 큰 순으로 K개(LDA 변환 차수만큼) 추출합니다.
4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환합니다.

<br>

##### 붓꽃 데이터 세트에 LDA 적용하기

붓꽃 데이터 세트를 사이킷런 LinearDiscriminantAnalysis 클래스를 이용해 변환하고, 그 결과를 품종별로 시각화해 보겠습니다.

<br>

붓꽃 데이터 세트를 로드하고 표준 정규 분포로 스케일링한 뒤, LDA를 적용했습니다.

이때 LDA는 지도 피처 추출이기 때문에 학습 시 결정값이 입력된 것을 확인할 수 있습니다.

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris

iris = load_iris()
iris_scaled = StandardScaler().fit_transform(iris.data)

lda = LinearDiscriminantAnalysis(n_components=2)
lda.fit(iris_scaled, iris.target)  #LDA는 지도 피처 추출이기 때문에 학습시 결정값이 필요
iris_lda = lda.transform(iris_scaled)
print(iris_ida.shape)
```

![image](https://user-images.githubusercontent.com/76269316/129447820-1a759057-8abe-4229-810e-acc25a465168.png)

<br>

변환된 데이터 값을 2차원 평면에 품종별로 표현해 보겠습니다.

```python
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

lda_columns = ['lda_component_1', 'lda_component_2']
irisDF_lda = pd.DataFrame(iris_lda, columns=lda_columns)
irisDF_lda['target'] = iris.target

#setosa 세모, versicolor 네모, virginica 동그라미로 표현
markers = ['^', 's', 'o']

#target 값은 setosa 0, versicolor 1, virginica 2. 각 target별로 다른 모양으로 표시
for i, marker in enumerate(markers):
    x_axis_data = irisDF_lda[irisDF_lda['target'] == i]['lda_component_1']
    y_axis_data = irisDF_lda[irisDF_lda['target'] == i]['lda_component_2']
    plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])
    
plt.legend(loc='upper right')  #범례를 오른쪽 상단에 배치
plt.xlabel('lda_component_1')
plt.ylabel('lda_component_2')
plt.show()
```

|                           PCA 적용                           |                           LDA 적용                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/129442549-300978e7-3e9b-4f32-b0f9-6b8095af16e6.png) | ![image](https://user-images.githubusercontent.com/76269316/129447978-57563513-9147-401c-b4a7-019004995438.png) |

PCA를 적용했을 때보다 클래스 값을 명확하게 분류한 것을 확인할 수 있습니다.

<br><br><br>

### SVD(Singular Value Decomposition)

PCA의 경우 정방행렬(행과 열의 크기가 같은 행렬)만을 고유벡터로 분해할 수 있지만, SVD(특이값 분해)는 정방행렬뿐만 아니라 행과 열의 크기가 다른 행렬에도 적용할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/129470985-6d27f4b4-50db-401a-8c42-f7b1ca067b1c.png)

행렬 U와 V에 속한 벡터는 특이벡터(singular vector)이며, 모든 특이벡터는 서로 직교하는 성질을 갖습니다.

![image](https://user-images.githubusercontent.com/76269316/129471006-91c5a4b5-acb0-4df4-b911-9e89778d630f.png)는 대각행렬이며, 행렬의 대각선에 행렬 A의 특이값이 위치하고 나머지 위치 값은 모두 0입니다.

SVD는 A의 차원이 mxn일 때 아래 그림과 같이 U의 차원 mxm, ![image](https://user-images.githubusercontent.com/76269316/129471006-91c5a4b5-acb0-4df4-b911-9e89778d630f.png)의 차원 mxn, <img src="https://user-images.githubusercontent.com/76269316/129471060-93d1bfd2-ddd4-4edc-bb27-121392c0000b.png" alt="image" style="zoom:80%;" />의 차원 nxn으로 분해합니다.

![image](https://user-images.githubusercontent.com/76269316/129471145-7fa4373b-8946-473b-8b79-8be3de432e3d.png)

하지만 일반적으로는 다음과 같이 ![image](https://user-images.githubusercontent.com/76269316/129471006-91c5a4b5-acb0-4df4-b911-9e89778d630f.png)의 비대각인 부분과 대각원소 중에 특이값이 0인 부분을 모두 제거하고 제거된 ![image](https://user-images.githubusercontent.com/76269316/129471006-91c5a4b5-acb0-4df4-b911-9e89778d630f.png)에 대응하는 U와 V의 원소도 함께 제거해 차원을 줄인 형태로 SVD를 적용합니다.

![image](https://user-images.githubusercontent.com/76269316/129471198-3ca62d28-b023-417c-aa87-3133ed003637.png)

컴팩트한 형태로 SVD를 적용하면 A의 차원이 mxn일 때, U의 차원을 mxp, ![image](https://user-images.githubusercontent.com/76269316/129471006-91c5a4b5-acb0-4df4-b911-9e89778d630f.png)의 차원을 pxp, <img src="https://user-images.githubusercontent.com/76269316/129471060-93d1bfd2-ddd4-4edc-bb27-121392c0000b.png" alt="image" style="zoom:80%;" />의 차원을 pxn으로 분해합니다.

<br>

Truncated SVD는 ![image](https://user-images.githubusercontent.com/76269316/129471006-91c5a4b5-acb0-4df4-b911-9e89778d630f.png)의 대각원소 중에 상위 몇 개만 추출해서 여기에 대응하는 U와 V의 원소도 함께 제거해 차원을 더욱 줄인 형태로 분해하는 것입니다.

<br>

일반적으로 SVD는 넘파이나 사이파이 라이브러리를 이용해 수행하는데, 넘파이 SVD를 사용해 SVD 연산을 수행해보도록 하겠습니다.

개별 row의 의존성을 없애기 위해 랜덤 행렬을 생성해서 사용했습니다.

```python
import numpy as np
from numpy.linalg import svd

#4X4 랜덤 행렬 a 생성
np.random.seed(121)
a = np.random.randn(4,4)
print(np.round(a, 3))
```

![image](https://user-images.githubusercontent.com/76269316/129471470-a7ec7b66-b07a-497b-b4e4-002ab76b3f21.png)

<br>

numpy의 SVD를 적용해 U, sigma, Vt를 도출해보겠습니다.

svd에 파라미터로 원본 행렬을 입력하면 U 행렬, Sigma 행렬, V 전치 행렬을 반환합니다.

![image](https://user-images.githubusercontent.com/76269316/129471006-91c5a4b5-acb0-4df4-b911-9e89778d630f.png)행렬의 경우 행렬의 대각에 위치한 값만 0이 아니고, 그렇지 않은 경우는 모두 0이므로 0이 아닌 값의 경우만 1차원 행렬로 반환합니다.

```python
U, Sigma, Vt = svd(a)
print(U.shape, Sigma.shape, Vt.shape)
print('U matrix\n', np.round(U, 3))
print('Sigma Value\n', np.round(Sigma, 3))
print('V transpose matrix\n', np.round(Vt, 3))
```

![image](https://user-images.githubusercontent.com/76269316/129471584-4a302e1f-cdb3-48f3-887d-454c684e9e1e.png)

분해된 U, Sigma, Vt를 이용해 다시 원본 행렬로 정확히 복원되는지 확인해 보겠습니다.

U, Sigma, Vt를 내적해서 복원하면 되는데, Sigma가 0이 아닌 값만 1차원으로 추출했으므로 다시 0을 포함한 대칭행렬로 변환한 뒤 내적을 수행했습니다.

```python
#Sigma를 다시 0을 포함한 대칭행렬로 변환
Sigma_mat = np.diag(Sigma)
a_ = np.dot(np.dot(U, Sigma_mat), Vt)
print(np.round(a_, 3))
```

|                        a_ (복원 행렬)                        |                        a (원본 행렬)                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/129471652-82a98a3c-81a4-468e-8058-ef95ac952d9e.png) | ![image](https://user-images.githubusercontent.com/76269316/129471470-a7ec7b66-b07a-497b-b4e4-002ab76b3f21.png) |

복원된 행렬 a_가 원본 행렬과 동일한 것을 확인할 수 있습니다.

<br>

데이터 세트가 row간 의존성이 있을 경우 Sigma 값이 어떻게 변하는지와 이에 따른 차원 축소가 진행될 수 있는지 확인해 보겠습니다.

의존성을 부여하기 위해 a 행렬의 3번째 row를 첫 번째 row + 두 번째 row로 업데이트하고, 4번째 row는 첫 번째 row와 같도록 업데이트해 줬습니다.

```python
a[2] = a[0] + a[1]
a[3] = a[0]
print(np.round(a, 3))
```

![image](https://user-images.githubusercontent.com/76269316/129471745-0eb1816d-7cb6-4526-822a-4b97d0e2c6a1.png)

업데이트 이후 row간 관계가 매우 높아졌습니다.

<br>

이 데이터를 다시 SVD로 분해해 보겠습니다.

```python
#다시 SVD를 수행해 Sigma 값 확인
U, Sigma, Vt = svd(a)
print(U.shape, Sigma.shape, Vt.shape)
print('Sigma Value\n', np.round(Sigma, 3))
```

![image](https://user-images.githubusercontent.com/76269316/129471816-4c65e22b-f16c-47f2-8687-18d7daa9c9f5.png)

이전과 차원은 같지만 Sigma 값 중 2개가 0으로 변했습니다.

즉 선형 독립인 row 벡터의 개수가 2개라는 의미입니다. (행렬의 rank가 2)

이렇게 분해된 U, Sigma, Vt를 이용해 다시 원본 행렬로 복원해 보겠습니다.

전체 데이터를 이용하지 않고, Sigma의 0에 대응되는 U, Sigma, Vt 데이터를 제외하고 복원해 보겠습니다.

```python
#U 행렬의 경우 Sigma와 내적을 수행하므로 Sigma 앞 2행에 대응되는 앞 2열만 추출
U_ = U[:, :2]
Sigma_ = np.diag(Sigma[:2])

#V 전치 행렬의 경우 앞 2행만 추출
Vt_ = Vt[:2]
print(U_.shape, Sigma_.shape, Vt_.shape)

#U, Sigma, Vt 내적을 수행하며 다시 원본 행렬 복원
a_ = np.dot(np.dot(U_, Sigma_), Vt_)
print(np.round(a_, 3))
```

|                        a_ (복원 행렬)                        |                        a (원본 행렬)                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/129471940-70ef2fdb-7bf1-464f-9c25-25144c38730c.png) | ![image](https://user-images.githubusercontent.com/76269316/129471470-a7ec7b66-b07a-497b-b4e4-002ab76b3f21.png) |

위 2개의 행을 제외하고는 원본 행렬과 다른 값으로 복원된 것을 확인할 수 있습니다.

<br>

이번에는 Truncated SVD를 이용해 행렬을 분해해 보겠습니다.

Truncated SVD는 ![image](https://user-images.githubusercontent.com/76269316/129471006-91c5a4b5-acb0-4df4-b911-9e89778d630f.png) 행렬에 있는 대각원소, 특이값 중 상위 일부 데이터만 추출해 분해하는 방식입니다.

이렇게 분해하면 인위적으로 더 작은 차원의 U, ![image](https://user-images.githubusercontent.com/76269316/129471006-91c5a4b5-acb0-4df4-b911-9e89778d630f.png),  <img src="https://user-images.githubusercontent.com/76269316/129471060-93d1bfd2-ddd4-4edc-bb27-121392c0000b.png" alt="image" style="zoom:80%;" />로 분해하기 때문에 원본 행렬을 정확하게 다시 원복할 수는 없습니다.

하지만 데이터 정보가 압축되어 분해됨에도 불구하고 상당한 수준으로 원본 행렬을 근사할 수 있습니다. (원래 차원의 차수에 가깝게 잘라낼수록(Truncate) 원본 행렬에 더 가깝게 복원할 수 있음)

<br>

사이파이 SVD, Truncated SVD 모듈을 사용해 임의의 원본 행렬 6x6을 Normal SVD로 분해해 분해된 행렬의 차원과 Sigma 행렬 내 특이값을 확인한 뒤, Truncated SVD로 분해해 행렬의 차원, Sigma 행렬 내의 특이값, Truncated SVD로 분해된 행렬 내적을 계산해 복원된 데이터와 원본 데이터를 비교해 보겠습니다.

```python
import numpy as np
from scipy.linalg import svd  #사이파이 SVD 모듈
from scipy.sparse.linalg import svds  #사이파이 Truncated SVD 모듈

#원본 행렬을 출력하고 SVD를 적용해 U, Sigma, Vt 차원 확인
np.random.seed(121)
matrix = np.random.random((6, 6))
print('원본 행렬\n', matrix)
U, Sigma, Vt = svd(matrix, full_matrices=False)
print('\n분해 행렬 차원: ', U.shape, Sigma.shape, Vt.shape)
print('\nSigma값 행렬: ', Sigma)

#Truncated SVD로 Sigma 행렬의 특이값을 4개로 하여 Truncated SVD 수행
num_components = 4
U_tr, Sigma_tr, Vt_tr = svds(matrix, k=num_components)
print('\nTruncated SVD 분해 행렬 차원: ', U_tr.shape, Sigma_tr.shape, Vt_tr.shape)
print('\nTruncated SVD Sigma 값 행렬: ', Sigma_tr)
matrix_tr = np.dot(np.dot(U_tr, np.diag(Sigma_tr)), Vt_tr)

print('\nTruncated SVD로 분해 후 복원 행렬\n', matrix_tr)
```

![image](https://user-images.githubusercontent.com/76269316/129472230-410f3d38-c999-4fe7-90fe-cb590d3f8b63.png)

6x6 행렬을 SVD 분해하면 U, Sigma, Vt가 각각 (6, 6) (6) (6, 6) 차원이지만 Truncated SVD의 n_components를 4로 설정해 U, Sigma, Vt를 (6, 4) (4) (4, 6)로 분해했습니다.

Truncated SVD로 다시 복원할 경우 완벽하게 복원되지 않고 근사적으로 복원됨을 확인할 수 있습니다.

<br><br>

##### 사이킷런 TruncatedSVD 클래스를 이용한 변환

사이킷런 TruncatedSVD 클래스는 사이파이의 svds처럼 Truncated SVD 연산을 수행해 원본 행렬을 분해한 U, Sigma, Vt 행렬을 반환하지 않고 PCA 클래스와 유사하게 fit()과 transform()을 호출해 원본 데이터를 몇 개의 컴포넌트(Truncated SVD K 컴포넌트 수)로 차원을 축소해 변환합니다.

<br>

붓꽃 데이터 세트에 TruncatedSVD를 적용해 보겠습니다.

```python
from sklearn.decomposition import TruncatedSVD, PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
%matplotlib inline

iris = load_iris()
iris_ftrs = iris.data

#2개의 주요 컴포넌트로 Truncated SVD 변환
tsvd = TruncatedSVD(n_components=2)
tsvd.fit(iris_ftrs)
iris_tsvd = tsvd.transform(iris_ftrs)

#산점도 2차원으로 TruncatedSVD 변환된 데이터 표현.
plt.scatter(x=iris_tsvd[:, 0], y=iris_tsvd[:, 1], c=iris.target)
plt.xlabel('Truncated Component 1')
plt.ylabel('Truncated Component 2')
```

![image](https://user-images.githubusercontent.com/76269316/129472398-db91c132-16b8-4e3a-a81e-9998e6795213.png)

변환 후 품종별로 어느 정도 클러스터링이 가능할 정도로 뛰어난 고유성을 갖고 있음을 확인할 수 있습니다.

<br>

사이킷런 TruncatedSVD와 PCA 클래스 구현을 자세히 들여다보면 두 개의 클래스 모두 SVD를 이용해 행렬을 분해하는 것을 알 수 있습니다.

붓꽃 데이터를 스케일링으로 변환한 뒤 TruncatedSVD와 PCA 클래스 변환을 해보면 두 개가 거의 동일한 것을 확인할 수 있습니다.

```python
from sklearn.preprocessing import StandardScaler

#붓꽃 데이터를 StandardScaler로 변환
scaler = StandardScaler()
iris_scaled = scaler.fit_transform(iris_ftrs)

#스케일링된 데이터를 기반으로 TruncatedSVD 변환 수행
tsvd = TruncatedSVD(n_components=2)
tsvd.fit(iris_scaled)
iris_tsvd = tsvd.transform(iris_scaled)

#스케일링된 데이터를 기반으로 PCA 변환 수행
pca = PCA(n_components=2)
pca.fit(iris_scaled)
iris_pca = pca.transform(iris_scaled)

#TruncatedSVD 변환 데이터를 왼쪽, PCA 변환 데이터를 오른쪽에 표현
fig, (ax1, ax2) = plt.subplots(figsize=(9, 4), ncols=2)
ax1.scatter(x=iris_tsvd[:, 0], y=iris_tsvd[:, 1], c=iris.target)
ax2.scatter(x=iris_pca[:, 0], y=iris_pca[:, 1], c=iris.target)
ax1.set_title('Truncated SVD Transformed')
ax2.set_title('PCA Transformed')
```

![image](https://user-images.githubusercontent.com/76269316/129472611-cae22329-9d25-402d-96a2-1ad2a6879e97.png)

두 개의 변환 행렬 값과 원본 속성별 컴포넌트 비율값을 비교해 보면 거의 같은 것을 확인할 수 있습니다.

```python
print((iris_pca - iris_tsvd).mean())
print((pca.components_ - tsvd.components_).mean())
```

![image](https://user-images.githubusercontent.com/76269316/129472647-ee67b995-49e0-48b7-898e-f79ce89854a9.png)

두 값의 차이가 모두 0에 가까운 값이므로 2개 변환이 서로 동일함을 알 수 있습니다.

데이터 세트가 스케일링으로 데이터 중심이 동일해지면 사이킷런 SVD와  PCA는 동일한 변환을 수행합니다. (이는 PCA가 SVD 알고리즘으로 구현됐음을 의미합니다.)

하지만 PCA는 밀집 행렬(Dense Matrix)에 대한 변환만 가능하며  SVD는 희소 행렬(Sparse Matrix)에 대한 변환도 가능합니다.

SVD는 PCA와 유사하게 컴퓨터 비전 영역에서 이미지 압축을 통한 패턴 인식과 신호 처리 분야에서 사용되며, 또한 텍스트 토픽 모델링 기법인 LSA(Latent Semantic Analysis) 기반 알고리즘입니다.
