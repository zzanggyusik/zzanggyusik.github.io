---
title: "9.Recommendations"
excerpt: "추천 시스템"
toc: true
toc_label: "Recommendations"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - 추천 시스템
  - 파이썬 머신러닝 완벽 가이드
last_modified_at: 2021-09-04
---

>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.

### 추천 시스템의 개요와 배경

##### 추천 시스템의 개요

하나의 콘텐츠를 선택했을 때 선택된 콘텐츠와 연관된 추천 콘텐츠가 얼마나 사용자의 관심을 끌고 개인에게 맞춤 콘텐츠를 추천했는지는 그 사이트의 평판을 좌우하는 매우 중요한 요소입니다.

추천 시스템의 진정한 묘미는 사용자 자신도 좋아하는지 몰랐던 취향을 시스템이 발견하고 그에 맞는 콘텐츠를 추천해주는 것입니다.

이러한 추천 시스템을 접한 사용자는 해당 사이트를 더 강하게 신뢰하게 되어 더 많은 추천 콘텐츠를 선택하게 되고, 결국 더 많은 데이터가 축적되면서 추천 시스템이 더욱 정확해지고 다양한 결과를 얻을 수 있는 선순환 시스템을 구축할 수 있게 됩니다.

넷플릭스, 유튜브, 아마존 등 많은 기업들이 사용자의 관심을 오랫동안 지속하기 위해 추천 시스템의 고도화에 큰 비용과 투자를 아끼지 않고 있습니다.

<br><br>

##### 온라인 스토어의 필수 요소, 추천 시스템

온라인 스토어는 많은 양과 고객의 상품 관련 데이터를 갖고 있는데, 너무 많은 상품과 콘텐츠는 오히려 사용자가 어떤 상품을 골라야 할지에 대한 압박감을 느끼게 만듧니다.

이때 좋은 추천 시스템은 사용자가 무엇을 원하는지 빠르게 찾아내 사용자의 온라인 쇼핑을 도와 이러한 상황을 타개해줍니다.

온라인 스토어는 많은 양의 고객과 상품 관련 데이터를 갖고 있는데, 다음과 같은 데이터가 추천 시스템을 구성하는데 사용될 수 있습니다.

- 사용자가 어떤 상품을 구매했는가?
- 사용자가 어떤 상품을 둘러보거나 장바구니에 넣었는가?
- 사용자가 평가한 영화 평점은? 제품 평가는?
- 사용자가 스스로 작성한 자신의 취향은?
- 사용자가 무엇을 클릭했는가?

이런 데이터를 기반으로 추천 시스템은 '당신만을 위한 최신 상품', '이 상품을 선택한 다른 사람들이 좋아하는 상품들', '이 상품을 좋아하나요?' 같은 문구로 사용자가 상품을 구매하도록 유혹합니다.

<br><br>

##### 추천 시스템의 유형

추천 시스템은 콘텐츠 기반 필터링(Content based filtering) 방식과 협업 필터링(Collaborative filtering) 방식으로 나뉩니다.

협업 필터링은 다시 최근접 이웃(Nearest Neighbor) 협업 필터링과 잠재 요인(Latent Factor) 협업 필터링으로 나뉩니다.

- 콘텐츠 기반 필터링
- 협업 필터링
  - 최근접 이웃 협업 필터링
  - 잠재 요인 협업 필터링



초창기에는 콘텐츠 기반 필터링이나 최근접 이웃 협업 필터링이 주로 사용됐지만, 넷플릭스 추천 시스템 경연 대회에서 행렬 분해(Matrix Factorization) 기법을 이용한 잠재 요인 협업 필터링 방식이 우승하면서 많은 온라인 스토어에서 잠재 요인 협업 필터링 기반의 추천 시스템을 적용하고 있습니다.

하지만 서비스 아이템의 특성에 따라 콘텐츠 기반 필터링이나 최근접 이웃 기반 필터링 방식을 유지하는 사이트들도 많고, 요즘에는 개인화 특성을 강화하기 위해 하이브리드 형식으로 결합해 사용하는 경우도 늘고 있습니다.

<br><br><br>

### 콘텐츠 기반 필터링 추천 시스템

콘텐츠 기반 필터링 방식은 사용자가 특정한 아이템을 매우 선호하는 경우, 그 아이템과 비슷한 콘텐츠를 가진 다른 아이템을 추천하는 방식입니다.

예를 들어 사용자가 '엔드게임'이라는 영화에 8점(10점 만점), '인피니티 워'에 9점이라는 높은 점수를 줬을 때,

'엔드게임'과 '인피니티 워'의 장르, 감독, 출연 배우, 키워드 등의 콘텐츠를 감안해 이와 유사한 영화를 추천합니다.

'엔드게임'의 경우 장르가 액션, 모험, 슈퍼히어로 영화이고, '인피니티 워'는 액션, 슈퍼히어로 영화, SF로 분류됩니다.

영화 감독의 경우 앤서니 루소, 조 루소입니다.

콘텐츠 기반 필터링 추천 시스템은 사용자가 높게 평가한 이러한 영화의 콘텐츠를 감안해 이와 적절하게 매칭되는 영화를 추천해줍니다.

'시빌 워'의 경우 사용자가 선호하는 감독, 장르 등의 콘텐츠를 다양하게 포함하고 있으므로 콘텐츠 기반 필터링 추천 시스템에 의해 추천되게 됩니다.

<img src="https://user-images.githubusercontent.com/76269316/131944660-f15b9b72-41f6-4e15-a4ca-02daacd590c2.png" alt="image"  />

<br><br><br>

### 최근접 이웃 협업 필터링

하지만 좋아하는 배우/감독만을 보고 영화를 선택했다가 실망한 경험이 있을겁니다.

그래서 우리는 대부분 영화를 본 가까운 친구들(취향이 비슷한)에게 영화가 어땠는지를 물어보고 영화를 보러 갈지 말지를 결정합니다.

이렇게 친구들에게 물어보는 것과 유사한 방식으로, 사용자가 아이템에 매긴 평점 정보나 상품 구매 이력과 같은 사용자 행동 양식(User Behavior)만을 기반으로 추천을 수행하는 것이 협업 필터링(Collaborative Filtering) 방식입니다.

<br>

협업 필터링의 주요 목표는 사용자-아이템 평점 행렬과 같은 축적된 사용자 행동 데이터를 기반으로 사용자가 아직 평가하지 않은 아이템을 예측 평가(Predicted Rating) 하는 것입니다.



<img src="https://user-images.githubusercontent.com/76269316/131945428-2ccd963c-db1f-4c34-aaf7-b730ba2e3e9e.png" alt="image" style="zoom:67%;" />

위 그림에서 User1은 Item 4에 대해 아직 평가하지 않았습니다.

협업 필터링은 사용자가 평가한 다른 아이템을 기반으로 사용자가 평가하지 않은 아이템의 예측 평가를 도출하는 방식입니다.



협업 필터링 기반 추천 시스템은 **최근접 이웃 방식**과 **잠재 요인 방식**으로 나뉘며, 두 방식 모두 사용자-아이템 평점 행렬 데이터에 의존해 추천을 수행합니다.

사용자-아이템 평점 행렬에서 행은 개별 사용자, 열은 개별 아이템으로 구성되며, 사용자 아이디 행, 아이템 아이디 열 위치에 해당하는 값이 평점을 나타내는 형태입니다.

<br>

만약 다음 그림 왼쪽과 같이 사용자-아이템 평점 데이터라면 판다스 pivot_table()과 같은 함수를 사용해 오른쪽과 같은 형태인 사용자-아이템 평점 행렬로 변경해야 합니다.

<img src="https://user-images.githubusercontent.com/76269316/131949621-e2eccbd7-dd81-49c7-ab0b-ac7ef00e78a5.png" alt="image" style="zoom: 67%;" />

일반적으로 이런 사용자-아이템 평점 행렬은 많은 아이템을 열로 갖는 다차원 행렬이며, 사용자가 아이템에 대한 평점을 매기는 경우가 많지 않기 때문에 희소 행렬(Sparse Matrix) 특성을 갖고 있습니다.

<br>

최근접 이웃 협업 필터링은 다시 사용자 기반과 아이템 기반으로 나뉠 수 있습니다.

사용자 기반(User-User) 최근접 이웃 방식은 특정 사용자와 타 사용자 간의 유사도(Similarity)를 측정한 뒤 가장 유사도가 높은 TOP-N 사용자를 추출해 그들이 선호하는 아이템을 추천하는 것입니다.



사용자 A의 주요 영화의 평점 정보는 사용자 C보다 사용자 B와 유사합니다.

만약 사용자 A에게 아직 보지 못한 두 개의 영화 중 하나를 추천한다면 사용자 B가 재미있게 본 '프로메테우스'를 추천하는 것이 사용자 기반 최근접 이웃 협업 필터링입니다.

<img src="https://user-images.githubusercontent.com/76269316/131950285-e080c309-aeff-46cd-916e-e8ed71226901.png" alt="image" style="zoom:67%;" />

<br>

아이템 기반 최근접 이웃 방식은 사용자들이 그 아이템을 좋아하는지/싫어하는지의 평가 척도가 유사한 아이템을 추천하는 알고리즘입니다.

아이템 '다크 나이트'는 '스타워즈-라스트 제다이'보다 '프로메테우스'와 사용자들의 평점 분포가 비슷합니다.

따라서 '다크 나이트'를 매우 좋아하는 사용자 D에게 아이템 기반 협업 필터링은 '프로메테우스'와 '스타워즈-라스트 제다이' 중 '프로메테우스'를 추천합니다.

<img src="https://user-images.githubusercontent.com/76269316/131950455-42c31d3f-7362-45a8-a43b-ebbf35b543f9.png" alt="image" style="zoom:67%;" />

<br>

일반적으로 비슷한 영화를 좋아한다고 사람들의 취향이 비슷하다고 판단하기는 어려운 경우가 많고, 사용자들이 평점을 매긴 영화(또는 상품)의 개수가 많지 않은 경우가 일반적이기 때문에 최근접 이웃 협업 필터링은 사용자 기반보다 아이템 기반 협업 필터링을 적용합니다.

<br><br><br>

### 잠재 요인 협업 필터링

잠재 요인 협업 필터링은 사용자-아이템 평점 행렬 속에 숨어 있는 잠재 요인을 추출해 추천 예측을 할 수 있게 하는 기법입니다.

대규모 다차원 행렬을 SVD와 같은 차원 감소 기법으로 분해하는 과정에서 잠재 요인을 추출하는데 이러한 기법을 행렬 분해(Matrix Factorization)라고 합니다.

다차원 희소 행렬인 사용자-아이템 행렬 데이터를 저차원 밀집 행렬의 **사용자-잠재 요인 행렬**과 **잠재 요인-아이템 행렬**(아이템-잠재 요인 행렬의 전치행렬)로 분해한 뒤, 두 행렬의 내적을 통해 새로운 예측 사용자-아이템 평점 행렬 데이터를 만들어서 사용자가 아직 평점을 부여하지 않은 아이템에 대한 예측 평점을 생성합니다.

![image](https://user-images.githubusercontent.com/76269316/131954931-702b42cf-613e-4fe1-bfbd-faac853171f8.png)

행렬 분해에 의해 추출되는 '잠재 요인'이 어떤 것인지 명확히 정의할 수는 없지만 가령 영화 평점 기반의 사용자-아이템 평점 행렬 데이터라면 영화가 갖는 장르별 특성 선호도로 가정할 수 있습니다.

따라서 사용자-잠재 요인 행렬은 사용자의 영화 장르에 대한 선호도, 아이템-잠재 요인 행렬은 영화의 장르별 특성값으로 정의할 수 있습니다.

<br>

사용자-아이템 평점 행렬 R에서 사용자(User)의 아이템(Item)에 대한 평점은 R(u, i)입니다. (u는 사용자 아이디, i는 아이템 아이디)

사용자-잠재 요인 행렬(사용자의 영화 장르별 선호도) P의 factor 1을 액션 선호도, factor 2를 로맨스 선호도로 가정하겠습니다. (설명상 편의를 위해 장르 2개만 사용했습니다)

아이템-잠재 요인 행렬(영화의 장르별 특성값) Q의 factor 1은 액션 요소 값, factor 2는 로맨스 요소 값으로 설정했습니다.

<img src="https://user-images.githubusercontent.com/76269316/131956840-cfd6bdf3-4349-412f-8cb8-b7e8deba941d.png" alt="image" style="zoom:67%;" />

평점이란 사용자의 특정 영화 장르에 대한 선호도와 개별 영화의 장르적 특성값을 반영해 결정됩니다.

예를 들어 사용자가 액션 영화를 매우 좋아하고, 특정 영화가 액션 영화의 특성이 매우 크다면 사용자가 해당 영화에 높은 평점을 줄 것입니다.

따라서 평점은 사용자의 장르별 선호도와 벡터와 영화의 장르별 특성 벡터를 곱해 만들 수 있습니다.

User 1의 item 1 평점인 R(1, 1)은 P 행렬의 User 1 벡터와 Q.T 행렬의 Item 1 벡터를 곱한 값입니다. (Q는 P와의 내적 계산을 통해 예측 평점을 계산하기 위해 행과 열 위치를 교환합니다)

<br>

마찬가지 방법으로 아직 User 1이 평점을 매기지 못한 Item 2에 대해 예측 평점을 수행할 수 있습니다.

<img src="https://user-images.githubusercontent.com/76269316/131959226-8f8436d5-3d43-4108-b3cf-1c6969d55af1.png" alt="image" style="zoom:67%;" />

R(1, 2)는 행렬 분해된 P 행렬의 User 1 벡터와 Q.T 행렬의 Item 2 벡터의 내적 결괏값인 2.56으로 예측할 수 있습니다.

이처럼 잠재 요인 협업 필터링은 숨겨져 있는 잠재 요인을 기반으로 분해된 행렬을 이용해 사용자가 아직 평가하지 않은 아이템에 대한 예측 평가를 수행하는 것입니다.

<br><br>

##### 행렬 분해의 이해

M개의 사용자(User) 행과 N개의 아이템(Item) 열을 갖는 평점 행렬 R은 M X N 차원으로 구성되며, 행렬 분해를 통해 사용자-잠재 요인 행렬 P(M X K 차원), 잠재 요인-아이템 행렬 Q.T(K X N 차원)으로 분해될 수 있습니다.

<img src="https://user-images.githubusercontent.com/76269316/131965286-1c26a006-90b4-4d86-b4c0-542d067cddea.png" alt="image" style="zoom:67%;" />

<br>

R 행렬의 u행 사용자와 i 열 아이템 위치에 있는 평점 데이터를 r(u, i)라고 하면 다음과 같이 계산할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/131965756-1fe6856e-d17d-4d3b-95d2-e15b84ff1ae9.png)

아직 사용자가 평가하지 않은 아이템에 대한 평점도 잠재 요인으로 분해된 P 행렬과 Q 행렬을 이용해 예측할 수 있습니다.

따라서 사용자-아이템 평점 행렬의 미정 값을 포함한 모든 평점 값은 행렬 분해를 통해 얻어진 P 행렬과 Q.T 행렬의 내적을 통해 예측 평점으로 다시 계산할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/131975692-c148066c-97b7-4a2b-9708-223d44a34b8e.png)

<br>

이렇게 행렬 분해는 다차원의 매트릭스를 저차원 매트릭스로 분해하는 기법으로서 대표적으로 SVD(Singular Vector Decomposition), NMF(Non-Negative Matrix Factorization) 등이 있는데,

SVD의 경우 널(NaN) 값이 없는 행렬에만 적용할 수 있기 때문에 평점이 없는 R 행렬에는 SVD 방식으로 분해할 수 없습니다.

이러한 경우에는 확률적 경사 하강법(SGD, Stochastic Gradient Descent)이나 ALS(Alternating Least Squares) 방식을 사용합니다.

<br><br>

##### 확률적 경사 하강법을 이용한 행렬 분해

확률적 경사 하강법을 이용한 행렬 분해는 P와 Q 행렬로 계산된 **예측 R 행렬 값**이 **실제 R 행렬 값**과 가장 최소의 오류를 가질 수 있도록 반복적인 비용 함수 최적화를 통해 P와 Q를 유추해냅니다.

<br>

**확률적 경사 하강법을 이용한 행렬 분해 절차**

1. P와 Q를 임의의 값을 가진 행렬로 설정합니다.
2. P와 Q.T 값을 곱해 예측 R 행렬을 계산하고 오류 값을 계산합니다.
3. 이 오류 값을 최소화할 수 있도록 P와 Q 행렬을 각각 업데이트 합니다.
4. 만족할만한 오류 값을 가질 때까지 2, 3번 작업을 반복하면서 P와 Q 값을 업데이트합니다.

<br>

실제 값과 예측값의 오류 최소화와 L2 규제를 고려한 비용 함수식은 다음과 같습니다.

![image](https://user-images.githubusercontent.com/76269316/131980741-ae6a8807-d5b9-4dd1-8129-f453e4a38dd8.png)

위 비용 함수를 최소화하기 위해 업데이트 되는<img src="https://user-images.githubusercontent.com/76269316/131980850-27a90463-6bbe-4891-80ab-75810260e9ca.png" alt="image" style="zoom:80%;" />와 <img src="https://user-images.githubusercontent.com/76269316/131980894-609324f6-0aef-4052-9274-9b2d0c97b94d.png" alt="image" style="zoom:80%;" />는 다음과 같이 계산할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/131980961-3eb8e4de-1d7f-42b9-9fdf-14386ddde329.png)

각각의 기호가 의미하는 바는 다음과 같습니다.

- <img src="https://user-images.githubusercontent.com/76269316/131981022-ee6606c9-7f9d-49a6-938c-e159e7561354.png" alt="image" style="zoom:80%;" />: P 행렬의 사용자 U행 벡터
- <img src="https://user-images.githubusercontent.com/76269316/131981160-e61167c0-f489-4e45-8f6b-c64fa85655c9.png" alt="image" style="zoom:80%;" />: Q 행렬의 아이템 i행의 전치 벡터
- <img src="https://user-images.githubusercontent.com/76269316/131981353-ee03aae0-b955-41b7-a376-d811e71186ad.png" alt="image" style="zoom:80%;" />: 실제 R 행렬의 u행, i열에 위치한 값
- <img src="https://user-images.githubusercontent.com/76269316/131981550-ff792752-8d05-4e51-8222-903a1ce966d6.png" alt="image" style="zoom:80%;" />: 예측 행렬 <img src="https://user-images.githubusercontent.com/76269316/131981655-262f6658-a664-4422-9dc9-e7bda7090f07.png" alt="image" style="zoom:80%;" />의 u행, i열에 위치한 값
- <img src="https://user-images.githubusercontent.com/76269316/131981757-5a89c579-c5e6-444a-9067-b06ac534813f.png" alt="image" style="zoom:80%;" />: u행, i열에 위치한 실제 행렬 값과 예측 행렬 값의 차이 오류 (<img src="https://user-images.githubusercontent.com/76269316/131982186-b68f074f-b559-4952-a2ee-a29c86c6d7e8.png" alt="image" style="zoom:80%;" />)
- <img src="https://user-images.githubusercontent.com/76269316/131982234-55197372-1c78-4d7f-9a95-793ef726ed95.png" alt="image" style="zoom:80%;" />: SGD 학습률
- <img src="https://user-images.githubusercontent.com/76269316/131982318-7ef01e0a-64f3-4507-a903-9be2c0037400.png" alt="image" style="zoom:80%;" />: L2 규제 계수

평점행렬을 경사 하강법을 이용해 행렬 분해하는 것도 [회귀 - 경사 하강법](https://seominseok4834.github.io/machine%20learning/5.regression/#%EB%B9%84%EC%9A%A9-%EC%B5%9C%EC%86%8C%ED%99%94%ED%95%98%EA%B8%B0---%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95gradient-descent-%EC%86%8C%EA%B0%9C)에서 회귀 계수의 업데이트 값(w1_update, w0_update)을 구해 이 업데이트 값을 반복적으로 적용하는 것과 비슷합니다.

SGD를 이용해 분해하려는 원본 행렬 R을 P와 Q로 분해한 뒤, 다시 P와 Q.T의 내적으로 예측 행렬을 만들어 보겠습니다.

원본 행렬 R을 미정인 널 값을 포함해 생성하고 분해 행렬 P와 Q는 정규 분포를 가진 랜덤 값으로 초기화했고, 잠재 요인 차원은 3차원으로 설정했습니다.

<br>

```python
import numpy as np

#원본 행렬 R 생성, 분해 행렬 P와 Q 초기화, 잠재요인 차원 K는 3 설정
R = np.array([[4, np.NaN, np.NaN, 2, np.NaN ],
              [np.NaN, 5, np.NaN, 3, 1 ],
              [np.NaN, np.NaN, 3, 4, 4 ],
              [5, 2, 1, 2, np.NaN ]])
num_users, num_items = R.shape
K=3  #잠재 요인 차원

#P와 Q 매트릭스의 크기를 지정하고 정규분포를 가진 random한 값으로 입력
np.random.seed(1)
P = np.random.normal(scale=1./K, size=(num_users, K))  #정규분포의 표준편차가 작지 않으면 무한대로 수렴하는 경우가 발생해서 1/k 적용
Q = np.random.normal(scale=1./K, size=(num_items, K))
```

<br>

**get_rmse()**

실제 행렬과 예측 행렬의 오차를 구하는 함수

```python
from sklearn.metrics import mean_squared_error

def get_rmse(R, P, Q, non_zeros):
    error = 0
    #두개의 분해된 행렬 P와 Q.T의 내적으로 예측 R 행렬 생성
    full_pred_matrix = np.dot(P, Q.T)
    
    #실제 R 행렬에서 널이 아닌 값의 위치 인덱스 추출하여 실제 R 행렬과 예측 행렬의 RMSE 추출
    x_non_zero_ind = [non_zero[0] for non_zero in non_zeros]
    y_non_zero_ind = [non_zero[1] for non_zero in non_zeros]
    R_non_zeros = R[x_non_zero_ind, y_non_zero_ind]
    full_pred_matrix_non_zeros = full_pred_matrix[x_non_zero_ind, y_non_zero_ind]
      
    mse = mean_squared_error(R_non_zeros, full_pred_matrix_non_zeros)
    rmse = np.sqrt(mse)
    
    return rmse
```

<br>

이제 SGD 기반으로 행렬 분해를 수행하겠습니다.

먼저 R에서 null 값을 제외한 데이터의 행렬 인덱스를 추출한 다음 non_zeros에 리스트로 저장합니다.

![image](https://user-images.githubusercontent.com/76269316/131980961-3eb8e4de-1d7f-42b9-9fdf-14386ddde329.png)

이후 위 식을 통해 1000번 동안 반복하면서 행렬 P와 Q를 업데이트하고, 50회 반복할 때마다 get_rmse() 함수를 통해 오류 값을 출력합니다.

```python
#R > 0 인 행 위치, 열 위치, 값을 non_zeros 리스트에 저장
non_zeros = [ (i, j, R[i,j]) for i in range(num_users) for j in range(num_items) if R[i,j] > 0 ]

steps=1000
learning_rate=0.01
r_lambda=0.01

#SGD 기법으로 P와 Q 매트릭스를 계속 업데이트
for step in range(steps):
    for i, j, r in non_zeros:
        # 실제 값과 예측 값의 차이인 오류 값 구함
        eij = r - np.dot(P[i, :], Q[j, :].T)
        # Regularization을 반영한 SGD 업데이트 공식 적용
        P[i,:] = P[i,:] + learning_rate*(eij * Q[j, :] - r_lambda*P[i,:])
        Q[j,:] = Q[j,:] + learning_rate*(eij * P[i, :] - r_lambda*Q[j,:])

    rmse = get_rmse(R, P, Q, non_zeros)
    if (step % 50) == 0 :
        print("### iteration step: ", step," rmse: ", rmse)
```

![image](https://user-images.githubusercontent.com/76269316/131994506-7def7bca-3e0d-4a3d-9fef-0c61c94fb86d.png)

<br>

분해된 P와 Q를 내적해 예측 행렬을 만들어 출력했습니다.

```python
pred_matrix = np.dot(P, Q.T)
print('예측 행렬\n', np.round(pred_matrix, 3))
```

|                          예측 행렬                           |                          원본 행렬                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/131994897-bb6c5cdb-59eb-4f38-8b30-bc66741a21cd.png) | ![image](https://user-images.githubusercontent.com/76269316/131995123-a5275898-4979-4c84-a283-ba4c664fcba0.png) |

원본 행렬과 비교해 null이 아닌 값은 큰 오차가 나지 않고, null이였던 값은 새로운 예측값으로 채워졌습니다.

<br><br><br>

### 콘텐츠 기반 필터링 실습 - TMDB 5000 영화 데이터 세트

TMDB 5000 영화 데이터 세트는 유명한 영화 데이터 정보 사이트인 IMDB에서 5000개 영화에 대한 메타 정보를 가공한 데이터 세트입니다.

이를 이용해 콘텐츠 기반 필터링을 수행해 보겠습니다.

[TMDB 5000 Movie DataSet](https://www.kaggle.com/tmdb/tmdb-movie-metadata)에서 tmdb_5000_credits.csv와 tmdb_5000_movies.csv 파일을 다운받으면 됩니다.

<br><br>

##### 데이터 로딩 및 가공

```python
import pandas as pd
import numpy as np
import warnings; warnings.filterwarnings('ignore')

movies =pd.read_csv('tmdb_5000_movies.csv')
print(movies.shape)
movies.head(1)
```

![image](https://user-images.githubusercontent.com/76269316/131996428-3b09ec33-9173-41bc-9509-6d4edab09be0.png)

4803개의 레코드와 20개의 피처로 구성돼 있습니다.

영화 제목, 개요, 인기도, 평점, 투표 수, 예산, 키워드 등 영화에 대한 다양한 메타 정보를 갖고 있는데, 콘텐츠 기반 필터링은 사용자가 좋아하는 영화와 비슷한 특성/속성, 구성 요소 등을 가진 다른 영화를 추천해주는 방식입니다.

따라서 id, title, genres, vote_average, vote_count, popularity, keywords, overview 컬럼만 사용하도록 하겠습니다.

```pythjon
movies_df = movies[['id','title', 'genres', 'vote_average', 'vote_count', 'popularity', 'keywords', 'overview']]
```

<br>

이 때 주의해야 할 점이 'genres', 'keywords' 등과 같은 컬럼을 보면 [{"id": 28, "name": "Action"}, {"id": 12, "name": "Adventure"}]와 같이 파이썬 리스트 내부에 딕셔너리가 있는 형태의 **문자열**로 저장돼 있습니다.

```python
pd.set_option('max_colwidth', 100)
movies_df[['genres','keywords']][:1]
```

![image](https://user-images.githubusercontent.com/76269316/131998094-4f2686e0-6127-42c5-9abb-020070b49a08.png)

이 개별 장르의 명칭은 딕셔너리의 키(key)인 'name'으로 추출할 수 있습니다.

genres 컬럼의 문자열을 분해해서 개별 장르를 파이썬 리스트 객체로 추출하겠습니다.

파이썬 ast 모듈 literal_eval() 함수를 사용하면 이 문자열을 문자열이 의미하는 list[dict1, dict2] 객체로 만들 수 있습니다.

```python
from ast import literal_eval

movies_df['genres'] = movies_df['genres'].apply(literal_eval)
movies_df['keywords'] = movies_df['keywords'].apply(literal_eval)
```

이제 genres 컬럼은 문자열이 아닌 리스트 내부에 여러 장르 딕셔너리로 구성된 객체입니다.

genres 컬럼에서 ['Action', 'Adventure'] 같은 장르명만 리스트 객체로 추출하겠습니다.

for loop로 리스트 내 여러 개의 딕셔너리 'name' 키에 해당하는 값을 찾아 이를 리스트 객체로 변환합니다.

```python
movies_df['genres'] = movies_df['genres'].apply(lambda x : [ y['name'] for y in x])
movies_df['keywords'] = movies_df['keywords'].apply(lambda x : [ y['name'] for y in x])
movies_df[['genres', 'keywords']][:1]
```

![image](https://user-images.githubusercontent.com/76269316/131999964-378f794c-a5b5-498b-a17d-8288de2187fa.png)

<br><br>

##### 장르 콘텐츠 유사도 측정

앞서 말했지만, 콘텐츠 기반 필터링 방식은 사용자가 특정한 영화를 선호하는 경우, 그 영화와 비슷한 콘텐츠를 가진 다른 아이템을 추천하는 방식입니다.

이렇게 영화 간의 유사성을 판단하는 기준은 장르, 감독, 배우, 평점, 키워드, 영화 설명 등등 매우 다양하지만 이 중 장르 값으로 유사도를 비교한 뒤 그 중 높은 평점을 갖는 영화를 추천하도록 하겠습니다.



현재 genres 컬럼에는 [Action, Adventure, Fantasy, Science Fiction]으로 돼 있는데, 이를 문자열로 변경한 뒤 [CountVectorizer](https://seominseok4834.github.io/machine%20learning/8.text-analytics/#%EC%82%AC%EC%9D%B4%ED%82%B7%EB%9F%B0%EC%9D%98-count-%EB%B0%8F-tf-idf-%EB%B2%A1%ED%84%B0%ED%99%94-%EA%B5%AC%ED%98%84-countvectorizer-tfidfvectorizer)로 피처 벡터화한 행렬 데이터 값에 [코사인 유사도](https://seominseok4834.github.io/machine%20learning/8.text-analytics/#%EB%AC%B8%EC%84%9C-%EC%9C%A0%EC%82%AC%EB%8F%84)를 적용해 유사성을 판단하겠습니다.

<br>

먼저 genres 컬럼을 문자열로 변환한 뒤 CountVectorizer를 이용해 피처 벡터 행렬로 변환하겠습니다.

```python
from sklearn.feature_extraction.text import CountVectorizer

#CountVectorizer를 적용하기 위해 공백문자로 word 단위가 구분되는 문자열로 변환
movies_df['genres_literal'] = movies_df['genres'].apply(lambda x : (' ').join(x))
count_vect = CountVectorizer(min_df=0, ngram_range=(1,2))
genre_mat = count_vect.fit_transform(movies_df['genres_literal'])
print(genre_mat.shape)
```

![image](https://user-images.githubusercontent.com/76269316/132001058-00838c22-fbf3-47c0-bb25-f2247a8a77c0.png)

<br>

카운트 기반 벡터화로 4803개 레코드, 276개의 개별 단어 피처로 구성된 피처 벡터 행렬이 생성됐습니다.

피처 벡터화된 행렬에 cosine_similarities()를 적용해 코사인 유사도를 구하고, 이 중 2개만 출력해 보겠습니다.

```python
from sklearn.metrics.pairwise import cosine_similarity

genre_sim = cosine_similarity(genre_mat, genre_mat)
print(genre_sim.shape)
print(genre_sim[:2])
```

![image](https://user-images.githubusercontent.com/76269316/132001538-2e6d34d7-fbad-4a26-be48-cfb0f60292b8.png)

genre_sim 객체는 movies_df의 행별 장르 유사도 값을 갖고 있습니다.

장르 값으로 유사도를 비교하기 위해 비교 대상이 되는 행의 유사도 값이 높은 순으로 정렬된 행렬의 위치 인덱스 값 넘파이 argsort() 함수를 사용해 추출하겠습니다.

```python
genre_sim_sorted_ind = genre_sim.argsort()[:, ::-1]
print(genre_sim_sorted_ind[:1])
```

![image](https://user-images.githubusercontent.com/76269316/132001891-bb450a01-7982-4ab4-b43d-62b5b29340f1.png)

자기 자신인 0번 레코드(자기 자신의 유사도는 1)를 제외하면 3494번 레코드가 가장 유사도가 높고, 그 다음이 813이며 가장 유사도가 낮은 레코드는 2041번 레코드라는 의미입니다.

<br><br>

##### 장르 콘텐츠 필터링을 이용한 영화 추천

장르 유사도에 따라 영화를 추천하는 함수 **find_sim_movie()**를 생성하겠습니다.

인자로 데이터 세트인 movies_df, 레코드별 장르 코사인 유사도를 갖고 있는 genre_sim_sorted_ind, 고객이 선정한 추천 기준이 되는 영화 제목, 추천할 영화 건수를 받겠습니다.

**finds_sim_movie()**

장르 유사도에 따라 영화를 추천하는 함수

```python
def find_sim_movie(df, sorted_ind, title_name, top_n=10):
    
    #인자로 입력된 movies_df DataFrame에서 'title' 컬럼이 입력된 title_name 값인 DataFrame추출
    title_movie = df[df['title'] == title_name]
    
    #title_named을 가진 DataFrame의 index 객체를 ndarray로 반환
    #sorted_ind 인자로 입력된 genre_sim_sorted_ind 객체에서 유사도 순으로 top_n 개의 index 추출
    title_index = title_movie.index.values
    similar_indexes = sorted_ind[title_index, :(top_n)]
    
    #추출된 top_n index들 출력. top_n index는 2차원 데이터
    #dataframe에서 index로 사용하기 위해서 1차원 array로 변경
    print(similar_indexes)
    similar_indexes = similar_indexes.reshape(-1)
    
    return df.iloc[similar_indexes]
```

<br>

finds_sim_movie() 함수를 사용해 영화 '대부'와 장르별로 유사한 영화 10개를 추천해 보겠습니다.

```python
similar_movies = find_sim_movie(movies_df, genre_sim_sorted_ind, 'The Godfather',10)
similar_movies[['title', 'vote_average']]
```

![image](https://user-images.githubusercontent.com/76269316/132002303-3059669b-690f-488c-987f-2701a0fd60a5.png)

<br><br>

'대부 2편'이 가장 먼저 추천됐습니다.

하지만 'Light Sleeper', 'Mi America', 'Kids' 등 대부를 좋아하는 고객에게 섣불리 추천하기 어려운 영화도 있습니다.

'Light Sleeper'의 경우 평점이 낮은 편이고, 'Mi America'의 경우 평점이 0입니다.

이를 개선하기 위해 일단 좀 더 많은 후보군을 선정한 뒤 평점에 따라 필터링해서 최종 추천하는 방식으로 변경해 보겠습니다.

이를 위해 평점 정보인 vote_average를 사용하겠습니다.

한 가지 주의해야 할 점이 vote_average는 여러 관객이 평가한 평점을 평균한 것으로 0부터 10점까지의 점수로 돼 있는데, 1명 혹은 2명의 소수 관객이 특정 영화에 만점이나 높은 평점을 부여해 왜곡된 데이터를 갖고 있습니다.

확인하기 위해 먼저 sort_values()를 이용해 vote_average를 오름차순으로 정렬해 10개만 출력해 보겠습니다.

```python
movies_df[['title','vote_average','vote_count']].sort_values('vote_average', ascending=False)[:10]
```

![image-20210903210731120](C:\Users\seominseok\AppData\Roaming\Typora\typora-user-images\image-20210903210731120.png)

평가 횟수가 매우 적은 영화들이 상위권에 있습니다.

이와 같이 왜곡된 평점 데이터를 회피하기 위해 IMDB에서 사용하는 가중치가 부여된 평점(Weighted Rating)을 사용하겠습니다.

<img src="https://user-images.githubusercontent.com/76269316/132003072-b1841dac-bec0-4a4f-acec-3c7949576a45.png" alt="image" style="zoom:67%;" />

각 변수의 의미는 다음과 같습니다.

- v: 개별 영화에 평점을 투표한 횟수 = movies_df['vote_count']
- m: 평점을 부여하기 위한 최소 투표 횟수
- R: 개별 영화에 대한 평균 평점 = movies+df['vote_average']
- C: 전체 영화에 대한 평균 평점 = movies_df['vote_average'].mean()

m 값은 투표 횟수에 따른 가중치를 조절하는 역할을 하는데, m 값을 높이면 평점 투표 횟수가 많은 영화에 더 많은 가중 평점을 부여합니다.

m 값은 상위 60%에 해당하는 횟수를 기준으로 하겠습니다.

```python
C = movies_df['vote_average'].mean()
m = movies_df['vote_count'].quantile(0.6)  #Series 객체 quantile()을 이용해 상위 60% 값 추출
print('C: ',round(C,3), 'm: ',round(m,3))
```

![image](https://user-images.githubusercontent.com/76269316/132003515-afaa0967-4951-4e07-9609-50a99d9fbd96.png)

<br>

기존 평점을 새로운 가중 평점으로 변경하는 함수 **weighted_vote_average()**를 생성하고, 이를 이용해 'vote_weighted' 컬럼을 생성하겠습니다.

**weighted_vote_average()**

기존 평점을 가중 평점으로 변경하는 함수

```python
def weighted_vote_average(record):
    v = record['vote_count']
    R = record['vote_average']
    
    return ( (v/(v+m)) * R ) + ( (m/(m+v)) * C )   
```

<br>

```python
movies_df['weighted_vote'] = movies_df.apply(weighted_vote_average, axis=1)
```

다시 새롭게 부여된 weighted_vote 평점이 높은 순으로 상위 10개의 영화를 출력해 보겠습니다.

```python
movies_df[['title','vote_average','weighted_vote','vote_count']].sort_values('weighted_vote', ascending=False)[:10]
```

![image](https://user-images.githubusercontent.com/76269316/132003925-2b1eb99f-9063-4ae6-a16b-0419c79ea489.png)

<br>

새롭게 정의된 평점 기준에 따라 장르 유사성이 높은 영화를 top_n의 2배수만큼 후보군으로 선정한 뒤 weighted_vote 값이 높은 순으로 top_n만큼 추출하겠습니다.

이를 위해 find_sim_movie() 함수를 변경했습니다.

**finds_sim_movie()**

장르 유사도에 따라 영화를 추천하는 함수

```python
def find_sim_movie(df, sorted_ind, title_name, top_n=10):
    title_movie = df[df['title'] == title_name]
    title_index = title_movie.index.values
    
    # top_n의 2배에 해당하는 쟝르 유사성이 높은 index 추출
    similar_indexes = sorted_ind[title_index, :(top_n*2)]
    similar_indexes = similar_indexes.reshape(-1)
    #기준 영화 index는 제외
    similar_indexes = similar_indexes[similar_indexes != title_index]
    
    # top_n의 2배에 해당하는 후보군에서 weighted_vote 높은 순으로 top_n만큼 추출 
    return df.iloc[similar_indexes].sort_values('weighted_vote', ascending=False)[:top_n]

similar_movies = find_sim_movie(movies_df, genre_sim_sorted_ind, 'The Godfather',10)
similar_movies[['title', 'vote_average', 'weighted_vote']]
```

![image](https://user-images.githubusercontent.com/76269316/132004220-ce3ffb80-0eeb-4185-b5ca-74b1c9eabe20.png)

이전 추천 영화보다 훨씬 나은 영화가 추천됐습니다.

<br><br><br>

### 아이템 기반 최근접 이웃 협업 필터링 실습

이번에는 아이템 기반 최근접 이웃 협업 필터링을 구현해 보겠습니다.

협업 필터링 기반 영화 추천을 위해서는 사용자가 영화의 평점을 매긴 사용자-영화 평점 행렬 데이터 세트가 필요합니다.

Grouplens 사이트에서 만든 [MovieLens Latest Datasets](https://grouplens.org/datasets/movielens/latest/)를 사용하도록 하겠습니다. (ml-latest-small.zip을 저장하면 됩니다.)

<br><br>

##### 데이터 가공 및 변환

내려받은 데이터 세트를 DataFrame으로 로딩해 보겠습니다.

```python
import pandas as pd
import numpy as np

movies = pd.read_csv('movies.csv')
ratings = pd.read_csv('ratings.csv')
print(movies.shape)
print(ratings.shape)
```

![image](https://user-images.githubusercontent.com/76269316/132005306-39cd8600-a651-4d2a-a312-b0965a22e16e.png)

<br>

movies.csv 파일은 영화에 대한 메타 정보인 title, genres를 갖고 있는 데이터 세트입니다.

```python
movies.head()
```

![image](https://user-images.githubusercontent.com/76269316/132005596-ee813476-c34c-402c-bad4-f0f1e9ca3d0d.png)

<br>

```python
ratings.head()
```

ratings.csv 파일은 사용자별로 영화에 대한 평점을 매긴 데이터 세트입니다.

사용자 아이디를 의미하는 userId, 영화 아이디를 의미하는 movieId, 평점인 rating 컬럼으로 구성됩니다. (timestamp는 현재로서 별 의미가 없는 컬럼입니다)

![image](https://user-images.githubusercontent.com/76269316/132005640-f0b741cc-f030-4222-8a40-e9a4a5ee0783.png)



협업 필터링에서는 row 레벨 형태의 원본 데이터 세트를 오른쪽과 같은 형태인 사용자-아이템 평점 행렬로 변경해야 하므로 DataFrame의 pivot_table() 함수를 사용했습니다.

<img src="https://user-images.githubusercontent.com/76269316/131949621-e2eccbd7-dd81-49c7-ab0b-ac7ef00e78a5.png" alt="image" style="zoom: 67%;" />

ratings.pivot_table('rating', index='userId', columns='movieId')을 호출하면 row는 userId, column은 모두  movieId 컬럼에 있는 이름으로 변경되고, 데이터는 rating 컬럼에 있는 값으로 할당됩니다.

```python
ratings = ratings[['userId', 'movieId', 'rating']]
ratings_matrix = ratings.pivot_table('rating', index='userId', columns='movieId')
ratings_matrix.head(3)
```

![image](https://user-images.githubusercontent.com/76269316/132006969-db999862-56bd-48a3-9091-608b65512a9e.png)

사용자가 평점을 매기지 않은 영화가 많아 희소 행렬이 생성됐습니다.

또한 컬럼명이 movieId의 숫자 값이라 어떤 영화인지 알기 어렵습니다. 가독성을 높이기 위해 영화명(title)으로 변경하겠습니다.

영화명은 movies 데이터 세트에 존재하기 때문에 ratings와 movies를 [join](https://seominseok4834.github.io/database/4.relational-algebra/#natural-joins-)해 title 컬럼을 가져온 뒤 pivot_table() 인자 columns에 'title'을 입력하겠습니다.

이후 null 값은 0으로 변환했습니다.

```python
#title 컬럼을 얻기 위해 movies와 join 수행
rating_movies = pd.merge(ratings, movies, on='movieId')

#columns='title'로 title 컬럼으로 pivot 수행
ratings_matrix = rating_movies.pivot_table('rating', index='userId', columns='title')

#NaN 값을 모두 0으로 변환
ratings_matrix = ratings_matrix.fillna(0)
ratings_matrix.head(3)
```

![image](https://user-images.githubusercontent.com/76269316/132007433-b78ffb91-2e6a-459b-a04b-86cc43a2ffb3.png)

<br><br>

##### 영화 간 유사도 추출

변환된 사용자-영화 평점 행렬 데이터 세트를 이용해 영화 간 유사도를 측정하겠습니다.

영화 간 유사도는 코사인 유사도를 기반으로 하고 사이킷런 cosine_similarity()를 이용해 측정하겠습니다.

![image](https://user-images.githubusercontent.com/76269316/132007979-1d9e4905-d3a2-4f74-9f78-bde33d9923a6.png)

cosine_similarity() 함수는 행을 기준으로 서로 다른 행을 비교해 유사도를 산출하기 때문에 rating_matrix에 적용시 영화 간의 유사도가 아닌, 사용자 간의 유사도가 생성되게 됩니다.

따라서 행과 열의 위치를 바꾼 뒤, 코사인 유사도를 구해야 합니다.

<br>

먼저 DataFrame transpose()를 적용해 행과 열을 바꾼 전치 행렬을 생성한 뒤,

```python
ratings_matrix_T = ratings_matrix.transpose()
ratings_matrix_T.head(3)
```

![image](https://user-images.githubusercontent.com/76269316/132008301-612ba229-d442-42d1-bc07-f83e86352fd5.png)

<br>

cosine_similarity()를 적용해 코사인 유사도를 구했습니다.

```python
from sklearn.metrics.pairwise import cosine_similarity

item_sim = cosine_similarity(ratings_matrix_T, ratings_matrix_T)

#cosine_similarity()로 반환된 넘파이 행렬을 영화명을 매핑하여 DataFrame으로 변환
item_sim_df = pd.DataFrame(data=item_sim, index=ratings_matrix.columns, columns=ratings_matrix.columns)
print(item_sim_df.shape)
item_sim_df.head(3)
```

![image](https://user-images.githubusercontent.com/76269316/132008526-195e3229-3d00-4203-a4ae-5e108416830c.png)

<br>

영화 '인셉션'과 유사도가 높은 상위 5개 영화('인셉션' 제외)를 추출해 보겠습니다.

```python
item_sim_df["Inception (2010)"].sort_values(ascending=False)[1:6]  #자신과의 유사도는 1이기 때문에 제외하고 출력
```

![image](https://user-images.githubusercontent.com/76269316/132008886-38361190-50c1-496e-81ea-9045d598d206.png)

'다크나이트'와 같은 스릴러와 액션이 가미된 영화가 높은 유사도를 나타내고 있습니다.

사용자의 평점 정보를 취합해 영화별 유사도를 구했습니다.

이 아이템 기반 유사도 데이터를 사용해 개인에게 특화된 영화 추천 알고리즘을 만들어 보겠습니다.

<br><br>

##### 아이템 기반 최근접 이웃 협업 필터링으로 개인화된 영화 추천

위에서 아이템 기반 영화 유사도 데이터를 가지고 추천한 영화들은 개인적인 취향을 반영하지 않고 오직 영화 간의 유사도만을 갖고 추천한 것입니다.

이번에는 최근접 이웃 협업 필터링으로 개인에게 최적화된 영화 추천을 구현해 보겠습니다.

개인화된 영화 추천의 가장 큰 특징은 아직 관람하지 않은 영화를 추천한다는 것입니다.

아직 관람하지 않은 영화에 대해 아이템 유사도와 기존 관람한 영화의 평점 데이터를 기반으로 모든 영화의 예측 평점을 계산한 후 높은 예측 평점을 갖는 영화를 추천하는 방식입니다.

이러한 아이템 기반 협업 필터링에서의 개인화된 예측 평점은 다음 식으로 구할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/132012011-30775abb-942a-405e-8a01-885728bee9a1.png)

변수의 의미는 다음과 같습니다.

- <img src="https://user-images.githubusercontent.com/76269316/132010238-3a23a4e8-d51e-46b2-b8c3-373af3823b50.png" alt="image" style="zoom:80%;" /> : 사용자 u, 아이템 i의 개인화된 예측 평점 값
- <img src="https://user-images.githubusercontent.com/76269316/132010425-b0db78db-6295-4bbc-a5cb-edbf04024d11.png" alt="image" style="zoom:80%;" />: 아이템 i와 가장 유사도가 높은 Top-N개 아이템의 유사도 벡터
- <img src="https://user-images.githubusercontent.com/76269316/132010523-7ee32644-aca0-485b-a1f1-2ebd97b75ef9.png" alt="image" style="zoom:80%;" />: 사용자 u의 아이템 i와 가장 유사도가 높은 Top-N개 아이템에 대한 실제 평점 벡터



<img src="https://user-images.githubusercontent.com/76269316/132010425-b0db78db-6295-4bbc-a5cb-edbf04024d11.png" alt="image" style="zoom:80%;" />과 <img src="https://user-images.githubusercontent.com/76269316/132010523-7ee32644-aca0-485b-a1f1-2ebd97b75ef9.png" alt="image" style="zoom:80%;" />에 나오는 N 값은 아이템의 최근접 이웃 범위 계수를 의미합니다. 이는 특정 아이템과 유사도가 가장 높은 Top-N개의 다른 아이템을 추출하는데 사용됩니다.

먼저 N의 범위에 제약을 두지 않고 모든 아이템으로 가정하고 예측 평점을 구해보겠습니다.

영화 간 유사도를 갖는 item_sim_df와 사용자-영화 평점 ratings_matrix DataFrame을 활용해 사용자별로 최적화된 평점 스코어를 예측하는 함수를 만들겠습니다.

<br>

**predict_rating()**

<img src="https://user-images.githubusercontent.com/76269316/132012011-30775abb-942a-405e-8a01-885728bee9a1.png" alt="image" style="zoom: 50%;" /> 식으로 개인화된 예측 평점을 계산하는 함수

```python
def predict_rating(ratings_arr, item_sim_arr ):
    ratings_pred = ratings_arr.dot(item_sim_arr)/ np.array([np.abs(item_sim_arr).sum(axis=1)])
    return ratings_pred
```

<br>

predict_rating() 함수를 사용해 개인화된 예측 평점을 구해보겠습니다.

```python
ratings_pred = predict_rating(ratings_matrix.values , item_sim_df.values)
ratings_pred_matrix = pd.DataFrame(data=ratings_pred, index= ratings_matrix.index, columns = ratings_matrix.columns)
ratings_pred_matrix.head(3)
```

![image](https://user-images.githubusercontent.com/76269316/132011517-5ff2ac90-63bb-4731-98c1-98e145281981.png)

예측 평점이 실제 평점에 비해 작을 수 있는데 내적 결과를 정규화하기 위해 코사인 유사도 벡터 합(<img src="https://user-images.githubusercontent.com/76269316/132012101-f55a0881-e7a0-4aa4-b90e-cd91ac2e95fe.png" alt="image" style="zoom: 50%;" />)으로 나눴기 때문입니다.

예측 결과가 실제 평점과 얼마나 차이가 있는지 확인해 보겠습니다.

기존에 평점이 부여된 데이터에 대해서만 오차 정도를 측정했습니다. (평가 지표는 MSE 사용)

**get_mse()**

평점을 부여한 영화에 대해서만 예측 성능을 구하는 함수

```python
from sklearn.metrics import mean_squared_error

#사용자가 평점을 부여한 영화에 대해서만 예측 성능 평가 MSE를 구함
def get_mse(pred, actual):
    # Ignore nonzero terms.
    pred = pred[actual.nonzero()].flatten()
    actual = actual[actual.nonzero()].flatten()
    return mean_squared_error(pred, actual)

print('아이템 기반 모든 인접 이웃 MSE: ', get_mse(ratings_pred, ratings_matrix.values ))
```

![image](https://user-images.githubusercontent.com/76269316/132012689-c3e337be-f68d-4a33-970d-8012a0a5fc0a.png)

MSE가 약 9.89가 나왔습니다.

predict_rating() 함수는 사용자별 영화의 예측 평점을 계산하기 위해 해당 영화와 다른 모든 영화 간의 유사도 벡터를 적용했기 때문에 이렇게 평점 예측이 많이 떨어졌습니다.

특정 영화와 가장 비슷한 유사도를 갖는 영화 유사도 벡터만 예측값을 계산하는데 적용해 보겠습니다.

이중 for loop를 사용해 행, 열 별로 TOP-N의 유사도 벡터를 계산해야 하기 때문에 수행시간이 걸립니다. (데이터 세트 크기가 작아서 금방되지만 크기가 커지면 매우 오래 걸리는 로직입니다)

**predict_rating_topsim(ratings_arr, item_sim_arr, n)**

TOP-N 유사도를 갖는 영화 유사도 벡터만을 사용해 예측 평점을 계산하는 함수

```python
def predict_rating_topsim(ratings_arr, item_sim_arr, n=20):
    #사용자-아이템 평점 행렬 크기만큼 0으로 채운 예측 행렬 초기화
    pred = np.zeros(ratings_arr.shape)

    #사용자-아이템 평점 행렬의 열 크기만큼 for loop 수행
    for col in range(ratings_arr.shape[1]):
        #유사도 행렬에서 유사도가 큰 순으로 n개 데이터 행렬의 index 반환
        top_n_items = [np.argsort(item_sim_arr[:, col])[:-n-1:-1]]
        #개인화된 예측 평점을 계산
        for row in range(ratings_arr.shape[0]):
            pred[row, col] = item_sim_arr[col, :][top_n_items].dot(ratings_arr[row, :][top_n_items].T) 
            pred[row, col] /= np.sum(np.abs(item_sim_arr[col, :][top_n_items]))        
    return pred
```

<br>

predict_rating_topsim() 함수를 사용해 예측 평점을 계산하고, 실제 평점과의 MSE를 다시 구해보겠습니다.

```python
ratings_pred = predict_rating_topsim(ratings_matrix.values , item_sim_df.values, n=20)
print('아이템 기반 인접 TOP-20 이웃 MSE: ', get_mse(ratings_pred, ratings_matrix.values))


# 계산된 예측 평점 데이터는 DataFrame으로 재생성
ratings_pred_matrix = pd.DataFrame(data=ratings_pred, index= ratings_matrix.index, columns = ratings_matrix.columns)
```

![image](https://user-images.githubusercontent.com/76269316/132014271-f1cf73e1-2e35-4585-8336-da6e8f3c138c.png)

MSE가 많이 향상됐습니다.

<br>

이제 userId = 9인 사용자에 대해 영화를 추천해 보겠습니다.

먼저 9번 사용자가 어떤 영화를 좋아하는지 확인해 보겠습니다. 사용자가 평점을 준 영화를 내림차순으로 나열해 출력했습니다.

```python
user_rating_id = ratings_matrix.loc[9, :]
user_rating_id[ user_rating_id > 0].sort_values(ascending=False)[:10]
```

![image](https://user-images.githubusercontent.com/76269316/132014746-226e82d6-ecc3-44b8-8c9b-703a988be912.png)

'오스틴 파워', '반지의 제왕' 등 대작 영화, 어드벤처 영화, 코미디 영화 등 전반적으로 흥행성이 좋은 영화에 높은 평점을 줬습니다.

개인에게 최적화된 영화 추천을 위해 get_unseen_movies(), recomm_movie_by_userid()를 생성했습니다.

<br>

**get_unseen_movies()**

평점을 주지 않은 영화를 리스트 객체로 반환하는 함수

```python
def get_unseen_movies(ratings_matrix, userId):
    #userId로 입력받은 사용자의 모든 영화정보 추출하여 Series로 반환
    #반환된 user_rating 은 영화명(title)을 index로 가지는 Series 객체
    user_rating = ratings_matrix.loc[userId,:]
    
    #user_rating이 0보다 크면 기존에 관람한 영화임. 대상 index를 추출하여 list 객체로 만듦
    already_seen = user_rating[ user_rating > 0].index.tolist()
    
    #모든 영화명을 list 객체로 만듦
    movies_list = ratings_matrix.columns.tolist()
    
    #list comprehension으로 already_seen에 해당하는 movie는 movies_list에서 제외
    unseen_list = [ movie for movie in movies_list if movie not in already_seen]
    
    return unseen_list
```

<br>

**recomm_movie_by_userid()**

사용자가 영화 평점을 주지 않은 추천 대상 영화 정보와 predict_rating_topsim()에서 추출한 사용자별 아이템 유사도에 기반한 예측 평점 데이터 세트를 사용해 최종적으로 사용자에게 영화를 추천하는 함수

```python
def recomm_movie_by_userid(pred_df, userId, unseen_list, top_n=10):
    #예측 평점 DataFrame에서 사용자id index와 unseen_list로 들어온 영화명 컬럼을 추출
    #가장 예측 평점이 높은 순으로 정렬
    recomm_movies = pred_df.loc[userId, unseen_list].sort_values(ascending=False)[:top_n]
    return recomm_movies
```

<br>

위 두 함수를 사용해 userId = 9인 사용자에 대해 영화를 추천해 보겠습니다.

```python
#사용자가 관람하지 않는 영화명 추출   
unseen_list = get_unseen_movies(ratings_matrix, 9)

#아이템 기반의 인접 이웃 협업 필터링으로 영화 추천 
recomm_movies = recomm_movie_by_userid(ratings_pred_matrix, 9, unseen_list, top_n=10)

#평점 데이터를 DataFrame으로 생성
recomm_movies = pd.DataFrame(data=recomm_movies.values,index=recomm_movies.index,columns=['pred_score'])
recomm_movies
```

![image](https://user-images.githubusercontent.com/76269316/132015767-530fea7e-5be1-4d1e-9807-2692a8c61daa.png)

'슈렉', '스파이더 맨', '인디아나 존스-2편', '매트릭스' 등 높은 흥행성을 가진 작품이 추천됐습니다.

<br><br><br>

### 행렬 분해를 이용한 잠재 요인 협업 필터링 실습

이번에는 행렬 분해를 이용한 잠재 요인 협업 필터링을 구현해 보겠습니다.

사용자-아이템 평점 행렬에는 사용자가 평점을 매기지 않은 null 데이터가 많아 SVD를 적용할 수 없습니다.

따라서 SGD 기반으로 행렬 분해를 구현하고 이를 기반으로 영화를 추천해 보겠습니다.

<br>

**get_rmse()**

실제 행렬과 예측 행렬의 오차를 구하는 함수

```python
from sklearn.metrics import mean_squared_error

def get_rmse(R, P, Q, non_zeros):
    error = 0
    #두개의 분해된 행렬 P와 Q.T의 내적으로 예측 R 행렬 생성
    full_pred_matrix = np.dot(P, Q.T)
    
    #실제 R 행렬에서 널이 아닌 값의 위치 인덱스 추출하여 실제 R 행렬과 예측 행렬의 RMSE 추출
    x_non_zero_ind = [non_zero[0] for non_zero in non_zeros]
    y_non_zero_ind = [non_zero[1] for non_zero in non_zeros]
    R_non_zeros = R[x_non_zero_ind, y_non_zero_ind]
    full_pred_matrix_non_zeros = full_pred_matrix[x_non_zero_ind, y_non_zero_ind]
      
    mse = mean_squared_error(R_non_zeros, full_pred_matrix_non_zeros)
    rmse = np.sqrt(mse)
    
    return rmse
```

<br>

**matrix_factorization(R, K, steps=200, learning_rate, r_lambda)**

SGD 기반의 행렬 분해를 구현한 함수

- R: 원본 사용자-아이템 평점 행렬
- K: 잠재 요인 차원 수
- steps: SGD 반복 횟수
- learning_rate: 학습률
- r_lambda: L2 규제 계수

```python
def matrix_factorization(R, K, steps=200, learning_rate=0.01, r_lambda = 0.01):
    num_users, num_items = R.shape
    #P와 Q 매트릭스의 크기를 지정하고 정규분포를 가진 랜덤한 값으로 입력
    np.random.seed(1)
    P = np.random.normal(scale=1./K, size=(num_users, K))
    Q = np.random.normal(scale=1./K, size=(num_items, K))

    break_count = 0
       
    #R > 0 인 행 위치, 열 위치, 값을 non_zeros 리스트 객체에 저장
    non_zeros = [ (i, j, R[i,j]) for i in range(num_users) for j in range(num_items) if R[i,j] > 0 ]
   
    #SGD기법으로 P와 Q 매트릭스를 계속 업데이트. 
    for step in range(steps):
        for i, j, r in non_zeros:
            #실제 값과 예측 값의 차이인 오류 값을 구함
            eij = r - np.dot(P[i, :], Q[j, :].T)
            #Regularization을 반영한 SGD 업데이트 공식 적용
            P[i,:] = P[i,:] + learning_rate*(eij * Q[j, :] - r_lambda*P[i,:])
            Q[j,:] = Q[j,:] + learning_rate*(eij * P[i, :] - r_lambda*Q[j,:])
       
        rmse = get_rmse(R, P, Q, non_zeros)
        if (step % 10) == 0 :
            print("### iteration step : ", step," rmse : ", rmse)
            
    return P, Q
```

<br>

먼저 영화 평점 행렬 데이터를 다시 로딩한 뒤 사용자-아이템 평점 행렬로 변환한 뒤,

```python
import pandas as pd
import numpy as np

movies = pd.read_csv('movies.csv')
ratings = pd.read_csv('ratings.csv')
ratings = ratings[['userId', 'movieId', 'rating']]
ratings_matrix = ratings.pivot_table('rating', index='userId', columns='movieId')

#title 컬럼을 얻기 위해 movies 와 join 수행
rating_movies = pd.merge(ratings, movies, on='movieId')

#columns='title' 로 title 컬럼으로 pivot 수행 
ratings_matrix = rating_movies.pivot_table('rating', index='userId', columns='title')
```

사용자-아이템 평점 행렬을 matrix_factorization() 함수를 이용해 행렬 분해해 보겠습니다.

수행 시간이 오래 걸리므로 steps를 200회로 지정했습니다.

```python
P, Q = matrix_factorization(ratings_matrix.values, K=50, steps=200, learning_rate=0.01, r_lambda = 0.01)
pred_matrix = np.dot(P, Q.T)
```

![image](https://user-images.githubusercontent.com/76269316/132083786-7164efb2-3ac4-4657-b726-ad71c2e569af.png)

<br>

쉽게 이해하기 위해 반환된 예측 사용자-아이템 평점 행렬을 영화 타이틀을 컬럼명으로 갖는 DataFrame으로 변경했습니다.

```python
ratings_pred_matrix = pd.DataFrame(data=pred_matrix, index= ratings_matrix.index, columns = ratings_matrix.columns)
ratings_pred_matrix.head(3)
```

![image](https://user-images.githubusercontent.com/76269316/132083792-c2326cdd-3034-4888-8f11-981bd9ad954f.png)

<br>

이렇게 만들어진 예측 사용자-아이템 평점 행렬 정보를 사용해 개인화된 영화 추천을 해보겠습니다.

위에서 사용했던 get_unseen_movies() 함수와 recomm_movie_by_userid() 함수를 사용해 추천 영화를 출력해 보겠습니다.

**get_unseen_movies()**

평점을 주지 않은 영화를 리스트 객체로 반환하는 함수

```python
def get_unseen_movies(ratings_matrix, userId):
    #userId로 입력받은 사용자의 모든 영화정보 추출하여 Series로 반환
    #반환된 user_rating 은 영화명(title)을 index로 가지는 Series 객체
    user_rating = ratings_matrix.loc[userId,:]
    
    #user_rating이 0보다 크면 기존에 관람한 영화임. 대상 index를 추출하여 list 객체로 만듦
    already_seen = user_rating[ user_rating > 0].index.tolist()
    
    #모든 영화명을 list 객체로 만듦
    movies_list = ratings_matrix.columns.tolist()
    
    #list comprehension으로 already_seen에 해당하는 movie는 movies_list에서 제외
    unseen_list = [ movie for movie in movies_list if movie not in already_seen]
    
    return unseen_list
```

<br>

**recomm_movie_by_userid()**

사용자가 영화 평점을 주지 않은 추천 대상 영화 정보와 predict_rating_topsim()에서 추출한 사용자별 아이템 유사도에 기반한 예측 평점 데이터 세트를 사용해 최종적으로 사용자에게 영화를 추천하는 함수

```python
def recomm_movie_by_userid(pred_df, userId, unseen_list, top_n=10):
    #예측 평점 DataFrame에서 사용자id index와 unseen_list로 들어온 영화명 컬럼을 추출
    #가장 예측 평점이 높은 순으로 정렬
    recomm_movies = pred_df.loc[userId, unseen_list].sort_values(ascending=False)[:top_n]
    return recomm_movies
```

<br>

```python
#사용자가 관람하지 않는 영화명 추출   
unseen_list = get_unseen_movies(ratings_matrix, 9)

#아이템 기반의 인접 이웃 협업 필터링으로 영화 추천 
recomm_movies = recomm_movie_by_userid(ratings_pred_matrix, 9, unseen_list, top_n=10)

#평점 데이터를 DataFrame으로 생성
recomm_movies = pd.DataFrame(data=recomm_movies.values,index=recomm_movies.index,columns=['pred_score'])
recomm_movies
```

![image](https://user-images.githubusercontent.com/76269316/132083838-e1fe0424-deb9-4875-8d80-2eb81612b4f4.png)

아이템 기반 협업 필터링 결과와는 추천된 영화가 많이 다릅니다.

알프레드 히치콕 감독의 스릴러 영화 '이창(Rear Window)', 애니메이션 영화 '사우스파크(South Park:Bigger, Longer & Uncut)', 맷 데이먼이 주연한 도박 영화 '라운더스(Rounders)' 등 약간 어둡고 무거운 주제의 영화가 추천됐습니다.

<br><br><br>

### 파이썬 추천 시스템 패키지 - Surprise

##### Surprise 패키지 소개

이번에는 파이썬 기반의 추천 시스템 구축을 위한 전용 패키지 [Surprise](https://surprise.readthedocs.io/en/stable/)를 소개하겠습니다.

Surprise는 파이썬 기반에서 사이킷런과 유사한 API와 프레임워크를 제공합니다. (사이킷런은 추천 전용 모듈을 제공하지 않습니다)

따라서 추천 시스템의 전반적인 알고리즘을 이해하고 사이킷런 사용 경험이 있으면 쉽게 사용할 수 있습니다.

<br>

Surprise 패키지의 주요 장점

- 다양한 추천 알고리즘 (사용자 또는 아이템 기반 최근접 이웃 협업 필터링, SVD, SVD++, NMF 기반 잠재 요인 협업 필터링을 쉽게 적용할 수 있음)
- Surprise 핵심 API가 사이킷런 핵심 API와 유사한 API명으로 작성됨
  fit(), predict() API로 추천 데이터 학습과 예측, train_test_split()으로 추천 학습 데이터 세트와 예측 데이터 세트 분리, cross_validate(), GridSearchCV 클래스를 통해 추천 시스템을 위한 모델 셀렉션, 평가, 하이퍼 파라미터 튜닝등

<br>

Anaconda Prompt에서 아래 명령어를 통해 설치합니다.

```
conda install -c conda-forge scikit-surprise
```

![image](https://user-images.githubusercontent.com/76269316/132083992-9b819469-7a1b-480e-b698-abf4f4aac83d.png)

<br><br>

##### Surprise를 이용한 추천 시스템 구축

Surprise에서 데이터 로딩은 Dataset 클래스를 이용해서만 가능한데, 포맷이 userID(사용자 ID), movieID(영화 ID), rating(평점)과 같은 row 레벨 형태로 돼 있는 포맷의 처리만 가능합니다.

<img src="https://user-images.githubusercontent.com/76269316/132084179-61f726a0-9a49-4c11-91ca-4c1f4e3bebcd.png" alt="image" style="zoom:50%;" />

<br>

Surprise는 MovieLens 사이트에서 제공하는 과거 버전의 데이터 세트를 가져오는 API를 제공합니다.

Surprise Dataset 클래스의 load_bulletin()은 'ml-100k' (10만개 평점 데이터) 또는 'ml-1m'(100만개 평점 데이터)를 아카이브 사이트로부터 내려받아 로컬 디렉토리에 저장한 뒤 데이터를 로딩합니다.

데이터를 로딩한 뒤 Surprise 패키지의 train_test_split() API를 사용해 학습 데이터와 테스트 데이터 세트로 분리해 보겠습니다.

```python
import surprise 
from surprise import Dataset
from surprise.model_selection import train_test_split

data = Dataset.load_builtin('ml-100k') 
trainset, testset = train_test_split(data, test_size=.25, random_state=0)
```

![image](https://user-images.githubusercontent.com/76269316/132084301-bb1ec65e-fa6e-40d1-8080-22c7927052af.png)

초기 다운로드 시 로컬 디렉토리에 데이터가 없기 때문에 다운로드 받을 것인지를 물어보는데 'Y'를 입력하면 됩니다.

내려받기가 완료되면 데이터가 저장된 디렉토리가 다음과 같이 표시됩니다. (C:\Users\seominseok/.surprise_data/ml-100k)

내려받은 데이터 세트는 row 레벨 데이터이며, 분리 문자가 탭(\\t) 문자입니다.

<br>

Surprise는 row 레벨의 데이터를 column 레벨로 변경해주기 때문에 원본인 row 레벨 데이터(사용자-아이템 평점 데이터)를 데이터 세트로 적용해야 합니다.

train_test_split으로 분리한 학습 데이터 세트(trainset)로 SVD 잠재 요인 협업 필터링을 수행하겠습니다.

먼저 fit()을 수행해 학습 데이터 세트 기반으로 추천 알고리즘을 학습합니다.

```python
from surprise import SVD

algo = SVD()  #알고리즘 객체 생성
algo.fit(trainset)  #학습 데이터 세트 기반으로 추천 알고리즘 학습
```

<br>

Surprise에서 추천을 예측하는 메소드는 test(), predict() 두 가지인데 test()는 사용자-아이템 평점 데이터 세트 **전체**에 대해서 추천을 예측하는 메소드이고, predict() 개별 사용자와 영화에 대한 추천 평점을 반환하는 메소드입니다.

먼저 test()로 테스트 데이터 세트 전체에 대해 추천 영화 평점 데이터를 생성한 뒤 5개만 확인해 보겠습니다.

```python
predictions = algo.test(testset)
print('prediction type:',type(predictions), ' size:',len(predictions))
print('prediction 결과의 최초 5개 추출')
predictions[:5]
```

![image](https://user-images.githubusercontent.com/76269316/132084618-31725341-8866-4704-b6f9-cc8131236253.png)

SVD 알고리즘 객체의 test() 메소드의 호출 결과는 파이썬 리스트이며, 크기는 입력 인자 데이터 세트 크기와 같은 25,000개입니다.

호출 결과로 반환된 리스트 객체는 25,000개의 Prediction 객체를 갖고 있습니다.

Prediction 객체는 Surprise 패키지에서 제공하는 데이터 타입으로 개별 사용자 아이디(uid), 영화(또는 아이템) 아이디(iid), 실제 평점(r_ui) 정보에 기반해 Surprise 추천 예측 평점 (est) 데이터를 tuple 형태로 갖고 있습니다.

details 속성은 내부 처리 시 추천 예측을 할 수 없는 경우에 로그용으로 데이터를 남기는 데 사용됩니다. (True이면 예측값을 생성할 수 없는 데이터라는 의미)

리스트 객체 내에 있는 Prediction 객체의 uid, iid, r_ui, est 등의 속성에 접근하려면 객체명.uid와 같은 형식으로 접근할 수 있습니다.

<br>

이번에는 Surprise 패키지의 predict() 메소드를 사용해 추천 예측을 해보겠습니다.

predict()는 개별 사용자의 아이템에 대한 추천 평점을 예측해 줍니다.

```python
# 사용자 아이디, 아이템 아이디는 문자열로 입력
uid = str(196)
iid = str(302)
pred = algo.predict(uid, iid)  #개별 사용자 아이디, 아이템 아이디를 파라미터로 입력
print(pred)
```

![image](https://user-images.githubusercontent.com/76269316/132084811-197df16d-607a-4fae-9edc-63cf9b130358.png)

predict()는 다음과 같이 개별 사용자와 아이템 정보를 입력하면 추천 예측 평점을 est로 반환합니다.

<br>

테스트 데이터 세트를 사용해 추천 예측 평점과 실제 평점의 차이를 평가해 보겠습니다.

Surprise의 accuracy 모듈은 RMSE, MSE 등의 방법으로 추천 시스템의 성능 평가를 제공합니다.

```python
from surprise import accuracy
accuracy.rmse(predictions)
```

![image](https://user-images.githubusercontent.com/76269316/132084866-7959e777-9736-4a90-bd68-5eaa53b1699b.png)

이렇게 Surprise 패키지를 사용하면 쉽게 추천 시스템을 구현할 수 있습니다.

<br><br>

##### Surprise 주요 모듈 소개

**Dataset**

Surprise는 user_id(사용자 아이디), item_id(아이템 아이디), rating(평점) 데이터가 row 레벨로 된 데이터 세트만 적용할 수 있습니다.

데이터의 첫 번째 컬럼을 사용자 아이디, 두 번째 컬럼을 아이템 아이디, 세 번째 컬럼을 평점으로 가정해 데이터를 로딩하고 네 번째 컬럼부터는 아예 로딩을 수행하지 않습니다.

MovieLens 아카이브 서버에서 자동으로 내려받는 데이터 뿐 아니라 일반 데이터 파일이나 판다스 DataFrame에서도 로딩할 수 있습니다.

단 데이터 세트의 컬럼 순서가 사용자 아이디, 아이템 아이디, 평점 순으로 반드시 돼 있어야 합니다.

|                  API 명                   |                             내용                             |
| :---------------------------------------: | :----------------------------------------------------------: |
|   Dataset.load_builtin(nmae='m1=100k')    | 무비렌즈 아카이브 FTP 서버에서 무비렌즈 데이터를 내려받습니다. (m1-100k, m1-1M를 내려받을 수 있습니다)<br />내려받은 데이터는 .surprise_data 디렉토리에 저장되고 해당 디렉토리에 데이터가 있으면 내려받지 않고 해당 데이터를 이용합니다.<br />입력 파라미터 name으로 대상 데이터가 m1-100k인지 m1-1m인지를 입력합니다. (default는 m1-100k) |
| Dataset.load_from_file(file_path, reader) | 콤마, 탭 등으로 컬럼이 분리된 포맷의 OS 파일에서 데이터를 로딩할 때 사용합니다.<br />입력 파라미터로 OS 파일명, Reader로 파일의 포맷을 지정합니다. |
|     Dataset.load_from_df(df, reader)      | 판다스의 DataFrame에서 데이터를 로딩합니다.<br />파라미터로 DataFrame을 입력받으며 반드시 3개의 컬럼 순서가 사용자 아이디, 아이템 아이디, 평점 순이여야 합니다.<br />입력 파라미터로 DataFrame 객체, Reader로 파일 포맷을 지정합니다. |

<br>

**OS  파일 데이터를 Surprise 데이터 세트로 로딩**

[아이템 기반 최근접 협업 필터링 실습](https://seominseok4834.github.io/machine%20learning/9.recommendations/#%EC%95%84%EC%9D%B4%ED%85%9C-%EA%B8%B0%EB%B0%98-%EC%B5%9C%EA%B7%BC%EC%A0%91-%EC%9D%B4%EC%9B%83-%ED%98%91%EC%97%85-%ED%95%84%ED%84%B0%EB%A7%81-%EC%8B%A4%EC%8A%B5)에서 사용한 ratings.csv와 movies.csv 파일을 Dataset.load_from_file()을 사용해 로딩해 보겠습니다.

주의할 점이 아래와 같이 ratings.csv 파일은 맨 처음 위치에 컬럼명을 헤더로 갖고 있습니다.

![image](https://user-images.githubusercontent.com/76269316/132085179-c7b11a8e-3645-430a-9139-27d90f8085fc.png)

따라서 먼저 판다스 DataFrame의 to_csv() 함수를 사용해 헤더를 삭제하고 새로운 파일(ratings_noh.csv) 파일로 저장하겠습니다.

```python
import pandas as pd

ratings = pd.read_csv('ratings.csv')
#ratings_noh.csv 파일로 unload시 index와 header를 모두 제거한 새로운 파일 생성 
ratings.to_csv('ratings_noh.csv', index=False, header=False)
```

<img src="https://user-images.githubusercontent.com/76269316/132085228-8770c910-6faf-496e-bbf6-755e4d614a5c.png" alt="image" style="zoom:67%;" />

<br>

새롭게 생성된 ratings_noh.csv 파일을 로딩해 보겠습니다.

Surprise 데이터 세트는 기본적으로 Movie Lens 데이터 형식을 따르므로 Movie Lens 데이터 형식이 아닌 다른 OS 파일의 경우 Reader 클래스를 먼저 설정해야 합니다.

**Reader 클래스 주요 생성 파라미터**

- line_format(string): 컬럼을 순서대로 나열, 입력된 문자열을 공백으로 분리해 컬럼으로 인식
- sep(char): 컬럼을 분리하는 분리자, 판다스 DataFrame에서 입력받을 경우에는 기재하지 않아도 됨(default는 '\t')
- rating_scale(tuple, optional): 평점 값의 최소~최대 평점을 설정 (default는 (1, 5))

<br>

Reader 클래스를 이용해 데이터 파일의 파싱 정보를 알려주기 위해 Reader 클래스의 생성자에 컬럼명, 컬럼 분리문자, 최소~최대 평점을 입력해 객체를 생성했습니다.

```python
from surprise import Reader

reader = Reader(line_format='user item rating timestamp', sep=',', rating_scale=(0.5, 5))
data=Dataset.load_from_file('ratings_noh.csv',reader=reader)
```

<br>

SVD 행렬 분해 기법을 적용해 추천을 예측해 보겠습니다.

잠재 요인 크기 K 값을 나타는 파라미터 n_factors를 50으로 설정해 학습한 뒤 테스트 데이터 세트를 적용한 다음, 예측 평점과 실제 평점 데이터를 RMSE로 평가했습니다.

```python
trainset, testset = train_test_split(data, test_size=.25, random_state=0)

#수행시마다 동일한 결과 도출을 위해 random_state 설정
algo = SVD(n_factors=50, random_state=0)

#학습 데이터 세트로 학습 후 테스트 데이터 세트로 평점 예측 후 RMSE 평가
algo.fit(trainset) 
predictions = algo.test(testset)
accuracy.rmse(predictions)
```

![image](https://user-images.githubusercontent.com/76269316/132085430-dd9f54a5-2a93-484d-b71c-4305c5ac4808.png)

<br>

**판다스 DataFrame에서 Surprise 데이터 세트 로딩**

Dataset.load_from_df()를 이용해 DataFrame을 Surprise 데이터 세트로 로딩해보겠습니다.

마찬가지로 DataFrame 역시 사용자 아이디, 아이템 아이디, 평점 컬럼 순서로 돼 있어야 합니다.

```python
import pandas as pd
from surprise import Reader, Dataset

ratings = pd.read_csv('ratings.csv') 
reader = Reader(rating_scale=(0.5, 5.0))

#ratings DataFrame 에서 컬럼은 사용자 아이디, 아이템 아이디, 평점 순서를 지켜야 함
data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)
trainset, testset = train_test_split(data, test_size=.25, random_state=0)

algo = SVD(n_factors=50, random_state=0)
algo.fit(trainset) 
predictions = algo.test( testset )
accuracy.rmse(predictions)
```

![image](https://user-images.githubusercontent.com/76269316/132085482-afda3d74-a618-4c6f-a123-d105baa7c141.png)

<br><br>

##### Surprise 추천 알고리즘 클래스

Surprise에서 추천 에측을 위해 자주 사용되는 추천 알고리즘 클래스는 다음과 같습니다.

|   클래스명   |                            설명                            |
| :----------: | :--------------------------------------------------------: |
|     SVD      | 행렬 분해를 통한 잠재 요인 협업 필터링을 위한 SVD 알고리즘 |
|   KNNBasic   |        최근접 이웃 협업 필터링을 위한 KNN 알고리즘         |
| BaselineOnly | 사용자 Bias와 아이템 Bias를 감안한 SGD 베이스라인 알고리즘 |

이 밖에도 SVD++, NMF등 다양한 유형의 알고리즘 클래스가 있습니다.

추천 알고리즘의 에측 성능 벤치마크 결과는 https://surpriselib.com에서 확인할 수 있습니다.

<br>

**베이스라인 평점**

호텔을 이용하고 난 뒤 설문지에 호텔을 평가해달라는 요구를 받아본 적이 있을 것입니다.

이런 평가는 각 개인의 성향에 따라 평가가 달라질 수 있습니다. (싫은 소리를 별로 안하는 사람의 경우는 평가에 후한 경향이 있습니다)

개인의 성향을 반영해 아이템 평가에 편향성(bias) 요소를 반영하여 평점을 부과하는 것을 베이스라인 평점(Baseline Rating)이라고 합니다.

보통 베이스라인 평점은 전체 평균 평점 + 사용자 편향 점수 + 아이템 편향 점수로 계산됩니다.

- 전체 평균 평점 = 모든 사용자의 아이템에 대한 평점을 평규한 값
- 사용자 편향 점수 = 사용자별 아이템 평점 평균 값 - 전체 평균 평점
- 아이템 편향 점수 = 아이템별 평점 평균 값

<br>

모든 사용자의 평균적인 영화 평점이 3.5점이고, '인피니티 워'의 평균 평점이 4.2점, 영화를 깐깐하게 평가하는 사용자 A의 평균 평점이 3.0점이였다면 사용자 A의 '인피니티 워' 베이스라인 평점은 다음과 같습니다.

<img src="https://user-images.githubusercontent.com/76269316/132085801-be5d9a6f-a5d5-4632-8ef2-e88c40cc3d68.png" alt="image" style="zoom:50%;" />

<br><br>

##### 교차 검증과 하이퍼 파라미터 튜닝

Surprise는 교차 검증과 하이퍼 파라미터 튜닝을 위해 사이킷런과 유사한 cross_validate()와 GridSearchCV 클래스를 제공합니다.

cross_validate()를 사용해 데이터를 5개의 학습/검증 폴드 데이터 세트로 분리해 교차 검증을 수행하고 RMSE, MAE로 성능 평가를 진행해 보겠습니다.

```python
from surprise.model_selection import cross_validate 

#Pandas DataFrame에서 Surprise Dataset으로 데이터 로딩
ratings = pd.read_csv('ratings.csv')  #reading data in pandas df
reader = Reader(rating_scale=(0.5, 5.0))
data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)

algo = SVD(random_state=0) 
cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
```

![image](https://user-images.githubusercontent.com/76269316/132085973-c2797416-0d69-44b8-9b44-a9a334f4479e.png)

위와 같이 폴드별 성능 평가 수치와 전체 폴드의 평균 성능 평가 수치를 함께 보여줍니다.

<br>

Surprise의 GridSearchCV를 이용해 SVD 하이퍼 파라미터 튜닝을 수행해 보겠습니다.

**SVD 클래스 입력 파라미터**

|  파라미터명  |                             내용                             |
| :----------: | :----------------------------------------------------------: |
|  n_factors   | 잠재 요인 K 개수 (default: 100)<br />커질수록 정확도가 높으나 과적합 문제가 발생할 수 있음 |
|   n_epochs   | SGD(Stochastic Gradient Descent) 수행 시 반복 횟수 (default: 20) |
| biased(bool) |       베이스라인 사용자 편향 적용 여부 (default: True)       |

```python
from surprise.model_selection import GridSearchCV

#최적화할 파라미터들을 딕셔너리 형태로 지정
param_grid = {'n_epochs': [20, 40, 60], 'n_factors': [50, 100, 200] }

#CV를 3개 폴드 세트로 지정, 성능 평가는 rmse, mse로 수행하도록 GridSearchCV 구성
gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)
gs.fit(data)

#최고 RMSE Evaluation 점수와 그때의 하이퍼 파라미터
print(gs.best_score['rmse'])
print(gs.best_params['rmse'])
```

![image](https://user-images.githubusercontent.com/76269316/132086098-3faa8946-9b36-4f36-be0b-ca4d37cdf730.png)

'n_epochs': 20, 'n_factors': 50일 때 3개 폴드 검증 데이터 세트에서 최적 RMSE가 0.8771로 도출됐습니다.

<br><br>

##### Surprise를 이용한 개인화 영화 추천 시스템 구축

Surprise 패키지로 학습된 추천 알고리즘을 기반으로 특정 사용자가 아직 관람하지 않은(평점을 매기지 않은) 영화 중에서 개인 취향에 가장 적절한 영화를 추천해 보겠습니다.

이번에는 학습 데이터와 테스트 데이터로 분리하지 않고 전체를 학습 데이터로 사용하겠습니다.

그런데 Surprise는 데이터 세트를 train_test_split()을 이용해 내부에서 사용하는 TrainSet 클래스 객체로 변환하지 않으면 fit()을 통해 학습할 수 없습니다.

```python
#아래 코드는 train_test_split( )으로 분리되지 않는 Dataset에 fit( )을 호출하여 오류 발생
data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)
algo = SVD(n_factors=50, random_state=0)
algo.fit(data)
```

![image](https://user-images.githubusercontent.com/76269316/132086125-969c614a-5837-4c45-9d51-0ae42b031bee.png)

<br>

따라서 DatasetAutoFolds 클래스를 사용해 DatasetAutoFolds 객체를 생성한 뒤 build_full_trainset() 메소드를 호출해 전체 데이터를 학습 데이터 세트로 만들겠습니다.

```python
from surprise.dataset import DatasetAutoFolds
from surprise import Reader, Dataset

reader = Reader(line_format='user item rating timestamp', sep=',', rating_scale=(0.5, 5))
#DatasetAutoFolds 클래스를 ratings_noh.csv 파일 기반으로 생성 
data_folds = DatasetAutoFolds(ratings_file='ratings_noh.csv', reader=reader)

#전체 데이터를 학습데이터로 생성
trainset = data_folds.build_full_trainset()
```

<br>

생성된 학습 데이터를 SVD를 사용해 학습을 수행합니다.

```python
from surprise import SVD

algo = SVD(n_epochs=20, n_factors=50, random_state=0)
algo.fit(trainset)
```

<br>

userId=9인 사용자가 아직 평점을 매기지 않은 영화를 movieId 42로 선정한 뒤 예측 평점을 계산해 보겠습니다.

먼저 userId가 9인 사용자가 영화 아이디 42에 평점을 줬는지 확인해 보고 영화 타이틀과 장르를 출력해 보겠습니다.

```python
import pandas as pd

ratings = pd.read_csv('ratings.csv')  #reading data in pandas df

#영화에 대한 상세 속성 정보 DataFrame로딩
movies = pd.read_csv('movies.csv')

#userId=9 의 movieId 데이터 추출하여 movieId=42 데이터가 있는지 확인
movieIds = ratings[ratings['userId']==9]['movieId']
if movieIds[movieIds==42].count() == 0:
    print('사용자 아이디 9는 영화 아이디 42의 평점 없음')

print(movies[movies['movieId']==42])
```

![image](https://user-images.githubusercontent.com/76269316/132086330-04a2b85e-990e-4da2-bb9d-2fbf9a91a738.png)

<br>

predict() 메소드를 사용해 예상 평점을 확인해 보겠습니다.

```python
uid = str(9)
iid = str(42)

pred = algo.predict(uid, iid, verbose=True)
```

![image](https://user-images.githubusercontent.com/76269316/132086342-6dd39b0c-ecbd-4f09-b783-cb1edf161cec.png)

추천 예측 평점은 3.13입니다.

<br>

이제 사용자가 아직 평점을 매기지 않은 영화를 추출해 예측 평점을 계산한 뒤 높은 순으로 정렬해 TOP-N개의 추천 영화를 출력해 보겠습니다.

이를 위해 먼저 추천 대상이 되는 영화를 추출하는 함수 get_unseen_surprise()와 예측 평점이 높은 순으로 정렬한 뒤 TOP-N개의 영화 아이디, 영화 제목, 예측 평점 정보를 추출하는 함수 recomm_movie_by_surprise()를 생성했습니다.

<br>

**get_unseen_surprise()**

사용자가 아직 평점을 매기지 않은 영화를 추출하는 함수

```python
def get_unseen_surprise(ratings, movies, userId):
    #입력값으로 들어온 userId에 해당하는 사용자가 평점을 매긴 모든 영화를 리스트로 생성
    seen_movies = ratings[ratings['userId']== userId]['movieId'].tolist()
    
    #모든 영화들의 movieId를 리스트로 생성
    total_movies = movies['movieId'].tolist()
    
    #모든 영화들의 movieId중 이미 평점을 매긴 영화의 movieId를 제외하여 리스트로 생성
    unseen_movies= [movie for movie in total_movies if movie not in seen_movies]
    print('평점 매긴 영화수:',len(seen_movies), '추천대상 영화수:',len(unseen_movies), '전체 영화수:',len(total_movies))
    
    return unseen_movies
```

<br>

**recomm_movie_by_surprise()**

```python
def recomm_movie_by_surprise(algo, userId, unseen_movies, top_n=10):
    #알고리즘 객체의 predict() 메소드를 평점이 없는 영화에 반복 수행한 후 결과를 list 객체로 저장
    predictions = [algo.predict(str(userId), str(movieId)) for movieId in unseen_movies]
    
    #predictions list 객체는 surprise의 Predictions 객체를 원소로 갖고 있음
    #[Prediction(uid='9', iid='1', est=3.69), Prediction(uid='9', iid='2', est=2.98),,,,]
    #이를 est 값으로 정렬하기 위해서 아래의 sortkey_est 함수를 정의
    #sortkey_est 함수는 list 객체의 sort() 함수의 키 값으로 사용되어 정렬 수행
    def sortkey_est(pred):
        return pred.est
    
    #sortkey_est( ) 반환값의 내림 차순으로 정렬 수행하고 top_n개의 최상위 값 추출
    predictions.sort(key=sortkey_est, reverse=True)
    top_predictions= predictions[:top_n]
    
    #top_n으로 추출된 영화의 정보 추출. 영화 아이디, 추천 예상 평점, 제목 추출
    top_movie_ids = [ int(pred.iid) for pred in top_predictions]
    top_movie_rating = [ pred.est for pred in top_predictions]
    top_movie_titles = movies[movies.movieId.isin(top_movie_ids)]['title']
    top_movie_preds = [ (id, title, rating) for id, title, rating in zip(top_movie_ids, top_movie_titles, top_movie_rating)]
    
    return top_movie_preds
```

<br>

9번 사용자에게 추천할 영화 10개는 다음과 같습니다.

```python
unseen_movies = get_unseen_surprise(ratings, movies, 9)
top_movie_preds = recomm_movie_by_surprise(algo, 9, unseen_movies, top_n=10)
print('##### Top-10 추천 영화 리스트 #####')

for top_movie in top_movie_preds:
    print(top_movie[1], ":", top_movie[2])
```

![image](https://user-images.githubusercontent.com/76269316/132086553-bf0cf0ef-eb08-457a-90da-0ccd100d7560.png)

<br><br><br>

### 정리

추천 시스템의 대표적인 방식으로 콘텐츠 기반 필터링과 협업 필터링이 있습니다.

콘텐츠 기반 필터링은 아이템(상품, 영화, 서비스 등)을 구성하는 여러 가지 콘텐츠 중 사용자가 좋아하는 콘텐츠를 필터링해 이에 맞는 아이템을 추천하는 방식입니다.

협업 필터링은 다시 최근접 이웃 협업 필터링과 잠재 요인 협업 필터링으로 나뉩니다.

최근접 이웃 협업 필터링은 사용자 기반과 아이템 기반으로 나뉘는데, 이 중 아이템 기반이 더 많이 사용됩니다.

아이템 기반 최근접 이웃 방식은 특정 아이템과 가장 근접하게 유사한(사용자들의 아이템에 대한 평가를 벡터화한 값을 기준으로 함) 다른 아이템들을 추천하는 방식입니다.

잠재 요인 협업 필터링은 많은 추천 시스템에서 활용하는 방식으로 사용자-아이템 평점 행렬 데이터에 숨어 있는 잠재 요인을 추출해 사용자가 아직 평점을 매기지 않은 아이템에 대한 평점을 예측해 이를 추천에 반영하는 방식입니다.
