---
title:  "4.Regression"
excerpt: "회귀"
toc: true
toc_label: "Regression"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - 회귀
  - 파이썬 머신러닝 완벽 가이드
last_modified_at: 2021-08-01
---

>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.



### 회귀 소개

회귀 분석은 유전적 특성을 연구하던 영국 통계학자 갈톤(Galton)이 수행한 연구에서 유래했다고 알려져있습니다.

부모, 자식 간의 키 상관관계를 분석했던 갈톤은 부모의 키가 모두 클 때 자식의 키가 크긴 하지만 부모를 능가할 정도로 크지 않았고,

부모의 키가 모두 아주 작을 때 그 자식의 키가 작기는 하지만 부모보다는 큰 경향을 발견했습니다.

즉, 사람의 키는 평균 키로 회귀하려는 경향을 가진다는 자연의 법칙이 있다는 것입니다.

회귀 분석은 이처럼 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법입니다.

<br>

머신러닝에서 회귀는 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 통칭합니다.

예를 들어 아파트 방 개수, 방 크기, 주변 학군 (독립변수)과 아파트 가격(종속변수)의 관계를 모델링하고 예측하는 것입니다.

<br>

이를 선형 회귀식으로 나타내면 다음과 같습니다.

**Y = W1\*X1 + W2\*X2 + W3\*X3 + ··· + Wn\*Xn**

- Y : 종속변수(아파트 가격)
- X1, X2, X3, ···, Xn : 독립변수(방 개수, 방 크기, 주변 학군등)
- W1, W2, W3, ··· Wn : 회귀 계수
  회귀 계수(Regression coefficients) : 독립변수에 영향을 미치는 값

<br>

회귀는 독립변수의 개수, **회귀 계수**의 선형/비선형 여부에 따라 다음과 같이 나뉩니다.

| 독립변수 개수       | 회귀 계수의 결합     |
| ------------------- | -------------------- |
| 1개 : 단일 회귀     | 선형 : 선형 회귀     |
| 여러 개 : 다중 회귀 | 비선형 : 비선형 회귀 |

<br>

지도학습은 두 가지 유형으로 나뉘는데, 분류와 회귀입니다.
두 기법의 가장 큰 차이는 예측 결과값입니다.

- 분류(Classification) : 카테고리 값(이산값)
- 회귀(Regression) : 숫자값(연속값)

<br>

여러 가지 회귀 중 선형 회귀가 가장 많이 사용됩니다.

선형 회귀는 실제 값과 예측값의 차이를 최소화하는 직선형 회귀선을 최적화하는 방식입니다.

대표적인 선형 회귀 모델은 다음과 같습니다.

- 일반 선형 회귀 : 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화하며, 규제(Regularization)를 적용하지 않은 모델
- 릿지(Ridge) : 선형 회귀에 L2 규제(큰 회귀 계수 값의 예측 영향도를 감소시키기 위한 규제)를 추가한 회귀 모델
- 라쏘(Lasso) : 선형 회귀에 L1 규제(예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않게 하는 규제)를 적용한 회귀 모델
- 엘라스틱넷(ElasticNet) : L2, L1 규제를 결합한 모델, 주로 피처가 많은 데이터 세트에 적용됨 (L1 규제로 피처 개수를 줄이고, L2 규제로 계수 값 크기 조정)
- 로지스틱 회귀(Logistic Regression) : 분류 알고리즘으로 분류에서 사용되는 선형 모델, 이진 분류뿐만 아니라 희소 영역(텍스트 분류)의 분류에서도 뛰어난 예측 성능을 보임



### 단순 선형 회귀를 통한 회귀 이해

먼저, 독립 변수도 하나 종속 변수도 하나인 단순 선형 회귀에 대해 설명하겠습니다.

<br>

예를 들어, 주택 가격이 주택 크기로만 결정된다고 가정하겠습니다.

일반적으로 주택 크기가 커지면 가격이 높아지는 경향이 있기 때문에 다음과 같이 주택 가격은 주택 크기에 대해 선형(직선 형태)의 관계로 표현할 수 있습니다.

<img src="https://user-images.githubusercontent.com/76269316/127769263-37405a99-4f5e-4c1f-8346-151bd8652cdc.png" alt="image" style="zoom: 67%;" />

X축이 주택 크기(평당 크기)이고, Y축이 주택 가격입니다.

따라서 회귀 모델을 다음과 같이 **Y = W0 + W1 * X**  1차 함수 형태로 모델링 할 수 있습니다.

단순 선형 회귀(독립변수, 종속변수가 한 개인 선형 회귀)에서 회귀 계수는 기울기 w1과 y절편 w0입니다.

*+절편은 intercept라고 합니다.*

<br>

위 모델로 실제 값과 회귀 모델의 차이에 따른 오류 값을 남은 오류, **잔차**라고 부릅니다.

<img src="https://user-images.githubusercontent.com/76269316/127769583-05c01982-9879-4644-996e-6ebe90c860b4.png" alt="image" style="zoom:67%;" />

최적의 회귀 모델을 만든다는 것은 전체 데이터의 잔차(오류 값) 합이 최소가 되는 모델을 만든다는 의미입니다.

즉, 오류 값 합이 최소가 될 수 있는 최적의 회귀 계수를 찾는다는 의미입니다.

<br>

오류 값은 +나 -가 될 수 있습니다.

그래서 전체 데이터의 오류 합을 단순히 더한다면 오류 합이 크게 줄어들 수 있습니다.

보통 오류 합을 계산할 대는 절댓값을 취해서 더하거나(Mean Absolute Error), 오류 값의 제곱을 구해서 더하는 방식(Residual Sum of Square)을 취합니다.

일반적으로 미분 등의 계산을 편리하게 하기 위해 RSS 방식으로 오류 합을 구합니다.

RSS = (#1 주택 가격 - (w0 + w1 #1 주택크기))^2 + (#2 주택 가격 - (w0 + w1 #2 주택 크기))^2 + (#3 주택 가격 - (w0 + W1 #3 주택크기))^2 + ···

<br>

RSS는 w0, w1 변수로 표현할 수 있으며, 이 RSS를 최소로 하는 w0, w1 회귀 계수를 학습을 통해 찾는 것이 머신러닝 기반 회귀의 핵심입니다.

**RSS는 회귀식의 독립변수 X, 종속변수 Y가 중심 변수가 아니라 w 회귀계수가 중심 변수입니다.**

<br>

RSS는 학습 데이터 건수로 나누어서 다음과 같이 정규화된 식으로 표현됩니다.

![image](https://user-images.githubusercontent.com/76269316/127770177-74d77c02-7a55-42d1-a6aa-aff9896e3556.png)

회귀에서 이 RSS는 비용(Cost)이며, w 변수(회귀 계수)로 구성되는 RSS를 비용 함수라고 합니다.

머신러닝 회귀 알고리즘은 계속해서 학습하면서 이 비용 함수가 반환하는 오류 값을 지속해서 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것입니다.

*+비용 함수를 손실 함수(loss function)라고도 합니다.*



### 비용 최소화하기 - 경사 하강법(Gradient Descent) 소개

경사 하강법은 점진적으로 반복적인 계산을 통해 W 파라미터 값을 업데이트하면서 오류 값이 최소가 되는 W 파라미터를 구하는 방식입니다.

<br>

경사 하강법은 산 정상에서 아래로 내려갈 때, 현재 위치보다 무조건 낮은 곳으로 계속 이동하다 보면 마침내 지상에 도착할 수 있을 거라 가정하고 이동하는 방식입니다. (내리막과 오르막이 반복되는 언덕에 갇혔을 수도 있는데, 이 문제는 나중에 해결하겠습니다.)

어떻게 보면 무식해 보이는 방법이지만 직관적이고 빠르게 비용 함수가 최소가 되는 W 파라미터 값을 구할 수 있습니다.

<br>

경사 하강법은 최초 오류 값이 100이였다면, 두 번째 오류 값은 100보다 작은 90, 세 번째는 80으로 지속해서 오류를 감소시키는 방향으로 W 값을 업데이트해 나갑니다.

그리고 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그 때의 W 값을 최적 파라미터로 반환합니다.

<br>

경사 하강법의 핵심은 **"어떻게 하면 오류가 작아지는 방향으로 W 값을 보정할 수 있을까?"**입니다.

비용 함수가 다음과 같은 포물선 형태의 2차 함수라면 경사 하강법은 미분을 적용한 다음, 이 미분 값이 계속 감소하는 방향으로 w를 업데이트 합니다.

![image](https://user-images.githubusercontent.com/76269316/127771891-bf9fa76f-9732-4fc4-a089-3e82deb0a0bb.png)

더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그 때의 w를 반환합니다.

<br>

<br>

위에서 언급한 비용 함수를 R(w)로 지칭하겠습니다.

![image](https://user-images.githubusercontent.com/76269316/127771922-24705125-2135-4902-bb45-8329cd71ad8d.png)

R(w)는 w0, w1으로 이루어진 함수이기 때문에 일반적인 미분을 적용할 수 없고, 편미분을 적용해야 합니다.

<img src="https://user-images.githubusercontent.com/76269316/127772224-0eb23cc2-8c74-4dbd-9158-a3c26037f4c1.png" alt="image" style="zoom: 80%;" />

w1, w0 편미분 결괏값을 반복적으로 보정하면서 w1, w0 값을 업데이트하면 비용 함수 R(w)가 최소가 되는 w1, w0의 값을 구할 수 있습니다.

업데이트는 새로운 w1을 이전 w1에서 편미분 결괏값을 마이너스하면서 적용합니다.

편미분 값이 너무 클 수도 있기 때문에 보정 계수 η(eta)를 곱하는데 이를 학습률이라고 합니다.

![image](https://user-images.githubusercontent.com/76269316/127772546-574f2dc0-6e36-4328-92c7-4a91c87e82f9.png)

<br>

요약하면 다음과 같습니다.

- Step 1 : w1, w0를 임의의 값으로 설정하고 첫 비용 함수의 값을 계산
- Step 2 : <img src="https://user-images.githubusercontent.com/76269316/127772619-1399a8dd-c496-4145-8620-632e1ce22c9d.png" alt="image" style="zoom: 50%;" /> , <img src="https://user-images.githubusercontent.com/76269316/127772628-e2daa798-26f6-496f-b4b4-1e3d520ac1e0.png" alt="image" style="zoom:50%;" />로 업데이트 한 뒤 다시 비용 함수 값을 계산합니다.
- Step 3 : 비용 함수의 값이 감소했으면 Step 2를 반복합니다. 더 이상 감소하지 않는 경우, 그 때의 w1, w0를 구하고 반복을 중지합니다.

<br>

<br>

<br>

지금까지 정리한 내용을 토대로 경사 하강법을 파이썬 코드로 구현해 보겠습니다.

y = 4X + 6을 근사하기 위한 100개의 데이터 세트를 만들고, 경사 하강법을 이용해 회귀 계수 w1, w0를 도출해 보겠습니다.

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

np.random.seed(0)
#y = 4X +6을 근사(w1=4, w0=6) 
X = 2 * np.random.rand(100, 1)
y = (6 + 4 * X) + np.random.randn(100, 1)  #4X + 6에다가 난수를 더함 (y = 4X + 6을 중심으로 무작위로 퍼지도록)

#X, y 데이터 세트 산점도로 시각화
plt.scatter(X, y)
```



