---
title:  "5.Regression"
excerpt: "회귀"
toc: true
toc_label: "Regression"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - 회귀
  - 파이썬 머신러닝 완벽 가이드
last_modified_at: 2021-08-02
---

>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.



### 회귀 소개

회귀 분석은 유전적 특성을 연구하던 영국 통계학자 갈톤(Galton)이 수행한 연구에서 유래했다고 알려져있습니다.

부모, 자식 간의 키 상관관계를 분석했던 갈톤은 부모의 키가 모두 클 때 자식의 키가 크긴 하지만 부모를 능가할 정도로 크지 않았고,

부모의 키가 모두 아주 작을 때 그 자식의 키가 작기는 하지만 부모보다는 큰 경향을 발견했습니다.

즉, 사람의 키는 평균 키로 회귀하려는 경향을 가진다는 자연의 법칙이 있다는 것입니다.

회귀 분석은 이처럼 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법입니다.

<br>

머신러닝에서 회귀는 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 통칭합니다.

예를 들어 아파트 방 개수, 방 크기, 주변 학군 (독립변수)과 아파트 가격(종속변수)의 관계를 모델링하고 예측하는 것입니다.

<br>

이를 선형 회귀식으로 나타내면 다음과 같습니다.

**Y = W1\*X1 + W2\*X2 + W3\*X3 + ··· + Wn\*Xn**

- Y : 종속변수(아파트 가격)
- X1, X2, X3, ···, Xn : 독립변수(방 개수, 방 크기, 주변 학군등)
- W1, W2, W3, ··· Wn : 회귀 계수
<br>
  회귀 계수(Regression coefficients) : 독립변수에 영향을 미치는 값

<br>

회귀는 독립변수의 개수, **회귀 계수**의 선형/비선형 여부에 따라 다음과 같이 나뉩니다.

| 독립변수 개수       | 회귀 계수의 결합     |
| ------------------- | -------------------- |
| 1개 : 단일 회귀     | 선형 : 선형 회귀     |
| 여러 개 : 다중 회귀 | 비선형 : 비선형 회귀 |

<br>

지도학습은 두 가지 유형으로 나뉘는데, 분류와 회귀입니다.
두 기법의 가장 큰 차이는 예측 결과값입니다.

- 분류(Classification) : 카테고리 값(이산값)
- 회귀(Regression) : 숫자값(연속값)

<br>

여러 가지 회귀 중 선형 회귀가 가장 많이 사용됩니다.

선형 회귀는 실제 값과 예측값의 차이를 최소화하는 직선형 회귀선을 최적화하는 방식입니다.

대표적인 선형 회귀 모델은 다음과 같습니다.

- 일반 선형 회귀 : 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화하며, 규제(Regularization)를 적용하지 않은 모델
- 릿지(Ridge) : 선형 회귀에 L2 규제(큰 회귀 계수 값의 예측 영향도를 감소시키기 위한 규제)를 추가한 회귀 모델
- 라쏘(Lasso) : 선형 회귀에 L1 규제(예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않게 하는 규제)를 적용한 회귀 모델
- 엘라스틱넷(ElasticNet) : L2, L1 규제를 결합한 모델, 주로 피처가 많은 데이터 세트에 적용됨 (L1 규제로 피처 개수를 줄이고, L2 규제로 계수 값 크기 조정)
- 로지스틱 회귀(Logistic Regression) : 분류 알고리즘으로 분류에서 사용되는 선형 모델, 이진 분류뿐만 아니라 희소 영역(텍스트 분류)의 분류에서도 뛰어난 예측 성능을 보임



### 단순 선형 회귀를 통한 회귀 이해

먼저, 독립 변수도 하나 종속 변수도 하나인 단순 선형 회귀에 대해 설명하겠습니다.

<br>

예를 들어, 주택 가격이 주택 크기로만 결정된다고 가정하겠습니다.

일반적으로 주택 크기가 커지면 가격이 높아지는 경향이 있기 때문에 다음과 같이 주택 가격은 주택 크기에 대해 선형(직선 형태)의 관계로 표현할 수 있습니다.

<img src="https://user-images.githubusercontent.com/76269316/127769263-37405a99-4f5e-4c1f-8346-151bd8652cdc.png" alt="image" style="zoom: 67%;" />

X축이 주택 크기(평당 크기)이고, Y축이 주택 가격입니다.

따라서 회귀 모델을 다음과 같이 **Y = W0 + W1 * X**  1차 함수 형태로 모델링 할 수 있습니다.

단순 선형 회귀(독립변수, 종속변수가 한 개인 선형 회귀)에서 회귀 계수는 기울기 w1과 y절편 w0입니다.

*+절편은 intercept라고 합니다.*

<br>

위 모델의 실제 값과 회귀 모델의 차이에 따른 오류 값을 남은 오류, **잔차**라고 부릅니다.

<img src="https://user-images.githubusercontent.com/76269316/127769583-05c01982-9879-4644-996e-6ebe90c860b4.png" alt="image" style="zoom:67%;" />

최적의 회귀 모델을 만든다는 것은 전체 데이터의 잔차(오류 값) 합이 최소가 되는 모델을 만든다는 의미입니다.

즉, 오류 값 합이 최소가 될 수 있는 최적의 회귀 계수를 찾는다는 의미입니다.

<br>

오류 값은 +나 -가 될 수 있습니다.

그래서 전체 데이터의 오류 합을 단순히 더한다면 오류 합이 크게 줄어들 수 있습니다.

보통 오류 합을 계산할 때는 절댓값을 취해서 더하거나(Mean Absolute Error), 오류 값의 제곱을 구해서 더하는 방식(Residual Sum of Square)을 취합니다.

일반적으로 미분 등의 계산을 편리하게 하기 위해 RSS 방식으로 오류 합을 구합니다.

RSS = (#1 주택 가격 - (w0 + w1 #1 주택크기))^2 + (#2 주택 가격 - (w0 + w1 #2 주택 크기))^2 + (#3 주택 가격 - (w0 + W1 #3 주택크기))^2 + ···

<br>

RSS는 w0, w1 변수로 표현할 수 있으며, 이 RSS를 최소로 하는 w0, w1 회귀 계수를 학습을 통해 찾는 것이 머신러닝 기반 회귀의 핵심입니다.

**RSS는 회귀식의 독립변수 X, 종속변수 Y가 중심 변수가 아니라 w 회귀계수가 중심 변수입니다.**

<br>

RSS는 학습 데이터 건수로 나누어서 다음과 같이 정규화된 식으로 표현됩니다.

![image](https://user-images.githubusercontent.com/76269316/127770177-74d77c02-7a55-42d1-a6aa-aff9896e3556.png)

회귀에서 이 RSS는 비용(Cost)이며, w 변수(회귀 계수)로 구성되는 RSS를 비용 함수라고 합니다.

머신러닝 회귀 알고리즘은 계속해서 학습하면서 이 비용 함수가 반환하는 오류 값을 지속해서 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것입니다.

*+비용 함수를 손실 함수(loss function)라고도 합니다.*



### 비용 최소화하기 - 경사 하강법(Gradient Descent) 소개

경사 하강법은 점진적으로 반복적인 계산을 통해 W 파라미터 값을 업데이트하면서 오류 값이 최소가 되는 W 파라미터를 구하는 방식입니다.

<br>

경사 하강법은 산 정상에서 아래로 내려갈 때, 현재 위치보다 무조건 낮은 곳으로 계속 이동하다 보면 마침내 지상에 도착할 수 있을 거라 가정하고 이동하는 방식입니다. (내리막과 오르막이 반복되는 언덕에 갇혔을 수도 있는데, 이 문제는 나중에 해결하겠습니다.)

어떻게 보면 무식해 보이는 방법이지만 직관적이고 빠르게 비용 함수가 최소가 되는 W 파라미터 값을 구할 수 있습니다.

<br>

경사 하강법은 최초 오류 값이 100이였다면, 두 번째 오류 값은 100보다 작은 90, 세 번째는 80으로 지속해서 오류를 감소시키는 방향으로 W 값을 업데이트해 나갑니다.

그리고 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그 때의 W 값을 최적 파라미터로 반환합니다.

<br>

경사 하강법의 핵심은 **"어떻게 하면 오류가 작아지는 방향으로 W 값을 보정할 수 있을까?"**입니다.

비용 함수가 다음과 같은 포물선 형태의 2차 함수라면 경사 하강법은 미분을 적용한 다음, 이 미분 값이 계속 감소하는 방향으로 w를 업데이트 합니다.

![image](https://user-images.githubusercontent.com/76269316/127771891-bf9fa76f-9732-4fc4-a089-3e82deb0a0bb.png)

계속 업데이트 하다가, 더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그 때의 w를 반환합니다.

<br>

<br>

위에서 언급한 비용 함수를 R(w)로 지칭하겠습니다.

![image](https://user-images.githubusercontent.com/76269316/127771922-24705125-2135-4902-bb45-8329cd71ad8d.png)

R(w)는 w0, w1으로 이루어진 함수이기 때문에 일반적인 미분을 적용할 수 없고, 편미분을 적용해야 합니다.

<img src="https://user-images.githubusercontent.com/76269316/127772224-0eb23cc2-8c74-4dbd-9158-a3c26037f4c1.png" alt="image" style="zoom: 80%;" />

w1, w0 편미분 결괏값을 반복적으로 보정하면서 w1, w0 값을 업데이트하면 비용 함수 R(w)가 최소가 되는 w1, w0의 값을 구할 수 있습니다.

업데이트는 새로운 w1을 이전 w1에서 편미분 결괏값을 마이너스하면서 적용합니다.

편미분 값이 너무 클 수도 있기 때문에 보정 계수 η(eta)를 곱하는데 이를 학습률이라고 합니다.

![image](https://user-images.githubusercontent.com/76269316/127772546-574f2dc0-6e36-4328-92c7-4a91c87e82f9.png)

<br>

요약하면 다음과 같습니다.

- Step 1 : w1, w0를 임의의 값으로 설정하고 첫 비용 함수의 값을 계산
- Step 2 : <img src="https://user-images.githubusercontent.com/76269316/127772619-1399a8dd-c496-4145-8620-632e1ce22c9d.png" alt="image" style="zoom: 50%;" /> , <img src="https://user-images.githubusercontent.com/76269316/127772628-e2daa798-26f6-496f-b4b4-1e3d520ac1e0.png" alt="image" style="zoom:50%;" />로 업데이트 한 뒤 다시 비용 함수 값을 계산합니다.
- Step 3 : 비용 함수의 값이 감소했으면 Step 2를 반복합니다. 더 이상 감소하지 않는 경우, 그 때의 w1, w0를 구하고 반복을 중지합니다.

<br>

<br>

지금까지 정리한 내용을 토대로 경사 하강법을 파이썬 코드로 구현해 보겠습니다.

y = 4X + 6을 근사하기 위한 100개의 데이터 세트를 만들고, 경사 하강법을 이용해 회귀 계수 w1, w0를 도출해 보겠습니다.

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

np.random.seed(0)
#y = 4X +6을 근사(w1=4, w0=6) 
X = 2 * np.random.rand(100, 1)
y = (6 + 4 * X) + np.random.randn(100, 1)  #4X + 6에다가 난수를 더함 (y = 4X + 6을 중심으로 무작위로 퍼지도록)

#X, y 데이터 세트 산점도로 시각화
plt.scatter(X, y)
```

![image](https://user-images.githubusercontent.com/76269316/127773617-a155e421-3077-4518-b806-714411a8a4fe.png)

**get_weight_updates()**

```python
#w1과 w0를 업데이트할 w1_update, w0_update를 반환
def get_weight_updates(w1, w0, X, y, learning_rate=0.01):
    N = len(y)
    #먼저 w1_update, w0_update를 각각 w1, w0의 shape와 동일한 크기를 가진 0 값으로 초기화
    w1_update = np.zeros_like(w1)
    w0_update = np.zeros_like(w0)
    #예측 배열 계산하고 예측과 실제 값의 차이 계산
    y_pred = np.dot(X, w1.T) + w0
    diff = y - y_pred
    
    #w0_update를 dot 행렬 연산 구하기 위해 모두 1 값을 가진 행렬 생성
    w0_factors = np.ones((N, 1))
    
    #w1과 w0을 업데이트할 w1_update와 w0_update 계산
    w1_update = -(2/N)*learning_rate*(np.dot(X.T, diff))
    w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T, diff))
    
    return w1_update, w0_update
```

100개 데이터 X(1, 2, ···, 100)가 있다면 예측값은 (w0 + X(1)\*w1) + (w0 + X(2)\*w1) + ··· +(w0 + X(100)\*w1)이며, 이는 입력 배열 X와 w1 배열의 내적에 w0를 더한 것과 동일합니다.

따라서 입력 배열 X 값에 대한 예측 배열 y_pred는 np.dot(X, w1.T) + w0로 구합니다.



이후 w1_update로 <img src="https://user-images.githubusercontent.com/76269316/127772619-1399a8dd-c496-4145-8620-632e1ce22c9d.png" alt="image" style="zoom: 50%;" />를 w0_update로 <img src="https://user-images.githubusercontent.com/76269316/127772628-e2daa798-26f6-496f-b4b4-1e3d520ac1e0.png" alt="image" style="zoom:50%;" />를 계산한 뒤 이를 반환합니다.

<br>

**gradient_descent_steps()**

```python
#입력 인자 iters로 주어진 횟수만큼 반복적으로 w1과 w0를 업데이트 적용함
def gradient_descent_steps(X, y, iters=10000):
    #w0와 w1을 모두 0으로 초기화
    w0 = np.zeros((1, 1))
    w1 = np.zeros((1, 1))
    
    #인자로 주어진 iters만큼 반복적으로 get_weight_updates() 호출해 w1, w0 업데이트 수행
    for ind in range(iters):
        w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01)
        w1 = w1 - w1_update
        w0 = w0 - w0_update

    return w1, w0
```

get_weight_updates()에서 구한 w1_update, w0_update를 가지고,  w1과 w0를 업데이트 해줍니다.

<br>

**get_cost()**

```python
def get_cost(y, y_pred):
    N = len(y)
    cost = np.sum(np.square(y - y_pred) / N)
    return cost

w1, w0 = gradient_descent_steps(X, y, iters=1000)
print('w1:{0:.3f} w0:{1:.3f}'.format(w1[0, 0], w0[0, 0]))
y_pred = w1[0, 0] * X + w0
print('Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))
```

![image](https://user-images.githubusercontent.com/76269316/127787609-17f89382-41b9-40ae-a082-14289de0dacc.png)

최종적으로 예측값과 실제값의 RSS 차이를 get_cost() 함수를 통해 계산했습니다.



정리하자면, grdient_descent_stpes() 호출 → get_weight_updates() 1,000번 호출하면서 w1, w0를 업데이트 → 최종 w1, w0로 y_pred 값 계산 → get_cost()에서 RSS 계산



앞에서 구한 y_pred에 기반해 회귀선을 그렸습니다.

```python
plt.scatter(X, y)
plt.plot(X, y_pred)
```

![image](https://user-images.githubusercontent.com/76269316/127787620-a7e49b35-609d-4085-ad64-a01b7b9eb116.png)

<br>

일반적으로 경사 하강법은 모든 학습 데이터에 대해 반복적으로 비용함수 최소화를 위한 값을 업데이트하기 때문에 수행시간이 매우 오래 걸립니다.

그 때문에 실전에서는 대부분 확률적 경사 하강법(Stochastic Gradient Descent) 또는 미니 배치 확률적 경사 하강법을 사용합니다.

확률적 경사 하강법은 전체 입력 데이터 중 일부 데이터만을 이용해 w가 업데이트 되는 값을  계산하므로 경사 하강법에 비해 빠른 속도를 보장합니다.



확률적 경사 하강법을 stochastic_gradient_descent_steps()로 구현했습니다.

**stochastic_gradient_descent_steps()**

```python
def stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000):
    w0 = np.zeros((1, 1))
    w1 = np.zeros((1, 1))
    prev_cost = 100000
    iter_index = 0
    
    for ind in range(iters):
        np.random.seed(ind)
        #전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이터를 추출해 sample_X, sample_y로 저장
        stochastic_random_index = np.random.permutation(X.shape[0])
        sample_X = X[stochastic_random_index[0:batch_size]]
        sample_y = y[stochastic_random_index[0:batch_size]]
        #랜덤하게 batch_size만큼 추출된 데이터 기반으로 w1_update, w0_update 계산 후 업데이트
        w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01)
        w1 = w1 - w1_update
        w0 = w0 - w0_update

    return w1, w0
```

<br>

```python
stochastic_random_index = np.random.permutation(X.shape[0])
sample_X = X[stochastic_random_index[0:batch_size]]
sample_y = y[stochastic_random_index[0:batch_size]]
```

전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이터를 추출해  이를 기반으로 w1_update, w0_update를 계산합니다.

<br>

**get_cost()**

```python
def get_cost(y, y_pred):
    N = len(y)
    cost = np.sum(np.square(y - y_pred) / N)
    return cost

w1, w0 = gradient_descent_steps(X, y, iters=1000)
print('w1:{0:.3f} w0:{1:.3f}'.format(w1[0, 0], w0[0, 0]))
y_pred = w1[0, 0] * X + w0
print('Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))
```

<br>

```python
w1, w0 = stochastic_descent_stpes(X, y, batch_size=10, iters=1000)
print('w1:', round(w1[0, 0], 3), 'w0:', round(w0[0, 0], 3))
y_pred = w1[0, 0] + X + w0
print('Stochastic Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))
```

![image](https://user-images.githubusercontent.com/76269316/127787998-be526376-5cda-497d-b54e-62ea4914dffb.png)

확률적 경사 하강법으로 구한  w0, w1 결과는 경사 하강법과 크게 차이가 없는 것을 볼 수 있습니다.

따라서 큰 데이터를 처리할 경우 일반적으로 확률적 경사 하강법을 사용합니다.

<br>

<br>
지금까지는 독립변수가 1개(피처가 1개)인 단순 선형 회귀에서 경사 하강법을 적용했습니다.

피처가 여러 개인 경우도 1개인 경우를 확장해 유사하게 도출할 수 있습니다.



피처가 M개 (X1, X2, ···, X100)이 있다면 예측 회귀식은 다음과 같이 되게 됩니다.

y ̂ = w0 + w1\*X1 + w2\*X2 + ··· + w100\*X100

이는 np.dot(X, w1.T) + w0로 계산할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/127788803-33a8f457-2ddc-44e9-b47f-d9e081ebb197.png)

Xmat의 맨 처음 열에 모든 데이터 값이 1인 피처 Feat0을 추가하면 좀 더 간결하게 y ̂ = Xmat + W^T 회귀 예측식을 도출할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/127788813-6ab1e8ea-1ec0-487d-93d3-8abc095d9ff4.png)

<br>

<br>

<br>

### 사이킷런 LinearRegression을 이용한 보스턴 주택 가격 예측

##### LinearRegression 클래스 - Ordinary Least Squares

LinearRegression 클래스는 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화해 OLS(Ordinary Least Squares) 추정 방식으로 구현한 클래스입니다.

Ordinary Least Squares 기반 회귀 계수 계산은 입력 피처의 독립성에 많은 영향을 받습니다.

피처 간의 상관관계가 높은 경우 분산이 매우 커져서 오류에 민감해지는데 이를 다중 공선성(multi-collinearity) 문제라고 합니다.

일반적으로 상관관계가 높은 피처가 많은 경우 독립적인 중요 피처만 남기고 제거하거나 규제를 적용합니다.



LinearRegression 클래스는 fit() 메소드로 X, y 배열을 입력 받으면 회귀 계수(Coefficients)인 W를 coef_ 속성에 저장합니다.

|                        입력 파라미터                         |                             속성                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| fit_intercept : 불린 값으로, 디폴트는 True<br />Intercept(절편) 값을 계산할 것인지를 지정, False로 지정하면 intercept가 사용되지 않고 0으로 지정됨<br /><img src="https://user-images.githubusercontent.com/76269316/127802879-6ed30a22-1739-4da2-b1ea-8688453556e1.png" alt="image" style="zoom:50%;" /> <img src="https://user-images.githubusercontent.com/76269316/127802913-14bc85eb-449c-4841-afe8-bf48a46d32c6.png" alt="image" style="zoom:50%;" /><br />normalize : 불린 값으로, 디폴트는 False<br />fit_intercept가 False인 경우 이 파라미터가 무시됨<br />True이면 회귀 수행 전에 입력 데이터 세트를 정규화함 | coef_ : fit() 메소드를 수행했을 때 회귀 계수가 배열 형태로 저장하는 속성 (Shape는 (Target 값 개수, 피처 개수))<br />intercept_ : intercept 값 |

<br>

<br>

##### 회귀 평가 지표

회귀의 평가를 위한 지표는 실제 값과 회귀 예측값의 차이 값을 기반으로 한 지표가 중심입니다.

실제 값과 예측값의 차이를 그냥 더하면 +와 -가 섞여 오류가 상쇄되기 때문에 오류의 절댓값 평균이나 제곱, 또는 제곱한 뒤 다시 루트를 씌운 평균값을 구합니다.

| 평가 지표 |                             설명                             |                             수식                             |
| :-------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
|    MAE    | Mean Absolute Error(MAE)이며 실제 값과 예측값의 차이를 절댓값으로 변환해 평균한 것 | ![image](https://user-images.githubusercontent.com/76269316/127806391-a69abab8-dfa8-42af-8e67-8c0ef451e1c4.png) |
|    MSE    | Mean Squared Error(MSE)이며 실제 값과 예측값의 차이를 제곱해 평균한 것 | ![image](https://user-images.githubusercontent.com/76269316/127808500-55f098d7-a0a1-4169-8651-396be98e695d.png) |
|   RMSE    | MSE 값은 오류의 제곱을 구하므로 실제 오류 평균보다 커지게 되므로  MSE에 루트를 씌운 것이 RMSE입니다. | ![image](https://user-images.githubusercontent.com/76269316/127808534-fb6cb217-8226-4725-a655-daf581124fab.png) |
|    R^2    | 분산 기반으로 예측 성능을 평가합니다.<br />실제 값의 분산 대비 예측값의 분산 비율을 지표로 하며, 1에 가까울수록 예측 정확도가 높습니다. | ![image](https://user-images.githubusercontent.com/76269316/127808567-169c67bf-cf86-4fc3-ae3c-7ecf7f07d566.png) |



다음은 각 평가 방법에 대한 사이킷런의 API및 cross_val_score나 GridSearchCV에서 평가 시 사용되는 scoring 파라미터의 적용 값입니다.

| 평가 방법 |   사이킷런 평가 지표 API    |   Scoring 함수 적용 값    |
| :-------: | :-------------------------: | :-----------------------: |
|    MAE    | metrics.mean_absolute_error | 'neg_mean_absolute_error' |
|    MSE    | metrics.mean_squared_error  | 'neg_mean_squared_error'  |
|    R^2    |      metrics.r2_score       |           'r2'            |

사이킷런은 RMSE를 제공하지 않기 때문에 RMSE를 구하기 위해서는 직접 MSE에 제곱근을 씌워서 계산해야 합니다.



※cross_val_score, GridSearchCV 같은 scoring 함수에 회귀 평가 지표를 적용할 때 주의해야 하는 점이 있습니다.

사이킷런의 scoring 함수는 score 값이 클수록 좋은 평과 결과로 자동 평가하는데, 실제 값과 예측값의 오류 차이를 기반으로 하는 회귀 평가 지표의 경우 값이 커지면 오히려 나쁜 모델이라는 의미를 갖습니다.

따라서 -1을 원래 평가 지표 값에 곱해 음수(negative)를 만들어 작은 오류 값이 더 큰 숫자로 인식하게 했습니다.

따라서 scoring 파라미터 값이 의미하는 것은 원래 값에 -1을 곱한 것이라는 점에 주의가 필요합니다.

<br>

<br>

##### LinearRegression을 이용해 보스턴 주택 가격 회귀 구현

사이킷런에 내장된 데이터 세트인 보스턴 주택 가격 데이터 세트에 LinearRegression 클래스를 사용해 선형 회귀 모델을 만들어보겠습니다.

사이킷런에 내장된 보스턴 주택 가격 데이터 세트의 피처는 다음과 같습니다.



- CRIM : 지역별 범죄 발생률
- ZN : 25,000평방피트를 초과하는 거주 지역의 비율
- INDUS : 비상업 지역 넓이 비율
- CHAS : 찰스강에 대한 더미 변수(강의 경계에 위치한 경우는 1, 아니면 0)
- NOX : 일산화질소 농도
- RM : 거주할 수 있는 방의 개수
- AGE : 1940년 이전에 건축된 소유 주택의 비율
- DIS : 5개 주요 고용센터까지의 가중 거리
- RAD : 고속도로 접근 용이도
- TAX : 10,000달러당 재산세율
- PTRATIO : 지역의 교사와 학생 수 비율
- B : 지역의 흑인 거주 비율
- LSTAT : 하위 계층의 비율
- MEDV : 본인 소유의 주택 가격(중앙값)



```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from scipy import stats
from sklearn.datasets import load_boston
%matplotlib inline

#boston 데이터 세트 로드
boston = load_boston()

#boston 데이터 세트 DataFrame 변환
bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)

#boston 데이터 세트의 target 배열은 주택 가격
bostonDF['PRICE'] = boston.target  #PRICE 컬럼으로 DataFrame에 추가
print('Boston 데이터 세트 크기 : ', bostonDF.shape)
bostonDF.head()
```

![image](https://user-images.githubusercontent.com/76269316/127813698-1261b264-cd39-47f7-ae4a-2590f57b9c91.png)



Null 값은 없고, 모두 float형입니다.

```python
bostonDF.info()
```

![image](https://user-images.githubusercontent.com/76269316/127813748-db5cd37c-02ab-4689-afa2-8803c6ea3108.png)



각 컬럼이 회귀 결과에 미치는 영향을 확인하기 위해 시각화해 알아보겠습니다.

```python
#2개의 행과 4개의 열을 갖는 subplots를 이용. axs는 2X4개의 ax를 가짐
fig, axs = plt.subplots(figsize=(16, 8), nrows=2, ncols=4)
lm_features = ['RM', 'ZN', 'INDUS', 'NOX', 'AGE', 'PTRATIO', 'LSTAT', 'RAD']
for i, feature in enumerate(lm_features):
    row = int(i / 4)
    col = i%4
    #시본의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현
    sns.regplot(x=feature, y='PRICE', data=bostonDF, ax=axs[row][col])
```

![image](https://user-images.githubusercontent.com/76269316/127814245-6f9f9ecb-4b00-4205-a673-2a0465d5c73e.png)



다른 컬럼보다 RM과 LSTAT의 PRICE 영향도가 가장 두드러진 것을 확인할 수 있습니다.

RM(방 개수)은 양의 방향 선형성(Positive Linearity) 즉, 방의 크기가 클수록 가격이 증가하고

LSTAT(하위 계층의 비율)은 음의 방향 선형성(Negative Linearity), LSTAT이 적을수록 PRICE가 증가하는 것을 볼 수 있습니다.

<br>

<br>

이제 LinearRegression 클래스를 사용해 보스턴 주택 가격의 회귀 모델을 만들어 보겠습니다.

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)

X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.3, random_state=156)

#선형 회귀 OLS로 학습/예측/평가 수행
lr = LinearRegression()
lr.fit(X_train, y_train)
y_preds = lr.predict(X_test)
mse = mean_squared_error(y_test, y_preds)
rmse = np.sqrt(mse)  #MSE에 제곱근을 씌워 RMSE 계산

print('MSE : {0:.3f}, RMSE : {1:.3f}'.format(mse, rmse))
print('Variance score : {0:.3f}'.format(r2_score(y_test, y_preds)))
```

![image](https://user-images.githubusercontent.com/76269316/127815865-bb50a127-2d52-4a13-ac7f-2ae4923c28ce.png)

<br>

LinearRegression 객체의 intercept\_, coef\_ 속성을 확인해 보겠습니다.

```python
print('절편 값:', lr.intercept_)
print('회귀 계수값:', np.round(lr.coef_, 1))
```

![image](https://user-images.githubusercontent.com/76269316/127816037-6060b747-fe6d-46eb-883a-93eabf3fbc64.png)

<br>

coef_ 속성은 회귀 계수 값만 갖고 있으므로 피처별로 매핑한 다음, 높은 값 순으로 출력해 보겠습니다.

```python
#회귀 계수를 큰 값 순으로 정렬하기 위해 Serie로 생성
coeff = pd.Series(data=np.round(lr.coef_, 1), index=X_data.columns)
coeff.sort_values(ascending=False)
```

![image](https://user-images.githubusercontent.com/76269316/127816299-3d1a1272-2c4f-4c06-8823-f7e05c99cc8d.png)

<br>

<br>

이번에는 cross_val_score()를 이용해 교차 검증으로 5개의 폴드 세트에서 MSE와 RMSE를 측정해 보겠습니다.

**이 때 neg_mean_squared 지정시 반환되는 수치 값이 음수 값이라는 점에 주의해야 합니다.**

```python
from sklearn.model_selection import cross_val_score

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)
lr = LinearRegression()

#cross_val_score()로 5개의 폴드 세트로 MSE를 구한 뒤 이를 기반으로 다시 RMSE를 구함
neg_mse_scores = cross_val_score(lr, X_data, y_target, scoring='neg_mean_squared_error', cv=5)
rmse_scores = np.sqrt(-1 * neg_mse_scores)
avg_rmse = np.mean(rmse_scores)

#cross_val_score(scoring="neg_mean_squared_error")로 반환된 값은 모두 음수
print('5 folds 개별 Negative MSE Scores : ', np.round(neg_mse_scores, 2))
print('5 folds 개별 RMSE scores : ', np.round(rmse_scores, 2))
print('5 folds 평균 RMSE : {0:.3f}'.format(avg_rmse))
```

![image](https://user-images.githubusercontent.com/76269316/127816887-10e07a56-3385-4c88-83b6-af04fa2be672.png)

<br>

<br>

<br>

### 다항 회귀와 과(대)적합/과소적합 이해

##### 다항 회귀 이해

이전까지 설명한 회귀는 y = w0 + w1\*X1 + w2\*X2 + ··· + wn\*Xn 과 같이 독립변수(feature)와 종속변수(target)의 관계가 일차 방정식 형태로 표현됐습니다.

하지만 세상 모든 관계를 직선으로만 표현할 수 없습니다.

일차 방정식 형태가 아닌 다음과 같이 2차, 3차 방정식과 같은 다항식으로 표현되는 것을 다항(Polynomial) 회귀라고 합니다.

y = w0 + w1\*X1 + w2\*X2 + w3 \*X1\*X2 + w4\*X1^2 + w5\*X2^2

<br>

주의할 점이, **다항 회귀는 선형 회귀**라는 점입니다.

회귀에서 선형 회귀/비선형 회귀를 나누는 기준은 회귀 계수가 선형/비선형인지에 따른 것이지 독립변수의 선형/비선형 여부와는 무관합니다.

<br>

사이킷런에서는 다항 회귀를 위한 클래스를 명시적으로 제공하지 않습니다.

하지만, 다항 회귀 역시 선형 회귀이기 때문에 비선형 함수를 선형 모델에 적용시키는 방법을 사용할 수 있습니다.

<br>

사이킷런은 PolynomialFeatures 클래스를 통해 단항식 피처를 degree에 해당하는 다항식 피처로 변환합니다.

아래 코드는 단항 계수 피처 [x1, x2]를 2차 다항 계수 [1, x1, x2, x1^2, x1x2, x2^2]으로 변환하는 예제 코드입니다.

```python
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

#다항식으로 변환한 단항식 생성, [[0, 1], [2, 3]]의 2X2 행렬 생성
X = np.arange(4).reshape(2, 2)
print('일차 단항식 계수 피처:\n', X)

#degree = 2인 2차 다항식으로 변환하기 위해 PolynomialFeatures를 이용해 변환
poly = PolynomialFeatures(degree=2)
poly.fit(X)
poly_ftr = poly.transform(X)
print('변환된 2차 다항식 계수 피처:\n', poly_ftr)
```

![image](https://user-images.githubusercontent.com/76269316/127819411-48ecd23e-4731-4904-a844-8a91a70beccf.png)

첫 번째 입력 단항 계수 피처 [x1=0, x2=1]은 [1, x1=0, x2=1, x1^2=0, x1x2=0, x2^2=1] 형태인 [1, 0, 1, 0, 0, 1]로 변환됐습니다.

두 번째 입력 단항 계수 피처도 마찬가지로 변환된 것을 확인할 수 있습니다.

이렇게 변환된 다항식 피처에 선형 회귀를 적용해 다항 회귀를 구현합니다.

<br>

<br>

일차 단항식 계수를 삼차 다항식 계수로 변환하고 이를 선형 회귀에 적용하면 다항 회귀로 구현됩니다.

```python
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

def polynomial_func(X):
    y = 1 + 2*X[:, 0] + 3*X[:, 0]**2 + 4*X[:, 1]**3
    return y

X = np.arange(4).reshape(2, 2)
print('일차 단항식 계수 feature:\n', X)
y = polynomial_func(X)
print('삼차 다항식 결정값:\n', y)

#3차 다항식 변환
poly_ftr = PolynomialFeatures(degree=3).fit_transform(X)
print('3차 다항식 계수 feature:\n', poly_ftr)

#LinearRegression에 3차 다항식 계수 feature와 3차 다항식 결정값으로 학습 후 회귀 계수 확인
model = LinearRegression()
model.fit(poly_ftr, y)
print('Polynomial 회귀 계수\n', np.round(model.coef_, 2))
print('Polynomial 회귀 shape: ', model.coef_.shape)
```

![image](https://user-images.githubusercontent.com/76269316/127821005-4d2ac84c-47cc-42ee-b431-444f2ba65540.png)

일차 단항식 계수 피처를 3차 다항식으로 변환한 다음 LinearRegression을 적용해 10개의 회귀 계수가 도출됐습니다.

원래 다항식 1 + 2X1 + 3X1^2 + 4X2^3의 계수 값인 [1, 2, 0, 3, 0, 0, 0, 0, 0, 4]와는 차이가 있지만 다항 회귀로 근사하고 있는 것을 볼 수 있습니다.

이처럼 사이킷런은 PolynomialFeatures로 피처를 변환한 뒤에 LinearRegression 클래스로 다항 회귀를 구현합니다.



아래는 간결하게 Pipeline 객체를 사용해 한 번에 다항 회귀를 구현한 코드입니다.

+파이프라인으로 결합된 모형은 원래 모형이 갖는 fit, predict 메소드를 가지며 각 메소드가 호출되면 파이프라인의 각 객체에 대해서 호출합니다.

파이프라인 fit() 메소드 호출 시 fit_transform() 메소드를 순서대로 호출한 결과를 다음 단계의 입력으로 전달합니다.

마지막 단계에서는 fit() 메소드만 호출합니다.

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
import numpy as np

def polynomial_func(X):
    y = 1 + 2*X[:, 0] + 3*X[:, 0]**2 + 4*X[:, 1]**3
    return y

#Pipeline 객체로 Streamline하게 Polynomial Feature 변환과 Linear Regression 연결
model = Pipeline([('poly', PolynomialFeatures(degree=3)), ('linear', LinearRegression())])
X = np.arange(4).reshape(2, 2)
y = polynomial_func(X)

model = model.fit(X, y)

print('Polynomial 회귀 계수\n', np.round(model.named_steps['linear'].coef_, 2))
```

![image](https://user-images.githubusercontent.com/76269316/127822557-da394654-7d75-4bae-a508-d1773c7321f2.png)

<br>

<br>

##### 다항 회귀를 이용한 과소적합 및 과적합 이해

다항회귀는 피처의 직선적 관계가 아닌 복잡한 다항 관계를 모델링할 수 있습니다.

하지만 다항식의 차수가 높아질수록 학습 데이터에만 너무 맞춘 학습이 이뤄져서 테스트 데이터에서는 예측 정확도가 떨어지게 됩니다.

즉, 차수가 높아질수록 과적합 문제가 발생합니다.

<br>

다음은 사이킷런 홈페이지에 있는 다항 회귀 과소적합, 과적합 문제를 잘 보여주는 예제입니다.

[https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#shpx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#shpx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py)



학습 데이터는 30개의 임의의 데이터인 X, X의 코사인 값에 약간의 잡음 변동 값을 더한 target인 y로 구성됩니다.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
%matplotlib inline

#임의의 값으로 구성된 X 값에 대해 코사인 변환 값을 반환
def true_fun(X):
    return np.cos(1.5 * np.pi * X)

#X는 0부터 1까지 30개 임의의 값을 순서대로 샘플링한 데이터
np.random.seed(0)
n_samples = 30
X = np.sort(np.random.rand(n_samples))

#y값은 코사인 기반 true_fun()에서 약간의 노이즈 변동 값을 더한 값
y = true_fun(X) + np.random.randn(n_samples) * 0.1
```



다항식 차수를 각각 1, 4, 15로 변경하면서 예측 결과를 비교해보겠습니다.

다항식 차수별로 학습을 수행한 뒤 cross_val_score()로 MSE 값을 구해 차수별 예측 성능을 평가합니다.

그리고 0부터 1까지 균일하게 구성된 100개 테스터용 데이터 세트를 이용해 차수별 회귀 예측 곡선을 그려보겠습니다.

```python
plt.figure(figsize=(14, 5))
degrees = [1, 4, 15]

#다항 회귀의 차수(degree)를 1, 4, 15로 변화시키면서 비교
for i in range(len(degrees)):
    ax = plt.subplot(1, len(degrees), i + 1)
    plt.setp(ax, xticks=(), yticks=())
    
    #개별 degree별로 Polynomial 변환
    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)  #include_bias=True 설정시 편향을 위한 변수 X0(1)가 추가됨
    linear_regression = LinearRegression()
    pipeline = Pipeline([('polynomial_features', PolynomialFeatures(degree=degrees[i])), ('linear_regression', LinearRegression())])
    pipeline.fit(X.reshape(-1, 1), y)
    
    #교차 검증으로 다항 회귀 평가
    scores = cross_val_score(pipeline, X.reshape(-1, 1), y, scoring="neg_mean_squared_error", cv=10)
    #Pipeline을 구성하는 세부 객체를 접근하는 named_steps['객체명]을 이요해 회귀계수 추출
    coefficients = pipeline.named_steps['linear_regression'].coef_
    print('\nDegree {0} 회귀 계수는 {1}입니다.'.format(degrees[i], np.round(coefficients, 2)))
    print('Degree {0} MSE는 {1}입니다.'.format(degrees[i], -1 * np.mean(scores)))
    
    #0부터 1까지 테스트 데이터 세트를 100개로 나눠 예측 수행
    #테스트 데이터 세트에 회귀 예측을 수행하고 예측 곡선과 실제 곡선을 그려서 비교
    X_test = np.linspace(0, 1, 100)
    #예측값 곡선
    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model")  #X_test[:, np.newaxis] : X_test에 새로운 축 추가
    #실제 값 곡선
    plt.plot(X_test, true_fun(X_test), '--', label="True function")
    plt.scatter(X, y, edgecolor='b', s=20, label="Samples")

    plt.xlabel("x"); plt.ylabel("y"); plt.xlim((0, 1)); plt.ylim((-2, 2)); plt.legend(loc="best")
    plt.title("Degree {}\nMSE = {:.2e}(+/- {:.2e})".format(degrees[i], -scores.mean(), scores.std()))

plt.show()
```

실선으로 표현된 예측 곡선은 다항 회귀 예측 곡선입니다.

점선으로 표현된 곡선은 실제 데이터 세트 X, Y의 코사인 곡선입니다.

학습 데이터는 0부터 1까지 30개의 임의의 X 값과 그에 따른 코사인 Y 값에 잡음을 변동 값으로 추가해 구성했으며 MSE(MeanSquaredError) 평가는 학습 데이터를 10개의 교차 검증 세트로 나누어 측정해서 평균한 것입니다.

![image](https://user-images.githubusercontent.com/76269316/127835695-862fcbe1-ddf4-4990-b1f9-b402315cd3f6.png)

- Degree 1 예측 곡선은 단순한 직선으로 단순 선형 회귀와 같습니다.
  예측 곡선이 학습 데이터의 패턴을 제대로 반영하지 못하고 있는 과소적합 모델이 됐습니다. MSE는 약 0.407입니다.
- Degree 4 예측 곡선은 실제 데이터 세트와 유사한 모습으로, 변동하는 잡음까지 예측하지는 못했지만 학습 데이터 세트를 비교적 잘 반영해 테스트 데이터를 잘 예측한 곡선을 가진 모델이 됐습니다.
  MSE 값은 약 0.043으로 가장 뛰어난 예측 성능을 보입니다.
- Degree 15 예측 곡선은 MSE 값이 181821493이 될 정도로  큰 오류값이 발생했습니다.
  예측 곡선을 보면 데이터 세트의 변동 잡음까지 지나치게 반영한 결과, 학습 데이터 세트만 정확히 예측하고 테스트 값의 실제 곡선과는 완전히 다른 과적합 모델이 됐습니다.

<br>

<br>

##### 편향-분산 트레이드오프(Bias-Variance Trade Off)

편향-분산 트레이드오프는 머신러닝이 극복해야할 가장 중요한 문제 중 하나입니다.

![image](https://user-images.githubusercontent.com/76269316/127836814-7490d1d3-6c73-4eb0-a6e6-c8aa628fcbb1.png)

[이미지 출처](http://scott.fortmann-roe.com/docs/BiasVariance.html)



위 '양궁 과녁' 그래프는 편향과 분산의 고/저 의미를 직관적으로 잘 표현하고 있습니다.

상단 왼쪽 그림은 **저편향/저분산(Low Bias/Low Variance)**입니다.

예측 결과가 실제 결과에 매우 근접하면서 예측 변동이 크지 않고 특정 부분에 집중돼 있습니다. (아주 드묾)



상단 오른쪽 그림은 **저편향/고분산(Low Bias/High Variance)**입니다.

예측 결과가 실제 결과에 비교적 근접하지만, 예측 결과가 실제 결과를 중심으로 꽤 넓은 부분에 분포돼 있습니다.



하단 왼쪽 그림은 **고편향/저분산(High Bias/Low Variance)**로 정확환 결과에서 벗어나면서도 예측 부분이 특정 부분에 집중돼 있습니다.

하단 오른쪽 그림은 **고편향/고분산(High Bias/High Variance)**로 정확환 예측 결과를 벗어나면서도 넓은 부분에 분포돼 있습니다.

<br>

일반적으로 편향과 분산은 한 쪽이 높으면 한 쪽이 낮아지는 경향이 있습니다.

![image](https://user-images.githubusercontent.com/76269316/127837911-052f9eb9-5a96-43ba-a688-01c279dc9a66.png)

[이미지 출처](http://scott.fortmann-roe.com/docs/BiasVariance.html)



편향이 너무 높으면 전체 오류가 높습니다.

편향을 점점 낮추면 동시에 분산이 높아지고 전체 오류는 낮아지게 됩니다.

편향을 낮추고 분산을 높이면서 전체 오류가 가장 낮아지는 '골디락스' 지점을 통과하면 오류 값이 오히려 증가하면서 예측 성능이 다시 저하되게 됩니다.

편향과 분산이 서로 트레이드오프를 이루면서 오류 cost 값이 최대로 낮아지는 모델을 구축하는 것이 중요합니다.



### 규제 선형 모델 - 릿지, 라쏘, 엘라스틱넷

##### 규제 선형 모델의 개요

이전까지 선형 모델의 비용 함수는 RSS를 최소화하는, 즉 실제 값과 예측값의 차이를 최소화하는 것만 고려했습니다.

그러다 보니 학습 데이터에 지나치게 맞추게 되고, 회귀 계수가 쉽게 커졌습니다.

이럴 경우 오히려 변동성이 심해져 테스트 데이터 세트에서는 예측 성능이 저하되기 쉽습니다.

따라서 비용 함수는 학습 데이터의 잔차 오류 값을 최소로 하는 RSS 최소화 방법과 과적합을 방지하기 위해 회귀 계수 값이 커지지 않도록 하는 방법이 서로 균형을 이뤄야 합니다.

▶ **최적 모델을 위한 cost 함수 구성요소 = 학습 데이터 잔차 오류 최소화 + 회귀계수 크기 제어**



<img src="https://user-images.githubusercontent.com/76269316/127839737-57516f40-a4e5-41a6-9cf7-3e5eba93c183.png" alt="image" style="zoom: 80%;" />

여기서 alpha는 학습 데이터 적합 정도와 회귀 계수 값의 크기 제어를 수행하는 튜닝 파라미터입니다.

alpha가 0(또는 매우 작은 값)이라면 비용 함수 식은 기존과 동일한 Min(RSS(W))가 될 것입니다.

alpha가 무한대(또는 매우 큰 값)라면 비용 함수 식은 RSS(W)에 비해 <img src="https://user-images.githubusercontent.com/76269316/127840084-1c291649-85a3-428b-8dfc-03f0601956b3.png" alt="image" style="zoom:50%;" /> 값이 너무 커지게 되므로 W 값을 0(또는 매우 작게)으로 만들어야 cost가 최소화되는 비용 함수 목표를 달성할 수 있습니다.

즉, alpha 값을 크게 하면 비용 함수는 회귀 계수 W 값을 작게 해 과적합을 개선할 수 있으며 alpha 값을 작게 하면 회귀 계수 W 값이 커져도 어느 정도 상쇄가 가능하므로 학습 데이터 적합을 더 개선할 수 있습니다.

<br>

이렇게 비용 함수에 alpha 값으로 페널티를 부여해 회귀 계수 값의 크기를 감소시켜 과적합을 개선하는 방식을 규제(Regularization)라고 합니다.

규제는 크게 L2 방식, L1 방식으로 구분됩니다.

<br>

L2 규제는 W의 제곱에 대해 페널티를 부여하는 방식을 말합니다.

L2 규제를 적용한 회귀를 릿지(Ridge) 회귀라고 합니다.

<br>

L1 규제는 W의 절댓값에 대해 페널티를 부여하는 방식입니다.

L1 규제를 적용한 회귀를 라쏘(Lasso) 회귀 라고 합니다.

L1 규제를 적용하면 영향력이 크지 않은 회귀 계수 값을 0으로 변환합니다. (따라서 피처 선택 기능으로도 불립니다.)

<br>

<br>

##### 릿지 회귀

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

#boston 데이터 세트 로드
boston = load_boston()

#boston 데이터 세트 DataFrame 변환
bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)

#boston 데이터 세트의 target 배열은 주택 가격
bostonDF['PRICE'] = boston.target  #PRICE 컬럼으로 DataFrame에 추가

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)

#alpha=10으로 설정해 릿지 회귀 수행
ridge = Ridge(alpha=10)  #alpha : L2 규제 계수
neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring="neg_mean_squared_error", cv=5)
rmse_scores = np.sqrt(-1 * neg_mse_scores)
avg_rmse = np.mean(rmse_scores)
print('5 folds 개별 Negative MSE scores: ', np.round(neg_mse_scores, 3))
print('5 folds 개별 RMSE scores: ', np.round(rmse_scores, 3))
print('5 folds 평균 RMSE: {0:.3f}'.format(avg_rmse))
```

|                       LinearRegression                       |                            Ridge                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/127816887-10e07a56-3385-4c88-83b6-af04fa2be672.png) | ![image](https://user-images.githubusercontent.com/76269316/127842554-53c361cd-b3a3-4cab-8015-e6d7d7f93c5c.png) |

위에서 다룬 [규제가 없는 LinearRegression](https://seominseok4834.github.io/machine%20learning/5.regression/#linearregression%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%B4-%EB%B3%B4%EC%8A%A4%ED%84%B4-%EC%A3%BC%ED%83%9D-%EA%B0%80%EA%B2%A9-%ED%9A%8C%EA%B7%80-%EA%B5%AC%ED%98%84)보다 뛰어난 예측 성능을 확인할 수 있습니다.



이번에는 alpha 값을 0, 0.1, 1, 10, 100으로 변화시키면서 RMSE와 회귀 계수 값의 변화를 살펴보겠습니다.

```python
#릿지에 사용될 alpha 파라미터 값 정의
alphas = [0, 0.1, 1, 10, 100]

#alphas list 값을 반복하면서 alpha에 따른 평균 rmse를 구함
for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    
    #cross_val_score를 이용해 5 폴드 평균 RMSE 계산
    neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring="neg_mean_squared_error", cv=5)
    avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
    print('alpha {0}일 때 5 folds의 평균 RMSE: {1:.3f}'.format(alpha, avg_rmse))
```

![image](https://user-images.githubusercontent.com/76269316/127843548-ec0dc0d5-c356-4e3e-a6f7-fa192eb69c5c.png)



```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

#각 alpha에 따른 회귀 계수 값을 시각화하기 위해 5개의 열로 된 맷플롯립 축 생성
fig, axs = plt.subplots(figsize=(18, 6), nrows=1, ncols=5)
#각 alpha에 따른 회귀 계수 값을 데이터로 저장하기 위한 DataFrame 생성
coeff_df = pd.DataFrame()

#alphas 리스트 값을 차례로 입력해 회귀 계수 값 시각화 및 데이터 저장. pos는 axis 위치 지정
for pos, alpha in enumerate(alphas):
    ridg = Ridge(alpha=alpha)
    ridge.fit(X_data, y_target)
    #alpha에 따른 피처별로 회귀 계수를 Sereis로 변환하고 이를 DataFrame 컬럼으로 추가
    coeff = pd.Series(data=ridge.coef_, index=X_data.columns)
    colname = 'alpha:' + str(alpha)
    coeff_df[colname] = coeff
    #막대 그래프로 각 alpha 값에서의 회귀 계수를 시각화(내림차순으로 정렬)
    coeff = coeff.sort_values(ascending=False)
    axs[pos].set_title(colname)
    axs[pos].set_xlim(-3, 6)
    sns.barplot(x=coeff.values, y=coeff.index, ax=axs[pos])
    
#for문 바깥에서 맷플롯립 show 호출 및 alpha에 따른 피처별 회귀 계수를 DataFrame으로 표시
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/127852493-b02e0476-0f00-4e0b-acfc-d7d4a4500a64.png)



```python
ridge_alphas = [0, 0.1, 1, 10, 100]
sort_column = 'alpha:' + str(ridge_alphas[0])
coeff_df.sort_values(by=sort_column, ascending=False)
```

![image](https://user-images.githubusercontent.com/76269316/127853090-a9a57fa1-24f2-443f-83b1-9dd3d19e30aa.png)

alpha 값을 계속 증가시킬수록 회귀 계수 값은 지속적으로 작아지는 것을 볼 수 있습니다.

하지만 릿지 회귀의 경우에는 회귀 계수를 0으로 만들지 않습니다.

<br>

<br>

##### 라쏘 회귀

W의 절댓값에 페널티를 부여하는 L1 규제를 선형 회귀에 적용한 것이 라쏘 회귀입니다.

L2 규제가 회귀 계수의 크기를 감소시키만 하는데 반해, L1 규제는 불필요한 회귀 계수를 0으로 만들고 제거합니다.

이러한 측면에서 L1 규제는 적절한 피처만 회귀에 포함시키는 피처 선택의 특성이 있습니다.



뒤에서 엘라스틱넷도 동일하게 alpha 값을 변화시키면서 RMSE와 각 피처의 회귀 계수를 출력할 것이므로 이를 위한 별도의 함수 get_linear_reg_eval()이라는 메소드를 만들었습니다.

**get_linear_reg_eval()**

```python
from sklearn.linear_model import Lasso, ElasticNet

#alpha 값에 따라 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환
#model_name : 회귀 모델의 이름, params : alpha 값들의 리스트, X_data_n : 피처 데이터 세트, y_target_n : 타깃 데이터 세트
def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True, return_coeff=True):
    coeff_df = pd.DataFrame()
    if verbose : print('#######', model_name, '#######')
    for param in params:
        if model_name == 'Ridge': model = Ridge(alpha=param)
        elif model_name == 'Lasso': model = Lasso(alpha=param)
        elif model_name == 'ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)
        neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring="neg_mean_squared_error", cv=5)
        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
        print('alpha {0}일 때 5 폴드 세트 평균 RMSE: {1:.3f}'.format(param, avg_rmse))

        #cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습해 회귀 계수 추출
        model.fit(X_data_n, y_target_n)
        if return_coeff:
            #alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame 컬럼으로 추가
            coeff = pd.Series(data=model.coef_, index=X_data_n.columns)
            colname = 'alpha:' + str(param)
            coeff_df[colname] = coeff

    return coeff_df
```

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

#boston 데이터 세트 로드
boston = load_boston()

#boston 데이터 세트 DataFrame 변환
bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)

#boston 데이터 세트의 target 배열은 주택 가격
bostonDF['PRICE'] = boston.target  #PRICE 컬럼으로 DataFrame에 추가

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)

#라쏘에 사용될 alpha 파라미터 값을 정의하고 get_linear_reg_eval() 함수 호출
lasso_alphas = [0.07, 0.1, 0.5, 1, 3]
coeff_lasso_df = get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)
```

<img src="https://user-images.githubusercontent.com/76269316/127940942-551750f3-8aea-46e2-a5fb-e608a4dde47f.png" alt="image" style="zoom:50%;" />

[릿지 회귀](https://seominseok4834.github.io/machine%20learning/5.regression/#%EB%A6%BF%EC%A7%80-%ED%9A%8C%EA%B7%80)보다는 약간 떨어지는 수치지만, LinearRegression 평균보다는 향상됐습니다.



alpha 값에 따른 피처별 회귀 계수입니다.

```python
#반환된 coeff_lasso_df를 첫 번째 컬럼순으로 내림차순 정렬해 회귀 계수 DataFrame 출력
sort_column = 'alpha:' + str(lasso_alphas[0])
coeff_lasso_df.sort_values(by=sort_column, ascending=False)
```

![image](https://user-images.githubusercontent.com/76269316/127857887-011e278f-0b42-4316-a4e3-d290ecedf3e5.png)

alpha의 크기가 증가함에 따라 일부 피처의 회귀 계수가 0으로 바뀌는 것을 볼 수 있습니다.

회귀 계수가 0인 피처는 회귀 식에서 제외되면서 피처 선택의 효과를 얻을 수 있습니다.

<br>

<br>

##### 엘라스틱넷 회귀

엘라스틱넷(Elastic Net) 회귀는 L2 규제와 L1 규제를 결합한 회귀입니다.

따라서 엘라스틱넷 회귀 비용 함수의 목표는 <img src="https://user-images.githubusercontent.com/76269316/127858186-17199c92-9202-4855-8a37-be0304f4f285.png" alt="image" style="zoom:50%;" /> 식을 최소화하는 W를 찾는 것입니다.

라쏘 회귀는 서로 상관관계가 높은 피처들 중 중요 피처만을 선택하고 다른 피처들은  모두 회귀 계수를 0으로 만드는 성향이 강합니다.

이러한 성향으로 alpha 값에 따라 회귀 계수 값이 급격히 변동할 수 있는데, 이를 완화하기 위해 L2 규제를 라쏘 회귀에 추가한 것입니다.

때문에 엘라스틱넷 회귀는 수행시간이 상대적으로 오래걸립니다.



사이킷런은 ElasticNet 클래스를 통해 엘라스틱넷 회귀를 구현합니다.

ElasticNet 클래스 주요 생성 파라미터는 alpha와 l1_ratio입니다.

엘라스틱넷 규제는 a * L1 + b * L2로 정의될 수 있으며, 이 때 a는 L1규제 alpha 값, b는 L2 규제 alpha 값입니다.

따라서 ElasticNet 클래스의 alpha 값 파라미터는 a + b 입니다.

ElasticNet 클래스의 l1_ratio는 a / (a + b)입니다. 

l1_ratio가 0이면 a가 0이므로 L2 규제와 동일하고, l1_ratio가 1이면 b가 0이므로 L1 규제와 동일합니다.



위에서 만든 get_linear_reg_eval() 메소드를 사용해서 alpha 값에 따른 RMSE와 각 피처의 회귀 계수 값을 보도록하겠습니다.

단순히 alpha 값에 따른 변화만 살피기 위해 l1_ratio를 0.7로 고정했습니다.

```python
elif model_name == 'ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)
```



**get_linear_reg_eval()**

```python
from sklearn.linear_model import Lasso, ElasticNet

#alpha 값에 따라 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환
#model_name : 회귀 모델의 이름, params : alpha 값들의 리스트, X_data_n : 피처 데이터 세트, y_target_n : 타깃 데이터 세트
def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True, return_coeff=True):
    coeff_df = pd.DataFrame()
    if verbose : print('#######', model_name, '#######')
    for param in params:
        if model_name == 'Ridge': model = Ridge(alpha=param)
        elif model_name == 'Lasso': model = Lasso(alpha=param)
        elif model_name == 'ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)
        neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring="neg_mean_squared_error", cv=5)
        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
        print('alpha {0}일 때 5 폴드 세트 평균 RMSE: {1:.3f}'.format(param, avg_rmse))

        #cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습해 회귀 계수 추출
        model.fit(X_data_n, y_target_n)
        if return_coeff:
            #alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame 컬럼으로 추가
            coeff = pd.Series(data=model.coef_, index=X_data_n.columns)
            colname = 'alpha:' + str(param)
            coeff_df[colname] = coeff

    return coeff_df
```

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

#boston 데이터 세트 로드
boston = load_boston()

#boston 데이터 세트 DataFrame 변환
bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)

#boston 데이터 세트의 target 배열은 주택 가격
bostonDF['PRICE'] = boston.target  #PRICE 컬럼으로 DataFrame에 추가

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)

#엘라스틱넷에 사용될 alpha 파라미터 값들을 정의하고 get_linear_reg_eval() 함수 호출
#l1_ratio는 0.7로 고정
elastic_alphas = [0.07, 0.1, 0.5, 1, 3]
coeff_elastic_df = get_linear_reg_eval('ElasticNet', params=elastic_alpahas, X_data_n=X_data, y_target_n=y_target)
```

![image](https://user-images.githubusercontent.com/76269316/127859426-404b0ac0-0915-4c6f-9bab-86bdb6ecb018.png)

```python
#반환된 coeff_elastic_df를 첫 번째 컬럼순으로 내림차순 정렬해 회귀 계수 DataFrame 출력
sort_column = 'alpha:' + str(elastic_alphas[0])
coeff_elastic_df.sort_values(by=sort_column, ascending=False)
```

<img src="https://user-images.githubusercontent.com/76269316/127859754-75349d36-10cb-4424-9bc1-97918fd19607.png" alt="image" style="zoom: 80%;" />

alpha 값에 따른 피처들의 회귀 계수 값이 라쏘보다는 상대적으로 0이 되는 값이 적음을 볼 수 있습니다.

<br>

<br>

##### 선형 회귀 모델을 위한 데이터 변환

선형 회귀 모델과 같은 선형 모델은 일반적으로 피처와 타깃값 간에 선형 관계가 있다고 가정하고, 선형 함수를 찾아내 결과값을 예측합니다.

또한 선형 회귀 모델은 피처값과 타깃값의 분포가 정규 분포 형태로 있는 것을 매우 선호합니다.

특히 타깃값의 경우 정규 분포 형태가 아니라 왜곡(Skew)된 형태의 분포도일 경우 예측 성능에 부정적인 영향을 미칠 가능성이 높습니다.

따라서 선형 회귀 모델을 적용하기 전 먼저 데이터에 대한 스케일링/정규화 작업을 수행하는 것이 일반적입니다.

하지만 스케일링/정규화 작업을 선행한다고 무조건 예측 성능이 향상되는 것은 아닙니다.



피처 데이터 세트에 적용하는 변환 작업은 다음과 같은 방법이 있습니다.

1. StandardScaler 클래스를 사용해 평균이 0, 분산이 1인 표준 정규 분포를 가진 데이터 세트로 변환하거나, MinMaxScaler 클래스를 이용해 최솟값이 0이고 최댓값이 1인 값으로 정규화
2. 스케일링/정규화를 수행한 데이터 세트에 다시 다항 특성을 적용 (1번 방법을 통해 예측 성능에 향상이 없을 경우 적용)
3. Log 변환(Log Transformation) 적용
   1번 방법의 경우 예측 성능 향상을 크게 기대하기 어렵고, 2번 방법의 경우 피처 개수가 많을 경우 다항 변환으로 생성되는 피처가 기하급수적으로 늘어나서 과적합 이슈가 발생할 수 있기 때문



타깃값의 경우 일반적으로 로그 변환을 적용합니다. (결정 값을 정규 분포나 다른 정규값으로 변환하면 원본 타깃값으로 원복하기 어려워서)



보스턴 주택가격 피처 데이터 세트에 위에서 언급한 표준 정규 분포 변환, 최댓값/최솟값 정규화, 로그 변환을 차례로 적용해 RMSE로 각 경우별 예측 성능을 측정해보겠습니다.

이를 위해 변환을 위한 get_scaled_data() 메소드를 생성했습니다.

**get_scaled_data()**

```python
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import PolynomialFeatures

#method는 표준 정규 분포 변환(Standard), 최댓값/최솟값 정규화(MinMax), 로그변환(Log) 결정
#p_degree는 다항식 특성을 추가할 때 적용, p_degree는 2 이상 부여하지 않음
def get_scaled_data(method='None', p_degree=None, input_data=None):
    if method == 'Standard':
        scaled_data = StandardScaler().fit_transform(input_data)
    elif method == 'MinMax':
        scaled_data = MinMaxScaler().fit_transform(input_data)
    elif method == 'Log':
        scaled_data = np.log1p(input_data)  #log() 함수 적용시 언더플로우가 발생하기 쉬워 1+log() 함수 적용
    else:
        scaled_data = input_data

    if p_degree != None:
        scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data)

    return scaled_data
```



**get_linear_reg_eval()**

```python
from sklearn.linear_model import Lasso, ElasticNet

#alpha 값에 따라 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환
#model_name : 회귀 모델의 이름, params : alpha 값들의 리스트, X_data_n : 피처 데이터 세트, y_target_n : 타깃 데이터 세트
def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True, return_coeff=True):
    coeff_df = pd.DataFrame()
    if verbose : print('#######', model_name, '#######')
    for param in params:
        if model_name == 'Ridge': model = Ridge(alpha=param)
        elif model_name == 'Lasso': model = Lasso(alpha=param)
        elif model_name == 'ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)
        neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring="neg_mean_squared_error", cv=5)
        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
        print('alpha {0}일 때 5 폴드 세트 평균 RMSE: {1:.3f}'.format(param, avg_rmse))

        #cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습해 회귀 계수 추출
        model.fit(X_data_n, y_target_n)
        if return_coeff:
            #alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame 컬럼으로 추가
            coeff = pd.Series(data=model.coef_, index=X_data_n.columns)
            colname = 'alpha:' + str(param)
            coeff_df[colname] = coeff

    return coeff_df
```



```python
#Ridge alpha 값을 다르게 적용하고 다양한 데이터 변환 방법에 따른 RMSE 추출
alphas = [0.1, 1, 10, 100]

#5개 방식으로 변환
#원본, 표준 정규 분포, 표준정규 분포,다항식 특성
#최대/최소 정규, 최대/최소 정규화 + 다항식 특성, 로그 변환
scaled_methods=[(None, None), ('Standard', None), ('Standard', 2),
               ('MinMax', None), ('MinMax', 2), ('Log', None)]

for scale_method in scaled_methods:
    X_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], input_data=X_data)
    print('\n## 변환 유형:{0}, Polynomial Degree:{1}'.format(scale_method[0], scale_method[1]))
    get_linear_reg_eval('Ridge', params=alphas, X_data_n=X_data_scaled, y_target_n=y_target, verbose=False, return_coeff=False)
```

![image](https://user-images.githubusercontent.com/76269316/127863515-60481827-5579-4f75-ac45-489bc7c28ff1.png)

<br>

표로 정리하면 다음과 같습니다.

![image](https://user-images.githubusercontent.com/76269316/127864862-ce2ddd51-d80f-45c6-914f-9b2fccc52eaa.png)


표준 정규 분포와 최솟값/최댓값 정규화로 피처 데이터 세트를 변경해도 성능상 개선은 없습니다.

alpha=1에서 최솟값/최댓값 정규화로 1차 변환 후 2차 다항식 변환을 했을 때 개선됐지만, 다항식 변환은 피처 개수가 많을 경우 과적합이 발생하기 때문에 적용하기 힘듧니다.

로그 변환은 alpha가 0.1, 1, 10인 경우 모두 좋은 성능 향상이 있는 것을 확인할 수 있습니다.

일반적으로 선형 회귀를 적용하려는 데이터 세트의 데이터 값이 심하게 왜곡되어 분포돼 있을 경우 로그 변환을 적용하는 것이 좋은 예측 결과를 기대할 수 있습니다.
