---
title:  "5.Regression"
excerpt: "회귀"
toc: true
toc_label: "Regression"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - 회귀
  - 파이썬 머신러닝 완벽 가이드
last_modified_at: 2021-08-06
---

>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.



### 회귀 소개

회귀 분석은 유전적 특성을 연구하던 영국 통계학자 갈톤(Galton)이 수행한 연구에서 유래했다고 알려져있습니다.

부모, 자식 간의 키 상관관계를 분석했던 갈톤은 부모의 키가 모두 클 때 자식의 키가 크긴 하지만 부모를 능가할 정도로 크지 않았고,

부모의 키가 모두 아주 작을 때 그 자식의 키가 작기는 하지만 부모보다는 큰 경향을 발견했습니다.

즉, 사람의 키는 평균 키로 회귀하려는 경향을 가진다는 자연의 법칙이 있다는 것입니다.

회귀 분석은 이처럼 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법입니다.

<br>

머신러닝에서 회귀는 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 통칭합니다.

예를 들어 아파트 방 개수, 방 크기, 주변 학군 (독립변수)과 아파트 가격(종속변수)의 관계를 모델링하고 예측하는 것입니다.

<br>

이를 선형 회귀식으로 나타내면 다음과 같습니다.

**Y = W1\*X1 + W2\*X2 + W3\*X3 + ··· + Wn\*Xn**

- Y : 종속변수(아파트 가격)

- X1, X2, X3, ···, Xn : 독립변수(방 개수, 방 크기, 주변 학군등)

- W1, W2, W3, ··· Wn : 회귀 계수

  <br>
  회귀 계수(Regression coefficients) : 독립변수에 영향을 미치는 값

<br>

회귀는 독립변수의 개수, **회귀 계수**의 선형/비선형 여부에 따라 다음과 같이 나뉩니다.

| 독립변수 개수       | 회귀 계수의 결합     |
| ------------------- | -------------------- |
| 1개 : 단일 회귀     | 선형 : 선형 회귀     |
| 여러 개 : 다중 회귀 | 비선형 : 비선형 회귀 |

<br>

지도학습은 두 가지 유형으로 나뉘는데, 분류와 회귀입니다.
두 기법의 가장 큰 차이는 예측 결과값입니다.

- 분류(Classification) : 카테고리 값(이산값)
- 회귀(Regression) : 숫자값(연속값)

<br>

여러 가지 회귀 중 선형 회귀가 가장 많이 사용됩니다.

선형 회귀는 실제 값과 예측값의 차이를 최소화하는 직선형 회귀선을 최적화하는 방식입니다.

대표적인 선형 회귀 모델은 다음과 같습니다.

- 일반 선형 회귀 : 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화하며, 규제(Regularization)를 적용하지 않은 모델
- 릿지(Ridge) : 선형 회귀에 L2 규제(큰 회귀 계수 값의 예측 영향도를 감소시키기 위한 규제)를 추가한 회귀 모델
- 라쏘(Lasso) : 선형 회귀에 L1 규제(예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않게 하는 규제)를 적용한 회귀 모델
- 엘라스틱넷(ElasticNet) : L2, L1 규제를 결합한 모델, 주로 피처가 많은 데이터 세트에 적용됨 (L1 규제로 피처 개수를 줄이고, L2 규제로 계수 값 크기 조정)
- 로지스틱 회귀(Logistic Regression) : 분류 알고리즘으로 분류에서 사용되는 선형 모델, 이진 분류뿐만 아니라 희소 영역(텍스트 분류)의 분류에서도 뛰어난 예측 성능을 보임



### 단순 선형 회귀를 통한 회귀 이해

먼저, 독립 변수도 하나 종속 변수도 하나인 단순 선형 회귀에 대해 설명하겠습니다.

<br>

예를 들어, 주택 가격이 주택 크기로만 결정된다고 가정하겠습니다.

일반적으로 주택 크기가 커지면 가격이 높아지는 경향이 있기 때문에 다음과 같이 주택 가격은 주택 크기에 대해 선형(직선 형태)의 관계로 표현할 수 있습니다.

<img src="https://user-images.githubusercontent.com/76269316/127769263-37405a99-4f5e-4c1f-8346-151bd8652cdc.png" alt="image" style="zoom: 67%;" />

X축이 주택 크기(평당 크기)이고, Y축이 주택 가격입니다.

따라서 회귀 모델을 다음과 같이 **Y = W0 + W1 * X**  1차 함수 형태로 모델링 할 수 있습니다.

단순 선형 회귀(독립변수, 종속변수가 한 개인 선형 회귀)에서 회귀 계수는 기울기 w1과 y절편 w0입니다.

*+절편은 intercept라고 합니다.*

<br>

위 모델의 실제 값과 회귀 모델의 차이에 따른 오류 값을 남은 오류, **잔차**라고 부릅니다.

<img src="https://user-images.githubusercontent.com/76269316/127769583-05c01982-9879-4644-996e-6ebe90c860b4.png" alt="image" style="zoom:67%;" />

최적의 회귀 모델을 만든다는 것은 전체 데이터의 잔차(오류 값) 합이 최소가 되는 모델을 만든다는 의미입니다.

즉, 오류 값 합이 최소가 될 수 있는 최적의 회귀 계수를 찾는다는 의미입니다.

<br>

오류 값은 +나 -가 될 수 있습니다.

그래서 전체 데이터의 오류 합을 단순히 더한다면 오류 합이 크게 줄어들 수 있습니다.

보통 오류 합을 계산할 때는 절댓값을 취해서 더하거나(Mean Absolute Error), 오류 값의 제곱을 구해서 더하는 방식(Residual Sum of Square)을 취합니다.

일반적으로 미분 등의 계산을 편리하게 하기 위해 RSS 방식으로 오류 합을 구합니다.

RSS = (#1 주택 가격 - (w0 + w1 #1 주택크기))^2 + (#2 주택 가격 - (w0 + w1 #2 주택 크기))^2 + (#3 주택 가격 - (w0 + W1 #3 주택크기))^2 + ···

<br>

RSS는 w0, w1 변수로 표현할 수 있으며, 이 RSS를 최소로 하는 w0, w1 회귀 계수를 학습을 통해 찾는 것이 머신러닝 기반 회귀의 핵심입니다.

**RSS는 회귀식의 독립변수 X, 종속변수 Y가 중심 변수가 아니라 w 회귀계수가 중심 변수입니다.**

<br>

RSS는 학습 데이터 건수로 나누어서 다음과 같이 정규화된 식으로 표현됩니다.

![image](https://user-images.githubusercontent.com/76269316/127770177-74d77c02-7a55-42d1-a6aa-aff9896e3556.png)

회귀에서 이 RSS는 비용(Cost)이며, w 변수(회귀 계수)로 구성되는 RSS를 비용 함수라고 합니다.

머신러닝 회귀 알고리즘은 계속해서 학습하면서 이 비용 함수가 반환하는 오류 값을 지속해서 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것입니다.

*+비용 함수를 손실 함수(loss function)라고도 합니다.*



### 비용 최소화하기 - 경사 하강법(Gradient Descent) 소개

경사 하강법은 점진적으로 반복적인 계산을 통해 W 파라미터 값을 업데이트하면서 오류 값이 최소가 되는 W 파라미터를 구하는 방식입니다.

<br>

경사 하강법은 산 정상에서 아래로 내려갈 때, 현재 위치보다 무조건 낮은 곳으로 계속 이동하다 보면 마침내 지상에 도착할 수 있을 거라 가정하고 이동하는 방식입니다. (내리막과 오르막이 반복되는 언덕에 갇혔을 수도 있는데, 이 문제는 나중에 해결하겠습니다.)

어떻게 보면 무식해 보이는 방법이지만 직관적이고 빠르게 비용 함수가 최소가 되는 W 파라미터 값을 구할 수 있습니다.

<br>

경사 하강법은 최초 오류 값이 100이였다면, 두 번째 오류 값은 100보다 작은 90, 세 번째는 80으로 지속해서 오류를 감소시키는 방향으로 W 값을 업데이트해 나갑니다.

그리고 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그 때의 W 값을 최적 파라미터로 반환합니다.

<br>

경사 하강법의 핵심은 **"어떻게 하면 오류가 작아지는 방향으로 W 값을 보정할 수 있을까?"**입니다.

비용 함수가 다음과 같은 포물선 형태의 2차 함수라면 경사 하강법은 미분을 적용한 다음, 이 미분 값이 계속 감소하는 방향으로 w를 업데이트 합니다.

![image](https://user-images.githubusercontent.com/76269316/127771891-bf9fa76f-9732-4fc4-a089-3e82deb0a0bb.png)

계속 업데이트 하다가, 더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그 때의 w를 반환합니다.

<br>

<br>

위에서 언급한 비용 함수를 R(w)로 지칭하겠습니다.

![image](https://user-images.githubusercontent.com/76269316/127771922-24705125-2135-4902-bb45-8329cd71ad8d.png)

R(w)는 w0, w1으로 이루어진 함수이기 때문에 일반적인 미분을 적용할 수 없고, 편미분을 적용해야 합니다.

<img src="https://user-images.githubusercontent.com/76269316/127772224-0eb23cc2-8c74-4dbd-9158-a3c26037f4c1.png" alt="image" style="zoom: 80%;" />

w1, w0 편미분 결괏값을 반복적으로 보정하면서 w1, w0 값을 업데이트하면 비용 함수 R(w)가 최소가 되는 w1, w0의 값을 구할 수 있습니다.

업데이트는 새로운 w1을 이전 w1에서 편미분 결괏값을 마이너스하면서 적용합니다.

편미분 값이 너무 클 수도 있기 때문에 보정 계수 η(eta)를 곱하는데 이를 학습률이라고 합니다.

![image](https://user-images.githubusercontent.com/76269316/127772546-574f2dc0-6e36-4328-92c7-4a91c87e82f9.png)

<br>

요약하면 다음과 같습니다.

- Step 1 : w1, w0를 임의의 값으로 설정하고 첫 비용 함수의 값을 계산
- Step 2 : <img src="https://user-images.githubusercontent.com/76269316/127772619-1399a8dd-c496-4145-8620-632e1ce22c9d.png" alt="image" style="zoom: 50%;" /> , <img src="https://user-images.githubusercontent.com/76269316/127772628-e2daa798-26f6-496f-b4b4-1e3d520ac1e0.png" alt="image" style="zoom:50%;" />로 업데이트 한 뒤 다시 비용 함수 값을 계산합니다.
- Step 3 : 비용 함수의 값이 감소했으면 Step 2를 반복합니다. 더 이상 감소하지 않는 경우, 그 때의 w1, w0를 구하고 반복을 중지합니다.

<br>

<br>

지금까지 정리한 내용을 토대로 경사 하강법을 파이썬 코드로 구현해 보겠습니다.

y = 4X + 6을 근사하기 위한 100개의 데이터 세트를 만들고, 경사 하강법을 이용해 회귀 계수 w1, w0를 도출해 보겠습니다.

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

np.random.seed(0)
#y = 4X +6을 근사(w1=4, w0=6) 
X = 2 * np.random.rand(100, 1)
y = (6 + 4 * X) + np.random.randn(100, 1)  #4X + 6에다가 난수를 더함 (y = 4X + 6을 중심으로 무작위로 퍼지도록)

#X, y 데이터 세트 산점도로 시각화
plt.scatter(X, y)
```

![image](https://user-images.githubusercontent.com/76269316/127773617-a155e421-3077-4518-b806-714411a8a4fe.png)

**get_weight_updates()**

```python
#w1과 w0를 업데이트할 w1_update, w0_update를 반환
def get_weight_updates(w1, w0, X, y, learning_rate=0.01):
    N = len(y)
    #먼저 w1_update, w0_update를 각각 w1, w0의 shape와 동일한 크기를 가진 0 값으로 초기화
    w1_update = np.zeros_like(w1)
    w0_update = np.zeros_like(w0)
    #예측 배열 계산하고 예측과 실제 값의 차이 계산
    y_pred = np.dot(X, w1.T) + w0
    diff = y - y_pred
    
    #w0_update를 dot 행렬 연산 구하기 위해 모두 1 값을 가진 행렬 생성
    w0_factors = np.ones((N, 1))
    
    #w1과 w0을 업데이트할 w1_update와 w0_update 계산
    w1_update = -(2/N)*learning_rate*(np.dot(X.T, diff))
    w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T, diff))
    
    return w1_update, w0_update
```

100개 데이터 X(1, 2, ···, 100)가 있다면 예측값은 (w0 + X(1)\*w1) + (w0 + X(2)\*w1) + ··· +(w0 + X(100)\*w1)이며, 이는 입력 배열 X와 w1 배열의 내적에 w0를 더한 것과 동일합니다.

따라서 입력 배열 X 값에 대한 예측 배열 y_pred는 np.dot(X, w1.T) + w0로 구합니다.



이후 w1_update로 <img src="https://user-images.githubusercontent.com/76269316/127772619-1399a8dd-c496-4145-8620-632e1ce22c9d.png" alt="image" style="zoom: 50%;" />를 w0_update로 <img src="https://user-images.githubusercontent.com/76269316/127772628-e2daa798-26f6-496f-b4b4-1e3d520ac1e0.png" alt="image" style="zoom:50%;" />를 계산한 뒤 이를 반환합니다.

<br>

**gradient_descent_steps()**

```python
#입력 인자 iters로 주어진 횟수만큼 반복적으로 w1과 w0를 업데이트 적용함
def gradient_descent_steps(X, y, iters=10000):
    #w0와 w1을 모두 0으로 초기화
    w0 = np.zeros((1, 1))
    w1 = np.zeros((1, 1))
    
    #인자로 주어진 iters만큼 반복적으로 get_weight_updates() 호출해 w1, w0 업데이트 수행
    for ind in range(iters):
        w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01)
        w1 = w1 - w1_update
        w0 = w0 - w0_update

    return w1, w0
```

get_weight_updates()에서 구한 w1_update, w0_update를 가지고,  w1과 w0를 업데이트 해줍니다.

<br>

**get_cost()**

```python
def get_cost(y, y_pred):
    N = len(y)
    cost = np.sum(np.square(y - y_pred) / N)
    return cost

w1, w0 = gradient_descent_steps(X, y, iters=1000)
print('w1:{0:.3f} w0:{1:.3f}'.format(w1[0, 0], w0[0, 0]))
y_pred = w1[0, 0] * X + w0
print('Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))
```

![image](https://user-images.githubusercontent.com/76269316/127787609-17f89382-41b9-40ae-a082-14289de0dacc.png)

최종적으로 예측값과 실제값의 RSS 차이를 get_cost() 함수를 통해 계산했습니다.



정리하자면, grdient_descent_stpes() 호출 → get_weight_updates() 1,000번 호출하면서 w1, w0를 업데이트 → 최종 w1, w0로 y_pred 값 계산 → get_cost()에서 RSS 계산



앞에서 구한 y_pred에 기반해 회귀선을 그렸습니다.

```python
plt.scatter(X, y)
plt.plot(X, y_pred)
```

![image](https://user-images.githubusercontent.com/76269316/127787620-a7e49b35-609d-4085-ad64-a01b7b9eb116.png)

<br>

일반적으로 경사 하강법은 모든 학습 데이터에 대해 반복적으로 비용함수 최소화를 위한 값을 업데이트하기 때문에 수행시간이 매우 오래 걸립니다.

그 때문에 실전에서는 대부분 확률적 경사 하강법(Stochastic Gradient Descent) 또는 미니 배치 확률적 경사 하강법을 사용합니다.

확률적 경사 하강법은 전체 입력 데이터 중 일부 데이터만을 이용해 w가 업데이트 되는 값을  계산하므로 경사 하강법에 비해 빠른 속도를 보장합니다.



확률적 경사 하강법을 stochastic_gradient_descent_steps()로 구현했습니다.

**stochastic_gradient_descent_steps()**

```python
def stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000):
    w0 = np.zeros((1, 1))
    w1 = np.zeros((1, 1))
    prev_cost = 100000
    iter_index = 0
    
    for ind in range(iters):
        np.random.seed(ind)
        #전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이터를 추출해 sample_X, sample_y로 저장
        stochastic_random_index = np.random.permutation(X.shape[0])
        sample_X = X[stochastic_random_index[0:batch_size]]
        sample_y = y[stochastic_random_index[0:batch_size]]
        #랜덤하게 batch_size만큼 추출된 데이터 기반으로 w1_update, w0_update 계산 후 업데이트
        w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01)
        w1 = w1 - w1_update
        w0 = w0 - w0_update

    return w1, w0
```

<br>

```python
stochastic_random_index = np.random.permutation(X.shape[0])
sample_X = X[stochastic_random_index[0:batch_size]]
sample_y = y[stochastic_random_index[0:batch_size]]
```

전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이터를 추출해  이를 기반으로 w1_update, w0_update를 계산합니다.

<br>

**get_cost()**

```python
def get_cost(y, y_pred):
    N = len(y)
    cost = np.sum(np.square(y - y_pred) / N)
    return cost

w1, w0 = gradient_descent_steps(X, y, iters=1000)
print('w1:{0:.3f} w0:{1:.3f}'.format(w1[0, 0], w0[0, 0]))
y_pred = w1[0, 0] * X + w0
print('Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))
```

<br>

```python
w1, w0 = stochastic_descent_stpes(X, y, batch_size=10, iters=1000)
print('w1:', round(w1[0, 0], 3), 'w0:', round(w0[0, 0], 3))
y_pred = w1[0, 0] + X + w0
print('Stochastic Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))
```

![image](https://user-images.githubusercontent.com/76269316/127787998-be526376-5cda-497d-b54e-62ea4914dffb.png)

확률적 경사 하강법으로 구한  w0, w1 결과는 경사 하강법과 크게 차이가 없는 것을 볼 수 있습니다.

따라서 큰 데이터를 처리할 경우 일반적으로 확률적 경사 하강법을 사용합니다.

<br>

<br>
지금까지는 독립변수가 1개(피처가 1개)인 단순 선형 회귀에서 경사 하강법을 적용했습니다.

피처가 여러 개인 경우도 1개인 경우를 확장해 유사하게 도출할 수 있습니다.



피처가 M개 (X1, X2, ···, X100)이 있다면 예측 회귀식은 다음과 같이 되게 됩니다.

y ̂ = w0 + w1\*X1 + w2\*X2 + ··· + w100\*X100

이는 np.dot(X, w1.T) + w0로 계산할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/127788803-33a8f457-2ddc-44e9-b47f-d9e081ebb197.png)

Xmat의 맨 처음 열에 모든 데이터 값이 1인 피처 Feat0을 추가하면 좀 더 간결하게 y ̂ = Xmat + W^T 회귀 예측식을 도출할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/127788813-6ab1e8ea-1ec0-487d-93d3-8abc095d9ff4.png)

<br>

<br>

<br>

### 사이킷런 LinearRegression을 이용한 보스턴 주택 가격 예측

##### LinearRegression 클래스 - Ordinary Least Squares

LinearRegression 클래스는 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화해 OLS(Ordinary Least Squares) 추정 방식으로 구현한 클래스입니다.

Ordinary Least Squares 기반 회귀 계수 계산은 입력 피처의 독립성에 많은 영향을 받습니다.

피처 간의 상관관계가 높은 경우 분산이 매우 커져서 오류에 민감해지는데 이를 다중 공선성(multi-collinearity) 문제라고 합니다.

일반적으로 상관관계가 높은 피처가 많은 경우 독립적인 중요 피처만 남기고 제거하거나 규제를 적용합니다.



LinearRegression 클래스는 fit() 메소드로 X, y 배열을 입력 받으면 회귀 계수(Coefficients)인 W를 coef_ 속성에 저장합니다.

|                        입력 파라미터                         |                             속성                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| fit_intercept : 불린 값으로, 디폴트는 True<br />Intercept(절편) 값을 계산할 것인지를 지정, False로 지정하면 intercept가 사용되지 않고 0으로 지정됨<br /><img src="https://user-images.githubusercontent.com/76269316/127802879-6ed30a22-1739-4da2-b1ea-8688453556e1.png" alt="image" style="zoom:50%;" /> <img src="https://user-images.githubusercontent.com/76269316/127802913-14bc85eb-449c-4841-afe8-bf48a46d32c6.png" alt="image" style="zoom:50%;" /><br />normalize : 불린 값으로, 디폴트는 False<br />fit_intercept가 False인 경우 이 파라미터가 무시됨<br />True이면 회귀 수행 전에 입력 데이터 세트를 정규화함 | coef_ : fit() 메소드를 수행했을 때 회귀 계수가 배열 형태로 저장하는 속성 (Shape는 (Target 값 개수, 피처 개수))<br />intercept_ : intercept 값 |

<br>

<br>

##### 회귀 평가 지표

회귀의 평가를 위한 지표는 실제 값과 회귀 예측값의 차이 값을 기반으로 한 지표가 중심입니다.

실제 값과 예측값의 차이를 그냥 더하면 +와 -가 섞여 오류가 상쇄되기 때문에 오류의 절댓값 평균이나 제곱, 또는 제곱한 뒤 다시 루트를 씌운 평균값을 구합니다.

| 평가 지표 |                             설명                             |                             수식                             |
| :-------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
|    MAE    | Mean Absolute Error(MAE)이며 실제 값과 예측값의 차이를 절댓값으로 변환해 평균한 것 | ![image](https://user-images.githubusercontent.com/76269316/127806391-a69abab8-dfa8-42af-8e67-8c0ef451e1c4.png) |
|    MSE    | Mean Squared Error(MSE)이며 실제 값과 예측값의 차이를 제곱해 평균한 것 | ![image](https://user-images.githubusercontent.com/76269316/127808500-55f098d7-a0a1-4169-8651-396be98e695d.png) |
|   RMSE    | MSE 값은 오류의 제곱을 구하므로 실제 오류 평균보다 커지게 되므로  MSE에 루트를 씌운 것이 RMSE입니다. | ![image](https://user-images.githubusercontent.com/76269316/127808534-fb6cb217-8226-4725-a655-daf581124fab.png) |
|    R^2    | 분산 기반으로 예측 성능을 평가합니다.<br />실제 값의 분산 대비 예측값의 분산 비율을 지표로 하며, 1에 가까울수록 예측 정확도가 높습니다. | ![image](https://user-images.githubusercontent.com/76269316/127808567-169c67bf-cf86-4fc3-ae3c-7ecf7f07d566.png) |



다음은 각 평가 방법에 대한 사이킷런의 API및 cross_val_score나 GridSearchCV에서 평가 시 사용되는 scoring 파라미터의 적용 값입니다.

| 평가 방법 |   사이킷런 평가 지표 API    |   Scoring 함수 적용 값    |
| :-------: | :-------------------------: | :-----------------------: |
|    MAE    | metrics.mean_absolute_error | 'neg_mean_absolute_error' |
|    MSE    | metrics.mean_squared_error  | 'neg_mean_squared_error'  |
|    R^2    |      metrics.r2_score       |           'r2'            |

사이킷런은 RMSE를 제공하지 않기 때문에 RMSE를 구하기 위해서는 직접 MSE에 제곱근을 씌워서 계산해야 합니다.



※cross_val_score, GridSearchCV 같은 scoring 함수에 회귀 평가 지표를 적용할 때 주의해야 하는 점이 있습니다.

사이킷런의 scoring 함수는 score 값이 클수록 좋은 평과 결과로 자동 평가하는데, 실제 값과 예측값의 오류 차이를 기반으로 하는 회귀 평가 지표의 경우 값이 커지면 오히려 나쁜 모델이라는 의미를 갖습니다.

따라서 -1을 원래 평가 지표 값에 곱해 음수(negative)를 만들어 작은 오류 값이 더 큰 숫자로 인식하게 했습니다.

따라서 scoring 파라미터 값이 의미하는 것은 원래 값에 -1을 곱한 것이라는 점에 주의가 필요합니다.

<br>

<br>

##### LinearRegression을 이용해 보스턴 주택 가격 회귀 구현

사이킷런에 내장된 데이터 세트인 보스턴 주택 가격 데이터 세트에 LinearRegression 클래스를 사용해 선형 회귀 모델을 만들어보겠습니다.

사이킷런에 내장된 보스턴 주택 가격 데이터 세트의 피처는 다음과 같습니다.



- CRIM : 지역별 범죄 발생률
- ZN : 25,000평방피트를 초과하는 거주 지역의 비율
- INDUS : 비상업 지역 넓이 비율
- CHAS : 찰스강에 대한 더미 변수(강의 경계에 위치한 경우는 1, 아니면 0)
- NOX : 일산화질소 농도
- RM : 거주할 수 있는 방의 개수
- AGE : 1940년 이전에 건축된 소유 주택의 비율
- DIS : 5개 주요 고용센터까지의 가중 거리
- RAD : 고속도로 접근 용이도
- TAX : 10,000달러당 재산세율
- PTRATIO : 지역의 교사와 학생 수 비율
- B : 지역의 흑인 거주 비율
- LSTAT : 하위 계층의 비율
- MEDV : 본인 소유의 주택 가격(중앙값)



```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from scipy import stats
from sklearn.datasets import load_boston
%matplotlib inline

#boston 데이터 세트 로드
boston = load_boston()

#boston 데이터 세트 DataFrame 변환
bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)

#boston 데이터 세트의 target 배열은 주택 가격
bostonDF['PRICE'] = boston.target  #PRICE 컬럼으로 DataFrame에 추가
print('Boston 데이터 세트 크기 : ', bostonDF.shape)
bostonDF.head()
```

![image](https://user-images.githubusercontent.com/76269316/127813698-1261b264-cd39-47f7-ae4a-2590f57b9c91.png)



Null 값은 없고, 모두 float형입니다.

```python
bostonDF.info()
```

![image](https://user-images.githubusercontent.com/76269316/127813748-db5cd37c-02ab-4689-afa2-8803c6ea3108.png)



각 컬럼이 회귀 결과에 미치는 영향을 확인하기 위해 시각화해 알아보겠습니다.

```python
#2개의 행과 4개의 열을 갖는 subplots를 이용. axs는 2X4개의 ax를 가짐
fig, axs = plt.subplots(figsize=(16, 8), nrows=2, ncols=4)
lm_features = ['RM', 'ZN', 'INDUS', 'NOX', 'AGE', 'PTRATIO', 'LSTAT', 'RAD']
for i, feature in enumerate(lm_features):
    row = int(i / 4)
    col = i%4
    #시본의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현
    sns.regplot(x=feature, y='PRICE', data=bostonDF, ax=axs[row][col])
```

![image](https://user-images.githubusercontent.com/76269316/127814245-6f9f9ecb-4b00-4205-a673-2a0465d5c73e.png)



다른 컬럼보다 RM과 LSTAT의 PRICE 영향도가 가장 두드러진 것을 확인할 수 있습니다.

RM(방 개수)은 양의 방향 선형성(Positive Linearity) 즉, 방의 크기가 클수록 가격이 증가하고

LSTAT(하위 계층의 비율)은 음의 방향 선형성(Negative Linearity), LSTAT이 적을수록 PRICE가 증가하는 것을 볼 수 있습니다.

<br>

<br>

이제 LinearRegression 클래스를 사용해 보스턴 주택 가격의 회귀 모델을 만들어 보겠습니다.

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)

X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.3, random_state=156)

#선형 회귀 OLS로 학습/예측/평가 수행
lr = LinearRegression()
lr.fit(X_train, y_train)
y_preds = lr.predict(X_test)
mse = mean_squared_error(y_test, y_preds)
rmse = np.sqrt(mse)  #MSE에 제곱근을 씌워 RMSE 계산

print('MSE : {0:.3f}, RMSE : {1:.3f}'.format(mse, rmse))
print('Variance score : {0:.3f}'.format(r2_score(y_test, y_preds)))
```

![image](https://user-images.githubusercontent.com/76269316/127815865-bb50a127-2d52-4a13-ac7f-2ae4923c28ce.png)

<br>

LinearRegression 객체의 intercept\_, coef\_ 속성을 확인해 보겠습니다.

```python
print('절편 값:', lr.intercept_)
print('회귀 계수값:', np.round(lr.coef_, 1))
```

![image](https://user-images.githubusercontent.com/76269316/127816037-6060b747-fe6d-46eb-883a-93eabf3fbc64.png)

<br>

coef_ 속성은 회귀 계수 값만 갖고 있으므로 피처별로 매핑한 다음, 높은 값 순으로 출력해 보겠습니다.

```python
#회귀 계수를 큰 값 순으로 정렬하기 위해 Serie로 생성
coeff = pd.Series(data=np.round(lr.coef_, 1), index=X_data.columns)
coeff.sort_values(ascending=False)
```

![image](https://user-images.githubusercontent.com/76269316/127816299-3d1a1272-2c4f-4c06-8823-f7e05c99cc8d.png)

<br>

<br>

이번에는 cross_val_score()를 이용해 교차 검증으로 5개의 폴드 세트에서 MSE와 RMSE를 측정해 보겠습니다.

**이 때 neg_mean_squared 지정시 반환되는 수치 값이 음수 값이라는 점에 주의해야 합니다.**

```python
from sklearn.model_selection import cross_val_score

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)
lr = LinearRegression()

#cross_val_score()로 5개의 폴드 세트로 MSE를 구한 뒤 이를 기반으로 다시 RMSE를 구함
neg_mse_scores = cross_val_score(lr, X_data, y_target, scoring='neg_mean_squared_error', cv=5)
rmse_scores = np.sqrt(-1 * neg_mse_scores)
avg_rmse = np.mean(rmse_scores)

#cross_val_score(scoring="neg_mean_squared_error")로 반환된 값은 모두 음수
print('5 folds 개별 Negative MSE Scores : ', np.round(neg_mse_scores, 2))
print('5 folds 개별 RMSE scores : ', np.round(rmse_scores, 2))
print('5 folds 평균 RMSE : {0:.3f}'.format(avg_rmse))
```

![image](https://user-images.githubusercontent.com/76269316/127816887-10e07a56-3385-4c88-83b6-af04fa2be672.png)

<br>

<br>

<br>

### 다항 회귀와 과(대)적합/과소적합 이해

##### 다항 회귀 이해

이전까지 설명한 회귀는 y = w0 + w1\*X1 + w2\*X2 + ··· + wn\*Xn 과 같이 독립변수(feature)와 종속변수(target)의 관계가 일차 방정식 형태로 표현됐습니다.

하지만 세상 모든 관계를 직선으로만 표현할 수 없습니다.

일차 방정식 형태가 아닌 다음과 같이 2차, 3차 방정식과 같은 다항식으로 표현되는 것을 다항(Polynomial) 회귀라고 합니다.

y = w0 + w1\*X1 + w2\*X2 + w3 \*X1\*X2 + w4\*X1^2 + w5\*X2^2

<br>

주의할 점이, **다항 회귀는 선형 회귀**라는 점입니다.

회귀에서 선형 회귀/비선형 회귀를 나누는 기준은 회귀 계수가 선형/비선형인지에 따른 것이지 독립변수의 선형/비선형 여부와는 무관합니다.

<br>

사이킷런에서는 다항 회귀를 위한 클래스를 명시적으로 제공하지 않습니다.

하지만, 다항 회귀 역시 선형 회귀이기 때문에 비선형 함수를 선형 모델에 적용시키는 방법을 사용할 수 있습니다.

<br>

사이킷런은 PolynomialFeatures 클래스를 통해 단항식 피처를 degree에 해당하는 다항식 피처로 변환합니다.

아래 코드는 단항 계수 피처 [x1, x2]를 2차 다항 계수 [1, x1, x2, x1^2, x1x2, x2^2]으로 변환하는 예제 코드입니다.

```python
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

#다항식으로 변환한 단항식 생성, [[0, 1], [2, 3]]의 2X2 행렬 생성
X = np.arange(4).reshape(2, 2)
print('일차 단항식 계수 피처:\n', X)

#degree = 2인 2차 다항식으로 변환하기 위해 PolynomialFeatures를 이용해 변환
poly = PolynomialFeatures(degree=2)
poly.fit(X)
poly_ftr = poly.transform(X)
print('변환된 2차 다항식 계수 피처:\n', poly_ftr)
```

![image](https://user-images.githubusercontent.com/76269316/127819411-48ecd23e-4731-4904-a844-8a91a70beccf.png)

첫 번째 입력 단항 계수 피처 [x1=0, x2=1]은 [1, x1=0, x2=1, x1^2=0, x1x2=0, x2^2=1] 형태인 [1, 0, 1, 0, 0, 1]로 변환됐습니다.

두 번째 입력 단항 계수 피처도 마찬가지로 변환된 것을 확인할 수 있습니다.

이렇게 변환된 다항식 피처에 선형 회귀를 적용해 다항 회귀를 구현합니다.

<br>

<br>

일차 단항식 계수를 삼차 다항식 계수로 변환하고 이를 선형 회귀에 적용하면 다항 회귀로 구현됩니다.

```python
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

def polynomial_func(X):
    y = 1 + 2*X[:, 0] + 3*X[:, 0]**2 + 4*X[:, 1]**3
    return y

X = np.arange(4).reshape(2, 2)
print('일차 단항식 계수 feature:\n', X)
y = polynomial_func(X)
print('삼차 다항식 결정값:\n', y)

#3차 다항식 변환
poly_ftr = PolynomialFeatures(degree=3).fit_transform(X)
print('3차 다항식 계수 feature:\n', poly_ftr)

#LinearRegression에 3차 다항식 계수 feature와 3차 다항식 결정값으로 학습 후 회귀 계수 확인
model = LinearRegression()
model.fit(poly_ftr, y)
print('Polynomial 회귀 계수\n', np.round(model.coef_, 2))
print('Polynomial 회귀 shape: ', model.coef_.shape)
```

![image](https://user-images.githubusercontent.com/76269316/127821005-4d2ac84c-47cc-42ee-b431-444f2ba65540.png)

일차 단항식 계수 피처를 3차 다항식으로 변환한 다음 LinearRegression을 적용해 10개의 회귀 계수가 도출됐습니다.

원래 다항식 1 + 2X1 + 3X1^2 + 4X2^3의 계수 값인 [1, 2, 0, 3, 0, 0, 0, 0, 0, 4]와는 차이가 있지만 다항 회귀로 근사하고 있는 것을 볼 수 있습니다.

이처럼 사이킷런은 PolynomialFeatures로 피처를 변환한 뒤에 LinearRegression 클래스로 다항 회귀를 구현합니다.



아래는 간결하게 Pipeline 객체를 사용해 한 번에 다항 회귀를 구현한 코드입니다.

+파이프라인으로 결합된 모형은 원래 모형이 갖는 fit, predict 메소드를 가지며 각 메소드가 호출되면 파이프라인의 각 객체에 대해서 호출합니다.

파이프라인 fit() 메소드 호출 시 fit_transform() 메소드를 순서대로 호출한 결과를 다음 단계의 입력으로 전달합니다.

마지막 단계에서는 fit() 메소드만 호출합니다.

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
import numpy as np

def polynomial_func(X):
    y = 1 + 2*X[:, 0] + 3*X[:, 0]**2 + 4*X[:, 1]**3
    return y

#Pipeline 객체로 Streamline하게 Polynomial Feature 변환과 Linear Regression 연결
model = Pipeline([('poly', PolynomialFeatures(degree=3)), ('linear', LinearRegression())])
X = np.arange(4).reshape(2, 2)
y = polynomial_func(X)

model = model.fit(X, y)

print('Polynomial 회귀 계수\n', np.round(model.named_steps['linear'].coef_, 2))
```

![image](https://user-images.githubusercontent.com/76269316/127822557-da394654-7d75-4bae-a508-d1773c7321f2.png)

<br>

<br>

##### 다항 회귀를 이용한 과소적합 및 과적합 이해

다항회귀는 피처의 직선적 관계가 아닌 복잡한 다항 관계를 모델링할 수 있습니다.

하지만 다항식의 차수가 높아질수록 학습 데이터에만 너무 맞춘 학습이 이뤄져서 테스트 데이터에서는 예측 정확도가 떨어지게 됩니다.

즉, 차수가 높아질수록 과적합 문제가 발생합니다.

<br>

다음은 사이킷런 홈페이지에 있는 다항 회귀 과소적합, 과적합 문제를 잘 보여주는 예제입니다.

[https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#shpx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#shpx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py)



학습 데이터는 30개의 임의의 데이터인 X, X의 코사인 값에 약간의 잡음 변동 값을 더한 target인 y로 구성됩니다.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
%matplotlib inline

#임의의 값으로 구성된 X 값에 대해 코사인 변환 값을 반환
def true_fun(X):
    return np.cos(1.5 * np.pi * X)

#X는 0부터 1까지 30개 임의의 값을 순서대로 샘플링한 데이터
np.random.seed(0)
n_samples = 30
X = np.sort(np.random.rand(n_samples))

#y값은 코사인 기반 true_fun()에서 약간의 노이즈 변동 값을 더한 값
y = true_fun(X) + np.random.randn(n_samples) * 0.1
```



다항식 차수를 각각 1, 4, 15로 변경하면서 예측 결과를 비교해보겠습니다.

다항식 차수별로 학습을 수행한 뒤 cross_val_score()로 MSE 값을 구해 차수별 예측 성능을 평가합니다.

그리고 0부터 1까지 균일하게 구성된 100개 테스터용 데이터 세트를 이용해 차수별 회귀 예측 곡선을 그려보겠습니다.

```python
plt.figure(figsize=(14, 5))
degrees = [1, 4, 15]

#다항 회귀의 차수(degree)를 1, 4, 15로 변화시키면서 비교
for i in range(len(degrees)):
    ax = plt.subplot(1, len(degrees), i + 1)
    plt.setp(ax, xticks=(), yticks=())
    
    #개별 degree별로 Polynomial 변환
    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)  #include_bias=True 설정시 편향을 위한 변수 X0(1)가 추가됨
    linear_regression = LinearRegression()
    pipeline = Pipeline([('polynomial_features', PolynomialFeatures(degree=degrees[i])), ('linear_regression', LinearRegression())])
    pipeline.fit(X.reshape(-1, 1), y)
    
    #교차 검증으로 다항 회귀 평가
    scores = cross_val_score(pipeline, X.reshape(-1, 1), y, scoring="neg_mean_squared_error", cv=10)
    #Pipeline을 구성하는 세부 객체를 접근하는 named_steps['객체명]을 이요해 회귀계수 추출
    coefficients = pipeline.named_steps['linear_regression'].coef_
    print('\nDegree {0} 회귀 계수는 {1}입니다.'.format(degrees[i], np.round(coefficients, 2)))
    print('Degree {0} MSE는 {1}입니다.'.format(degrees[i], -1 * np.mean(scores)))
    
    #0부터 1까지 테스트 데이터 세트를 100개로 나눠 예측 수행
    #테스트 데이터 세트에 회귀 예측을 수행하고 예측 곡선과 실제 곡선을 그려서 비교
    X_test = np.linspace(0, 1, 100)
    #예측값 곡선
    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model")  #X_test[:, np.newaxis] : X_test에 새로운 축 추가
    #실제 값 곡선
    plt.plot(X_test, true_fun(X_test), '--', label="True function")
    plt.scatter(X, y, edgecolor='b', s=20, label="Samples")

    plt.xlabel("x"); plt.ylabel("y"); plt.xlim((0, 1)); plt.ylim((-2, 2)); plt.legend(loc="best")
    plt.title("Degree {}\nMSE = {:.2e}(+/- {:.2e})".format(degrees[i], -scores.mean(), scores.std()))

plt.show()
```

실선으로 표현된 예측 곡선은 다항 회귀 예측 곡선입니다.

점선으로 표현된 곡선은 실제 데이터 세트 X, Y의 코사인 곡선입니다.

학습 데이터는 0부터 1까지 30개의 임의의 X 값과 그에 따른 코사인 Y 값에 잡음을 변동 값으로 추가해 구성했으며 MSE(MeanSquaredError) 평가는 학습 데이터를 10개의 교차 검증 세트로 나누어 측정해서 평균한 것입니다.

![image](https://user-images.githubusercontent.com/76269316/127835695-862fcbe1-ddf4-4990-b1f9-b402315cd3f6.png)

- Degree 1 예측 곡선은 단순한 직선으로 단순 선형 회귀와 같습니다.
  예측 곡선이 학습 데이터의 패턴을 제대로 반영하지 못하고 있는 과소적합 모델이 됐습니다. MSE는 약 0.407입니다.
- Degree 4 예측 곡선은 실제 데이터 세트와 유사한 모습으로, 변동하는 잡음까지 예측하지는 못했지만 학습 데이터 세트를 비교적 잘 반영해 테스트 데이터를 잘 예측한 곡선을 가진 모델이 됐습니다.
  MSE 값은 약 0.043으로 가장 뛰어난 예측 성능을 보입니다.
- Degree 15 예측 곡선은 MSE 값이 181821493이 될 정도로  큰 오류값이 발생했습니다.
  예측 곡선을 보면 데이터 세트의 변동 잡음까지 지나치게 반영한 결과, 학습 데이터 세트만 정확히 예측하고 테스트 값의 실제 곡선과는 완전히 다른 과적합 모델이 됐습니다.

<br>

<br>

##### 편향-분산 트레이드오프(Bias-Variance Trade Off)

편향-분산 트레이드오프는 머신러닝이 극복해야할 가장 중요한 문제 중 하나입니다.

![image](https://user-images.githubusercontent.com/76269316/127836814-7490d1d3-6c73-4eb0-a6e6-c8aa628fcbb1.png)

[이미지 출처](http://scott.fortmann-roe.com/docs/BiasVariance.html)



위 '양궁 과녁' 그래프는 편향과 분산의 고/저 의미를 직관적으로 잘 표현하고 있습니다.

상단 왼쪽 그림은 **저편향/저분산(Low Bias/Low Variance)**입니다.

예측 결과가 실제 결과에 매우 근접하면서 예측 변동이 크지 않고 특정 부분에 집중돼 있습니다. (아주 드묾)



상단 오른쪽 그림은 **저편향/고분산(Low Bias/High Variance)**입니다.

예측 결과가 실제 결과에 비교적 근접하지만, 예측 결과가 실제 결과를 중심으로 꽤 넓은 부분에 분포돼 있습니다.



하단 왼쪽 그림은 **고편향/저분산(High Bias/Low Variance)**로 정확환 결과에서 벗어나면서도 예측 부분이 특정 부분에 집중돼 있습니다.

하단 오른쪽 그림은 **고편향/고분산(High Bias/High Variance)**로 정확환 예측 결과를 벗어나면서도 넓은 부분에 분포돼 있습니다.

<br>

일반적으로 편향과 분산은 한 쪽이 높으면 한 쪽이 낮아지는 경향이 있습니다.

![image](https://user-images.githubusercontent.com/76269316/127837911-052f9eb9-5a96-43ba-a688-01c279dc9a66.png)

[이미지 출처](http://scott.fortmann-roe.com/docs/BiasVariance.html)



편향이 너무 높으면 전체 오류가 높습니다.

편향을 점점 낮추면 동시에 분산이 높아지고 전체 오류는 낮아지게 됩니다.

편향을 낮추고 분산을 높이면서 전체 오류가 가장 낮아지는 '골디락스' 지점을 통과하면 오류 값이 오히려 증가하면서 예측 성능이 다시 저하되게 됩니다.

편향과 분산이 서로 트레이드오프를 이루면서 오류 cost 값이 최대로 낮아지는 모델을 구축하는 것이 중요합니다.



### 규제 선형 모델 - 릿지, 라쏘, 엘라스틱넷

##### 규제 선형 모델의 개요

이전까지 선형 모델의 비용 함수는 RSS를 최소화하는, 즉 실제 값과 예측값의 차이를 최소화하는 것만 고려했습니다.

그러다 보니 학습 데이터에 지나치게 맞추게 되고, 회귀 계수가 쉽게 커졌습니다.

이럴 경우 오히려 변동성이 심해져 테스트 데이터 세트에서는 예측 성능이 저하되기 쉽습니다.

따라서 비용 함수는 학습 데이터의 잔차 오류 값을 최소로 하는 RSS 최소화 방법과 과적합을 방지하기 위해 회귀 계수 값이 커지지 않도록 하는 방법이 서로 균형을 이뤄야 합니다.

▶ **최적 모델을 위한 cost 함수 구성요소 = 학습 데이터 잔차 오류 최소화 + 회귀계수 크기 제어**



<img src="https://user-images.githubusercontent.com/76269316/127839737-57516f40-a4e5-41a6-9cf7-3e5eba93c183.png" alt="image" style="zoom: 80%;" />

여기서 alpha는 학습 데이터 적합 정도와 회귀 계수 값의 크기 제어를 수행하는 튜닝 파라미터입니다.

alpha가 0(또는 매우 작은 값)이라면 비용 함수 식은 기존과 동일한 Min(RSS(W))가 될 것입니다.

alpha가 무한대(또는 매우 큰 값)라면 비용 함수 식은 RSS(W)에 비해 <img src="https://user-images.githubusercontent.com/76269316/127840084-1c291649-85a3-428b-8dfc-03f0601956b3.png" alt="image" style="zoom:50%;" /> 값이 너무 커지게 되므로 W 값을 0(또는 매우 작게)으로 만들어야 cost가 최소화되는 비용 함수 목표를 달성할 수 있습니다.

즉, alpha 값을 크게 하면 비용 함수는 회귀 계수 W 값을 작게 해 과적합을 개선할 수 있으며 alpha 값을 작게 하면 회귀 계수 W 값이 커져도 어느 정도 상쇄가 가능하므로 학습 데이터 적합을 더 개선할 수 있습니다.

<br>

이렇게 비용 함수에 alpha 값으로 페널티를 부여해 회귀 계수 값의 크기를 감소시켜 과적합을 개선하는 방식을 규제(Regularization)라고 합니다.

규제는 크게 L2 방식, L1 방식으로 구분됩니다.

<br>

L2 규제는 W의 제곱에 대해 페널티를 부여하는 방식을 말합니다.

L2 규제를 적용한 회귀를 릿지(Ridge) 회귀라고 합니다.

<br>

L1 규제는 W의 절댓값에 대해 페널티를 부여하는 방식입니다.

L1 규제를 적용한 회귀를 라쏘(Lasso) 회귀 라고 합니다.

L1 규제를 적용하면 영향력이 크지 않은 회귀 계수 값을 0으로 변환합니다. (따라서 피처 선택 기능으로도 불립니다.)

<br>

<br>

##### 릿지 회귀

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

#boston 데이터 세트 로드
boston = load_boston()

#boston 데이터 세트 DataFrame 변환
bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)

#boston 데이터 세트의 target 배열은 주택 가격
bostonDF['PRICE'] = boston.target  #PRICE 컬럼으로 DataFrame에 추가

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)

#alpha=10으로 설정해 릿지 회귀 수행
ridge = Ridge(alpha=10)  #alpha : L2 규제 계수
neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring="neg_mean_squared_error", cv=5)
rmse_scores = np.sqrt(-1 * neg_mse_scores)
avg_rmse = np.mean(rmse_scores)
print('5 folds 개별 Negative MSE scores: ', np.round(neg_mse_scores, 3))
print('5 folds 개별 RMSE scores: ', np.round(rmse_scores, 3))
print('5 folds 평균 RMSE: {0:.3f}'.format(avg_rmse))
```

|                       LinearRegression                       |                            Ridge                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/127816887-10e07a56-3385-4c88-83b6-af04fa2be672.png) | ![image](https://user-images.githubusercontent.com/76269316/127842554-53c361cd-b3a3-4cab-8015-e6d7d7f93c5c.png) |

위에서 다룬 [규제가 없는 LinearRegression](https://seominseok4834.github.io/machine%20learning/5.regression/#linearregression%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%B4-%EB%B3%B4%EC%8A%A4%ED%84%B4-%EC%A3%BC%ED%83%9D-%EA%B0%80%EA%B2%A9-%ED%9A%8C%EA%B7%80-%EA%B5%AC%ED%98%84)보다 뛰어난 예측 성능을 확인할 수 있습니다.



이번에는 alpha 값을 0, 0.1, 1, 10, 100으로 변화시키면서 RMSE와 회귀 계수 값의 변화를 살펴보겠습니다.

```python
#릿지에 사용될 alpha 파라미터 값 정의
alphas = [0, 0.1, 1, 10, 100]

#alphas list 값을 반복하면서 alpha에 따른 평균 rmse를 구함
for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    
    #cross_val_score를 이용해 5 폴드 평균 RMSE 계산
    neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring="neg_mean_squared_error", cv=5)
    avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
    print('alpha {0}일 때 5 folds의 평균 RMSE: {1:.3f}'.format(alpha, avg_rmse))
```

![image](https://user-images.githubusercontent.com/76269316/127843548-ec0dc0d5-c356-4e3e-a6f7-fa192eb69c5c.png)



```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

#각 alpha에 따른 회귀 계수 값을 시각화하기 위해 5개의 열로 된 맷플롯립 축 생성
fig, axs = plt.subplots(figsize=(18, 6), nrows=1, ncols=5)
#각 alpha에 따른 회귀 계수 값을 데이터로 저장하기 위한 DataFrame 생성
coeff_df = pd.DataFrame()

#alphas 리스트 값을 차례로 입력해 회귀 계수 값 시각화 및 데이터 저장. pos는 axis 위치 지정
for pos, alpha in enumerate(alphas):
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_data, y_target)
    #alpha에 따른 피처별로 회귀 계수를 Sereis로 변환하고 이를 DataFrame 컬럼으로 추가
    coeff = pd.Series(data=ridge.coef_, index=X_data.columns)
    colname = 'alpha:' + str(alpha)
    coeff_df[colname] = coeff
    #막대 그래프로 각 alpha 값에서의 회귀 계수를 시각화(내림차순으로 정렬)
    coeff = coeff.sort_values(ascending=False)
    axs[pos].set_title(colname)
    axs[pos].set_xlim(-3, 6)
    sns.barplot(x=coeff.values, y=coeff.index, ax=axs[pos])
    
#for문 바깥에서 맷플롯립 show 호출 및 alpha에 따른 피처별 회귀 계수를 DataFrame으로 표시
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/127852493-b02e0476-0f00-4e0b-acfc-d7d4a4500a64.png)



```python
ridge_alphas = [0, 0.1, 1, 10, 100]
sort_column = 'alpha:' + str(ridge_alphas[0])
coeff_df.sort_values(by=sort_column, ascending=False)
```

![image](https://user-images.githubusercontent.com/76269316/127853090-a9a57fa1-24f2-443f-83b1-9dd3d19e30aa.png)

alpha 값을 계속 증가시킬수록 회귀 계수 값은 지속적으로 작아지는 것을 볼 수 있습니다.

하지만 릿지 회귀의 경우에는 회귀 계수를 0으로 만들지 않습니다.

<br>

<br>

##### 라쏘 회귀

W의 절댓값에 페널티를 부여하는 L1 규제를 선형 회귀에 적용한 것이 라쏘 회귀입니다.

L2 규제가 회귀 계수의 크기를 감소시키만 하는데 반해, L1 규제는 불필요한 회귀 계수를 0으로 만들고 제거합니다.

이러한 측면에서 L1 규제는 적절한 피처만 회귀에 포함시키는 피처 선택의 특성이 있습니다.



뒤에서 엘라스틱넷도 동일하게 alpha 값을 변화시키면서 RMSE와 각 피처의 회귀 계수를 출력할 것이므로 이를 위한 별도의 함수 get_linear_reg_eval()이라는 메소드를 만들었습니다.

**get_linear_reg_eval()**

```python
from sklearn.linear_model import Lasso, ElasticNet

#alpha 값에 따라 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환
#model_name : 회귀 모델의 이름, params : alpha 값들의 리스트, X_data_n : 피처 데이터 세트, y_target_n : 타깃 데이터 세트
def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True, return_coeff=True):
    coeff_df = pd.DataFrame()
    if verbose : print('#######', model_name, '#######')
    for param in params:
        if model_name == 'Ridge': model = Ridge(alpha=param)
        elif model_name == 'Lasso': model = Lasso(alpha=param)
        elif model_name == 'ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)
        neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring="neg_mean_squared_error", cv=5)
        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
        print('alpha {0}일 때 5 폴드 세트 평균 RMSE: {1:.3f}'.format(param, avg_rmse))

        #cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습해 회귀 계수 추출
        model.fit(X_data_n, y_target_n)
        if return_coeff:
            #alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame 컬럼으로 추가
            coeff = pd.Series(data=model.coef_, index=X_data_n.columns)
            colname = 'alpha:' + str(param)
            coeff_df[colname] = coeff

    return coeff_df
```

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

#boston 데이터 세트 로드
boston = load_boston()

#boston 데이터 세트 DataFrame 변환
bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)

#boston 데이터 세트의 target 배열은 주택 가격
bostonDF['PRICE'] = boston.target  #PRICE 컬럼으로 DataFrame에 추가

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)

#라쏘에 사용될 alpha 파라미터 값을 정의하고 get_linear_reg_eval() 함수 호출
lasso_alphas = [0.07, 0.1, 0.5, 1, 3]
coeff_lasso_df = get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)
```

<img src="https://user-images.githubusercontent.com/76269316/127940942-551750f3-8aea-46e2-a5fb-e608a4dde47f.png" alt="image" style="zoom:50%;" />

[릿지 회귀](https://seominseok4834.github.io/machine%20learning/5.regression/#%EB%A6%BF%EC%A7%80-%ED%9A%8C%EA%B7%80)보다는 약간 떨어지는 수치지만, LinearRegression 평균보다는 향상됐습니다.



alpha 값에 따른 피처별 회귀 계수입니다.

```python
#반환된 coeff_lasso_df를 첫 번째 컬럼순으로 내림차순 정렬해 회귀 계수 DataFrame 출력
sort_column = 'alpha:' + str(lasso_alphas[0])
coeff_lasso_df.sort_values(by=sort_column, ascending=False)
```

![image](https://user-images.githubusercontent.com/76269316/127857887-011e278f-0b42-4316-a4e3-d290ecedf3e5.png)

alpha의 크기가 증가함에 따라 일부 피처의 회귀 계수가 0으로 바뀌는 것을 볼 수 있습니다.

회귀 계수가 0인 피처는 회귀 식에서 제외되면서 피처 선택의 효과를 얻을 수 있습니다.

<br>

<br>

##### 엘라스틱넷 회귀

엘라스틱넷(Elastic Net) 회귀는 L2 규제와 L1 규제를 결합한 회귀입니다.

따라서 엘라스틱넷 회귀 비용 함수의 목표는 <img src="https://user-images.githubusercontent.com/76269316/127858186-17199c92-9202-4855-8a37-be0304f4f285.png" alt="image" style="zoom:50%;" /> 식을 최소화하는 W를 찾는 것입니다.

라쏘 회귀는 서로 상관관계가 높은 피처들 중 중요 피처만을 선택하고 다른 피처들은  모두 회귀 계수를 0으로 만드는 성향이 강합니다.

이러한 성향으로 alpha 값에 따라 회귀 계수 값이 급격히 변동할 수 있는데, 이를 완화하기 위해 L2 규제를 라쏘 회귀에 추가한 것입니다.

때문에 엘라스틱넷 회귀는 수행시간이 상대적으로 오래걸립니다.



사이킷런은 ElasticNet 클래스를 통해 엘라스틱넷 회귀를 구현합니다.

ElasticNet 클래스 주요 생성 파라미터는 alpha와 l1_ratio입니다.

엘라스틱넷 규제는 a * L1 + b * L2로 정의될 수 있으며, 이 때 a는 L1규제 alpha 값, b는 L2 규제 alpha 값입니다.

따라서 ElasticNet 클래스의 alpha 값 파라미터는 a + b 입니다.

ElasticNet 클래스의 l1_ratio는 a / (a + b)입니다. 

l1_ratio가 0이면 a가 0이므로 L2 규제와 동일하고, l1_ratio가 1이면 b가 0이므로 L1 규제와 동일합니다.



위에서 만든 get_linear_reg_eval() 메소드를 사용해서 alpha 값에 따른 RMSE와 각 피처의 회귀 계수 값을 보도록하겠습니다.

단순히 alpha 값에 따른 변화만 살피기 위해 l1_ratio를 0.7로 고정했습니다.

```python
elif model_name == 'ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)
```



**get_linear_reg_eval()**

```python
from sklearn.linear_model import Lasso, ElasticNet

#alpha 값에 따라 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환
#model_name : 회귀 모델의 이름, params : alpha 값들의 리스트, X_data_n : 피처 데이터 세트, y_target_n : 타깃 데이터 세트
def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True, return_coeff=True):
    coeff_df = pd.DataFrame()
    if verbose : print('#######', model_name, '#######')
    for param in params:
        if model_name == 'Ridge': model = Ridge(alpha=param)
        elif model_name == 'Lasso': model = Lasso(alpha=param)
        elif model_name == 'ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)
        neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring="neg_mean_squared_error", cv=5)
        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
        print('alpha {0}일 때 5 폴드 세트 평균 RMSE: {1:.3f}'.format(param, avg_rmse))

        #cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습해 회귀 계수 추출
        model.fit(X_data_n, y_target_n)
        if return_coeff:
            #alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame 컬럼으로 추가
            coeff = pd.Series(data=model.coef_, index=X_data_n.columns)
            colname = 'alpha:' + str(param)
            coeff_df[colname] = coeff

    return coeff_df
```

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

#boston 데이터 세트 로드
boston = load_boston()

#boston 데이터 세트 DataFrame 변환
bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)

#boston 데이터 세트의 target 배열은 주택 가격
bostonDF['PRICE'] = boston.target  #PRICE 컬럼으로 DataFrame에 추가

y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)

#엘라스틱넷에 사용될 alpha 파라미터 값들을 정의하고 get_linear_reg_eval() 함수 호출
#l1_ratio는 0.7로 고정
elastic_alphas = [0.07, 0.1, 0.5, 1, 3]
coeff_elastic_df = get_linear_reg_eval('ElasticNet', params=elastic_alpahas, X_data_n=X_data, y_target_n=y_target)
```

![image](https://user-images.githubusercontent.com/76269316/127859426-404b0ac0-0915-4c6f-9bab-86bdb6ecb018.png)

```python
#반환된 coeff_elastic_df를 첫 번째 컬럼순으로 내림차순 정렬해 회귀 계수 DataFrame 출력
sort_column = 'alpha:' + str(elastic_alphas[0])
coeff_elastic_df.sort_values(by=sort_column, ascending=False)
```

<img src="https://user-images.githubusercontent.com/76269316/127859754-75349d36-10cb-4424-9bc1-97918fd19607.png" alt="image" style="zoom: 80%;" />

alpha 값에 따른 피처들의 회귀 계수 값이 라쏘보다는 상대적으로 0이 되는 값이 적음을 볼 수 있습니다.

<br>

<br>

##### 선형 회귀 모델을 위한 데이터 변환

선형 회귀 모델과 같은 선형 모델은 일반적으로 피처와 타깃값 간에 선형 관계가 있다고 가정하고, 선형 함수를 찾아내 결과값을 예측합니다.

또한 선형 회귀 모델은 피처값과 타깃값의 분포가 정규 분포 형태로 있는 것을 매우 선호합니다.

특히 타깃값의 경우 정규 분포 형태가 아니라 왜곡(Skew)된 형태의 분포도일 경우 예측 성능에 부정적인 영향을 미칠 가능성이 높습니다.

따라서 선형 회귀 모델을 적용하기 전 먼저 데이터에 대한 스케일링/정규화 작업을 수행하는 것이 일반적입니다.

하지만 스케일링/정규화 작업을 선행한다고 무조건 예측 성능이 향상되는 것은 아닙니다.



피처 데이터 세트에 적용하는 변환 작업은 다음과 같은 방법이 있습니다.

1. StandardScaler 클래스를 사용해 평균이 0, 분산이 1인 표준 정규 분포를 가진 데이터 세트로 변환하거나, MinMaxScaler 클래스를 이용해 최솟값이 0이고 최댓값이 1인 값으로 정규화
2. 스케일링/정규화를 수행한 데이터 세트에 다시 다항 특성을 적용 (1번 방법을 통해 예측 성능에 향상이 없을 경우 적용)
3. Log 변환(Log Transformation) 적용
   1번 방법의 경우 예측 성능 향상을 크게 기대하기 어렵고, 2번 방법의 경우 피처 개수가 많을 경우 다항 변환으로 생성되는 피처가 기하급수적으로 늘어나서 과적합 이슈가 발생할 수 있기 때문



타깃값의 경우 일반적으로 로그 변환을 적용합니다. (결정 값을 정규 분포나 다른 정규값으로 변환하면 원본 타깃값으로 원복하기 어려워서)



보스턴 주택가격 피처 데이터 세트에 위에서 언급한 표준 정규 분포 변환, 최댓값/최솟값 정규화, 로그 변환을 차례로 적용해 RMSE로 각 경우별 예측 성능을 측정해보겠습니다.

이를 위해 변환을 위한 get_scaled_data() 메소드를 생성했습니다.

**get_scaled_data()**

```python
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import PolynomialFeatures

#method는 표준 정규 분포 변환(Standard), 최댓값/최솟값 정규화(MinMax), 로그변환(Log) 결정
#p_degree는 다항식 특성을 추가할 때 적용, p_degree는 2 이상 부여하지 않음
def get_scaled_data(method='None', p_degree=None, input_data=None):
    if method == 'Standard':
        scaled_data = StandardScaler().fit_transform(input_data)
    elif method == 'MinMax':
        scaled_data = MinMaxScaler().fit_transform(input_data)
    elif method == 'Log':
        scaled_data = np.log1p(input_data)  #log() 함수 적용시 언더플로우가 발생하기 쉬워 1+log() 함수 적용
    else:
        scaled_data = input_data

    if p_degree != None:
        scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data)

    return scaled_data
```



**get_linear_reg_eval()**

```python
from sklearn.linear_model import Lasso, ElasticNet

#alpha 값에 따라 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환
#model_name : 회귀 모델의 이름, params : alpha 값들의 리스트, X_data_n : 피처 데이터 세트, y_target_n : 타깃 데이터 세트
def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True, return_coeff=True):
    coeff_df = pd.DataFrame()
    if verbose : print('#######', model_name, '#######')
    for param in params:
        if model_name == 'Ridge': model = Ridge(alpha=param)
        elif model_name == 'Lasso': model = Lasso(alpha=param)
        elif model_name == 'ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)
        neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring="neg_mean_squared_error", cv=5)
        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
        print('alpha {0}일 때 5 폴드 세트 평균 RMSE: {1:.3f}'.format(param, avg_rmse))

        #cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습해 회귀 계수 추출
        model.fit(X_data_n, y_target_n)
        if return_coeff:
            #alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame 컬럼으로 추가
            coeff = pd.Series(data=model.coef_, index=X_data_n.columns)
            colname = 'alpha:' + str(param)
            coeff_df[colname] = coeff

    return coeff_df
```



```python
#Ridge alpha 값을 다르게 적용하고 다양한 데이터 변환 방법에 따른 RMSE 추출
alphas = [0.1, 1, 10, 100]

#5개 방식으로 변환
#원본, 표준 정규 분포, 표준정규 분포,다항식 특성
#최대/최소 정규, 최대/최소 정규화 + 다항식 특성, 로그 변환
scaled_methods=[(None, None), ('Standard', None), ('Standard', 2),
               ('MinMax', None), ('MinMax', 2), ('Log', None)]

for scale_method in scaled_methods:
    X_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], input_data=X_data)
    print('\n## 변환 유형:{0}, Polynomial Degree:{1}'.format(scale_method[0], scale_method[1]))
    get_linear_reg_eval('Ridge', params=alphas, X_data_n=X_data_scaled, y_target_n=y_target, verbose=False, return_coeff=False)
```

![image](https://user-images.githubusercontent.com/76269316/127863515-60481827-5579-4f75-ac45-489bc7c28ff1.png)

<br>

표로 정리하면 다음과 같습니다.

![image](https://user-images.githubusercontent.com/76269316/127864862-ce2ddd51-d80f-45c6-914f-9b2fccc52eaa.png)

표준 정규 분포와 최솟값/최댓값 정규화로 피처 데이터 세트를 변경해도 성능상 개선은 없습니다.

alpha=1에서 최솟값/최댓값 정규화로 1차 변환 후 2차 다항식 변환을 했을 때 개선됐지만, 다항식 변환은 피처 개수가 많을 경우 과적합이 발생하기 때문에 적용하기 힘듧니다.

로그 변환은 alpha가 0.1, 1, 10인 경우 모두 좋은 성능 향상이 있는 것을 확인할 수 있습니다.

일반적으로 선형 회귀를 적용하려는 데이터 세트의 데이터 값이 심하게 왜곡되어 분포돼 있을 경우 로그 변환을 적용하는 것이 좋은 예측 결과를 기대할 수 있습니다.

<br>

<br>

<br>

### 로지스틱 회귀

로지스틱 회귀는 선형 회귀 방식을 분류에 적용한 알고리즘입니다. (로지스틱 회귀는 이름은 회귀지만 분류에 사용됩니다)

로지스틱 회귀가 선형 회귀와 다른 점은 학습을 통해서 선형 함수의 회귀 최적선을 찾는 것이 아니라 시그모이드(Sigmoid) 함수 최적선을 찾고 이 시그모이드 함수의 반환 값을 확률로 간주해 확률에 따라 분류를 결정한다는 점입니다.

<img src="https://user-images.githubusercontent.com/76269316/128439846-d21a9200-e595-4d57-8c41-77f4f619559c.png" alt="image" style="zoom:50%;" />

많은 자연, 사회 현상에서 특정 변수의 확률 값은 선형이 아니라 시그모이드 함수 같이 S자 커브 형태를 갖습니다.

시그모이드 함수 정의는 <img src="https://user-images.githubusercontent.com/76269316/128439960-e392cd3e-d13e-4c33-82e9-64b36ffa262b.png" alt="image" style="zoom:50%;" />입니다. 

시그모이드 함수는 항상 0과 1 사이 값을 반환합니다.

x 값이 커지면 1에 근사하며, x 값이 작아지면 0에 근사합니다.

<br>

로지스틱 회귀를 사용해 위스콘신 유방암 데이터 세트에서 암 여부를 판단해 보겠습니다.

로지스틱 회귀는 선형 회귀 계열이기 때문에 데이터 정규분포도에 따라 예측 성능 영향을 받을 수 있으므로, 정규 분포로 스케일링 한 다음 진행했습니다.

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score
%matplotlib inline

cancer = load_breast_cancer()

#StandardScaler()로 평균이 0, 분산 1로 데이터 분포도 변환
scaler = StandardScaler()
data_scaled = scaler.fit_transform(cancer.data)

#데이터를 학습 데이터 세트, 테스트 데이터 세트로 나눔
X_train, X_test, y_train, y_test = train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0)

#로지스틱 회귀를 이용해 학습 및 예측 수행
lr_clf = LogisticRegression()
lr_clf.fit(X_train, y_train)
lr_preds = lr_clf.predict(X_test)

#정확도와 roc_auc 측정
print('accuracy: {0:.3f}'.format(accuracy_score(y_test, lr_preds)))
print('roc_auc: {0:.3f}'.format(roc_auc_score(y_test, lr_preds)))
```

![image](https://user-images.githubusercontent.com/76269316/128440874-a116d0cc-8873-4b66-87fb-9ff1e6dc961f.png)

<br>

사이킷런 LogisticRegression 클래스 주요 하이퍼 파라미터로 penalty와 C가 있습니다.

penalty는 규제(Regularization) 유형을 설정하며 'l2'로 설정 시 L2 규제, 'l1'으로 설정 시 L1 규제를 뜻합니다. (default는 'l2')

C는 규제 강도를 조절하는 alpha 값의 역수입니다. (<img src="https://user-images.githubusercontent.com/76269316/128440981-3007aead-a32d-431e-bcda-417fcb823e00.png" alt="image" style="zoom:50%;" />)

C 값이 작을 수록 규제 강도가 큽니다.

<br>

GridSearchCV를 이용해 위스콘신 데이터 세트에서 하이퍼 파라미터를 최적화 해봤습니다.

```python
from sklearn.model_selection import GridSearchCV

params = {'penalty' : ['l2', 'l1'], 'C' : [0.01, 0.1, 1, 5, 10]}

grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring='accuracy', cv=3)
grid_clf.fit(data_scaled, cancer.target)
print('최적 하이퍼 파라미터:{0}, 최적 평균 정확도:{1:.3f}'.format(grid_clf.best_params_, grid_clf.best_score_))
```

![image](https://user-images.githubusercontent.com/76269316/128441552-fd189b6d-7d7c-4343-b4c7-4c1e92225aa6.png)

<br>

로지스틱 회귀는 가볍고 빠를뿐 아니라, 이진 분류 예측 성능도 뛰어납니다.

또한 희소한 데이터 세트 분류에도 뛰어난 성능을 보여서 텍스트 분류에서도 자주 사용됩니다.

<br>

<br>

<br>

### 회귀 트리

이번에는 회귀 함수를 기반으로 하지 않고, 결정 트리 같이 트리를 기반으로 하는 회귀 방식에 대해 알아보겠습니다.

트리 기반 회귀는 회귀를 위한 트리를 생성하고 이를 기반으로 회귀 예측을 하는 것입니다.

다만 리프 노드에서 예측 결정 값을 만드는 과정에 차이가 있는데, 

분류 트리가 특정 클래스 레이블을 결정하는 것과 다르게 회귀 트리는 리프 노드에 속한 데이터의 평균값을 구해 회귀 예측값을 계산합니다.

<br>

간단한 데이터 세트를 이용해 회귀 트리가 어떻게 동작하는지 설명하겠습니다.

피처가 하나인 X 피처 데이터 세트와 결정값 Y가 2차원 평면상에 다음 그림과 같이 있다고 가정하겠습니다.

<img src="https://user-images.githubusercontent.com/76269316/128442497-6475c1c5-5b73-4d69-9e21-2b51d8919c2d.png" alt="image" style="zoom:67%;" />

<br>

이 데이터 세트 X 피처를 결정 트리 기반으로 분할하면 X 값의 균일도를 반영한 지니 계수에 따라 다음 그림과 같이 분할할 수 있습니다.

<img src="https://user-images.githubusercontent.com/76269316/128442791-7b480936-0598-4c72-aa35-1ccbd2c38bde.png" alt="image" style="zoom: 67%;" />

루트 노드를 Split 0 기준으로 분할하고, 이렇게 분할된 규칙 노드에서 Split 1과 Split 2 규칙 노드로 분할합니다.

그리고 Split 2는 다시 재귀적으로 Split 3 규칙 노드로 분할합니다.

<br>

<img src="https://user-images.githubusercontent.com/76269316/128443187-f99474cc-4791-4cc7-b05c-e8de22446105.png" alt="image" style="zoom:67%;" />

<br>

리프 노드 생성 기준에 부합하는 트리 분할이 완료됐다면 리프 노드에 소속된 데이터 값의 평균값을 구해 최종적으로 리프 노드에 결정 값으로 할당합니다.

<img src="C:\Users\seominseok\AppData\Roaming\Typora\typora-user-images\image-20210806104543446.png" alt="image-20210806104543446" style="zoom:67%;" />

<img src="https://user-images.githubusercontent.com/76269316/128443455-0ad7874b-9fce-4b4e-bedd-a88f5b8df64c.png" alt="image" style="zoom: 67%;" />

<br>

<br>

[4번째 포스트](https://seominseok4834.github.io/machine%20learning/4.classification/)에서 소개한 모든 트리 기반 알고리즘(결정 트리, 랜덤 포레스트, GBM, XGBoost, LightGBM)은 트리 생성을 CART(Classification And Regression Trees)에 기반하기 때문에 분류 뿐만 아니라 회귀에서도 사용 가능합니다.

|     알고리즘      |   회귀 Estimator 클래스   |   분류 Estimator 클래스    |
| :---------------: | :-----------------------: | :------------------------: |
|   Decision Tree   |   DecisionTreeRegressor   |   DecisionTreeClassifier   |
| Gradient Boosting | GradientBoostingRegressor | GradientBoostingClassifier |
|      XGBoost      |       XGBRegressor        |       XGBClassifier        |
|     LightGBM      |       LGBMRegressor       |       LGBMClassifier       |

사이킷런 회귀 트리 Regressor 하이퍼 파라미터는 분류 트리 Classifier의 하이퍼 파라미터와 거의 동일하므로 [참고](https://seominseok4834.github.io/machine%20learning/4.classification/)하시길 바랍니다.

<br>

<br>

사이킷런 랜덤 포레스트 회귀 트리인 RandomForestRegressor를 이용해 보스턴 주택 가격 예측을 수행해 보겠습니다.

```python
import pandas as pd
import numpy as np
from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor

#보스턴 데이터 세트 로드
boston = load_boston()
bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)

bostonDF['PRICE'] = boston.target
y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)

rf = RandomForestRegressor(random_state=0, n_estimators=1000)
neg_mse_scores = cross_val_score(rf, X_data, y_target, scoring="neg_mean_squared_error", cv=5)
rmse_scores = np.sqrt(-1 * neg_mse_scores)
avg_rmse = np.mean(rmse_scores)

print('5 교차 검증 개별 Negative MSE Scores: ', np.round(neg_mse_scores, 2))
print('5 교차 검증 개별 RMSE Scores: ', np.round(rmse_scores, 2))
print('5 교차 검증 평균 RMSE: {0:.3f}'.format(avg_rmse))
```

![image](https://user-images.githubusercontent.com/76269316/128444473-fcb5b6d0-6102-4284-8d11-b947701396d8.png)

<br>

<br>

랜덤 포레스트 뿐 아니라, 결정 트리, GBM, XGBoost, LightGBM Regressor를 모두 사용해 보스턴 주택 가격 예측을 수행하겠습니다.

이를 위해 **get_model_cv_prediction()** 함수를 만들었습니다.

<br>

**get_model_cv_prediction()**

모델과 데이터 세트를 입력 받아 교차 검증으로 평균 RMSE를 계산해주는 함수

```python
def get_model_cv_prediction(model, X_data, y_target):
    neg_mse_scores = cross_val_score(model, X_data, y_target, scoring="neg_mean_squared_error", cv=5)
    rmse_scores = np.sqrt(-1 * neg_mse_scores)
    avg_rmse = np.mean(rmse_scores)
    
    print('#### ', model.__class__.__name__, ' ####')
    print('5 교차 검증 평균 RMSE: {0:.3f}'.format(avg_rmse))
```

<br>

결정 트리, GBM, XGBoost, LightGBM Regressor로 보스턴 주택 가격을 예측

```python
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

dt_reg = DecisionTreeRegressor(random_state=0, max_depth=4)
gb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000)
xgb_reg = XGBRegressor(n_estimators=1000)
lgb_reg = LGBMRegressor(n_estimators=1000)

#트리 기반 회귀 모델들의 평가 수행
models = [dt_reg, gb_reg, xgb_reg, lgb_reg]
for model in models:
    get_model_cv_prediction(model, X_data, y_target)
```

![image](https://user-images.githubusercontent.com/76269316/128445517-34d66943-093c-497b-902c-4237e6ca1ab8.png)

<br>

회귀 트리 Regressor 클래스는 선형 회귀 방식과 다르기 때문에 회귀 계수를 제공하는 coef_ 속성이 없습니다.

대신 feature_importance_ 속성으로 피처별 중요도를 알 수 있습니다.

```python
import seaborn as sns
%matplotlib inline

rf_reg = RandomForestRegressor(n_estimators=1000)

#앞 예제에서 만들어진 X_data, y_target 데이터 세트를 적용해 학습
rf_reg.fit(X_data, y_target)

feature_series = pd.Series(data=rf_reg.feature_importances_, index=X_data.columns)
feature_series = feature_series.sort_values(ascending=False)
sns.barplot(x=feature_series, y=feature_series.index)
```

![image](https://user-images.githubusercontent.com/76269316/128445873-a7a27ccf-f2bb-4f94-a433-d1dd9d9a21dc.png)

<br>

<br>

이번에는 회귀 트리 Regressor가 어떻게 예측값을 판단하는지 선형 회귀와 비교해 시각화해 보겠습니다.

보스턴 주택 데이터 세트 100개만 샘플링해 사용하고, 2차원 평면상에 회귀 예측선을 쉽게 표현하기 위해 Price와 가장 밀접한 양의 상관관계를 갖는 RM 컬럼만 추출하겠습니다.

```python
import pandas as pd
from sklearn.datasets import load_boston
import matplotlib.pyplot as plt
%matplotlib inline

#보스턴 데이터 세트 로드
boston = load_boston()
bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)
bostonDF['PRICE'] = boston.target

#2차원 평면상에 X축에 독립변수 RM, Y축에 종속변수인 PRICE를 산점도로 시각화
bostonDF_sample = bostonDF[['RM', 'PRICE']]
bostonDF_sample = bostonDF_sample.sample(n=100, random_state=0)
print(bostonDF_sample.shape)
plt.figure()
plt.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c="darkorange")
```

![image](https://user-images.githubusercontent.com/76269316/128446585-d7caa9a6-ef3e-4b8c-9e96-e5aa78e532f1.png)

<br>

다음으로 보스턴 데이터 세트에 대해 LinearRegression과 DecisionTreeRegressor를 max_depth 2, 7로 해서 학습하고,

학습된 Regressor에 RM 값이 4.5~8.5인 100개의 테스트 데이터 세트를 예측한 값을 구해보겠습니다.

```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor

#선형 회귀와 결정 트리 기반 Regressor 생성, DecisionTreeRegressor의 max_depth는 각각 2, 7
lr_reg = LinearRegression()
rf_reg2 = DecisionTreeRegressor(max_depth=2)
rf_reg7 = DecisionTreeRegressor(max_depth=7)

#실제 예측을 적용할 테스트용 데이터 세트를 4.5~8.5까지 100개 데이터 세트로 생성
X_test = np.arange(4.5, 8.5, 0.04).reshape(-1, 1)

#보스턴 주택 가격 데이터에서 시각화를 위해 RM 피처, 결정 데이터엔 PRICE만 추출
X_feature = bostonDF_sample['RM'].values.reshape(-1, 1)
y_target = bostonDF_sample['PRICE'].values.reshape(-1, 1)

#학습과 예측 수행
lr_reg.fit(X_feature, y_target)
rf_reg2.fit(X_feature, y_target)
rf_reg7.fit(X_feature, y_target)

pred_lr = lr_reg.predict(X_test)
pred_rf2 = rf_reg2.predict(X_test)
pred_rf7 = rf_reg7.predict(X_test)
```

<br>

학습된 LinearRegression과 DecisionTreeRegressor에서 예측한 Price 회귀선을 그려보겠습니다.

```python
fig, (ax1, ax2, ax3) = plt.subplots(figsize=(14, 4), ncols=3)

#X축 값을 4.~8.5로 변환하며 입력했을 때 선형 회귀와 결정 트리 회귀 예측선 시각화
#선형 회귀로 학습된 모델 회귀 예측선
ax1.set_title('Linear Regression')
ax1.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c="darkorange")
ax1.plot(X_test, pred_lr, label='linear', linewidth=2)

#DecisionTreeRegressor(max_depth=2) 회귀 예측선
ax2.set_title('Decision Tree Regression\nmax_depth=2')
ax2.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c="darkorange")
ax2.plot(X_test, pred_rf2, label="max_depth:2", linewidth=2)

#DecisionTreeRegressor(max_depth=7) 회귀 예측선
ax3.set_title('Decision Tree Regression\nmax_depth=7')
ax3.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c="darkorange")
ax3.plot(X_test, pred_rf7, label="max_depth:7", linewidth=2)
```

![image](https://user-images.githubusercontent.com/76269316/128447945-a067e57a-e9a7-424b-b246-a41f74ac2bb8.png)

선형 회귀는 직선으로 예측 회귀선을 표현하는데 반해, 회귀 트리의 경우 분할되는 데이터 지점에 따라 브랜치를 만들면서 계단 형태의 회귀선을 만듧니다.

DecisionTreeRegressor의 max_depth=7인 경우에는 학습 데이터 세트의 이상치(outlier) 데이터도 학습하면서(과적합) 복잡한 계단 형태의 회귀선을 만들었습니다.

<br>

<br>

<br>

### 회귀 실습 - 자전거 대여 수요 예측

[캐글 자전거 대여 수요 예측 경연 (Bike Sharing Demand)](https://www.kaggle.com/c/bike-sharing-demand/data)에서 사용된 데이터 세트를 이용해 선형 회귀와 트리 기반 회귀를 비교해 보겠습니다.

저는 train.csv 파일을 **bike_train.csv**로 파일명을 변경하여 사용했습니다.

<br>

데이터 세트의 주요 컬럼은 다음과 같습니다.

- datetime : hourly date + timestamp
- season : 1 = 봄, 2 = 여름, 3 = 가을, 4 = 겨울
- holiday : 1 = 주말을 제외한 국경일 등의 휴일, 0 = 휴일이 아닌 날
- workingday : 1 =주말 및 휴일이 아닌 주중, 0 = 주말 및 휴일
- weather :
  - 1 = 맑음. 약간 구름 낀 흐림
  - 2 = 안개, 안개 + 흐림
  - 3 = 가벼운 눈, 가벼운 비 + 천둥
  - 4 = 심한 눈/비, 천둥/번개
- temp : 온도(섭씨)
- atemp : 체감온도(섭씨)
- humidity :  상대습도
- windspeed : 풍속
- casual : 사전에 등록되지 않는 사용자가 대여한 횟수
- registered : 사전에 등록된 사용자가 대여한 횟수
- count : 대여 횟수 (casual + registered)

<br>

<br>

##### 데이터 클렌징 및 가공

```python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore", category=RuntimeWarning)
%matplotlib inline

bike_df = pd.read_csv('./bike_train.csv')
print(bike_df.shape)
bike_df.head()
```

![image](https://user-images.githubusercontent.com/76269316/128454081-2660aafb-9034-49e7-9fbc-4b4f66360098.png)

10886개의 레코드와 12개의 컬럼으로 구성돼 있습니다.

<br>

```python
bike_df.info()
```

![image](https://user-images.githubusercontent.com/76269316/128454123-8c507012-91e4-4a97-9caa-22071f7a10af.png)

null 값은 없고, 대부분 컬럼이 int 또는float형인데, datetime 컬럼이 object형입니다.

Datetime 컬럼은 년-월-일 시:분:초 형식으로 돼 있으므로, 이를 년, 월, 일, 시간 4개의 속성으로 분리하겠습니다.

변환을 편리하게 하기 위해 pd.to_datetime 메소드로 'datetime' 타입으로 변경하겠습니다. (pandas의 datetime 타입입니다)

```python
#문자열을 datetime 타입으로 변경
bike_df['datetime'] = bike_df.datetime.apply(pd.to_datetime)

#datetime 타입에서 년, 월, 일 시간 추출
bike_df['year'] = bike_df.datetime.apply(lambda x : x.year)
bike_df['month'] = bike_df.datetime.apply(lambda x : x.month)
bike_df['day'] = bike_df.datetime.apply(lambda x : x.day)
bike_df['hour'] = bike_df.datetime.apply(lambda x : x.hour)
bike_df.head(3)
```

![image](https://user-images.githubusercontent.com/76269316/128455226-3dded325-843a-4a76-8eaf-740def4f4d25.png)

<br>

datetime 컬럼은 year, month, day, hour 컬럼으로 분리했기 때문에 더 이상 필요하지 않고,

count 컬럼이 casual + registered 컬럼이므로 casual 컬럼과 registered 컬럼을 지우도록 하겠습니다.

```python
drop_columns = ['datetime', 'casual', 'registered']
bike_df.drop(drop_columns, axis=1, inplace=True)
```

<br>

이제 다양한 회귀 모델을 사용해 예측 성능을 측정해보겠습니다.

이를 위해 MAE, RMSE, RMSLE를 한 번에 평가하는 함수 **evaluate_regr()**를 생성했습니다.

##### evaluate_regr()

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error

#사이킷런은 RMSLE를 제공하지 않아 RMSLE를 직접 계산하는 함수 생성
#log 값 변환 시 언더플로우가 발생하기 쉬워 log1p()를 이용해 계산
def rmsle(y, pred):
    log_y = np.log1p(y)
    log_pred = np.log1p(pred)
    squared_error = (log_y - log_pred) ** 2
    rmsle = np.sqrt(np.mean(squared_error))
    return rmsle

#사이킷런 mean_squared_error()를 이용해 RMSE 계산
def rmse(y, pred):
    return np.sqrt(mean_squared_error(y, pred))

#MSE, RMSE, MSLE 계산
def evaluate_regr(y, pred):
    rmsle_val = rmsle(y, pred)
    rmse_val = rmse(y, pred)
    #MAE는 사이킷런 mean_absolute_error()로 계산
    mae_val = mean_absolute_error(y, pred)
    print('RMSLE: {0:.3f}, RMSE: {1:.3f}, MAE: {2:.3f}'.format(rmsle_val, rmse_val, mae_val))
```

<br>

+아래처럼 RMSLE를 구할 수도 있지만, 데이터 값의 크기에 따라 오버플로/언더플로가 발생하기 쉬워 위 코드처럼 구현했습니다.

```python
def rmsle(y, pred):
    msle = mean_squared_log_error(y, pred)
    rmsle = np.sqrt(mse)
    return rmsle
```

<br>

```python
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression

y_target = bike_df['count']
X_features = bike_df.drop(['count'], axis=1, inplace=False)

X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0)

lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)
pred = lr_reg.predict(X_test)

evaluate_regr(y_test,pred)
```

![image](https://user-images.githubusercontent.com/76269316/128456521-0d7f8417-565b-4fba-a1fe-296fe2f54ee1.png)

실행 결과 비교적 큰 오류값이 나왔습니다.

 <br>

실제 값과 예측값이 어느정도 차이 나는지 DataFrame 컬럼으로 만든 뒤 상위 5개만 확인해보겠습니다.

```python
def get_top_error_data(y_test, pred, n_tops=5):
    #DataFrame 컬럼으로 실제 대여 횟수(count)와 예측값의 차이 생성
    result_df = pd.DataFrame(y_test.values, columns=['real_count'])
    result_df['predicted_count'] = np.round(pred)
    result_df['diff'] = np.abs(result_df['real_count'] - result_df['predicted_count'])
    
    #예측값과 실제 값이 가장 큰 데이터 순으로 n_tops개만 출력
    print(result_df.sort_values('diff', ascending=False)[:n_tops])
    
get_top_error_data(y_test, pred, n_tops=5)
```

![image](https://user-images.githubusercontent.com/76269316/128457158-cd95d8be-682b-47a1-a9ac-26fa9b3a1c4d.png)

상위 5개 오류 값은 546~568로 예측 오류가 큰 것을 확인할 수 있습니다.

이렇게 큰 예측 오류가 발생할 경우 Target 값 분포가 왜곡된 형태를 이루고 있는지 먼저 확인해야 합니다.

자전거 대여 데이터 세트의 Target 값인 count 컬럼이 정규 분포를 이루는지 확인해 보겠습니다.

```python
y_target.hist()
```

![image](https://user-images.githubusercontent.com/76269316/128457279-ba702d69-4546-471f-9ac5-8aef36cce020.png)

count 컬럼 값이 정규 분포가 아닌 0~200 사이에 왜곡돼 있는 것을 확인할 수 있습니다.

이렇게 왜곡된 값을 정규 분포 형태로 바꾸는 가장 일반적인 방법은 로그를 적용하는 것입니다.

<br>
<br>

##### 로그 변환, 피처 인코딩과 모델 학습/예측/평가

log1p()를 사용해 Target 값을 변환해 보겠습니다.

*+log1p를 적용하는 이유는 expm1() 함수를 적용해 원래 값으로 원상 복구하기 쉽기 때문입니다.*

```python
y_log_transform = np.log1p(y_target)
y_log_transform.hist()
```

![image](https://user-images.githubusercontent.com/76269316/128457890-64e7c9db-7fb3-44d3-a93a-09da5d8f6060.png)

정규 분포 형태는 아니지만 변환 전보다 왜곡 정도가 줄어든 것을 확인할 수 있습니다.

<br>

다시 학습한 후 평가를 수행해 보겠습니다.

```python
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression

y_target = bike_df['count']
y_target_log = np.log1p(y_target)
X_features = bike_df.drop(['count'], axis=1, inplace=False)

X_train, X_test, y_train, y_test = train_test_split(X_features, y_target_log, test_size=0.3, random_state=0)

lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)
pred = lr_reg.predict(X_test)

#테스트 데이터 세트 Target 값은 로그 변환됐으므로, 다시 expm1을 사용해 원래 스케일로 변환
y_test_exp = np.expm1(y_test)

#예측값도 다시 expm1을 사용해 원래 스케일로 변환
pred_exp = np.expm1(pred)

evaluate_regr(y_test_exp,pred_exp)
```

|                         로그 변환 전                         |                         로그 변환 후                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/128456521-0d7f8417-565b-4fba-a1fe-296fe2f54ee1.png) | ![image](https://user-images.githubusercontent.com/76269316/128458210-e5cc2132-b22b-40bb-8c3f-8cfaad4e30ed.png) |

RMSLE 값은 줄었지만, RMSE 값은 오히려 늘어났습니다.

각 피처의 회귀 계수 값을 시각화해 보겠습니다.

```python
coef = pd.Series(lr_reg.coef_, index=X_features.columns)
coef_sort = coef.sort_values(ascending=False)
sns.barplot(x=coef_sort.values, y=coef_sort.index)
```

![image](https://user-images.githubusercontent.com/76269316/128458425-c0c38480-01e1-4900-956c-1d2667ee6e68.png)

year 피처 회귀 계수 값이 독보적으로 큰 회귀 계수 값을 갖고 있습니다.

year는 2011, 2012 두 개의 값 밖에 없을 뿐더러, 자전거 대여 횟수에 크게 영향을 미친다는 것도 이상합니다.

<br>

year 컬럼은 숫자형 카테고리 값으로 선형 회귀에서 회귀 계수를 연산할 때 이 숫자형 값에 크게 영향을 받아서 저렇게 큰 회귀 계수 값이 나온 것으로 추정됩니다.

따라서 year 컬럼에 [레이블 인코딩](https://seominseok4834.github.io/machine%20learning/2.scikit-learn-machine-learning-in-python/#%EB%A0%88%EC%9D%B4%EB%B8%94-%EC%9D%B8%EC%BD%94%EB%94%A9-label-encoding)이 아닌 [원-핫 인코딩](https://seominseok4834.github.io/machine%20learning/2.scikit-learn-machine-learning-in-python/#%EC%9B%90-%ED%95%AB-%EC%9D%B8%EC%BD%94%EB%94%A9-one-hot-encoding)을 적용한 뒤 다시 예측 성능을 확인해 보겠습니다.

<br>

여러 모델에서 테스트하기 위해 **get_model_predict()** 메소드를 생성했습니다.

**get_model_predict()**

```python
#모델과 학습/테스트 세트를 입력하면 성능 평가 수치를 반환
def get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=False):
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    #테스트 데이터 세트, 예측값이 로그 변환 됐으므로 원래 스케일로 변환
    if is_expm1:
        y_test = np.expm1(y_test)
        pred = np.expm1(pred)
    print('###', model.__class__.__name__, '###')
    evaluate_regr(y_test, pred)
```

<br>

**evaluate_regr()**

MAE, RMSE, RMSLE를 한 번에 평가하는 함수

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error

#사이킷런은 RMSLE를 제공하지 않아 RMSLE를 직접 계산하는 함수 생성
#log 값 변환 시 언더플로우가 발생하기 쉬워 log1p()를 이용해 계산
def rmsle(y, pred):
    log_y = np.log1p(y)
    log_pred = np.log1p(pred)
    squared_error = (log_y - log_pred) ** 2
    rmsle = np.sqrt(np.mean(squared_error))
    return rmsle

#사이킷런 mean_squared_error()를 이용해 RMSE 계산
def rmse(y, pred):
    return np.sqrt(mean_squared_error(y, pred))

#MSE, RMSE, MSLE 계산
def evaluate_regr(y, pred):
    rmsle_val = rmsle(y, pred)
    rmse_val = rmse(y, pred)
    #MAE는 사이킷런 mean_absolute_error()로 계산
    mae_val = mean_absolute_error(y, pred)
    print('RMSLE: {0:.3f}, RMSE: {1:.3f}, MAE: {2:.3f}'.format(rmsle_val, rmse_val, mae_val))
```

<br>

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso

bike_df = pd.read_csv('./bike_train.csv')

#문자열을 datetime 타입으로 변경
bike_df['datetime'] = bike_df.datetime.apply(pd.to_datetime)

#datetime 타입에서 년, 월, 일 시간 추출
bike_df['year'] = bike_df.datetime.apply(lambda x : x.year)
bike_df['month'] = bike_df.datetime.apply(lambda x : x.month)
bike_df['day'] = bike_df.datetime.apply(lambda x : x.day)
bike_df['hour'] = bike_df.datetime.apply(lambda x : x.hour)

#불필요한 피처 drop
drop_columns = ['datetime', 'casual', 'registered']
bike_df.drop(drop_columns, axis=1, inplace=True)

#피처 데이터 세트와 레이블 데이터 세트 생성
y_target = bike_df['count']
y_target_log = np.log1p(y_target)
X_features = bike_df.drop(['count'], axis=1, inplace=False)

#'year', 'month', 'day', 'hour' 등의 피처를 One-Hot Encoding
X_features_ohe = pd.get_dummies(X_features, columns=['year', 'month', 'day', 'hour', 'holiday', 'workingday', 'season', 'weather'])

#원-핫 인코딩이 적용된 피처 데이터 세트 기반으로 학습/예측 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.3, random_state=0)

#모델별로 평가 수행
lr_reg = LinearRegression()
ridge_reg = Ridge(alpha=10)
lasso_reg = Lasso(alpha=0.01)

for model in [lr_reg, ridge_reg, lasso_reg]:
    get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=True)
```

|                     Label Encoding 적용                      |                    One-Hot Encoding 적용                     |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/128465134-63add2b8-dac4-4962-a88b-263f55f59186.png) | ![image-20210806151534435](C:\Users\seominseok\AppData\Roaming\Typora\typora-user-images\image-20210806151534435.png) |

원-핫 인코딩 적용 후 예측 성능이 크게 향상된 것을 볼 수 있습니다.

<br>

원-핫 인코딩된 데이터 세트에서 회귀 계수가 높은 피처를 시각화해 보겠습니다.

원-핫 인코딩으로 피처가 늘어났으므로 회귀 계수 상위 25개 피처만 추출해 보겠습니다.

```python
import seaborn as sns
%matplotlib inline

coef = pd.Series(lr_reg.coef_, index=X_features_ohe.columns)
coef_sort = coef.sort_values(ascending=False)[:20]
sns.barplot(x=coef_sort.values, y=coef_sort.index)
```

![image](https://user-images.githubusercontent.com/76269316/128465765-e328bb7a-a5cd-435c-a5ea-3ffbb15d12d6.png)

month_9, month_8, month_7등 월 관련 피처들과 working_day, hour 관련 피처들의 회귀 계수가 높은 것을 볼 수 있습니다.

월, 주말/주중, 시간대등 상식선에서 자전거를 타는데 필요한 피처 회귀 계수가 높아졌습니다.

이처럼 선형 회귀 수행 시에는 피처를 어떻게 인코딩하는가가 성능에 큰 영향을 미칠 수 있습니다.

<br>

위에서 적용한 Target 값의 로그 변환된 값과 원-핫 인코딩된 피처 데이터 세트를 그대로 사용해 랜덤 포레스트, GBM, XGBoost, LightGBM을 순차적으로 성능 평가해 보겠습니다.

<br>

**get_model_predict()**

여러 모델에서 테스트, 평가하기 위한 함수

```python
#모델과 학습/테스트 세트를 입력하면 성능 평가 수치를 반환
def get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=False):
    model.fit(X_train, y_train)
    pred = model.predict(X_test)
    #테스트 데이터 세트, 예측값이 로그 변환 됐으므로 원래 스케일로 변환
    if is_expm1:
        y_test = np.expm1(y_test)
        pred = np.expm1(pred)
    print('###', model.__class__.__name__, '###')
    evaluate_regr(y_test, pred)
```

<br>

**evaluate_regr()**

MAE, RMSE, RMSLE를 한 번에 평가하는 함수

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error

#사이킷런은 RMSLE를 제공하지 않아 RMSLE를 직접 계산하는 함수 생성
#log 값 변환 시 언더플로우가 발생하기 쉬워 log1p()를 이용해 계산
def rmsle(y, pred):
    log_y = np.log1p(y)
    log_pred = np.log1p(pred)
    squared_error = (log_y - log_pred) ** 2
    rmsle = np.sqrt(np.mean(squared_error))
    return rmsle

#사이킷런 mean_squared_error()를 이용해 RMSE 계산
def rmse(y, pred):
    return np.sqrt(mean_squared_error(y, pred))

#MSE, RMSE, MSLE 계산
def evaluate_regr(y, pred):
    rmsle_val = rmsle(y, pred)
    rmse_val = rmse(y, pred)
    #MAE는 사이킷런 mean_absolute_error()로 계산
    mae_val = mean_absolute_error(y, pred)
    print('RMSLE: {0:.3f}, RMSE: {1:.3f}, MAE: {2:.3f}'.format(rmsle_val, rmse_val, mae_val))
```

<br>

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

bike_df = pd.read_csv('./bike_train.csv')

#문자열을 datetime 타입으로 변경
bike_df['datetime'] = bike_df.datetime.apply(pd.to_datetime)

#datetime 타입에서 년, 월, 일 시간 추출
bike_df['year'] = bike_df.datetime.apply(lambda x : x.year)
bike_df['month'] = bike_df.datetime.apply(lambda x : x.month)
bike_df['day'] = bike_df.datetime.apply(lambda x : x.day)
bike_df['hour'] = bike_df.datetime.apply(lambda x : x.hour)

#불필요한 피처 drop
drop_columns = ['datetime', 'casual', 'registered']
bike_df.drop(drop_columns, axis=1, inplace=True)

#피처 데이터 세트와 레이블 데이터 세트 생성
y_target = bike_df['count']
y_target_log = np.log1p(y_target)
X_features = bike_df.drop(['count'], axis=1, inplace=False)

#'year', 'month', 'day', 'hour' 등의 피처를 One-Hot Encoding
X_features_ohe = pd.get_dummies(X_features, columns=['year', 'month', 'day', 'hour', 'holiday', 'workingday', 'season', 'weather'])

#원-핫 인코딩이 적용된 피처 데이터 세트 기반으로 학습/예측 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.3, random_state=0)

#랜덤 포레스트, GBM, XGBoost, LightGBM model별로 평가 수행
rf_reg = RandomForestRegressor(n_estimators=500)
gbm_reg = GradientBoostingRegressor(n_estimators=500)
xgb_reg = XGBRegressor(n_estimators=500)
lgbm_reg = LGBMRegressor(n_estimators=500)

for model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]:
    #XGBoost의 경우 DataFrame이 입력될 경우 버전에 따라 오류 발생 가능하기 때문에 ndarray로 변환
    get_model_predict(model, X_train.values, X_test.values, y_train.values, y_test.values, is_expm1=True)
```

![image](https://user-images.githubusercontent.com/76269316/128467289-c087e62f-4933-4a10-b045-dcb69d92af52.png)

선형 회귀 모델보다 예측 성능이 개선됐습니다.

그렇다고 항상 회귀 트리가 선형 회귀보다 더 좋은 성능을 가지는 것은 아닙니다.

<br>

<br>

<br>

### 회귀 실습 - 캐글 주택 가격: 고급 회귀 기법

이번에는 캐글에서 제공하는 [캐글 주택 가격 : 고급 회귀 기법(House Prices: Advanced Regression Techniques)](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) 데이터 세트를 사용해 회귀 분석을 더 심층적으로 해보겠습니다.

저는 train.csv 파일을 **house_price.csv**로 파일명을 변경해서 사용했습니다.

위 데이터 세트는 80개의 피처로 구성된 미국 아이오와 주의 에임스 지방의 주택 가격 정보를 갖고 있습니다.

<br>

먼저 데이터를 확인해 보도록 하겠습니다.

데이터가공을 많이 수행할 예정이므로 원본 csv 파일 기반 DataFrame은 보관하고 복사해서 데이터를 가공하도록 하겠습니다.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
%matplotlib inline
warnings.filterwarnings('ignore')

house_df_org = pd.read_csv('house_price.csv')
house_df = house_df_org.copy()
house_df.head()
```

![image](https://user-images.githubusercontent.com/76269316/128468590-1d65de49-7e45-4daa-bac2-89f84b402699.png)

![image](https://user-images.githubusercontent.com/76269316/128468669-3c8a262a-84fb-467a-8381-8ca817b87d4a.png)

target 값은 맨 마지막 컬럼인 SalePrice입니다.

<br>

데이터 세트 전체 크기와 컬럼 타입, Null값이 있는 컬럼과 건수를 내림차순으로 출력해 보겠습니다.

```python
print('데이터 세트 Shape: ', house_df.shape)
print('\n전체 피처 type\n', house_df.dtypes.value_counts())
isnull_series = house_df.isnull().sum()
print('\nNull 컬럼과 개수\n', isnull_series[isnull_series > 0].sort_values(ascending=False))
```

![image](https://user-images.githubusercontent.com/76269316/128468958-48c6ede1-3c8b-410b-81de-7ef3fb3adb70.png)

데이터 세트는 1460개의 레코드와 81개 피처로 구성돼 있으며, 피처 타입에 문자형이 많이 포함돼 있습니다.



<br>

Target 값의 분포가 정규 분포인지 확인해 보겠습니다.

```python
plt.title('Original Sale Price Histogram')
sns.distplot(house_df['SalePrice'])
```

![image](https://user-images.githubusercontent.com/76269316/128469324-843ce948-e4ea-4c71-a55a-222d53f6438b.png)

중심에서 왼쪽으로 치우친 형태로, 정규 분포에서 벗어나 있습니다.

앞서 자전거 대여 수요 예측처럼 로그 변환(Log Transformation)을 적용해 정규 분포 형태로 변환해 보겠습니다.

```python
plt.title('Log Transformed Sale Price Histogram')
log_SalePrice = np.log1p(house_df['SalePrice'])
sns.distplot(log_SalePrice)
```

![image](https://user-images.githubusercontent.com/76269316/128586680-705a6998-f7a0-4d32-b159-c111c3129211.png)

정규 분포 형태로 변환된 것을 확인할 수 있습니다.

<br>

PoolQC, MiscFeature, Alley, Fence, FireplaceQu는 너무 많은 데이터가 Null이기 때문에 드롭하고, 나머지는 평균값으로 대체하겠습니다. (LotFrontage도 259개로 null 값이 많으나 평균값으로 대체했습니다)

또한 Id 컬럼도 단순 식별자이므로 삭제했습니다.

<br>

아래 코드는 target 값(SalePrice) 로그 변환, null 값이 많은 피처 드롭, 드롭하지 않은 숫자형 피처들의 null 값을 평균값으로 대체하는 코드입니다.

```python
#SalePrice 로그 변환
original_SalePrice = house_df['SalePrice']
house_df['SalePrice'] = np.log1p(house_df['SalePrice'])

#Null 값이 너무 많은 컬럼과 불필요한 컬럼 삭제
house_df.drop(['Id', 'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], axis=1, inplace=True)

#드롭하지 않은 숫자형 컬럼의 null 값은 평균값으로 대체
house_df.fillna(house_df.mean(), inplace=True)
```

<br>

Null 값이 남아있는지 확인해보겠습니다.

```python
#Null 값이 있는 피처명과 타입 추출
null_column_count = house_df.isnull().sum()[house_df.isnull().sum() > 0]
print('## Null 피처 Type\n', house_df.dtypes[null_column_count.index])
```

![image](https://user-images.githubusercontent.com/76269316/128586964-293925da-d975-48ac-8f97-e18cf56c2c3b.png)

문자형 피처를 제외하고는 Null 값이 없는 것을 확인할 수 있습니다.

<br>

문자형 피처에 판다스 get_dummies()를 사용해 모두 원-핫 인코딩으로 변환하겠습니다.

*+get_dummies()는 자동으로 문자열 피처를 원-핫 인코딩으로 변환하면서 Null 값을 'None' 컬럼으로 대체해주기  때문에 Null 값을 대체하는 로직을 작성할 필요가 없습니다.*

<br>

```python
print('get_dummies() 수행 전 데이터 Shape: ', house_df.shape)
house_df_ohe = pd.get_dummies(house_df)
print('get_dummies() 수행 후 데이터 Shape: ', house_df_ohe.shape)

null_column_count = house_df_ohe.isnull().sum()[house_df_ohe.isnull().sum() > 0]
print('## Null 피처 Type: ', house_df_ohe.dtypes[null_column_count.index])
```

![image](https://user-images.githubusercontent.com/76269316/128588175-8d215676-9029-4d3c-8aed-eaf5b952a581.png)

원-핫 인코딩 적용 후 피처가 75개에서 271개로 증가했습니다.

Null 값을 갖는 피처가 더 이상 존재하지 않는 것을 확인할 수 있습니다.

<br>

<br>

### 궁금한 점

| 원-핫 인코딩으로 변환시 null 값을 None 컬럼으로 변경해준다고 했는데, null 값이 아닌 값들처럼 피처명_None 컬럼으로 변경하는 것인지 아니면 다른 피처의 null 값을 하나에 저장하기 위해 None 컬럼으로 변경하는 것인지 잘 모르겠습니다,, |
| ------------------------------------------------------------ |

<br>

```python
print('Null 값의 개수: ', house_df.isnull().sum().sum())
```

![image](https://user-images.githubusercontent.com/76269316/128587965-2d0da2ee-cc90-44e9-803c-68c564f22f02.png)

원-핫 인코딩 적용 전에, Object 타입 피처들의 Null 값은 총 520개가 있다고 나와있는데,

```python
house_df_ohe = pd.get_dummies(house_df)

for column in house_df_ohe.columns:
    print(column)
```

원-핫 인코딩 적용 후 컬럼명을 출력해보면 None이 붙은 컬럼명은 한 개뿐입니다.

<br>

```python
print('get_dummies() 수행 전 데이터 Shape: ', house_df.shape)
house_df_ohe = pd.get_dummies(house_df)
print('get_dummies() 수행 후 데이터 Shape: ', house_df_ohe.shape)

null_column_count = house_df_ohe.isnull().sum()[house_df_ohe.isnull().sum() > 0]
print('## Null 피처 Type: ', house_df_ohe.dtypes[null_column_count.index])
```

![image](https://user-images.githubusercontent.com/76269316/128587993-842d56cc-7d3f-4db3-8caa-1df5412bd011.png)

그래서 모든 피처의 null 값을 MasVnrType_None 컬럼에 저장한 건가하고 확인해보니까

```python
MasVnrType_None = house_df_ohe['MasVnrType_None']
print('MasVnrType_None 피처 값\n', MasVnrType_None.value_counts())
```



![image](https://user-images.githubusercontent.com/76269316/128588021-b8bbf6a5-4822-4aae-a22b-98d02d29eef2.png)

1이 864개가 나와서 위에서 카운트한 null 값이랑 안 맞습니다.

흠.. 아시는 분 알려주세요ㅠ

<br>

<br>

##### 선형 회귀 모델 학습/예측/평가

사이킷런 LinearRegression, Ridge, Lasso를 사용해 선형 회귀 모델을 만들어 보겠습니다.

예측 평가는 RMSLE(실제 값과 예측값의 오류를 로그 변환한 뒤 RMSE 적용)로 하겠습니다.

이미 앞에서 target(SalePrice) 값을 로그 변환을 해서, SalePrice를 기반으로 예측한 예측값 역시 로그 변환된 값입니다.

따라서 예측 결과 오류에 RMSE만 적용하면 됩니다.

<br>

여러 모델의 로그 변환된 RMSE를 측정할 것이므로 get_rmse()를 작성했습니다.



**get_rmses()**

get_rmse()에서 단일 모델의 RMSE를 계산

```python
def get_rmse(model):
    pred = model.predict(X_test)
    mse = mean_squared_error(y_test, pred)
    rmse = np.sqrt(mse)
    print(model.__class__.__name__, '로그 변환된 RMSE: ', np.round(rmse, 3))
    return rmse

def get_rmses(models):
    rmses = []
    for model in models:
        rmse = get_rmse(model)
        rmses.append(rmse)
    return rmses
```

<br>

```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

y_target = house_df_ohe['SalePrice']
X_features = house_df_ohe.drop('SalePrice', axis=1, inplace=False)
X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)

#LinearRegression, Ridge, Lasso 학습, 예측, 평가
lr_reg = LinearRegression()
lr_reg.fit(X_train, y_train)
ridge_reg = Ridge()
ridge_reg.fit(X_train, y_train)
lasso_reg = Lasso()
lasso_reg.fit(X_train, y_train)

models = [lr_reg, ridge_reg, lasso_reg]
get_rmses(models)
```

![image](https://user-images.githubusercontent.com/76269316/128588512-c2615fbf-3e2b-42b4-8a6f-2047d7db5944.png)

라쏘 회귀의 경우 다른 방식보다 성능이 떨어지는 결과가 나왔습니다.

먼저 피처별 회귀 계수를 시각화해서 모델별로 어떤 피처의 회귀 계수로 구성 됐는지 확인해 보겠습니다.

피처가 많으니 상위 10개, 하위 10개의 피처명과 회귀 계수 값만 확인하겠습니다.

<br>

**get_top_bottom_coef()**

회귀 계수 값 상위 10개, 하위 10개의 피처명과 회귀 계수 값을 갖는 Series 객체를 반환하는 함수

```python
def get_top_bottom_coef(model, n = 10):
    #coef_ 속성을 기반으로 Series 객체 생성, index는 컬럼명
    coef = pd.Series(model.coef_, index=X_features.columns)
    
    #상위 10개, 하위 10개 회귀 계수 추출
    coef_high = coef.sort_values(ascending=False).head(n)
    coef_low = coef.sort_values(ascending=False).tail(n)
    return coef_high, coef_low
```

<br>

**visualize_coefficient()**

get_top_bottom_coef()에서 반환받은 상위 10개, 하위 10개의 회귀 계수를 시각화

```python
def visualize_coefficient(models):
    #3개 회귀 모델의 시각화를 위해 3개 컬럼을 갖는 subplot 생성
    fig, axs = plt.subplots(figsize=(24, 10), nrows=1, ncols=3)
    fig.tight_layout()  #서브플롯 간 간격 자동 유지
    
    #입력 인자로 받은 list 객체 models에서 차례로 model을 추출해 회귀 계수 시각화
    for i_num, model in enumerate(models):
        #상위 10개, 하위 10개 회귀 계수를 구하고, 판다스 concat으로 결합
        coef_high, coef_low = get_top_bottom_coef(model)
        coef_concat = pd.concat([coef_high, coef_low])
        #ax subplot에 barchar로 표현. 한 화면에 표현하기 위해 tick label 위치, font 크기 조정
        axs[i_num].set_title(model.__class__.__name__ + 'Coefficients', size=25)
        axs[i_num].tick_params(axis="y", direction="in", pad=-120)
        for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()):
            label.set_fontsize(22)
        sns.barplot(x=coef_concat.values, y=coef_concat.index, ax=axs[i_num])
```

<br>

<br>

visualize_coefficient()를 사용해서 위에서 생성한 3개 모델의 회귀 계수를 시각화해 보겠습니다.

```python
#앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델 회귀 계수 시각화
models = [lr_reg, ridge_reg, lasso_reg]
visualize_coefficient(models)
```

![image](https://user-images.githubusercontent.com/76269316/128588817-3f3c5664-4e11-430a-98cb-c3f2e243d27a.png)

OLS 기반의 LinearRegression과 Ridge의 경우 회귀 계수가 유사한 반면, 라쏘는 전체적으로 회귀 계수 값이 작고 두 모델과 다른 형태를 보이고 있습니다.

<br>

이번에는 5개의 교차 검증 폴드 세트로 분할해 평균 RMSLE를 측정해 보겠습니다.
