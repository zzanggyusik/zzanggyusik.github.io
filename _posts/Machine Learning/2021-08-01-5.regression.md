---
title:  "5.Regression"
excerpt: "회귀"
toc: true
toc_label: "Regression"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - 회귀
  - 파이썬 머신러닝 완벽 가이드
last_modified_at: 2021-08-01
---

>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.



### 회귀 소개

회귀 분석은 유전적 특성을 연구하던 영국 통계학자 갈톤(Galton)이 수행한 연구에서 유래했다고 알려져있습니다.

부모, 자식 간의 키 상관관계를 분석했던 갈톤은 부모의 키가 모두 클 때 자식의 키가 크긴 하지만 부모를 능가할 정도로 크지 않았고,

부모의 키가 모두 아주 작을 때 그 자식의 키가 작기는 하지만 부모보다는 큰 경향을 발견했습니다.

즉, 사람의 키는 평균 키로 회귀하려는 경향을 가진다는 자연의 법칙이 있다는 것입니다.

회귀 분석은 이처럼 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법입니다.

<br>

머신러닝에서 회귀는 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 통칭합니다.

예를 들어 아파트 방 개수, 방 크기, 주변 학군 (독립변수)과 아파트 가격(종속변수)의 관계를 모델링하고 예측하는 것입니다.

<br>

이를 선형 회귀식으로 나타내면 다음과 같습니다.

**Y = W1\*X1 + W2\*X2 + W3\*X3 + ··· + Wn\*Xn**

- Y : 종속변수(아파트 가격)
- X1, X2, X3, ···, Xn : 독립변수(방 개수, 방 크기, 주변 학군등)
- W1, W2, W3, ··· Wn : 회귀 계수
  회귀 계수(Regression coefficients) : 독립변수에 영향을 미치는 값

<br>

회귀는 독립변수의 개수, **회귀 계수**의 선형/비선형 여부에 따라 다음과 같이 나뉩니다.

| 독립변수 개수       | 회귀 계수의 결합     |
| ------------------- | -------------------- |
| 1개 : 단일 회귀     | 선형 : 선형 회귀     |
| 여러 개 : 다중 회귀 | 비선형 : 비선형 회귀 |

<br>

지도학습은 두 가지 유형으로 나뉘는데, 분류와 회귀입니다.
두 기법의 가장 큰 차이는 예측 결과값입니다.

- 분류(Classification) : 카테고리 값(이산값)
- 회귀(Regression) : 숫자값(연속값)

<br>

여러 가지 회귀 중 선형 회귀가 가장 많이 사용됩니다.

선형 회귀는 실제 값과 예측값의 차이를 최소화하는 직선형 회귀선을 최적화하는 방식입니다.

대표적인 선형 회귀 모델은 다음과 같습니다.

- 일반 선형 회귀 : 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화하며, 규제(Regularization)를 적용하지 않은 모델
- 릿지(Ridge) : 선형 회귀에 L2 규제(큰 회귀 계수 값의 예측 영향도를 감소시키기 위한 규제)를 추가한 회귀 모델
- 라쏘(Lasso) : 선형 회귀에 L1 규제(예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않게 하는 규제)를 적용한 회귀 모델
- 엘라스틱넷(ElasticNet) : L2, L1 규제를 결합한 모델, 주로 피처가 많은 데이터 세트에 적용됨 (L1 규제로 피처 개수를 줄이고, L2 규제로 계수 값 크기 조정)
- 로지스틱 회귀(Logistic Regression) : 분류 알고리즘으로 분류에서 사용되는 선형 모델, 이진 분류뿐만 아니라 희소 영역(텍스트 분류)의 분류에서도 뛰어난 예측 성능을 보임



### 단순 선형 회귀를 통한 회귀 이해

먼저, 독립 변수도 하나 종속 변수도 하나인 단순 선형 회귀에 대해 설명하겠습니다.

<br>

예를 들어, 주택 가격이 주택 크기로만 결정된다고 가정하겠습니다.

일반적으로 주택 크기가 커지면 가격이 높아지는 경향이 있기 때문에 다음과 같이 주택 가격은 주택 크기에 대해 선형(직선 형태)의 관계로 표현할 수 있습니다.

<img src="https://user-images.githubusercontent.com/76269316/127769263-37405a99-4f5e-4c1f-8346-151bd8652cdc.png" alt="image" style="zoom: 67%;" />

X축이 주택 크기(평당 크기)이고, Y축이 주택 가격입니다.

따라서 회귀 모델을 다음과 같이 **Y = W0 + W1 * X**  1차 함수 형태로 모델링 할 수 있습니다.

단순 선형 회귀(독립변수, 종속변수가 한 개인 선형 회귀)에서 회귀 계수는 기울기 w1과 y절편 w0입니다.

*+절편은 intercept라고 합니다.*

<br>

위 모델로 실제 값과 회귀 모델의 차이에 따른 오류 값을 남은 오류, **잔차**라고 부릅니다.

<img src="https://user-images.githubusercontent.com/76269316/127769583-05c01982-9879-4644-996e-6ebe90c860b4.png" alt="image" style="zoom:67%;" />

최적의 회귀 모델을 만든다는 것은 전체 데이터의 잔차(오류 값) 합이 최소가 되는 모델을 만든다는 의미입니다.

즉, 오류 값 합이 최소가 될 수 있는 최적의 회귀 계수를 찾는다는 의미입니다.

<br>

오류 값은 +나 -가 될 수 있습니다.

그래서 전체 데이터의 오류 합을 단순히 더한다면 오류 합이 크게 줄어들 수 있습니다.

보통 오류 합을 계산할 대는 절댓값을 취해서 더하거나(Mean Absolute Error), 오류 값의 제곱을 구해서 더하는 방식(Residual Sum of Square)을 취합니다.

일반적으로 미분 등의 계산을 편리하게 하기 위해 RSS 방식으로 오류 합을 구합니다.

RSS = (#1 주택 가격 - (w0 + w1 #1 주택크기))^2 + (#2 주택 가격 - (w0 + w1 #2 주택 크기))^2 + (#3 주택 가격 - (w0 + W1 #3 주택크기))^2 + ···

<br>

RSS는 w0, w1 변수로 표현할 수 있으며, 이 RSS를 최소로 하는 w0, w1 회귀 계수를 학습을 통해 찾는 것이 머신러닝 기반 회귀의 핵심입니다.

**RSS는 회귀식의 독립변수 X, 종속변수 Y가 중심 변수가 아니라 w 회귀계수가 중심 변수입니다.**

<br>

RSS는 학습 데이터 건수로 나누어서 다음과 같이 정규화된 식으로 표현됩니다.

![image](https://user-images.githubusercontent.com/76269316/127770177-74d77c02-7a55-42d1-a6aa-aff9896e3556.png)

회귀에서 이 RSS는 비용(Cost)이며, w 변수(회귀 계수)로 구성되는 RSS를 비용 함수라고 합니다.

머신러닝 회귀 알고리즘은 계속해서 학습하면서 이 비용 함수가 반환하는 오류 값을 지속해서 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것입니다.

*+비용 함수를 손실 함수(loss function)라고도 합니다.*



### 비용 최소화하기 - 경사 하강법(Gradient Descent) 소개

경사 하강법은 점진적으로 반복적인 계산을 통해 W 파라미터 값을 업데이트하면서 오류 값이 최소가 되는 W 파라미터를 구하는 방식입니다.

<br>

경사 하강법은 산 정상에서 아래로 내려갈 때, 현재 위치보다 무조건 낮은 곳으로 계속 이동하다 보면 마침내 지상에 도착할 수 있을 거라 가정하고 이동하는 방식입니다. (내리막과 오르막이 반복되는 언덕에 갇혔을 수도 있는데, 이 문제는 나중에 해결하겠습니다.)

어떻게 보면 무식해 보이는 방법이지만 직관적이고 빠르게 비용 함수가 최소가 되는 W 파라미터 값을 구할 수 있습니다.

<br>

경사 하강법은 최초 오류 값이 100이였다면, 두 번째 오류 값은 100보다 작은 90, 세 번째는 80으로 지속해서 오류를 감소시키는 방향으로 W 값을 업데이트해 나갑니다.

그리고 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그 때의 W 값을 최적 파라미터로 반환합니다.

<br>

경사 하강법의 핵심은 **"어떻게 하면 오류가 작아지는 방향으로 W 값을 보정할 수 있을까?"**입니다.

비용 함수가 다음과 같은 포물선 형태의 2차 함수라면 경사 하강법은 미분을 적용한 다음, 이 미분 값이 계속 감소하는 방향으로 w를 업데이트 합니다.

![image](https://user-images.githubusercontent.com/76269316/127771891-bf9fa76f-9732-4fc4-a089-3e82deb0a0bb.png)

계속 업데이트 하다가, 더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그 때의 w를 반환합니다.

<br>

<br>

위에서 언급한 비용 함수를 R(w)로 지칭하겠습니다.

![image](https://user-images.githubusercontent.com/76269316/127771922-24705125-2135-4902-bb45-8329cd71ad8d.png)

R(w)는 w0, w1으로 이루어진 함수이기 때문에 일반적인 미분을 적용할 수 없고, 편미분을 적용해야 합니다.

<img src="https://user-images.githubusercontent.com/76269316/127772224-0eb23cc2-8c74-4dbd-9158-a3c26037f4c1.png" alt="image" style="zoom: 80%;" />

w1, w0 편미분 결괏값을 반복적으로 보정하면서 w1, w0 값을 업데이트하면 비용 함수 R(w)가 최소가 되는 w1, w0의 값을 구할 수 있습니다.

업데이트는 새로운 w1을 이전 w1에서 편미분 결괏값을 마이너스하면서 적용합니다.

편미분 값이 너무 클 수도 있기 때문에 보정 계수 η(eta)를 곱하는데 이를 학습률이라고 합니다.

![image](https://user-images.githubusercontent.com/76269316/127772546-574f2dc0-6e36-4328-92c7-4a91c87e82f9.png)

<br>

요약하면 다음과 같습니다.

- Step 1 : w1, w0를 임의의 값으로 설정하고 첫 비용 함수의 값을 계산
- Step 2 : <img src="https://user-images.githubusercontent.com/76269316/127772619-1399a8dd-c496-4145-8620-632e1ce22c9d.png" alt="image" style="zoom: 50%;" /> , <img src="https://user-images.githubusercontent.com/76269316/127772628-e2daa798-26f6-496f-b4b4-1e3d520ac1e0.png" alt="image" style="zoom:50%;" />로 업데이트 한 뒤 다시 비용 함수 값을 계산합니다.
- Step 3 : 비용 함수의 값이 감소했으면 Step 2를 반복합니다. 더 이상 감소하지 않는 경우, 그 때의 w1, w0를 구하고 반복을 중지합니다.

<br>

<br>

지금까지 정리한 내용을 토대로 경사 하강법을 파이썬 코드로 구현해 보겠습니다.

y = 4X + 6을 근사하기 위한 100개의 데이터 세트를 만들고, 경사 하강법을 이용해 회귀 계수 w1, w0를 도출해 보겠습니다.

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

np.random.seed(0)
#y = 4X +6을 근사(w1=4, w0=6) 
X = 2 * np.random.rand(100, 1)
y = (6 + 4 * X) + np.random.randn(100, 1)  #4X + 6에다가 난수를 더함 (y = 4X + 6을 중심으로 무작위로 퍼지도록)

#X, y 데이터 세트 산점도로 시각화
plt.scatter(X, y)
```

![image](https://user-images.githubusercontent.com/76269316/127773617-a155e421-3077-4518-b806-714411a8a4fe.png)

**get_weight_updates()**

```python
#w1과 w0를 업데이트할 w1_update, w0_update를 반환
def get_weight_updates(w1, w0, X, y, learning_rate=0.01):
    N = len(y)
    #먼저 w1_update, w0_update를 각각 w1, w0의 shape와 동일한 크기를 가진 0 값으로 초기화
    w1_update = np.zeros_like(w1)
    w0_update = np.zeros_like(w0)
    #예측 배열 계산하고 예측과 실제 값의 차이 계산
    y_pred = np.dot(X, w1.T) + w0
    diff = y - y_pred
    
    #w0_update를 dot 행렬 연산 구하기 위해 모두 1 값을 가진 행렬 생성
    w0_factors = np.ones((N, 1))
    
    #w1과 w0을 업데이트할 w1_update와 w0_update 계산
    w1_update = -(2/N)*learning_rate*(np.dot(X.T, diff))
    w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T, diff))
    
    return w1_update, w0_update
```

100개 데이터 X(1, 2, ···, 100)가 있다면 예측값은 (w0 + X(1)\*w1) + (w0 + X(2)\*w1) + ··· +(w0 + X(100)\*w1)이며, 이는 입력 배열 X와 w1 배열의 내적에 w0를 더한 것과 동일합니다.

따라서 입력 배열 X 값에 대한 예측 배열 y_pred는 np.dot(X, w1.T) + w0로 구합니다.



이후 w1_update로 <img src="https://user-images.githubusercontent.com/76269316/127772619-1399a8dd-c496-4145-8620-632e1ce22c9d.png" alt="image" style="zoom: 50%;" />를 w0_update로 <img src="https://user-images.githubusercontent.com/76269316/127772628-e2daa798-26f6-496f-b4b4-1e3d520ac1e0.png" alt="image" style="zoom:50%;" />를 계산한 뒤 이를 반환합니다.

<br>

**gradient_descent_steps()**

```python
#입력 인자 iters로 주어진 횟수만큼 반복적으로 w1과 w0를 업데이트 적용함
def gradient_descent_steps(X, y, iters=10000):
    #w0와 w1을 모두 0으로 초기화
    w0 = np.zeros((1, 1))
    w1 = np.zeros((1, 1))
    
    #인자로 주어진 iters만큼 반복적으로 get_weight_updates() 호출해 w1, w0 업데이트 수행
    for ind in range(iters):
        w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01)
        w1 = w1 - w1_update
        w0 = w0 - w0_update

    return w1, w0
```

get_weight_updates()에서 구한 w1_update, w0_update를 가지고,  w1과 w0를 업데이트 해줍니다.

<br>

**get_cost()**

```python
def get_cost(y, y_pred):
    N = len(y)
    cost = np.sum(np.square(y - y_pred) / N)
    return cost

w1, w0 = gradient_descent_steps(X, y, iters=1000)
print('w1:{0:.3f} w0:{1:.3f}'.format(w1[0, 0], w0[0, 0]))
y_pred = w1[0, 0] * X + w0
print('Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))
```

![image](https://user-images.githubusercontent.com/76269316/127787609-17f89382-41b9-40ae-a082-14289de0dacc.png)

최종적으로 예측값과 실제값의 RSS 차이를 get_cost() 함수를 통해 계산했습니다.



정리하자면, grdient_descent_stpes() 호출 → get_weight_updates() 1,000번 호출하면서 w1, w0를 업데이트 → 최종 w1, w0로 y_pred 값 계산 → get_cost()에서 RSS 계산



앞에서 구한 y_pred에 기반해 회귀선을 그렸습니다.

```python
plt.scatter(X, y)
plt.plot(X, y_pred)
```

![image](https://user-images.githubusercontent.com/76269316/127787620-a7e49b35-609d-4085-ad64-a01b7b9eb116.png)

<br>

일반적으로 경사 하강법은 모든 학습 데이터에 대해 반복적으로 비용함수 최소화를 위한 값을 업데이트하기 때문에 수행시간이 매우 오래 걸립니다.

그 때문에 실전에서는 대부분 확률적 경사 하강법(Stochastic Gradient Descent) 또는 미니 배치 확률적 경사 하강법을 사용합니다.

확률적 경사 하강법은 전체 입력 데이터 중 일부 데이터만을 이용해 w가 업데이트 되는 값을  계산하므로 경사 하강법에 비해 빠른 속도를 보장합니다.



확률적 경사 하강법을 stochastic_gradient_descent_steps()로 구현했습니다.

**stochastic_gradient_descent_steps()**

```python
def stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000):
    w0 = np.zeros((1, 1))
    w1 = np.zeros((1, 1))
    prev_cost = 100000
    iter_index = 0
    
    for ind in range(iters):
        np.random.seed(ind)
        #전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이터를 추출해 sample_X, sample_y로 저장
        stochastic_random_index = np.random.permutation(X.shape[0])
        sample_X = X[stochastic_random_index[0:batch_size]]
        sample_y = y[stochastic_random_index[0:batch_size]]
        #랜덤하게 batch_size만큼 추출된 데이터 기반으로 w1_update, w0_update 계산 후 업데이트
        w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01)
        w1 = w1 - w1_update
        w0 = w0 - w0_update

    return w1, w0
```



```python
stochastic_random_index = np.random.permutation(X.shape[0])
sample_X = X[stochastic_random_index[0:batch_size]]
sample_y = y[stochastic_random_index[0:batch_size]]
```

앞의 gradient_descent_stpes()와 전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이를 추출해  이를 기반으로 w1_update, w0_update를 계산하는 부분만 차이가 있습니다.

<br>

**get_cost()**

```python
def get_cost(y, y_pred):
    N = len(y)
    cost = np.sum(np.square(y - y_pred) / N)
    return cost

w1, w0 = gradient_descent_steps(X, y, iters=1000)
print('w1:{0:.3f} w0:{1:.3f}'.format(w1[0, 0], w0[0, 0]))
y_pred = w1[0, 0] * X + w0
print('Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))
```

<br>

```python
w1, w0 = stochastic_descent_stpes(X, y, batch_size=10, iters=1000)
print('w1:', round(w1[0, 0], 3), 'w0:', round(w0[0, 0], 3))
y_pred = w1[0, 0] + X + w0
print('Stochastic Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred)))
```

![image](https://user-images.githubusercontent.com/76269316/127787998-be526376-5cda-497d-b54e-62ea4914dffb.png)

확률적 경사 하강법으로 구한  w0, w1 결과는 경사 하강법과 크게 차이가 없는 것을 볼 수 있습니다.

따라서 큰 데이터를 처리할 경우 일반적으로 확률적 경사 하강법을 사용합니다.

<br>

<br>
지금까지는 독립변수가 1개(피처가 1개)인 단순 선형 회귀에서 경사 하강법을 적용했습니다.

피처가 여러 개인 경우도 1개인 경우를 확장해 유사하게 도출할 수 있습니다.



피처가 M개 (X1, X2, ···, X100)이 있다면 예측 회귀식은 다음과 같이 되게 됩니다.

y ̂ = w0 + w1\*X1 + w2\*X2 + ··· + w100\*X100

이는 np.dot(X, w1.T) + w0로 계산할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/127788803-33a8f457-2ddc-44e9-b47f-d9e081ebb197.png)

Xmat의 맨 처음 열에 모든 데이터 값이 1인 피처 Feat0을 추가하면 좀 더 간결하게 y ̂ = Xmat + W^T 회귀 예측식을 도출할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/127788813-6ab1e8ea-1ec0-487d-93d3-8abc095d9ff4.png)

<br>

<br>

### 사이킷런 LinearRegression을 이용한 보스턴 주택 가격 예측

##### LinearRegression 클래스 - Ordinary Least Squares

LinearRegression 클래스는 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화해 OLS(Ordinary Least Squares) 추정 방식으로 구현한 클래스입니다.

LinearRegression 클래스는 fit() 메소드로 X, y 배열을 입력 받으면 회귀 계수(Coefficients)인 W를 coef_ 속성에 저장합니다.

|                        입력 파라미터                         |                             속성                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| fit_intercept : 불린 값으로, 디폴트는 True<br />Intercept(절편) 값을 계산할 것인지를 지정, False로 지정하면 intercept가 사용되지 않고 0으로 지정됨<br /><img src="https://user-images.githubusercontent.com/76269316/127802879-6ed30a22-1739-4da2-b1ea-8688453556e1.png" alt="image" style="zoom:50%;" /> <img src="https://user-images.githubusercontent.com/76269316/127802913-14bc85eb-449c-4841-afe8-bf48a46d32c6.png" alt="image" style="zoom:50%;" /><br />normalize : 불린 값으로, 디폴트는 False<br />fit_intercept가 False인 경우 이 파라미터가 무시됨<br />True이면 회귀 수행 전에 입력 데이터 세트를 정규화함 | coef_ : fit() 메소드를 수행했을 때 회귀 계수가 배열 형태로 저장하는 속성 (Shape는 (Target 값 개수, 피처 개수))<br />intercept_ : intercept 값 |

