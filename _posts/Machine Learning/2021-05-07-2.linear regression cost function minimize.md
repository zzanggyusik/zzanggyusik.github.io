---
published: false
---

[ML lec  01 - 기본적인 Machine Learning의 용어와 개념 설명](https://youtu.be/qPMeuL2LIqY)

### Machine Learning

우리가 사용하는 일반 프로그램의 경우 우리가 입력을 주면 입력을 기반으로 데이터를 보여준다거나하는 출력을 주는데, 그런 프로그램을 **explicit(분명한) program**이라고 한다.

하지만 이렇게 explicit하게 구현하기 어려운 프로그램들이 존재함.  그 중 하나가 이메일 스팸 필터인데 규칙들이 너무 많아서 이런 경우에는 explicit하게 만들 수 없다. 요새 대두되고 있는 자율주행의 경우도 그렇다.

이렇게 일일이 모든 경우에 대해서 프로그래밍을 할 수 없으니까

프로그램 자체에서 어떤 데이터를 보고  스스로 학습해서 알아서 출력값을 결정하는 프로그램의 일종을 머신러닝이라고 한다.



### Supervised / Unsupervised Learning

학습하는 방법에 따라서 supervise와 unsupervised로 나뉩니다.



### supervised learning

<u>labeled 데이터(training set)</u>를 가지고 학습하는 것

supervised learning의 대표적인 예가 Image labeling입니다.

고양이 그림, 강아지 그림, 컵 그림, 모자 그림 이렇게 labeled된 데이터들을 가지고 학습을 하는 것이 supervised learning입니다.

![image](https://user-images.githubusercontent.com/76269316/117240987-01cfdc80-ae6d-11eb-86c3-60d014d36e8e.png)



### unsupervised learning

un-labeled 데이터를 가지고 학습하는 것

unsupervised의 대표적인 예가 google news grouping(유사한 뉴스들을 그룹핑), word clustering(비슷한 단어들끼리 모음) 있습니다.



저희는 supervised learning에 대해 주로 다루는데, 이를 통해 Image labeling, Email spam filter, Predicting exam score등을 할 수 있습니다.



### Training data set

labeled 데이터(training data set)를 가지고 학습시켜 머신러닝 모델을 생성한 다음, 내가 모르는 X test의 값을 주면 모델이 생각한 답(Y=3)을 주는 것이 supervised learning입니다. 알파고가 대표적인 예입니다.

![image](https://user-images.githubusercontent.com/76269316/117242728-c59e7b00-ae70-11eb-8367-625e1272356f.png)



그런데 supervised learning도 결과에 따라 다르게 나뉘는데,

시험 성적을 예측하는 시스템의 경우 0~100점 사이의 점수를 예측해야 하는데, 이런 예측을 하는 것을 **regression**이라고 하고,

점수로 매기지 않고, 이 사람이 pass했느냐 non pass했느냐 두 가지 결과중 하나로 분류하는 것을 **binary classfication**이라고 합니다.

학점(A, B, C, D, F)으로 예측하는 것을 **multi-label classfication**이라고 합니다.



[ML lec 02 - Linear Regression의 Hypothesis 와 cost 설명](https://youtu.be/Hax03rCn3UI)

### Predicting exam score : regression

| x(hours) | y(score) |
| -------- | -------- |
| 10       | 90       |
| 9        | 80       |
| 3        | 50       |
| 2        | 30       |

어떤 학생이 몇 시간 공부했더니, 얼마 정도의 성적이 나왔는지 데이터를 가지고 supervised learning을 해서 생성한 모델에게 7시간 공부한 학생의 성적이 얼마일까? 하고 물어보면, 학습된 데이터를 기반으로 65점 정도 받을거라고 예측하는데, 이것을 **Linear Regression**이라고 합니다.

![image](https://user-images.githubusercontent.com/76269316/117244486-0b107780-ae74-11eb-985b-0cf0c6b4a8e4.png)

### Regression

주어진 데이터가 x가 1일 때 y는 1이고, x가 2일 때 y는 2, x가 3일 때 y는 3이라고 하면 이를 그래프로 나타내면

| x    | y    |
| ---- | ---- |
| 1    | 1    |
| 2    | 2    |
| 3    | 3    |

x : 예측을 하기 위한 기본적인 자료(feature라고 합니다.)

y : 예측해야하는 대상



다음과 같이 직선 형태로 나타나게 됩니다. 이런 Linear Regression 모델을 학습시키기 위해서는 가설(Hypothesis)을 세울 필요가 있는데 이런 직선 형태의 가설을 Linear Hypothesis라고 합니다.

세상의 많은 데이터(현상)들이 Linear 형태로 표현될 수 있습니다. 공부양과 성적의 상관관계(공부를 많이 할수록 성적↑), 집의 크기와 가격의 상관관계(집의 크기가 클수록 가격 ↑)등이  Linear하게 가설을 세울 수 있습니다.

직선이기 때문에 이렇게 표현할 수 있습니다. 

##### **H(x) = Wx + b**

![linear hypothesis1](https://user-images.githubusercontent.com/76269316/117245919-a3a7f700-ae76-11eb-9e17-c98cbc50ff27.png)



아래처럼 여러 가설을 세울 수 있는데, 이 직선 중 어떤 직선이 우리가 갖고 있는 데이터와 잘 맞는 직선인지를 찾아야 하는데, 

![linear hypothesis2](https://user-images.githubusercontent.com/76269316/117245942-ac98c880-ae76-11eb-89c5-be6f960ffb7d.png)



이는 실제 데이터와  가설이 나타내는 점 사이의 거리( **H(x) - y**)를 계산해냄으로써 알 수 있는데,

![linear hypothesis3](https://user-images.githubusercontent.com/76269316/117246376-7e67b880-ae77-11eb-877d-f85cb04eaed8.png)



### Cost function (Loss function)

이를 cost function이라고 합니다.

 **H(x) - y** 하지만 이 값은 양수가 될 수도 있고, 음수가 될 수도 있기 때문에 그냥 사용하지 않고, 다음과 같이 제곱해서 사용합니다.

![image](https://user-images.githubusercontent.com/76269316/117246677-0221a500-ae78-11eb-9429-d831c2c5b85c.png)

이렇게 제곱할 경우 항상 양수로 값이 표현되고, 제곱이기 때문에 차이가 클 경우 더  패널티를 많이 주게됩니다. (거리가 작을수록 좋으니까)



위 그래프의 cost function은 다음과 같습니다. 데이터와 직선 위의 점 사이의 거리를 각각 제곱한 뒤 이를 3(데이터의 개수)으로 나눠서 평균을 낸 것입니다.

![image](https://user-images.githubusercontent.com/76269316/117246793-34cb9d80-ae78-11eb-9328-bb7ecb52a636.png)

cost function을 formal하게 정리하면 다음과 같습니다. (m = 학습데이터의 개수)

![image](https://user-images.githubusercontent.com/76269316/117246916-5fb5f180-ae78-11eb-94e1-b8771c024541.png)

위에서 얘기한 것처럼 Hypothesis는 직선이기 때문에 **H(x) = Wx + b** 다음과 같이 표현할 수 있고, 이를 cost function에 대입하면 cost function은 w와 b의 function이 되게 됩니다.

![image](https://user-images.githubusercontent.com/76269316/117247052-9429ad80-ae78-11eb-8d47-b95b95677d7a.png)

**결국, Linear Regression의 학습은 이 cost를 가장 작게하는 W와 b값을 찾는 것입니다.**
[# ML lec 03 - Linear Regression의 cost 최소화 알고리즘의 원리 설명](https://youtu.be/TxIVr-nk1so)

### Hypothesis and Cost
![image](https://user-images.githubusercontent.com/76269316/117428981-8865e600-af61-11eb-8f8b-3a088fef1891.png)

지난 포스트에서 우리의 hypothesis가 Wx + b의 형태로 주어지고, 이것을 기반으로 **실제 데이터와 얼마나 다른가를 나타내는 cost function** 을 정의했습니다.

이 cost function을 최소화하는 w와 b 값을 우리가 갖고 있는 데이터를 통해서 구하는 것이 저희의 목적이라고 했었습니다.

간단한 설명을 위해 H(x)를 Wx라고 놓겠습니다. 그러면 cost function은 다음과 같이 되게됩니다.

![image](https://user-images.githubusercontent.com/76269316/117429028-93b91180-af61-11eb-9753-be90931c5973.png)


주어진 데이터가 다음과 같고, w가 1이라면 cost function의 값은 어떻게 될까요?

| X | Y |
|--|--|
| 1 | 1 |
| 2 | 2 |
| 3 | 3 |

w가 1이라면 다음과 같이 cost function의 값이 0이 되게 됩니다.


![image](https://user-images.githubusercontent.com/76269316/117428841-62404600-af61-11eb-826e-f6fab351d023.png)


w가 0이라면 cost function이 4.67이 되게됩니다.

![image](https://user-images.githubusercontent.com/76269316/117428801-594f7480-af61-11eb-8bae-5cdbeb9c3366.png)

x축을 w, y축을 cost function의 값으로 둔 그래프는 다음과 같습니다.
저희의 목적은 이 cost function 즉, y값이 최소가 되는 w의 값(위에서 b의 값을 생략했기 때문에 여기서는 w값 만을 얘기하겠습니다.)을 찾는 것이였습니다.

![image](https://user-images.githubusercontent.com/76269316/117429770-53a65e80-af62-11eb-9230-db261d7c848e.png)

눈으로 봤을 때는 당연하게 저 빨간 원의 점이 최소값인 걸 알겠는데, 이를 기계적으로 어떻게 찾아낼까요?

바로 Gradient descent algorithm을 통해서 알아낼 수 있습니다.

### Gradient descent algorithm
gradient descent는 굉장히 많은 machine learning의 minimization problems에 적용됩니다.

저희가 찾고자했던 w와 b값을 찾아줄 수도 있고 심지어 많은 값들(w1, w2 ···)이 있는 cost function에도 적용할 수 있습니다.

경사를 나타내는 gradient와 내려간다는 의미의 descent가 합쳐진 단어로, 경사를 따라 내려감으로써 최소 값을 찾는 알고리즘입니다.

아무 점에서나 시작해도 됩니다. 저는 가장 오른쪽 파란 점에서 시작했다고 하겠습니다.
그런 다음 w를 조금씩 바꿔가면서 이 점을 이동시킵니다. (경사를 따라 내려가는 쪽으로 이동시킵니다.)
그러다보면은 경사가 0이되서 최소값에 도달할 수 있게됩니다.

![image](https://user-images.githubusercontent.com/76269316/117431097-ca902700-af63-11eb-87cb-114bd5869ae7.png)

다음에 이동할 w를 찾기 위해 경사를 따라 내려가는 쪽으로 이동한다고 했는데, 경사는 기울기를 의미하므로, 미분을 이용해서 다음 w값을 찾아보도록 하겠습니다.
α값은 learning rate(학습률)인데 일단은 그냥 상수라고 생각하셔도 됩니다.
α값 옆을 보면 cost function을 w에 대해서 미분했습니다. 이는 해당 w에서의 기울기를 의미하게 되는데 
w의 값 - 해당 w에서의 기울기를 통해 다음 w 값을 찾게 됩니다.

![image](https://user-images.githubusercontent.com/76269316/117432382-42ab1c80-af65-11eb-9706-ec10fee25d13.png)

cost function을 미분하는 과정은 다음과 같습니다.

![image](https://user-images.githubusercontent.com/76269316/117433248-3ffcf700-af66-11eb-92c9-6d7e8639aa4b.png)


이 때 1/2m을 곱해주는데 1/m을 minimize하는 것과 1/2m을 minimize하는 것은 같은 의미를 지니기 때문에 미분을 간단하게 하기 위해 1/2m을 사용하였습니다.

![image](https://user-images.githubusercontent.com/76269316/117431945-c1538a00-af64-11eb-910b-e1249e3aae4c.png)

최종적으로 이 수식을 사용하면 cost function을 최소값으로 만드는 w의 값을 찾을 수 있게됩니다.

![image](https://user-images.githubusercontent.com/76269316/117433462-763a7680-af66-11eb-9b68-b8c7f83f1e63.png)

**하지만 언제나 최소값을 구할 수 있는 것은 아닙니다.**

![image](https://user-images.githubusercontent.com/76269316/117433976-109aba00-af67-11eb-8b71-a4e02ab638ba.png)

만약 우리의 cost function이 이런 형태였다면, 빨간 점에서 시작했을 때와 파란 점에서 시작했을 떄의 최소값이 달라지게 됩니다..

..? 아까는 아무 점에서나 시작해도 된다면서요.

![image](https://user-images.githubusercontent.com/76269316/117434525-be0dcd80-af67-11eb-963e-1958b76a476d.png)

그건 잘 됩니다 ^^..

아까 저희가 사용한 cost function을 그래프는 아래와 같은 모양을 가졌습니다.
이런 함수들을 convex function이라고 하는데, 그래프만 봐도 아무데서 시작해도 한 곳으로 가게 생겼죠?
아무튼, cost function마다 아무데서나 시작해도 최소값을 구할 수 있는 함수도 있고 그렇지 못한 함수도 있다는 점을 주의하셔야합니다.

![image](https://user-images.githubusercontent.com/76269316/117434104-3758f080-af67-11eb-8761-a65bfaed24a1.png)
