[# ML lec 03 - Linear Regression의 cost 최소화 알고리즘의 원리 설명](https://youtu.be/TxIVr-nk1so)

### Hypothesis and Cost
![image](https://user-images.githubusercontent.com/76269316/117428981-8865e600-af61-11eb-8f8b-3a088fef1891.png)

지난 포스트에서 우리의 hypothesis가 Wx + b의 형태로 주어지고, 이것을 기반으로 **실제 데이터와 얼마나 다른가를 나타내는 cost function** 을 정의했습니다.

이 cost function을 최소화하는 w와 b 값을 우리가 갖고 있는 데이터를 통해서 구하는 것이 저희의 목적이라고 했었습니다.

간단한 설명을 위해 H(x)를 Wx라고 놓겠습니다. 그러면 cost function은 다음과 같이 되게됩니다.
![image](https://user-images.githubusercontent.com/76269316/117429028-93b91180-af61-11eb-9753-be90931c5973.png)


주어진 데이터가 다음과 같고, w가 1이라면 cost function의 값은 어떻게 될까요?
| X | Y |
|--|--|
| 1 | 1 |
| 2 | 2 |
| 3 | 3 |
w가 1이라면 다음과 같이 cost function의 값이 0이 되게 됩니다.
![image](https://user-images.githubusercontent.com/76269316/117428841-62404600-af61-11eb-826e-f6fab351d023.png)


w가 0이라면 cost function이 4.67이 되게됩니다.
![image](https://user-images.githubusercontent.com/76269316/117428801-594f7480-af61-11eb-8bae-5cdbeb9c3366.png)

x축을 w, y축을 cost function의 값으로 둔 그래프는 다음과 같습니다.
저희의 목적은 이 cost function 즉, y값이 최소가 되는 w의 값(위에서 b의 값을 생략했기 때문에 여기서는 w값 만을 얘기하겠습니다.)을 찾는 것이였습니다.

![image](https://user-images.githubusercontent.com/76269316/117429770-53a65e80-af62-11eb-9230-db261d7c848e.png)
눈으로 봤을 때는 당연하게 저 빨간 원의 점이 최소값인 걸 알겠는데, 이를 기계적으로 어떻게 찾아낼까요?

바로 Gradient descent algorithm을 통해서 알아낼 수 있습니다.

### Gradient descent algorithm
gradient descent는 굉장히 많은 machine learning의 minimization problems에 적용됩니다.

저희가 찾고자했던 w와 b값을 찾아줄 수도 있고 심지어 많은 값들(w1, w2 ···)이 있는 cost function에도 적용할 수 있습니다.

경사를 나타내는 gradient와 내려간다는 의미의 descent가 합쳐진 단어로, 경사를 따라 내려감으로써 최소 값을 찾는 알고리즘입니다.

아무 점에서나 시작해도 됩니다. 저는 가장 오른쪽 파란 점에서 시작했다고 하겠습니다.
그런 다음 w를 조금씩 바꿔가면서 이 점을 이동시킵니다. (경사를 따라 내려가는 쪽으로 이동시킵니다.)
그러다보면은 경사가 0이되서 최소값에 도달할 수 있게됩니다.
![image](https://user-images.githubusercontent.com/76269316/117431097-ca902700-af63-11eb-87cb-114bd5869ae7.png)

다음에 이동할 w를 찾기 위해 경사를 따라 내려가는 쪽으로 이동한다고 했는데, 경사는 기울기를 의미하므로, 미분을 이용해서 다음 w값을 찾아보도록 하겠습니다.
α값은 learning rate(학습률)인데 일단은 그냥 상수라고 생각하셔도 됩니다.
α값 옆을 보면 cost function을 w에 대해서 미분했습니다. 이는 해당 w에서의 기울기를 의미하게 되는데 
w의 값 - 해당 w에서의 기울기를 통해 다음 w 값을 찾게 됩니다.
![image](https://user-images.githubusercontent.com/76269316/117432382-42ab1c80-af65-11eb-9706-ec10fee25d13.png)

cost function을 미분하는 과정은 다음과 같습니다.
![image](https://user-images.githubusercontent.com/76269316/117433248-3ffcf700-af66-11eb-92c9-6d7e8639aa4b.png)


이 때 1/2m을 곱해주는데 1/m을 minimize하는 것과 1/2m을 minimize하는 것은 같은 의미를 지니기 때문에 미분을 간단하게 하기 위해 1/2m을 사용하였습니다.
![image](https://user-images.githubusercontent.com/76269316/117431945-c1538a00-af64-11eb-910b-e1249e3aae4c.png)

최종적으로 이 수식을 사용하면 cost function을 최소값으로 만드는 w의 값을 찾을 수 있게됩니다.
![image](https://user-images.githubusercontent.com/76269316/117433462-763a7680-af66-11eb-9b68-b8c7f83f1e63.png)

**하지만 언제나 최소값을 구할 수 있는 것은 아닙니다.**

![image](https://user-images.githubusercontent.com/76269316/117433976-109aba00-af67-11eb-8b71-a4e02ab638ba.png)
만약 우리의 cost function이 이런 형태였다면, 빨간 점에서 시작했을 때와 파란 점에서 시작했을 떄의 최소값이 달라지게 됩니다..

..? 아까는 아무 점에서나 시작해도 된다면서요.

![image](https://user-images.githubusercontent.com/76269316/117434525-be0dcd80-af67-11eb-963e-1958b76a476d.png)

그건 잘 됩니다 ^^..
아까 저희가 사용한 cost function을 그래프는 아래와 같은 모양을 가졌습니다.
이런 함수들을 convex function이라고 하는데, 그래프만 봐도 아무데서 시작해도 한 곳으로 가게 생겼죠?
아무튼, cost function마다 아무데서나 시작해도 최소값을 구할 수 있는 함수도 있고 그렇지 못한 함수도 있다는 점을 주의하셔야합니다.
![image](https://user-images.githubusercontent.com/76269316/117434104-3758f080-af67-11eb-8761-a65bfaed24a1.png)
