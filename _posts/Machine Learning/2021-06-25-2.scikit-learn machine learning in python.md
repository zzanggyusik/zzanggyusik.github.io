---
title:  "2.scikit-learn Machine Learning in Python"
excerpt: "사이킷런으로 시작하는 머신러닝"
toc: true
toc_label: "scikit-learn Machine Learning in Python"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - Machine Learning
last_modified_at: 2021-06-25



---





>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.



사이킷런(scikit-learn)은 파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리입니다.

사이킷런의 특징은 다음과 같습니다.

- 쉽고 가장 파이썬스러운 API를 제공
- 머신러닝을 위한 매우 다양한 알고리즘과 편리한 프레임워크, API를 제공
- 오랜 기간 실전 환경에서 검증되어 많은 환경에서 사용되는 성숙한 라이브러리임



Anaconda를 설치하면 기본으로 사이킷런까지 설치가 완료되기 때문에 별도로 설치할 필요가 없습니다.

+재설치 해야하는 경우 (conda 명령어로 설치하는 것을 권장)

```python
conda install scikit-learn  #Anaconda의 conda 명령어로 설치할 경우 사이킷런 구동에 필요한 다양한 라이브러리를 동시에 설치해줌
pip install scikit-learn
```



### 붓꽃 품종 예측하기

사이킷런에는 예제로 활용할 수 있는 간단한 데이터 세트가 내장돼 있습니다. 

이 중 하나인 붓꽃 데이터 세트를 이용해서 붓꽃 품종 예측 머신러닝 모델을 만들어보겠습니다.



붓꽃 품종 예측 머신러닝 모델이라는 것은 붓꽃 데이터 세트의 꽃잎의 길이와 너비, 꽃받침의 길이와 너비 feature를 기반으로 꽃의 품종을 분류(Classfication)한다는 것입니다.

분류(Classfication)는 대표적인 [지도학습](https://seominseok4834.github.io/1.machine-learning-term-and-concept/#supervised-learning) 방법 중 하나인데,

![image](https://user-images.githubusercontent.com/76269316/123435462-1dc64380-d5bd-11eb-940a-fdad9059c48f.png)

지도학습은 학습을 위한 다양한 feature와 분류 결정값인 label 데이터로 모델을 학습한 뒤, 별도의 테스트 데이터 세트에서 미지의 label을 예측합니다. (명확한 정답이 주어진 데이터를 먼저 학습한 뒤 미지의 정답을 예측하는 방식)

이 때, 학습을 위해 주어진 데이터 세트를 **학습 데이터 세트**, 머신러닝 모델의 예측 성능을 평가하기 위해 별도로 주어진 데이터 세트를 **테스트 데이터 세트**라고 합니다.



feature와 label이 뭔지 감이 안 오실텐데, feature는 단어 그대로 특징을 의미합니다. 

붓꽃 데이터 세트에서 feature는 꽃잎의 길이와 너비, 꽃받침의 길이와 너비를 의미합니다.

label도 마찬가지로 우리가 상표나 품명을 인쇄해서 붙여놓는 종잇조각인 라벨처럼, 각각의 데이터들이 무엇인지를 결정해주는 값입니다. 붓꽃 데이터 세트에서는 붓꽃의 품종(setosa, vesicolor, virginica)을 의미합니다.



즉, 학습 데이터 세트를 통해서 학습을 시킬 때는 각각의 feature들이 어떤 품종인지를 나타내는 label 값을 주고 학습을 시킨 다음, 실제로 학습시킨 머신 러닝 모델의 예측 성능을 평가할 때는 이 label값을 주지 않고 feature 값만 줌으로써 (이 때 주어지는 feature는 학습시킬 때의 feature와는 다른 값입니다.) label을 얼마나 잘 예측했는지를 평가합니다.



사이킷런의 여러 모듈들을 사용해서 실제로 붓꽃 품종 예측 머신러닝 모델을 만들어보겠습니다.

```python
import pandas as pd
import sklearn

from sklearn.datasets import load_iris  #sklearn.datasets : 사이킷런에서 자체적으로 제공하는 데이터 세트를 생성하는데 사용되는 모듈
from sklearn.tree import DecisionTreeClassifier  #sklearn.tree : 트리 기반 ML 알고리즘을 구현한 클래스의 모임
from sklearn.model_selection import train_test_split  #sklearn.model_selection : 학습 데이터와 검증데이터, 예측 데이터로 데이터를 분리하거나 최적의 하이퍼 파라미터로 평가하기 위한 모듈
from sklearn.metrics import accuracy_score  #정확도 측정을 위한 accuracy_score() 함수 사용

#붓꽃 데이터 세트 로딩
iris = load_iris()

#iris.data는 데이터 세트에서 feature만으로 된 데이터를 numpy ndarray로 갖고 있음
iris_data = iris.data

#iris.target은 붓꽃 데이터 세트에서 label만으로 된 데이터를 numpy ndarray로 갖고 있음
iris_label = iris.target

#붓꽃 데이터 세트를 자세히 보기위해 DataFrame으로 변환
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)
iris_df['label'] = iris.target  #label column 추가
iris_df  #붓꽃 데이터 세트를 DataFrame으로 변환한 걸 출력

#학습용 데이터와 테스트용 데이터로 분리
X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=11)

#의사 결정 트리를 이용해 학습과 예측 수행
dt_clf = DecisionTreeClassifier(random_state=11)  #DecisionTreeClassifier 객체 생성

#학습 수행
dt_clf.fit(X_train, y_train)

#학습이 완료된 DecisionTreeClassifier 객체에서 테스트 데이터 세트로 예측 수행
pred = dt_clf.predict(X_test)
                      
#정확도 측정
print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))
```

![image](https://user-images.githubusercontent.com/76269316/123438220-73294180-d60b-11eb-8f78-19524a19fd18.png)



한 줄 한 줄 살펴보면

```python
iris_data
print('iris_data type:', type(iris_data))
```

![image](https://user-images.githubusercontent.com/76269316/123496805-db584180-d664-11eb-9eb1-efc839ee0c7a.png)



다음과 같이, iris_data는 feature 값(꽃잎의 길이와 너비, 꽃받침의 길이와 너비)을 저장한 numpy의 ndarray type이라는 것을 알 수 있습니다.



```python
iris_label
print('iris_label type:',type(iris_label))
print('iris target명:', iris.target_names)
```

![image](https://user-images.githubusercontent.com/76269316/123442355-c30a0780-d60f-11eb-8455-cf21b4902470.png)



iris_label 값을 보면 0, 1, 2로만 이루어진 것을 알 수 있는데, 이는 각각 setosa, versicolor, virginica 품종을 의미합니다.



```python
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)
iris_df
```

![image](https://user-images.githubusercontent.com/76269316/123442659-13816500-d610-11eb-826b-6d6f152f2202.png)



label 값을 추가하지 않고 dataframe으로 변환시 해당 feature가 어떤 품종인지 알 수 없습니다.



따라서 다음과 같이 label column을 추가해줌으로써 해당 feature가 어떤 품종인지 보여줍니다.

```python
iris_df['label'] = iris.target
iris_df
```

![image](https://user-images.githubusercontent.com/76269316/123442915-59d6c400-d610-11eb-8b65-c89cfbb47d36.png)

이 작업은 그냥 feature와 label이 어떤 관계인지를 보여주기 위해 label column을 추가해서 dataframe으로 나타낸 것이고, 

실제로 학습용 데이터와 테스트용 데이터로 분리할 때는 이렇게 feature와 label을 합치지 않고 분리합니다.



이제 각각의 feature들이 어떤 품종인지에 대한 데이터가 있으니까 이를 학습용 데이터와 테스트용 데이터로 분리해 보겠습니다.

```python
X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=11)
```

train_test_split의 첫 번째 파라미터인 iris_data는 feature 데이터 세트입니다.

두 번째 파라미터인 iris_label은 label 데이터 세트입니다.

세 번째 파라미터인 test_size는 전체 데이터 세트 중 테스트 데이터 세트의 비율입니다.

마지막 random_state는 train_test_split() 호출 시 무작위로 데이터를 분리하므로 random_state를 지정해서 수행할 때마다 동일한 데이터 세트로 분리하기 위해 임의의 숫자(11)를 부여했습니다.

위 코드를 실행하면 다음과 같은 값들이 반환됩니다.

- X_train : 학습용 feature 데이터 세트
- X_test : 테스트용 feature 데이터 세트
- y_train : 학습용 label 데이터 세트
- y_test : 테스트용 label 데이터 세트



이제 이 데이터를 기반으로 머신러닝 분류 알고리즘 중 하나인 의사 결정 트리 (Decision Tree)를 이용해 학습과 예측을 수행하겠습니다.

아직 의사 결정 트리 같은 주요 머신러닝 알고리즘에 대해 설명하지 않았지만 일단은 데이터를 학습하고 예측하는 머신러닝 기법을 구현한 주요 알고리즘 정도로 이해하면 될 것 같습니다.



```python
dt_clf = DecisionTreeClassifier(random_state=11)
```

사이킷런의 의사 결정 트리인 DecisionTreeClassfier를 객체로 생성합니다. (마찬가지로, 위 예시 코드를 수행할 때마다 동일한 학습/예측 결과를 출력하기 위해 random_state를 임의의 숫자값으로 지정해줬습니다.)



```python
dt_clf.fit(X_train, y_train)
```

DecisionTreeClassifier 객체의 fit() 메소드에 학습용 feature 데이터와 label 데이터를 입력해 호출하면 학습을 수행합니다.

즉, 학습시킬 때는 해당 feature 값에 해당하는 품종이 무엇인지를 알려주고 학습을 시키는 것입니다.



```python
pred = dt_clf.predict(X_test)
```

학습된 DecisionTreeClassfier 객체를 이용해 predict() 메소드를 사용해서 예측을 수행합니다.

이 때 파라미터에 X_test만 주었는데, 예측할 때는 해당 feature가 어떤 label일지를 예측해야하므로 label 데이터는 주지 않습니다.



```python
print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))
```

예측 결과를 기반으로 의사 결정 트리 기반의 DecisionTreeClassifier의 예측 성능을 평가하겠습니다. 

일반적으로 머신러닝 모델 성능 평가 방법은 여러가지가 있으나, 저희는 정확도를 측정해 평가하겠습니다.

정확도는 예측 결과가 실제 label 값과 얼마나 정확하게 맞는지를 평가하는 것입니다.

accuracy_score() 메소드의 파라미터로 테스트용 label 데이터 세트 (실제 결과값 : y_test)과 예측 레이블 데이터 세트(pred)를 입력합니다.



![image](https://user-images.githubusercontent.com/76269316/123496686-46554880-d664-11eb-9e1b-a356009da809.png)

실행 결과는 다음과 같습니다. 예측 정확도가 약 0.9333(93.33%)로 측정됐습니다.



붓꽃 데이터 세트로 분류를 예측하는 과정을 정리하면 다음과 같습니다.

1. 데이터 세트 분리 : 데이터를 학습 데이터와 테스트 데이터로 분리합니다.
2. 모델 학습 : 학습 데이터를 기반으로 ML 알고리즘을 적용해 모델을 학습시킵니다.
3. 예측 수행 : 학습된 ML 모델을 이용해 테스트 데이터의 분류(붓꽃 종류)를 예측합니다.
4. 평가 : 예측된 결괏값과 테스트 데이터의 실제 결괏값을 비교해 ML 모델 성능을 평가합니다.



### 사이킷런의 기반 프레임워크 익히기

사이킷런은 API 일관성과 개발 편의성을 제공하기 위한 노력이 돋보이는 패키지입니다. 

ML 모델 학습을 위해 fit(), 학습된 모델을 예측하기 위해 predict() 메소드를 제공하는데,

사이킷런에서 지도학습의 분류 알고리즘을 구현한 Classifier와 회귀 알고리즘을 구현한 Regressor 클래스에서도 fit과 predict를 사용해 간단하게 학습과 예측 결과를 확인할 수 있습니다.

이들 Classifier아 Regressor를 합쳐서 Estimator 클래스라고 부르는데, Estimator 클래스에서도 마찬가지로 fit과 predict를 제공합니다.

![Estimator](https://user-images.githubusercontent.com/76269316/123498747-caabc980-d66c-11eb-8d84-c7d0864e25e8.png)



또한 비지도학습인 차원 축소, 클러스터링, 피처 추출(Feature Extraction) 등을 구현한 클래스에서도 fit()과 transform()을 적용합니다.

비지도학습과 피처추출에서 fit은 지도학습에서의 fit과 같이 학습을 의미하는 것이 아니라, 입력 데이터의 형태에 맞춰 데이터를 변환하기 위한 사전 구조를 맞추는 작업입니다.

fit으로 변환을 위한 사전 구조를 맞추면 이후 입력 데이터의 차원 변환, 클러스터링, 피처 추출등의 작업은 transform으로 수행합니다.

fit과 transform을 합친 fit_transform() 메소드도 있는데, 사용에 약간의 차이가 있습니다. 비지도학습 포스팅에서 설명하도록 하겠습니다.



### 사이킷런의 주요 모듈

![scikit-learn module](https://user-images.githubusercontent.com/76269316/123499526-70156c00-d672-11eb-8f0d-86fb928bfe37.png)

지금은 개괄적으로만 이해해도 충분합니다. 이후 포스팅에서 상세하게 다루도록 하겠습니다.



### 내장된 예제 데이터 세트

사이킷런에는 예제로 활용할 수 있는 간단하면서 좋은 데이터 세트가 내장돼 있습니다. 저희가 위에서 사용한 붓꽃 데이터 세트도 그 중 하나입니다.

다음과 같이 5개의 데이터 세트가 내장되어 있습니다.

| API명                         | 설명                                                         |
| ----------------------------- | ------------------------------------------------------------ |
| datasets.load_boston()        | 회귀 용도이며, 미국 보스턴의 집 피처들과 가격에 대한 데이터 세트 |
| datasets.load_breast_cancer() | 분류 용도이며, 위스콘신 유방암 피처들과 악성/음성 레이블 데이터 세트 |
| datasets.load_diabetes()      | 회귀 용도이며, 당뇨 데이터 세트                              |
| datasets.load_digits()        | 분류 용도이며, 0에서 9까지 숫자의 이미지 픽셀 데이터 세트    |
| datasets.load_iris()          | 분류 용도이며, 붓꽃에 대한 피처를 가진 데이터 세트           |



fetch 계열 명령은 데이터 크기가 커서 패키지에 저장돼 있지 않고, 인터넷에서 다운받아 홈 디렉토리 아래 scikit_learn_data라는 서브 디렉토리에 저장한 후 추후 불러들여야 합니다.

- fetch_covtype() : 회귀 분석용 토지 조사 자료
- fetch_20newsgroups() : 뉴스 그룹 텍스트 자료
- fetch_olivetti_faces() : 얼굴 이미지 자료
- fetch_lfw_people() : 얼굴 이미지 자료
- fetch_lfw_pairs() : 얼굴 이미지 자료
- fetch_rcv1() : 로이터 뉴스 말뭉치
- fetch_mldata() : ML 웹사이트에서 다운로드



분류와 클러스터링을 위한 표본 데이터 생성기

| API명                           | 설명                                                         |
| ------------------------------- | ------------------------------------------------------------ |
| datasets.make_classifications() | 분류를 위한 데이터 세트를 생성.<br />높은 상관도, 불필요한 속성 등의 노이즈 효과를 위한 데이터를 무작위로 생성. |
| datasets.make_blobs()           | 클러스터링을 위한 데이터 세트를 무작위로 생성.<br />군집 지정 개수에 따라 여러가지 클러스터링을 위한 데이터 세트를 쉽게 만들어줌. |

이외에도 표본 데이터 생성기가 있지만, 위 두 개로도 충분하기 때문에 위 두 개만 소개하겠습니다.



연습용 예제 데이터가 어떻게 구성돼 있는지 보도록 하겠습니다.

사이킷런에 내장된 데이터 세트는 dictionary 형태로 있습니다.

key는 data, target, target_names, feature_names, DESCR로 구성돼 있습니다.

- data : feature의 데이터 세트 (numpy ndarray)
- target : 분류 시 label값, 회귀일 때는 숫자 결괏값 데이터 세트 (numpy ndarray)
- target_names : 개별 label의 이름 (numpy ndarray 또는 python list) 
- feature_names : feature의 이름 (numpy ndarray 또는 python list) 
- DESCR : 데이터 세트에 대한 설명과 각 feature의 설명 (python string)



먼저 붓꽃 데이터를 생성한 다음, type을 확인해보면 

```python
import sklearnfrom sklearn.datasets import load_irisiris_data = load_iris()print(type(iris_data))
```

![image](https://user-images.githubusercontent.com/76269316/123499816-40fffa00-d674-11eb-98ef-5edafaeb391c.png)

sklearn.utils.Bunch 클래스라고 나옵니다. 

Bunch 클래스는 파이썬 dictionary 자료형과 유사합니다. dictionary 형태이므로 key값을 확인해보면 

```python
keys = iris_data.keys()print('붓꽃 데이터 세트의 key:', keys)
```

![image](https://user-images.githubusercontent.com/76269316/123499877-c1bef600-d674-11eb-92ad-b946970eb1a3.png)

다음과 같이 'data', 'target', 'target_names', 'DESCR', 'feature_names'가 key값인 것을 확인할 수 있습니다.



![image](https://user-images.githubusercontent.com/76269316/123500045-157e0f00-d676-11eb-9452-fe6af8b7592f.png)

data는 feature들의 데이터 값을 가리킵니다. 데이터 세트가 dictionary 형태이기 때문에 feature 데이터 값을 추출하기 위해서는 데이터 세트.data (또는 데이터 세트['data'])를 이용하면 됩니다.

```python
iris_data.data  #iris_data['data']도 같음
```



![image](https://user-images.githubusercontent.com/76269316/123500157-ef0ca380-d676-11eb-9233-7a629bf621b0.png)



target, target_names, feature_names, DESCR 모두 동일하게 출력할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/123500203-490d6900-d677-11eb-89d8-c70821b451d3.png)



### Model Selection 모듈 소개

[붓꽃 품종 예측하기](https://seominseok4834.github.io/machine%20learning/2.scikit-learn-machine-learning-in-python/#%EB%B6%93%EA%BD%83-%ED%92%88%EC%A2%85-%EC%98%88%EC%B8%A1%ED%95%98%EA%B8%B0)에서 학습 데이터와 테스트 데이터 세트로 분리해서 예측을 진행했는데, 학습 데이터 세트로만 학습하고, 예측할 때 발생하는 문제를 살펴보겠습니다.

```python
from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scoreiris = load_iris()dt_clf = DecisionTreeClassifier()train_data = iris.datatrain_label = iris.targetdt_clf.fit(train_data, train_label)  #학습 데이터 세트로 학습#학습 데이터 세트로 예측pred = dt_clf.predict(train_data)print('예측 정확도:', accuracy_score(train_label, pred))
```

![image](https://user-images.githubusercontent.com/76269316/123500390-b241ac00-d678-11eb-9516-f983226bceae.png)

정확도가 100%가 나왔습니다.

예측 결과가 100% 정확한 이유는 이미 학습한 학습 데이터를 기반으로 예측했기 때문입니다. 

모의고사를 한 번 보고 모의고사 답을 다 외운 상태인데, 모의고사 문제와 똑같은 문제가 시험에 나와서 다 맞은 상황인 것입니다.



따라서 예측을 수행하는 데이터 세트는 학습을 수행한 학습용 데이터 세트가 아닌 테스트 전용 데이터 세트여야 합니다.

그래서 train_test_split()을 사용해 원본 데이터 세트에서 학습 및 테스트 데이터 세트로 분리한 것입니다.



train_test_split() 파라미터를 보면

```python
import sklearn
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#붓꽃 데이터 세트 로딩
iris = load_iris()

#iris.data는 데이터 세트에서 feature만으로 된 데이터를 numpy로 갖고 있음
iris_data = iris.data

#iris.target은 붓꽃 데이터 세트에서 label만으로 된 데이터를 numpy로 갖고 있음
iris_label = iris.target

#의사 결정 트리를 이용해 학습과 예측 수행
dt_clf = DecisionTreeClassifier(random_state=11)  #DecisionTreeClassifier 객체 생성

#학습용 데이터와 테스트용 데이터로 분리
X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.3, random_state=121)

#학습 데이터 세트로 학습
dt_clf.fit(X_train, y_train)

#테스트 데이터 세트로 예측
pred = dt_clf.predict(X_test)
print('예측 정확도:{0:.4f}'.format(accuracy_score(y_test, pred)))
```

첫 번째 파라미터로는 feature 데이터 세트, 두 번째 파라미터로 label 데이터 세트를 입력 받습니다.

그리고 선택적으로 다음과 같은 파라미터들을 입력받습니다.

- test_size : 전체 데이터에서 테스트 데이터 세트 크기를 얼마로 샘플링 할 것인가를 결정 (default : 0.25 (25%))
- train_size : 전체 데이터에서 학습용 데이터 세트 크기를 얼마로 샘플링 할 것인가를 결정 (train_size를 사용하는게 일반적이라 잘 사용 안함)
- shuffle : 데이터를 분리하기 전 미리 섞을지를 결정 (default : True), 데이터를 분산시켜서 효율적인 학습 및 데이터 세트를 만드는데 사용됨.
- random_state : 호출할 때마다 동일한 학습/테스트용 데이터 세트를 생성하기 위해 주는 난수 값. random_state를 지정하지 않으면 수행할 때마다 다른 학습/테스트용 데이터를 생성함.



위 코드를 실행하면 다음과 같이 예측 정확도가 95.56%가 나옵니다. (붓꽃 데이터는 150개 데이터로 데이터 양이 크지 않아 테스트 데이터가 전체의 30% (45개) 밖에 되지 않아 알고리즘 성능을 판단하기에는 적절하지 않지는 않습니다.)

![image](https://user-images.githubusercontent.com/76269316/123500611-5415c880-d67a-11eb-9fb0-c41a503b55f2.png)





##### 교차 검증

위에서 알고리즘을 학습시키는 학습 데이터와 예측 성능을 평가하기 위한 별도의 테스트 데이터가 필요하다고 했는데,

만약 모델이 학습 데이터에만 과도하게 최적화 되어 실제 예측을 다른 데이터로 수행할 경우 예측 성능이 과도하게 떨어지는 과적합(Overfitting) 문제가 발생할 수도 있습니다.

그렇다고 고정된 학습 데이터와 테스트 데이터로 평가를 하다보면 테스트 데이터에만 최적의 성능을 발휘하는 편향된 모델이 유도되게 됩니다.

이러한 문제점을 개선하기 위해 교차 검증을 사용해 다양한 학습과 평가를 수행합니다.



교차 검증은 데이터 편중을 막기 위해 여러 세트로 구성된 학습 데이터 세트와 검증 데이터 세트에서 학습과 평가를 수행합니다.

각 세트에서 수행한 평가 결과에 따라 하이퍼 파라미터 튜닝 등의 모델 최적화를 할 수 있습니다.

<img src="https://user-images.githubusercontent.com/76269316/123762617-f1ccfa00-d8fd-11eb-9493-7b0f38cdf083.png" alt="image" style="zoom:50%;" />



##### K 폴드 교차 검증

가장 보편적으로 사용되는 교차 검증 기법으로, K개의 데이터 폴드 세트를 만들어서 K번만큼 각 폴드 세트에 검증 평가를 반복적으로 수행하는 방법입니다.

![image](https://user-images.githubusercontent.com/76269316/123506053-3148db80-d69d-11eb-9f26-e31a159977d1.png)

K=5라고 할 경우, 5개의 폴드된 데이터 세트를 학습과 검증을 위한 데이터 세트로 변경하면서 5번 평가를 수행한 뒤, 이 5개 평가를 평균한 결과를 가지고 예측 성능을 평가합니다.

이렇게 학습 데이터 세트와 검증 데이터 세트를 점진적으로 변경하면서 마지막 5번째까지 학습과 검증을 수행하는 것이 K 폴드 교차 검증입니다.



사이킷런에서는 K폴드 교차 검증 프로세스를 구현하기 위해 KFold와 StratifiedKFold 클래스를 제공합니다.

KFold 클래스를 사용해 붓꽃 데이터 세트를 교차 검증하고 예측 정확도를 알아보겠습니다.

```python
import numpy as npimport sklearnfrom sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import KFold#붓꽃 데이터 세트 로딩iris = load_iris() #feature 데이터를 가져옴features = iris.data#label 데이터를 가져옴label = iris.target#DecisionTreeClassifier 객체 생성dt_clf = DecisionTreeClassifier(random_state=156)#5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성kfold = KFold(n_splits=5)cv_accuracy = []print('붓꽃 데이터 세트 크기:',features.shape[0])
```

![image](https://user-images.githubusercontent.com/76269316/123506321-8cc79900-d69e-11eb-8ea3-40f9902357ec.png)



```python
Kfold = KFold(n_splits=5)
```

KFold객체를 생성했으니 이제 생성된 KFold 객체의 split()을 호출해 전체 붓꽃 데이터를 5개의 폴드 데이터 세트로 분리합니다.

전체 붓꽃 데이터는 모두 150개이므로 학습용 데이터 세트는 이 중 4/5인 120개, 검증용 데이터 세트는 30개로 분할됩니다.



```python
n_iter = 0for train_index, test_index in kfold.split(features):    #kfold.split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출    X_train, X_test = features[train_index], features[test_index]    y_train, y_test = label[train_index], label[test_index]        #학습 및 예측    dt_clf.fit(X_train, y_train)    pred = dt_clf.predict(X_test)    n_iter += 1        #반복 시 마다 정확도 측정    accuracy = np.round(accuracy_score(y_test, pred),4)  #소숫점 다섯째자리에서 반올림 (넷째자리까지 표시)    train_size = X_train.shape[0]    test_size = X_test.shape[0]    print('\n#{0} 교차 검증 정확도: {1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'.format(n_iter, accuracy, train_size, test_size))    print('#{0} 검증 세트 인덱스: {1}'.format(n_iter, test_index))    	#정확도를 cv_accuracy list에 넣음    cv_accuracy.append(accuracy)    #개별 iteration별 정확도를 합하여 평균 정확도 계산print('\n## 평균 검증 정확도: ', np.mean(cv_accuracy))
```

![image](https://user-images.githubusercontent.com/76269316/123513525-a7af0300-d6c8-11eb-9214-161ae2a8323d.png)



KFold 클래스의 split()을 호출하면 학습용/검증용 데이터로 분할할 수 있는 인덱스를 반환합니다. 따라서 학습용 인덱스를 train_index, 검증용 인덱스를 test_index로 받았습니다.

```python
for train_index, test_index in kfold.split(features):
```



이후 앞에서 했던 것과 마찬가지로 의사 결정 트리 DecisionTreeClassifier를 이용해 예측 성능을 평가하고 이를 cv_accuracy list에 추가해주었습니다.

5개의 예측 평가를 구한뒤 이를 평균해서 평균 정확도를 계산했습니다.

```python
print('\n## 평균 검증 정확도: ', np.mean(cv_accuracy))
```



##### Stratified K 폴드

Stratified K 폴드는 불균형한 분포도를 가진 label(결정 클래스) 데이터 집합을 위한 K 폴드 방식입니다.

불균형한 분포도를 가진 label 데이터 집합은 특정 label 값이 특이하게 많거나 매우 적어서 값의 분포가 한 쪽으로 치우치는 것을 말합니다.



예를 들어, 대출 사기 데이터를 예측한다고 할 때 이 데이터 세트는 1억건이고 수십 개의 feature와 대출 사기 여부를 뜻하는 label (대출 사기: 1, 정상 대출: 0)로 구성돼 있습니다.

1억 건중 대출 사기가 1,000건이 있다고 하면 전체의 0.0001%의 아주 작은 확률로 대출 사기 label이 존재합니다. 

이렇게 작은 비율로 1 label이 존재한다면 K 폴드로 랜덤하게 학습 및 테스트 세트의 인덱스를 고르더라도 0과 1의 비율을 제대로 반영하지 못하게 됩니다. (1이 특정 학습/테스트 데이터 세트에는 상대적으로 많이 들어있고, 다른 학습/테스트 데이터 세트에는 그렇지 못하게 됨.)

대출 사기 label이 1인 레코드는 건수는 작지만 대출 사기를 예측하기 위한 중요한 feature 값을 갖고 있기 때문에 매우 중요한 데이터 세트입니다.

**따라서 원본 데이터와 유사한 label 값의 분포를 학습/테스트 데이터 세트에도 유지하는 것이 중요합니다.**



Stratified K 폴드는 원본 데이터의 label 분포를 먼저 고려한 다음, 이 분포와 동일하게 학습과 검증 데이터 세트를 분배합니다.



먼저 K 폴드가 어떤 문제를 갖고있는지 보도록 하겠습니다.

```python
import pandas as pdimport sklearnfrom sklearn.datasets import load_irisfrom sklearn.model_selection import KFoldiris = load_iris()iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)iris_df['label'] = iris.target#3개의 폴드 세트로 분리하는 KFold 객체 생성kfold = KFold(n_splits=3)n_iter = 0for train_index, test_index in kfold.split(iris_df):    n_iter += 1    label_train = iris_df['label'].iloc[train_index]    label_test = iris_df['label'].iloc[test_index]    print('## 교차 검증: {0}'.format(n_iter))    print('학습 레이블 데이터 분포:\n', label_train.value_counts())    print('검증 레이블 데이터 분포:\n', label_test.value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/123512853-c3180f00-d6c4-11eb-867d-d70458eb39b2.png)



교차 검증시마다 3개의 폴드 세트로 만들어지는 학습 label과 검증 label이 완전히 다른 값으로 추출되었습니다.

첫 번째 교차 검증에서는 학습 label이 1, 2값으로만 50개씩 추출되었고 검증 label에 0 값이 50개 추출되었습니다. 

학습 label이 1, 2 밖에 없으므로 0의 경우는 전혀 학습하지 못하게되고, 이 경우 예측 정확도는 0이 될 수 밖에 없습니다.



StraitifiedKFold는 이렇게 KFold로 분할된 label 데이터 세트가 전체 label 값의 분포도를 반영하지 못하는 문제를 해결해 줍니다.

이번에는 StraitifiedKFold를 사용해 학습/검증 label 데이터 분포도를 확인해보겠습니다.

```python
import pandas as pd
import sklearn
from sklearn.datasets import load_iris
from sklearn.model_selection import StratifiedKFold

iris = load_iris()
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris_df['label'] = iris.target

#3개의 폴드 세트로 분리하는 StratifiedKFold 객체 생성
skf = StratifiedKFold(n_splits=3)
n_iter = 0

#StratifiedKFold split() 호출 시에는 *label 데이터 세트 추가 입력 필요
for train_index, test_index in skf.split(iris_df, iris_df['label']):
    n_iter += 1
    label_train = iris_df['label'].iloc[train_index]
    label_test = iris_df['label'].iloc[test_index]
    print('## 교차 검증:{0}'.format(n_iter))
    print('학습 레이블 데이터 분포:\n', label_train.value_counts())
    print('검증 레이블 데이터 분포:\n', label_test.value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/123513175-97962400-d6c6-11eb-931c-a11d28a06730.png)



출력 결과를 보면 학습 label과 검증 label 데이터 값의 분포도가 동일하게 할당된 것을 볼 수 있습니다.

이렇게 분할이 되어야 label 0, 1, 2 값에 대해 모두 학습할 수 있고 이에 기반해 검증할 수 있습니다. 이제 이 데이터를 이용해 검증해보도록 하겠습니다.



```python
import numpy as npimport sklearnfrom sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import StratifiedKFold#붓꽃 데이터 세트 로딩iris = load_iris() #feature 데이터를 가져옴features = iris.data#label 데이터를 가져옴label = iris.targetdt_clf = DecisionTreeClassifier(random_state=156)skfold = StratifiedKFold(n_splits=3)n_iter = 0cv_accuracy = []#StratifiedKFold split() 호출 시에는 *label 데이터 세트 추가 입력 필요for train_index, test_index in skfold.split(features, label):    #split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출    X_train, X_test = features[train_index], features[test_index]    y_train, y_test = label[train_index], label[test_index]        #학습 및 예측    dt_clf.fit(X_train, y_train)    pred = dt_clf.predict(X_test)    n_iter += 1        #반복 시 마다 정확도 측정    accuracy = np.round(accuracy_score(y_test, pred), 4)  #소숫점 다섯째자리에서 반올림 (넷째자리까지 표시)    train_size = X_train.shape[0]    test_size = X_test.shape[0]    print('\n#{0} 교차 검증 정확도: {1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'.format(n_iter, accuracy, train_size, test_size))    print('#{0} 검증 세트 인덱스: {1}'.format(n_iter, test_index))    	#정확도를 cv_accuracy list에 넣음    cv_accuracy.append(accuracy)    #교차 검증별 정확도 및 평균 정확도 계산print('\n## 교차 검증별 정확도:', np.round(cv_accuracy, 4))print('\n## 평균 검증 정확도: ', np.round(np.mean(cv_accuracy), 4))    
```

![image](https://user-images.githubusercontent.com/76269316/123513768-e98c7900-d6c9-11eb-82af-a381a3a4d6ba.png)



3개의 Straitifed K 폴드로 교차 검증한 결과 평균 검증 정확도가 약 96.67%가 측정됐습니다.

Stratified K 폴드의 경우 원본 데이터의 label 분포도 특성을 반영한 학습 및 검증 데이터 세트를 만들 수 있으므로 왜곡된 label 데이터 세트에서는 반드시 Straitifed K 폴드를 이용해 교차 검증해야 합니다.



+분류(Classification)에서의 교차 검증은 K 폴드가 아니라 Stratified K 폴드로 분할돼야 합니다. (회귀(Regression)의 결정값(label)이 이산값 형태가 아니라 연속된 숫자값이기 때문에 Straitifed K 폴드가 지원되지 않음)



##### 교차 검증을 보다 간편하게 - cross_val_score()

이전까지는

1. 폴드 세트를 설정
2. for 루프에서 반복으로 학습 및 테스트 데이터 인덱스 추출
3. 반복적으로 학습과 예측을 수행하고 예측 성능 반환

위 3가지 과정을 거쳤습니다. 사이킷런의 cross_val_score()에서는 이런 일련의 과정을 한꺼번에 수행해주는 API를 제공합니다.



```python
cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs')
```

파라미터 중 estimator, X, y, scoring, cv가 주요 파라미터입니다.

- estimator : 사이킷런 분류 알고리즘 클래스(Classifier) 또는 회귀 알고리즘 클래스(Regressor)를 의미합니다.
- X : 피처 데이터 세트
- y : 레이블 데이터 세트
- scoring : 예측 성능 평가 지표
- cv : 교차 검증 폴드 수



+cross_val_score()는 classifier가 입력되면 Straitifed K 폴드 방식으로 레이블 값의 분포에 따라 학습/테스트 세트를 분할합니다. (Regressor인 경우 K 폴드 방식으로 분할)



```python
import numpy as npimport sklearnfrom sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import cross_val_scoreiris_data = load_iris()dt_clf = DecisionTreeClassifier(random_state=156)data = iris_data.datalabel = iris_data.target#성능 지표는 정확도(accuracy), 교차 검증 세트는 3개scores = cross_val_score(dt_clf, data, label, scoring='accuracy', cv=3)print('교차 검증별 정확도:', np.round(scores, 4))print('평균 검증 정확도:', np.round(np.mean(scores), 4))
```

![image](https://user-images.githubusercontent.com/76269316/123515011-d3ce8200-d6d0-11eb-95b6-f6a1dfd35cb2.png)



cross_val_score 수행 후 scoring 파라미터로 지정된 성능 평가 값을 배열 형태로 반환합니다.

일반적으로 이를 평균해 평가 수치로 사용합니다.

cross_val_score는 내부적으로 학습(fit), 예측(predict), 평가(evaluation)를 시켜주므로 간단하게 교차 검증을 수행할 수 있습니다.



또한 바로 위에서 StraitifedKFold의 수행 결과와 동일한 결과가 나온 것을 알 수 있습니다.

이는 cross_val_score가 내부적으로 StraitifedKFold를 사용하기 때문입니다.



비슷한 API로 cross_validate()가 있습니다. 

cross_val_score는 하나의 평가 지표만 가능하지만 cross_validate는 여러 개의 평가 지표를 반환할 수 있습니다. 

또한 학습 데이터에 대한 성능 평가 지표와 수행 시간도 같이 제공합니다. 아래 import문을 통해 사용할 수 있습니다.

```python
from sklearn.model_selection import cross_validate
```



##### GridSearchCV - 교차 검증과 최적 하이퍼 파라미터 튜닝을 한 번에

아직 머신러닝 알고리즘을 구성하는 하이퍼 파라미터에 대한 상세한 설명을 하지 않았기 때문에 이 부분은 그냥 한 번 읽기만 해도 될 것 같습니다.

하이퍼 파라미터는 머신러닝 알고리즘을 구성하는 주요 구성 요소로써, 값을 조정해 알고리즘의 예측 성능을 개선할 수 있습니다.



사이킷런에서는 GridSearchCV API를 통해 분류나 회귀와 같은 알고리즘에 사용되는 하이퍼 파라미터 딕셔너리를 만들고 이를 교차 검증으로 순차 적용하면서  최적의 파라미터를 찾을 수 있도록 합니다.



만약 다음과 같은 하이퍼 파라미터 딕셔너리가 있고, CV*교차 검증 폴드 수)가 3회라면

```python
gird_parameters = {'max_depth': [1, 2, 3], 'min_samples_split': [2, 3]}
```

| 순번 | max_depth | min_samples_split |
| :--: | :-------: | :---------------: |
|  1   |     1     |         2         |
|  2   |     1     |         3         |
|  3   |     2     |         2         |
|  4   |     2     |         3         |
|  5   |     3     |         2         |
|  6   |     3     |         3         |

CV 3회 X 6개 파라미터 조합 = 18회의 학습/평가가 이루어지게 됩니다.



GridSearchCV 클래스 주요 파라미터는 다음과 같습니다.

- estimator : classifier, regressor, pipeline이 사용될 수 있습니다.
- param_grid : key + 리스트 값을 갖는 딕셔너리가 주어집니다. (estimator 튜닝을 위해 파라미터명과 파라미터 값을 지정)
- scoring : 예측 성능을 측정할 평가 방법 지정 (보통 사이킷런 성능 평가 지표를 지정하는 문자열(예 - 정확도 : accuracy)로 지정하나 별도의 성능 평가 지표 함수도 지정 가능)
- cv : 교차 검증을 위해 분할되는 학습/테스트 세트 개수
- refit : True로 생성시 가장 최적의 하이퍼 파라미터를 찾은 뒤 입력된 estimator 객체를 해당 하이퍼 파라미터로 재학습시킴. (default : true)



```python
import pandas as pdimport sklearnfrom sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import GridSearchCV#데이터를 로딩하고 학습 데이터와 테스틑 데이터로 분리iris_data = load_iris()X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=121)#DecisionTreeClassifier 객체 생성dtree = DecisionTreeClassifier(random_state=156)#하이퍼 파라미터를 딕셔너리 형태로 설정parameters = {'max_depth':[1, 2, 3], 'min_samples_split':[2, 3]}#param_grid 하이퍼 파라미터를 3개의 train, test 세트 폴드로 나누어 테스트 수행 설정grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)  #refit=True가 default (최적의 파라미터로 재학습 시킴)#붓꽃 학습 데이터로 param_grid 하이퍼 파라미터를 순차적으로 학습/평가grid_dtree.fit(X_train, y_train)#GridSearchCV 결과를 추출해 DataFrame으로 변환scores_df = pd.DataFrame(grid_dtree.cv_results_)scores_df[['params', 'mean_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score']]
```

![image](https://user-images.githubusercontent.com/76269316/123516229-029b2700-d6d6-11eb-8b5a-63a2e0271d36.png)



rank_test_score가 1이라는 의미는 해당 parameter를 갖고 평가한 결과 예측 성능이 1위라는 의미입니다. 

인덱스 5, 6번이 mean_test_score(split0, split1, split2 세 개의 성능 수치를 평균한 값)가 0.975로 가장 높은 것을 볼 수 있습니다.



- parmas column : 수행할 때마다 적용된 개별 하이퍼 파라미터 값
- rank_test_score : 하이퍼 파라미터별로 성능이 좋은 score 순위 (1이 가장 뛰어난 순위이며 이때의 파라미터가 최적의 하이퍼 파라미터)
- mean_test_score : 개별 하이퍼 파라미터별로 CV의 폴딩 테스트 세트에 대해 총 수행한 평가 평균값



GridSearchCV 객체의 fit을 수행하면 최고 성능을 나타낸 하이퍼 파라미터의 값과 그 떄의 평과 결과 값이 best_params\_, best_score\_ 속성에 기록됩니다. (rank_test_score가 1일 때의 값)

```python
print('GridSearchCV 최적 파라미터:', grid_dtree.best_params_)print('GridSearchCV 최고 정확도:{0:.4f}'.format(grid_dtree.best_score_))
```

![image](https://user-images.githubusercontent.com/76269316/123516457-1dba6680-d6d7-11eb-82f7-101b8881d7e7.png)



refit=True인 경우 GridSearchCV가 최적 성능을 나타내는 하이퍼 파라미터로 estimator를 학습해 best_estimator_로 저장합니다.

best\_estimator\_를 이용해 train_test_split()으로 분리한 테스트 데이터 세트에 대해 예측하고 성능을 평가해보겠습니다.

```python
#GridSearchCV의 refit으로 이미 학습된 estimator 반환
estimator = grid_dtree.best_estimator_

#GridSearchCV의 best_estimator_는 이미 최적 학습 됐으므로 별도 학습이 필요 없음
pred = estimator.predict(X_test)
print('테스트 데이터 세트 정확도:{0:.4f}'.format(accuracy_score(y_test, pred)))
```

![image](https://user-images.githubusercontent.com/76269316/123516536-902b4680-d6d7-11eb-86e9-e657bd50a92b.png)

약 96.67%의 결과가 나왔습니다.



일반적으로 학습 데이터를 GridSearchCV를 이용해 최적 하이퍼 파라미터 튜닝을 수행하고 별도의 테스트 세트에서 이를 평가하는 것이 일반적인 머신러닝 모델 적용 방법입니다.



### 데이터 전처리

ML 알고리즘은 데이터에 기반하기 때문에 어떤 데이터를 입력으로 가지느냐에 따라 결과가 크게 달라집니다. (Garbage In, Garbage Out)

그래서  ML 알고리즘을 적용하기 전에 데이터에 대해 미리 처리해야 할 사항들이 있습니다.

- 결손값(Nan)을 허용하지 않음 → Null 값은 다른 값으로 변환되어야 함.

feature 값 중 Null 값이 얼마되지 않는다면 feautre의 평균값 등으로 대체할 수 있지만 Null값이 대부분이라면 해당 feature는 drop 하는 것이 좋습니다.

(해당 feature가 중요도가 높은 feature이고, Null 값을 단순히 평균값으로 대체할 경우 예측 왜곡이 심할 수 있는 경우 더 정밀한 대체 값을 선정해야 합니다.)

- 문자열 값을 입력 값으로 허용하지 않음 → 인코딩해서 숫자형으로 변환해야 함.

문자열 feature는 카테고리형 / 텍스트형으로 나뉘는데 텍스트형은 feature vectorization 등의 기법으로 벡터화해야하는데, 나중에 텍스트 분석 포스트에서 설명하도록 하겠습니다.

문자열 feature의 경우에도 불필요한 feature라고 판단되는 경우 삭제하는 게 좋습니다. (주민번호나 단순 문자열 아이디와 같은 경우 단순히 row를 식별하는데만 사용되고, 알고리즘을 오히려 복잡하게 만들고 예측 성능을 떨어뜨리기 때문)



#### 데이터 인코딩



##### 레이블 인코딩 (Label Encoding)

카테고리 feature를 코드형 숫자 값으로 변환하는 것입니다.

TV, 냉장고, 전자레인지, 컴퓨터, 선풍기, 믹서 값이 있다면 TV: 1, 냉장고: 2, 전자레인지: 3 ··· 같은 숫자형 값으로 변환합니다.

주의해야 할 점은 '01', '02' 같은 코드값 역시 문자열이므로 1, 2 같은 숫자형 값으로 변환돼야 합니다.

```python
import sklearnfrom sklearn.preprocessing import LabelEncoderitems=['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']#LabelEncoder를 객체로 생성한 후, fit()과 transform()으로 인코딩 encoder = LabelEncoder()encoder.fit(items)labels = encoder.transform(items)print('인코딩 변화값:', labels)
```

![image](https://user-images.githubusercontent.com/76269316/123532402-d02a1200-d747-11eb-8e06-5172521aced2.png)

<img src="https://user-images.githubusercontent.com/76269316/123532978-cf47af00-d74c-11eb-98ad-2d63090f8d20.png" alt="image" style="zoom:50%;" />



TV는 0, 냉장고 1, 전자레인지 4, 컴퓨터 5, 선풍기 3, 믹서는 2로 변환됐습니다.

위 코드는 데이터가 작아서 문자열 값이 어떤 숫자 값으로 인코딩 됐는지 바로 알 수 있지만 보통은 한 번에 알아보기 힘듧니다.

```python
print('인코딩 클래스:', encoder.classes_)
```

![image](https://user-images.githubusercontent.com/76269316/123532454-1c755200-d748-11eb-8500-6e7c8c18ab30.png)



classes_ 속성은 0번부터 변환된 인코딩 값에 대한 원본 값을 갖고 있습니다.

inverse_transform()을 사용해서 인코딩된 값을 다시 디코딩 할 수 있습니다.

```python
print('디코딩 원본값:', encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3]))
```

![image](https://user-images.githubusercontent.com/76269316/123532478-4e86b400-d748-11eb-8ac0-6b2ed03efa0a.png)



레이블 인코딩은 간단하게 문자열 값을 숫자형 카테고리 값으로 변환합니다.

하지만 몇몇 ML 알고리즘에서 이를 적용할 경우 숫자 값의 크고 작음에 대한 특성이 작용해서 예측 성능이 떨어지는 경우가 발생할 수 있습니다.

예를 들어 냉장고가 1, 믹서가 2로 변환되면 특정 알고리즘에서 가중치가 더 부여되어 믹서를 더 중요하게 인식할 수 있다는 의미입니다.

하지만 냉장고와 믹서는 단순히 카테고리 feature이기 때문에 숫자 값에 따라 중요도로 인식되면 안됩니다.

이런 특성 때문에 레이블 인코딩은 선형 회귀와 같은 ML 알고리즘에는 적용하지 않아야합니다. (트리 계열의 ML 알고리즘은 이러한 특성을 반영하지 않으므로 적용 가능)

이런 문제점을 개선하기 위한 인코딩 방식이 원-핫 인코딩입니다.



##### 원-핫 인코딩 (One-Hot Encoding)

원-핫 인코딩은 feature 값의 유형에 따라 새로운 feature를 추가해 고유 값에 해당하는 column에만 1을 표시하고 나머지 column에는 0으로 표시하는 방식입니다.

<img src="https://user-images.githubusercontent.com/76269316/123532627-e638d200-d749-11eb-8da6-fec03078447e.png" alt="image" style="zoom:50%;" />



원본 데이터는 8개의 레코드로 돼 있으며 고유 값은 ['TV', '냉장고', '믹서', '선풍기', '전자레인지', '컴퓨터']로 모두 6개입니다.

원-핫 인코딩은 고유 값에 따라 상품 분류 feature를 6개의 상품 분류 고유 값 feature로 변환합니다. (상품분류\_TV, 상품분류\_냉장고 ···)

그리고 해당 레코드의 상품 분류가 TV인 경우는 상품 분류_TV  feature에만 1을 입력하고 나머지 feature는 모두 0을 입력합니다.



```python
import numpy as npimport sklearnfrom sklearn.preprocessing import LabelEncoder, OneHotEncoderitems=['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']#LabelEncoder로 먼저 숫자 값으로 변환encoder = LabelEncoder()encoder.fit(items)labels = encoder.transform(items)#2차원 데이터로 변환labels = labels.reshape(-1, 1)  #8X1으로 변환#원-핫 인코딩 적용oh_encoder = OneHotEncoder()oh_encoder.fit(labels)oh_labels = oh_encoder.transform(labels)print('원-핫 인코딩 데이터\n', oh_labels.toarray())print('원-핫 인코딩 데이터 차원\n', oh_labels.shape)
```

![image](https://user-images.githubusercontent.com/76269316/123532793-4714da00-d74b-11eb-852c-787e6b93a607.png)



<img src="https://user-images.githubusercontent.com/76269316/123532962-b939ee80-d74c-11eb-80ab-f5858a07b816.png" alt="image" style="zoom:50%;" />



+Pandas의 get_dummies()를 사용하면 문자열 카테고리 값을 숫자 형으로 변환할 필요 없이 바로 변환할 수 있습니다.

```python
import pandas as pddf = pd.DataFrame({'item': ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']})pd.get_dummies(df)
```

![image](https://user-images.githubusercontent.com/76269316/123533031-23529380-d74d-11eb-874f-d9e11e02a05e.png)



##### 피처 스케일링과 정규화

서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업을 피처 스케일링(feature scailing)이라고 합니다.

대표적으로 표준화(Standardization)와 정규화(Normalization)가 있습니다.



표준화는 데이터의 feature 각각이 평균이 0이고 분산이 1인 가우시안 정규 분포를 가진 값으로 변환하는 것을 의미합니다.

표준화를 통해 변환될 feature x의 i번째 데이터를 xi_new라고 한다면 이 값은 원래 값에서 feature x의 평균을 뺀 값을 feature x의 표준편차로 나눈 값으로 계산할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/123533185-344fd480-d74e-11eb-9e87-4a4e16aadb26.png)



정규화는 서로 다른 feature의 크기를 통일하기 위해 크기를 변환해주는 개념입니다.

예를 들어 feature A는 거리를 나타내는 변수로서 값이 0 ~ 100KM이고, feature B는 금액을 나타내는 속성으로 값이 0 ~ 1,000,000,000,000원으로 주어진다면 두 feature를 모두 동일한 크기 단위로 비교하기 위해 최솟값 0 ~ 최댓값 1로 변환하는 것입니다. (개별 데이터 크기를 모두 같은 단위로 변경)

![image](https://user-images.githubusercontent.com/76269316/123533227-8b55a980-d74e-11eb-850c-0f7107bf2968.png)

데이터 xi_new는 원래 값에서 feature x의 최솟값을 뺀 값을 feature x의 최댓값과 최솟값의 차이로 나눠 변환할 수 있습니다.



사이킷런의 전처리에서 제공하는 Normalizer 모듈과 일반적인 정규화는 약간의 차이가 있습니다. (큰 개념은 같음)

사이킷런의 Normalizer 모듈은 선형대수에서의 정규화 개념이 적용됐으며, 개별 벡터의 크기를 맞추기 위해 변환하는 것을 의미합니다.

즉, 개별 벡터를 모든 feature 벡터의 크기로 나눠줍니다.

세 개의 feature x, y, z가 있다고 하면 새로운 데이터 xi_new는 원래 값에서 세개의 feature의 i번째 feature 값에 해당하는 크기를 합한 값으로 나눠 줍니다.

![image](https://user-images.githubusercontent.com/76269316/123533316-4a11c980-d74f-11eb-9c47-32a7558afa0c.png)



혼선을 방지하기 위해 일반적인 의미의 표준화·정규화를 피처 스케일링이라 통칭하고 선형대수 개념의 정규화를 벡터 정규화로 지칭하겠습니다.

먼저 사이킷런에서 제공하는 피처 스케일링 클래스인 StandardScaler를 알아보겠습니다.



##### StandaradScaler

StandardScaler는 표준화를 쉽게 지원하기 위한 클래스입니다. 개별 피처를 평균이 0이고, 분산이 1인 값으로 변환해줍니다.

+서포트 벡터 머신(Support Vector Machine)이나 선형 회귀(Linear Regression), 로지스틱 회귀(Logistic Regression)는 데이터가 가우시안 분포를 가지고 있다고 가정하고 구현됐기 때문에 사전에 표준화를 적용해야 합니다.



```python
import sklearn
import pandas as pd
from sklearn.datasets import load_iris

#붓꽃 데이터 세트를 로딩하고 DataFrame으로 변환
iris = load_iris()
iris_data = iris.data
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)

print('feature들의 평균 값')
print(iris_df.mean())
print('\nfeature 들의 분산 값')
print(iris_df.var())
```

![image](https://user-images.githubusercontent.com/76269316/123545040-1fe0fb80-d791-11eb-830c-118dcea550f8.png)



StandardScaler를 이용해 모든 feature를 한 번에 표준화를 해보겠습니다.

```python
from sklearn.preprocessing import StandardScaler#StandardScaler 객체 생성scaler = StandardScaler()#StandardScaler로 데이터 세트 변환scaler.fit(iris_df)iris_scaled = scaler.transform(iris_df)#transform() 시 스케일 변환된 데이터 세트가 Numpy ndarray로 반환돼 이를 DataFrame으로 반환iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)print('feature 들의 평균 값')print(iris_df_scaled.mean())print('\nfeature 들의 분산 값')print(iris_df_scaled.var())
```

![image](https://user-images.githubusercontent.com/76269316/123545232-0a200600-d792-11eb-8561-0a6988e46a4d.png)

모든 column 값 평균이 0에 아주 가까운 값으로, 분산은 1에 가까운 값으로 변환됐음을 알 수 있습니다.



##### MinMaxScaler

이번에는 MinMaxScaler를 이용해서 표준화를 해보겠습니다.

MinMaxScaler는 데이터 값을 0에서 1사이의 범위 값으로 변환합니다. (음수 값이 있으면 -1에서 1 값으로 변환)

데이터 분포가 가우시안 분포가 아닐 경우에 MinMaxScale을 적용해 볼 수 있습니다.



```python
import sklearnimport pandas as pdfrom sklearn.datasets import load_irisfrom sklearn.preprocessing import MinMaxScaler#붓꽃 데이터 세트를 로딩하고 DataFrame으로 변환iris = load_iris()iris_data = iris.datairis_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)#MinMaxScaler 객체 생성scaler = MinMaxScaler()#MinMaxScaler로 데이터 세트 변환scaler.fit(iris_df)iris_scaled = scaler.transform(iris_df)#transform() 시 스케일 변환된 데이터 세트가 Numpy ndarray로 반환돼 이를 DataFrame으로 반환iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)print('feature 들의 최솟값')print(iris_df_scaled.min())print('\nfeature 들의 최댓값')print(iris_df_scaled.max())
```

![image](https://user-images.githubusercontent.com/76269316/123545545-8535ec00-d793-11eb-8009-baaea91dec91.png)



##### 학습 데이터와 테스트 데이터의 스케일링 변환시 유의점

StandardScaler나 MinMaxScaler 같은 Scaler 객체를 이용해 데이터 스케일링 변환 시

fit()은 데이터 변환을 위한 기준 정보 설정 (데이터 세트의 최댓값/최솟값 설정 등)을 적용하며

transform()은 설정된 정보를 이용해 데이터를 변환합니다.

+fit_transform()은 fit()과 transform()을 한 번에 적용



이렇게 학습 데이터 세트와 테스트 데이터 세트에 fit()과 transform()을 적용할 때 주의할 점이 있습니다.

**학습 데이터 세트로 fit()과 transform()을 적용하면 테스트 데이터 세트에서는 fit()을 수행하지 않고, 학습 데이터 세트로 fit()이 적용된 스케일링 기준 정보를 그대로 transform()을 수행해 테스트 데이터에 적용해야 한다는 것입니다.**

그렇지 않을 경우, 학습 데이터와 테스트 데이터의 스케일링 기준 정보가 달라져 올바른 예측 결과를 도출하지 못하게 되기 때문입니다.



```python
import numpy as npimport sklearnfrom sklearn.preprocessing import MinMaxScaler#학습 데이터는 0부터 10까지, 테스트 데이터는 0부터 5까지 값을 갖도록 데이터 세트 생성#Scaler 클래스의 fit(), transform()은 2차원 이상 데이터만 가능하므로 reshape(-1, 1)로 차원 변경train_array = np.arange(0, 11).reshape(-1, 1)test_array = np.arange(0, 6).reshape(-1, 1)#MinMaxScaler 객체에 별도의 feature_range 파라미터 값을 지정하지 않으면 0 ~ 1 값으로 변환scaler = MinMaxScaler()#학습 데이터 train_array부터 MinMaxScaler를 이용해 변환#fit() 실행시 train_array 데이터 최솟값: 0, 최댓값: 10으로 설정됨scaler.fit(train_array)#1/10 scale로 train_array 데이터 변환train_scaled = scaler.transform(train_array)print('원본 train_array 데이터: ', np.round(train_array.reshape(-1), 2))print('Scale된 train_array 데이터: ', np.round(train_scaled.reshape(-1), 2))#테스트 데이터 test_array에 다시 fit()을 호출해 스케일링 기준 정보를 변경한 뒤 변환#fit() 실행시 test_array 데이터 최솟값: 0, 최댓값: 5로 설정됨scaler.fit(test_array)#1/5 scale로 test_array 데이터 변환test_scaled = scaler.transform(test_array)print('원본 train_array 데이터: ', np.round(test_array.reshape(-1), 2))print('Scale된 train_array 데이터: ', np.round(test_scaled.reshape(-1), 2))
```

![image](https://user-images.githubusercontent.com/76269316/123545913-836d2800-d795-11eb-8e4f-4089ab9f8b25.png)



위 코드의 실행 결과를 보면 학습 데이터와 테스트 데이터의 스케일이 맞지 않는 것을 볼 수 있습니다.

학습 데이터는 원본값 2가 0.2로 변환됐는데, 테스트 데이터에서는 원본값 1이 0.2로 변환됐습니다.

머신러닝 모델은 학습 데이터를 기반으로 학습되기 때문에 반드시 테스트 데이터는 학습 데이터 스케일링 기준에 따라야하며, 따라서 테스트 데이터에 다시 fit()을 적용해서는 안됩니다.



```python
#MinMaxScaler 객체에 별도의 feature_range 파라미터 값을 지정하지 않으면 0 ~ 1 값으로 변환
scaler = MinMaxScaler()

#학습 데이터 train_array부터 MinMaxScaler를 이용해 변환
#fit() 실행시 train_array 데이터 최솟값: 0, 최댓값: 10으로 설정됨
scaler.fit(train_array)

#1/10 scale로 train_array 데이터 변환
train_scaled = scaler.transform(train_array)

print('원본 train_array 데이터: ', np.round(train_array.reshape(-1), 2))
print('Scale된 train_array 데이터: ', np.round(train_scaled.reshape(-1), 2))

#train_array로 fit()을 수행한 MinMaxScaler 객체로 변환
test_scaled = scaler.transform(test_array)

print('원본 train_array 데이터: ', np.round(test_array.reshape(-1), 2))
print('Scale된 train_array 데이터: ', np.round(test_scaled.reshape(-1), 2))
```

![image](https://user-images.githubusercontent.com/76269316/123546046-068e7e00-d796-11eb-9a14-3f31d6477b0f.png)



fit_transform()을 적용할 때도 마찬가지입니다. fit_transform()은 fit()과 transform()을 순차적으로 수행하는 메소드라 학습 데이터에서는 상관없지만 테스트 데이터에서는 절대 사용해서는 안됩니다.



학습 데이터와 테스트 데이터 스케일링 변환시 유의점을 요약하면 다음과 같습니다.

1. 가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이터로 분리
2. 1을 할 수 없을 경우, 테스트 데이터 변환 시에는 fit(), fit_transform()을 적용하지 않고 학습 데이터로 이미 fit()된 Scaler 객체를 이용해 transform()으로 변환



### 사이킷런으로 수행하는 타이타닉 생존자 예측

캐글에서 제공하는 타이타닉 탑승자 데이터를 기반으로 생존자 예측을 사이킷런으로 수행해보겠습니다.

[Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)에서 train.csv를 저장해서 사용하면 됩니다. (저는 titanic_train.csv로 이름을 변경해서 사용했습니다.)



타이타닉 탑승자 데이터

- Passengerid: 탑승자 데이터 일련번호
- survived: 생존 여부, 0 = 사망, 1 = 생존
- pclass: 티켓의 선실 등급, 1 = 일등석, 2 = 이등석, 3 = 삼등석
- sex: 탑승자 성별
- name: 탑승자 이름
- Age: 탑승자 나이
- sibsp: 같이 탑승한 형제 자매 또는 배우자 인원수
- parch: 같이 탑승한 부모님 또는 어린이 인원수
- ticket: 티켓 번호
- fare: 요금
- cabin: 선실 번호
- embarked: 중간 정착 항구 C = Cherbourg, Q = Queenstown, S = Southampton



```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt  #맷플롯립과 시본을 사용해 차트와 그래프로 시각화
import seaborn as sns

#쥬피터 노트북을 실행한 브라우저에서 차트와 그래프가 바로 그려지도록 함
%matplotlib inline

titanic_df = pd.read_csv('./titanic_train.csv')

print('### 학습 데이터 정보 ### \n')
print(titanic_df.info())  #로딩된 타이타닉 데이터의 column type 확인
```

![image](https://user-images.githubusercontent.com/76269316/123546519-d21bc180-d797-11eb-8e74-18b5b1d26685.png)





RangeIndex는 DataFrame 인덱스 범위를 나타내므로 전체 row 수를 알 수 있습니다. 현재 891개의 row로 구성되어 있습니다. column 수는 12개입니다.

2개의 column이 float64, 5개의 column이 int64, 5개의 column이 object 타입입니다. (pandas에서 object type은 string type으로 봐도 무방합니다. *판다스는 numpy 기반으로 만들어졌고 numpy의 string type은 길이 제한이 있어 이에 대한 구분을 위해 object type으로 명기*)



Non-Null Count는 Null값이 아닌 데이터의 개수를 반환해주는데 Age, Cabin, Embarked column은 각각 177개, 687개, 2개의 Null 값을 갖고 있습니다.



##### 1. Null 값 처리

사이킷런 머신러닝 알고리즘은 [Null 값을 허용하지 않으므로](https://seominseok4834.github.io/machine%20learning/2.scikit-learn-machine-learning-in-python/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC) 위 3개 column의 null 값을 어떻게 처리할 지 결정해야 합니다.

Age의 경우는 평균 나이, 나머지 column은 'N' 값으로 변경하도록 하겠습니다.



```python
print('변경 전 데이터 세트 Null 값 개수 ', titanic_df.isnull().sum().sum())

titanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace=True)
titanic_df['Cabin'].fillna('N', inplace=True)
titanic_df['Embarked'].fillna('N', inplace=True)

#모든 column Null 값이 제대로 바뀌었는지 확인
print('변경 후 데이터 세트 Null 값 개수 ', titanic_df.isnull().sum().sum())
```

![image](https://user-images.githubusercontent.com/76269316/123546871-72261a80-d799-11eb-9867-88cee3bc8da5.png)



##### 2. 문자열 피처 처리

문자열 피처가 Name, Ticket, Sex, Cabin, Embarked 다섯 개가 있었는데 이 중 Name과 Ticket은 단순히 row를 식별하는데만 사용되므로 drop하고, Sex, Cabin, Embarked에 대해서만 문자열 처리를 해주도록 하겠습니다.

[Null 값과 마찬가지로 문자열 값도 허용하지 않으므로](https://seominseok4834.github.io/machine%20learning/2.scikit-learn-machine-learning-in-python/#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC) 이를 어떻게 처리할지 고민하기 전에, 먼저 피처들의 값 분류를 살펴보도록 하겠습니다.

```python
print('Sex 값 분포 :\n', titanic_df['Sex'].value_counts())
print('\nCabin 값 분포 :\n', titanic_df['Cabin'].value_counts())
print('\nEmbarked 값 분포 :\n', titanic_df['Embarked'].value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/123546993-e5c82780-d799-11eb-9f01-1cdba506952b.png)



Sex, Embarked 값은 별 문제가 없으나 Cabin(선실)의 경우 N이 687건으로 가장 많고 또한 속성값이 제대로 정리가 되지 않았습니다. ('C23 C25 C27'처럼 여러 Cabin이 한꺼번에 표기된 속성이 있음)

Cabin의 경우 선실 번호 중 선실 등급을 나타내느 첫 번째 알파벳이 중요하므로 앞 문자만 추출하도록 하겠습니다. (당시에는 지금보다 더 부자와 가난한 사람에 대한 차별이 있던 시절이었기에 일등석에 투숙한 사람이 삼등실에 투숙한 사람보다 더 살아날 확률이 높았을 것이므로)



```python
titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]
print(titanic_df['Cabin'].value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/123547186-c087e900-d79a-11eb-9815-b72ae1c72dbe.png)

##### 3. 데이터 분석

예측을 수행하기 전에, 어떤 유형의 승객이 생존 확률이 높았는지 확인해 보겠습니다.

아마도, 여성과 아이들, 노약자가 우선 구조 대상이였을 것입니다. 다음은 부자나 유명인이였을 것입니다,,



먼저 성별에 따른 생존자 수를 비교해보겠습니다.

- 성별에 따른 생존자 수

```python
#Survived 0 : 사망, 1 : 생존
titanic_df.groupby(['Sex', 'Survived'])['Survived'].count()
```

![image](https://user-images.githubusercontent.com/76269316/123547621-799af300-d79c-11eb-88af-9013b8c53f60.png)

```python
#시본 패키지로 시각화
sns.barplot(x='Sex', y='Survived', data=titanic_df)
```

![image](https://user-images.githubusercontent.com/76269316/123547631-815a9780-d79c-11eb-8804-d7c5467d4cc8.png)

여자는 314명 중 233명이 생존했고 (약 74.2%) 남자는 577명 중 109명 (약 18.8%)이 생존했습니다.



- 부에 따른 생존자 수

이번엔 부자와 가난한 사람 간의 생존확률을 비교해보겠습니다.

부의 기준을 객실 등급 속성(Pclass)으로 정의하겠습니다.

```python
sns.barplot(x='Pclass', y='Survived', data=titanic_df)
```

![image](https://user-images.githubusercontent.com/76269316/123547821-3ab96d00-d79d-11eb-91bd-7b5b7836090e.png)

일등실이 삼등실보다 눈에 띄게 생존 확률이 높은 것을 볼 수 있습니다.



이번에는 성별을 함께 고려해보겠습니다.

```python
#hue 파라미터를 사용해 객실 등급별 성별에 따른 생존 확률을 시각화
sns.barplot(x='Pclass', y='Survived', hue='Sex', data=titanic_df)
```

![image](https://user-images.githubusercontent.com/76269316/123547895-8e2bbb00-d79d-11eb-824b-44c1127f837e.png)

여성의 경우 일, 이등실에 따른 생존 확률의 차이가 크지 않았으나, 삼등실의 경우 생존 확률이 상대적으로 많이 떨어졌고,

남성의 경우 일등실의 생존 확률이 이등실, 삼등실의 생존확률 보다 월등히 높은 것을 볼 수 있습니다.



- 나이에 따른 생존자 수

이번에는 나이에 따른 생존 확률을 알아보도록 하겠습니다.

Age column은 값 종류가 많기 때문에 범위별로 분류해 카테고리 값을 할당해서 사용하겠습니다.

-1 이하 : Unknown

0 ~ 5세 : Baby

6 ~ 12세 : Child

13 ~ 18세 : Teenager

19 ~ 25세 : Student

26 ~ 35세 : Young Adult

36 ~ 60세 : Adult

61세 이상 : Elderly

```python
def get_category(age):
    cat = ''
    if age <= -1: cat = 'Unknown'
    elif age <= 5: cat = 'Baby'
    elif age <= 12: cat = 'Child'
    elif age <= 18: cat = 'Teenager'
    elif age <= 25: cat = 'Student'
    elif age <= 35: cat = 'Young Adult'
    elif age <= 60: cat = 'Adult'
    else: cat = 'Elderly'
        
    return cat

#막대그래프 크기 figure를 더 크게 설정
plt.figure(figsize=(10, 6))

#X축 값을 순차적으로 표시하기 위해 설정
group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Elderly']

#lambda 식에 get_category() 함수를 반환값으로 지정.
#get_category(X)는 입력값으로 'Age' column 값을 받아서 해당하는 cat 반환
titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x : get_category(x))

#연령별 성별에 땨른 생존 확률을 시각화
sns.barplot(x='Age_cat', y='Survived', hue='Sex', data=titanic_df, order=group_names)

#Age_cat column 삭제
titanic_df.drop('Age_cat', axis=1, inplace=True)
```

![image](https://user-images.githubusercontent.com/76269316/123548211-ced80400-d79e-11eb-9702-66d8c70bdc1a.png)

여자 Baby의 경우 생존 확률이 높았고, 여자 Child의 경우는 다른 연령대에 비해 생존 확률이 낮은 반면, 여자 Elderly의 경우 매우 생존 확률이 높은 것을 볼 수 있습니다.



##### 4.문자열 카테고리 피처 변환

LabelEncoder 클래스를 사용해서 Cabin, Sex, Embarked column에 대해 [레이블 인코딩](https://seominseok4834.github.io/machine%20learning/2.scikit-learn-machine-learning-in-python/#%EB%A0%88%EC%9D%B4%EB%B8%94-%EC%9D%B8%EC%BD%94%EB%94%A9-label-encoding)을 적용하도록 하겠습니다.

```python
from sklearn.preprocessing import LabelEncoder

def encode_features(dataDF):
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = preprocessing.LabelEncoder()
        le = le.fit(dataDF[feature])
        dataDF[feature] = le.transform(dataDF[feature])
        
        return dataDF
    
titanic_df = encode_features(titanic_df)
titanic_df.head()
```

![image](https://user-images.githubusercontent.com/76269316/123551956-00f16200-d7af-11eb-94d9-80d87b0389fe.png)

Sex, Cabin, Embarked 속성이 숫자형으로 바뀌었습니다.



##### 5. 모델 학습

1 ~ 4에서 했던 과정들을 transform_features() 함수로 만들어 원본 데이터를 한 번에 가공한 다음, 모델을 학습시켜보도록 하겠습니다.



##### transform_features()

```python
#Null 처리 함수
def fillna(df):
    df['Age'].fillna(titanic_df['Age'].mean(), inplace=True)
    df['Cabin'].fillna('N', inplace=True)
    df['Embarked'].fillna('N', inplace=True)
    return df

#머신러닝 알고리즘에 불필요한 속성 제거
def drop_features(df):
    df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)
    return df

#레이블 인코딩 수행
def format_features(df):
    df['Cabin'] = df['Cabin'].str[:1]
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(df[feature])
        df[feature] = le.transform(df[feature])
    return df

def transform_features(df):
    df = fillna(df)
    df = drop_features(df)
    df = format_features(df)
    return df
```

```python
import pandas as pd
import sklearn
from sklearn.preprocessing import LabelEncoder

#원본 데이터를 재로딩하고, 피처 데이터 세트와 레이블 데이터 세트 추출
titanic_df = pd.read_csv('./titanic_train.csv')
y_titanic_df = titanic_df['Survived']
X_titanic_df = titanic_df.drop('Survived', axis=1)

#transform_features()를 적용해 데이터 가공
X_train_df = transform_features(X_titanic_df)
```



##### 5-1. 결정 트리, 랜덤 포레스트, 로지스틱 회귀를 이용해 타이타닉 생존자 예측

```python
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df, test_size=0.2, random_state=11)

#결정트리, Random Forest, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성
dt_clf = DecisionTreeClassifier(random_state=11)
rf_clf = RandomForestClassifier(random_state=11)
lr_clf = LogisticRegression()

#DecisionTreeClassifier 학습/예측/평가
dt_clf.fit(X_train , y_train)
dt_pred = dt_clf.predict(X_test)
print('DecisionTreeClassifier 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))

#RandomForestClassifier 학습/예측/평가
rf_clf.fit(X_train , y_train)
rf_pred = rf_clf.predict(X_test)
print('RandomForestClassifier 정확도:{0:.4f}'.format(accuracy_score(y_test, rf_pred)))

#LogisticRegression 학습/예측/평가
lr_clf.fit(X_train , y_train)
lr_pred = lr_clf.predict(X_test)
print('LogisticRegression 정확도: {0:.4f}'.format(accuracy_score(y_test, lr_pred)))
```

![image](https://user-images.githubusercontent.com/76269316/123552587-c9d08000-d7b1-11eb-9cf0-fe50af273e16.png)



3개의 알고리즘 중 LogisticRegression이 높은 정확도로 나왔는데, 아직 최적화 작업을 수행하지 않았고, 데이터 양도 충분하지 않기 때문에 어떤 알고리즘이 성능이 가장 좋다고 평가할 수 없습니다.



##### 5-2. 교차 검증

- KFold 교차 검증



**transform_features()**

```python
#Null 처리 함수
def fillna(df):
    df['Age'].fillna(titanic_df['Age'].mean(), inplace=True)
    df['Cabin'].fillna('N', inplace=True)
    df['Embarked'].fillna('N', inplace=True)
    return df

#머신러닝 알고리즘에 불필요한 속성 제거
def drop_features(df):
    df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)
    return df

#레이블 인코딩 수행
def format_features(df):
    df['Cabin'] = df['Cabin'].str[:1]
    features = ['Cabin', 'Sex', 'Embarked']
    for feature in features:
        le = LabelEncoder()
        le = le.fit(df[feature])
        df[feature] = le.transform(df[feature])
    return df

def transform_features(df):
    df = fillna(df)
    df = drop_features(df)
    df = format_features(df)
    return df
```

```python
import numpy as np
import pandas as pd
import sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold

titanic_df = pd.read_csv('titanic_train.csv')

#원본 데이터를 재로딩하고, 피처 데이터 세트와 레이블 데이터 세트 추출
titanic_df = pd.read_csv('./titanic_train.csv')
y_titanic_df = titanic_df['Survived']
X_titanic_df = titanic_df.drop('Survived', axis=1)

#transform_features()를 적용해 데이터 가공
X_train_df = transform_features(X_titanic_df)

#결정트리 클래스 생성
dt_clf = DecisionTreeClassifier(random_state=11)

def exec_kfold(clf, folds=5):
    # 폴드 세트를 5개인 KFold객체를 생성, 폴드 수만큼 예측결과 저장을 위한  리스트 객체 생성
    kfold = KFold(n_splits=folds)
    scores = []

    # KFold 교차 검증 수행. 
    for iter_count , (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):
        # X_titanic_df 데이터에서 교차 검증별로 학습과 검증 데이터를 가리키는 index 생성
        X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]
        y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]

        # Classifier 학습, 예측, 정확도 계산 
        clf.fit(X_train, y_train) 
        predictions = clf.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
        scores.append(accuracy)
        print("교차 검증 {0} 정확도: {1:.4f}".format(iter_count, accuracy))     

    # 5개 fold에서의 평균 정확도 계산. 
    mean_score = np.mean(scores)
    print("평균 정확도: {0:.4f}".format(mean_score)) 
    
# exec_kfold 호출
exec_kfold(dt_clf , folds=5) 
```

![image](https://user-images.githubusercontent.com/76269316/123563726-3d45b200-d7f1-11eb-9379-9e35c5c20ebb.png)

평균 정확도가 약 78.23%가 나왔습니다.



- cross_val_score()

이번에는 cross_val_score()를 이용해 교차 검증 해보겠습니다.

```python
import pandas as pd
import sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

titanic_df = pd.read_csv('titanic_train.csv')

#원본 데이터를 재로딩하고, 피처 데이터 세트와 레이블 데이터 세트 추출
titanic_df = pd.read_csv('./titanic_train.csv')
y_titanic_df = titanic_df['Survived']
X_titanic_df = titanic_df.drop('Survived', axis=1)

#transform_features()를 적용해 데이터 가공
X_train_df = transform_features(X_titanic_df)

#결정트리 클래스 생성
dt_clf = DecisionTreeClassifier(random_state=11)

scores = cross_val_score(dt_clf, X_titanic_df , y_titanic_df , cv=5)
for iter_count,accuracy in enumerate(scores):
    print("교차 검증 {0} 정확도: {1:.4f}".format(iter_count, accuracy))
    
print("평균 정확도: {0:.4f}".format(np.mean(scores)))
```

![image](https://user-images.githubusercontent.com/76269316/123563791-97df0e00-d7f1-11eb-8ba9-3adfa2e63373.png)

cross_val_score()와 K 폴드의 평균 정확도가 약간 다른데, 이는 cross_val_score가 StratifiedKFold를 이용해 폴드 세트를 분할하기 때문입니다.



- GridSearchCV

마지막으로 GridSearchCV를 이용해 DecisionTreeClassifier의 최적 하이퍼 파라미터를 찾고 예측 성능을 측정해보겠습니다.

```python
import pandas as pd
import sklearn
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV

#원본 데이터를 재로딩하고, 피처 데이터 세트와 레이블 데이터 세트 추출
titanic_df = pd.read_csv('./titanic_train.csv')
y_titanic_df = titanic_df['Survived']
X_titanic_df = titanic_df.drop('Survived', axis=1)

#transform_features()를 적용해 데이터 가공
X_train_df = transform_features(X_titanic_df)

#테스트 데이터 세트 20% 추출
X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df, test_size=0.2, random_state=11)

#결정트리 클래스 생성
dt_clf = DecisionTreeClassifier(random_state=11)

parameters = {'max_depth':[2, 3, 5, 10], 'min_samples_split':[2, 3, 5], 'min_samples_leaf': [1, 5, 8]}

grid_dclf = GridSearchCV(dt_clf, param_grid=parameters, scoring='accuracy', cv=5)
grid_dclf.fit(X_train, y_train)

print('GridSearchCV 최적 하이퍼 파라미터: ',grid_dclf.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dclf.best_score_))
best_dclf = grid_dclf.best_estimator_

#GridSearchCV 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행
dpredictions = best_dclf.predict(X_test)
accuracy = accuracy_score(y_test, dpredictions)
print("테스트 세트에서의 DecisionTreeClassifier 정확도: {0:.4f}".format(accuracy))
```

![image](https://user-images.githubusercontent.com/76269316/123564252-a8908380-d7f3-11eb-9110-7378af8b3ec8.png)



최적화된 하이퍼 파라미터 (max_depth=3, min_samples_leaf=1, min_samples_split=2)로 DecisionTreeClassifier로 학습 시킨뒤 예측 정확도가 약 87.15%로 튜닝 됐습니다.

이전 K 폴드, cross_val_score 교차 검증 결과보다 8% 이상 증가했는데, 테스트용 데이터 세트가 작기 때문에 많이 증가한 것이지 이 정도 수준으로 증가하기는 매우 어렵습니다.
