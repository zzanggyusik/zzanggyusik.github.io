---
title:  "4.Classification"
excerpt: "분류"
toc: true
toc_label: "Classification"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - 평가
  - 파이썬 머신러닝 완벽 가이드
last_modified_at: 2021-07-18


---



>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.



### 분류의 개요

지도학습은 명시적인 정답(label)이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식입니다.

지도학습의 대표적인 유형인 분류(Classification)는 학습 데이터로 주어진 데이터의 피처와 레이블값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 

이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것입니다.

**즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 학습한 다음 새롭게 관측된 데이터에 대한 레이블을 판별하는 것입니다.**



분류는 다양한 머신러닝 알고리즘으로 구현할 수 있습니다.

- 베이즈(Bayes) 통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)
- 독립변수와 종속변수의 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression)
- 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)
- 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine)
- 근접 거리를 기준으로 하는 최소 근접(Nearest Neighbor) 알고리즘
- 심층 연결 기반의 신경망 (Neural Network)
- 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)



### 결정 트리(Decision Tree)

결정 트리는 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 알고리즘입니다.

데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우합니다.



아래 그림은 결정 트리의 구조를 간략하게 나타낸 것입니다.

규칙 노드(Decision Node)로 표시된 노드는 규칙 조건이 되고, 새로운 규칙 조건마다 서브 트리(Sub Tree)가 생성됩니다.

리프 노드(Leaf Node)로 표시된 노드는 결정된 클래스 값입니다.

![image](https://user-images.githubusercontent.com/76269316/126070248-adec61f6-9f8e-44cb-a288-9a5abb278598.png)



데이터 세트에는 피처가 있고 피처가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어집니다.

하지만 많은 규칙이 있다는 건 분류를 결정하는 방식이 복잡해진다는 것이고, 이는 과적합으로 이어지게 됩니다.

즉, 트리의 깊이(depth)가 깊어질수록 결정 트리의 예측 성능이 저하되게 됩니다.

따라서 가능한 한 적은 결정 노드로 높은 예측 정확도를 가져야 하는데 이를 위해서는, 최대한 균일한 데이터 세트를 구성할 수 있도록  분할(Split)하는 것이 필요합니다.



균일한 데이터 세트가 갖는 의미에 대해 알아보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126084685-9a654ae8-37ed-4f7b-8a8c-06028853d29e.png)



다음 데이터 세트를 균일한 순서로 나열하면 C → B → A 입니다.

C의 경우 모두 검은 공으로 구성되므로 데이터가 모두 균일하고,

B의 경우는 일부 하얀 공을 갖고 있지만, 대부분 검은 공으로 구성되어 다음으로 균일도가 높습니다.

A의 경우는 검은 공 못지않게 많은 하얀 공을 갖고 있어 균일도가 낮습니다.



눈을 가린 채 데이터 세트 C에서 하나의 데이터를 뽑았을 때 데이터에 대한 별다른 정보 없이도 검은 공이라고 쉽게 예측할 수 있는 반면,

A의 경우 상대적으로 혼잡도가 높고 균일도가 낮기 때문에 같은 조건에서 데이터를 판단하는데 있어 많은 정보가 필요합니다.



결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만듧니다.

즉, 정보 균일도가 데이터 세트로 쪼개질 수 있는 조건을 찾아 서브 데이터 세트를 만들고, 

다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트로 쪼개는 방식으로 내려가면서 반복하는 방식으로 데이터 값을 예측합니다.



예를 들어 박스 안에 30개의 레고 블록이 있을 때, 각 레고 블록이 갖는 속성은 다음과 같습니다. 

- 형태 : 동그라미, 네모, 세모

- 색깔 : 노랑, 빨강, 파랑



이 중 노랑색 블록은 모두 동그라미이고 빨강과 파랑 블록의 경우 동그라미, 네모, 세모가 골고루 섞여있다고 하면

각 레고 블록을 형태와 색깔 속성으로 분류하고자 할 때 가장 첫 번째로 만들어지는 규칙은 **if 색깔 == '노란색'**입니다.

왜냐하면 노란색 블록이면 모두 노란 동그라미 블록으로 예측할 수 있고, 그 다음 나머지 블록에 대해 다시 균일도 조건을 찾아 분류하는 것이 가장 효율적이기 때문입니다.



정보 균일도를 측정하는 방법으로 엔트로피를 이용한 정보 이득(Information Gain) 지수와 지니 계수가 있습니다.

- 정보 이득 : 엔트로피(주어진 데이터 집합의 혼잡도) 개념을 기반으로 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮게 되는데 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값입니다.
  결정 트리는 정보 이득 지수가 높은 속성을 기준으로 분할합니다.
- 지니 계수 : 경제학에서 불평등 지수를 나타낼 때 사용하는 계수로 0이 가장 평등하고 1로 갈수록 불평등합니다.
  머신러닝에 적용될 때는 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할합니다.



결정 트리 알고리즘을 사이킷런에서 구현한 DecisionTreeClassifier에서는 지니 계수를 이용해 데이터 세트를 분할합니다.

일반적인 결정 트리 알고리즘은 데이터 세트를 분할하는데 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정합니다.

![image](https://user-images.githubusercontent.com/76269316/126085195-1a8d5692-a2be-4e23-af48-965e10d5e980.png)





##### 결정 트리 모델의 특징

결정 트리의 장점은 정보의 '균일도'라는 룰을 기반으로 하고 있어 알고리즘이 쉽고 직관적이라는 것입니다.

룰이 매우 명확하고, 이에 기반해 어떻게 규칙 노드와 리프 노드가 만들어지는지 알 수 있고 시각화까지 할 수 있습니다.

또한 정보 균일도만 신경 쓰면 되므로 특별한 경우를 제외하고는 각 피처의 스케일링과 정규화 같은 전처리 작업이 필요 없습니다.



결정 트리의 단점은 과적합으로 정확도가 떨어진다는 것입니다.

학습 데이터 기반 모델의 정확도를 높이기 위해 모든 데이터 상황을 만족하는 완벽한 규칙을 만들려고 하게되고(그럴 수 없음에도 불구하고) 결국, 트리의 깊이가 깊어지고 트리가 복잡해져서 예측 성능이 떨어지게 됩니다.
