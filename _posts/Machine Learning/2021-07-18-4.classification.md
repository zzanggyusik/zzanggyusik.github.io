---
title:  "4.Classification"
excerpt: "분류"
toc: true
toc_label: "Classification"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - 평가
  - 파이썬 머신러닝 완벽 가이드
last_modified_at: 2021-07-18

---

>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.



### 분류의 개요

지도학습은 명시적인 정답(label)이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식입니다.

지도학습의 대표적인 유형인 분류(Classification)는 학습 데이터로 주어진 데이터의 피처와 레이블값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 

이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것입니다.

**즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 학습한 다음 새롭게 관측된 데이터에 대한 레이블을 판별하는 것입니다.**



분류는 다양한 머신러닝 알고리즘으로 구현할 수 있습니다.

- 베이즈(Bayes) 통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)
- 독립변수와 종속변수의 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression)
- 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)
- 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine)
- 근접 거리를 기준으로 하는 최소 근접(Nearest Neighbor) 알고리즘
- 심층 연결 기반의 신경망 (Neural Network)
- 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)



### 결정 트리(Decision Tree)

결정 트리는 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 알고리즘입니다.

데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우합니다.



아래 그림은 결정 트리의 구조를 간략하게 나타낸 것입니다.

규칙 노드(Decision Node)로 표시된 노드는 규칙 조건이 되고, 새로운 규칙 조건마다 서브 트리(Sub Tree)가 생성됩니다.

리프 노드(Leaf Node)로 표시된 노드는 결정된 클래스 값입니다.

![image](https://user-images.githubusercontent.com/76269316/126070248-adec61f6-9f8e-44cb-a288-9a5abb278598.png)



데이터 세트에는 피처가 있고 피처가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어집니다.

하지만 많은 규칙이 있다는 건 분류를 결정하는 방식이 복잡해진다는 것이고, 이는 과적합으로 이어지게 됩니다.

즉, 트리의 깊이(depth)가 깊어질수록 결정 트리의 예측 성능이 저하되게 됩니다.

따라서 가능한 한 적은 결정 노드로 높은 예측 정확도를 가져야 하는데 이를 위해서는, 최대한 균일한 데이터 세트를 구성할 수 있도록  분할(Split)하는 것이 필요합니다.



균일한 데이터 세트가 갖는 의미에 대해 알아보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126084685-9a654ae8-37ed-4f7b-8a8c-06028853d29e.png)



다음 데이터 세트를 균일한 순서로 나열하면 C → B → A 입니다.

C의 경우 모두 검은 공으로 구성되므로 데이터가 모두 균일하고,

B의 경우는 일부 하얀 공을 갖고 있지만, 대부분 검은 공으로 구성되어 다음으로 균일도가 높습니다.

A의 경우는 검은 공 못지않게 많은 하얀 공을 갖고 있어 균일도가 낮습니다.



눈을 가린 채 데이터 세트 C에서 하나의 데이터를 뽑았을 때 데이터에 대한 별다른 정보 없이도 검은 공이라고 쉽게 예측할 수 있는 반면,

A의 경우 상대적으로 혼잡도가 높고 균일도가 낮기 때문에 같은 조건에서 데이터를 판단하는데 있어 많은 정보가 필요합니다.



결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만듧니다.

즉, 정보 균일도가 데이터 세트로 쪼개질 수 있는 조건을 찾아 서브 데이터 세트를 만들고, 

다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트로 쪼개는 방식으로 내려가면서 반복하는 방식으로 데이터 값을 예측합니다.



예를 들어 박스 안에 30개의 레고 블록이 있을 때, 각 레고 블록이 갖는 속성은 다음과 같습니다. 

- 형태 : 동그라미, 네모, 세모

- 색깔 : 노랑, 빨강, 파랑



이 중 노랑색 블록은 모두 동그라미이고 빨강과 파랑 블록의 경우 동그라미, 네모, 세모가 골고루 섞여있다고 하면

각 레고 블록을 형태와 색깔 속성으로 분류하고자 할 때 가장 첫 번째로 만들어지는 규칙은 **if 색깔 == '노란색'**입니다.

왜냐하면 노란색 블록이면 모두 노란 동그라미 블록으로 예측할 수 있고, 그 다음 나머지 블록에 대해 다시 균일도 조건을 찾아 분류하는 것이 가장 효율적이기 때문입니다.



정보 균일도를 측정하는 방법으로 엔트로피를 이용한 정보 이득(Information Gain) 지수와 지니 계수가 있습니다.

- 정보 이득 : 엔트로피(주어진 데이터 집합의 혼잡도) 개념을 기반으로 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮게 되는데 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값입니다.
  결정 트리는 정보 이득 지수가 높은 속성을 기준으로 분할합니다.
- 지니 계수 : 경제학에서 불평등 지수를 나타낼 때 사용하는 계수로 0이 가장 평등하고 1로 갈수록 불평등합니다.
  머신러닝에 적용될 때는 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할합니다.



결정 트리 알고리즘을 사이킷런에서 구현한 DecisionTreeClassifier에서는 지니 계수를 이용해 데이터 세트를 분할합니다.

일반적인 결정 트리 알고리즘은 데이터 세트를 분할하는데 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정합니다.

![image](https://user-images.githubusercontent.com/76269316/126085195-1a8d5692-a2be-4e23-af48-965e10d5e980.png)





##### 결정 트리 모델의 특징

결정 트리의 장점은 정보의 '균일도'라는 룰을 기반으로 하고 있어 알고리즘이 쉽고 직관적이라는 것입니다.

룰이 매우 명확하고, 이에 기반해 어떻게 규칙 노드와 리프 노드가 만들어지는지 알 수 있고 시각화까지 할 수 있습니다.

또한 정보 균일도만 신경 쓰면 되므로 특별한 경우를 제외하고는 각 피처의 스케일링과 정규화 같은 전처리 작업이 필요 없습니다.



결정 트리의 단점은 과적합으로 정확도가 떨어진다는 것입니다.

학습 데이터 기반 모델의 정확도를 높이기 위해 모든 데이터 상황을 만족하는 완벽한 규칙을 만들려고 하게되고(그럴 수 없음에도 불구하고) 결국, 트리의 깊이가 깊어지고 트리가 복잡해져서 예측 성능이 떨어지게 됩니다.



따라서 모든 데이터 상황을 만족하는 완벽한 규칙은 만들 수 없다고 인정하고 트리의 크기를 사전에 제한하는 것이 성능 향상에 도움이 됩니다.

|                             장점                             |                             단점                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| - 직관적이다.<br />- 피처의 스케일리이나 정규화 등의 사전 가공 영향도가 크지 않다. | - 과적합으로 알고리즘 성능이 떨어진다.<br />이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝이 필요 |



##### 결정 트리 파라미터

사이킷런 결정 트리 알고리즘은 **CART(Classification And Regression Trees)** 기반으로 구현한 **DecisionTreeClassifier**와 **DecisionTreeRegressor** 클래스를 제공합니다. (CART : 분류뿐만 아니라 회귀에서도 사용될 수 있는 트리 알고리즘)

DecisionTreeClassifier는 분류를 위한 클래스이고, DecisionTreeRegressor는 회귀를 위한 클래스입니다.

|    파라미터명     |                             설명                             |
| :---------------: | :----------------------------------------------------------: |
| min_samples_split | 노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는데 사용됨.<br />디폴트는 2이고 작게 설정할수록 분할되는 노드가 많아져 과적합 가능성 증가 |
| min_samples_leaf  | 말단 노드(Leaf)가 되기 위한 최소한의 샘플 데이터 수<br />min_samples_split과 유사하게 과적합 제어 용도로 사용됨.<br />비대칭적(imbalanced) 데이터의 경우 특정 클래스 데이터가 극도로 작을 수 있으므로 작게 설정 필요 |
|   max_features    | 최적의 분할을 위해 고려할 최대 피처 개수. 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할 수행.<br />int 형으로 지정하면 대상 피처의 개수, float 형으로 지정하면 전체 피처 중 대상 피처의 퍼센트<br />sqrt는 <img src="https://user-images.githubusercontent.com/76269316/126085844-bc299bcc-3f39-4a67-845d-7d39a9f98854.png" alt="image" style="zoom: 50%;" />만큼 선정<br />auto로 지정하면 sqrt와 동일<br />log는 전체 피처 중 <img src="https://user-images.githubusercontent.com/76269316/126085922-9abdd74f-e044-4079-8be5-42eea3643b88.png" alt="image" style="zoom: 50%;" />만큼 선정<br />None은 전체 피처 선정 |
|     max_depth     | 트리의 최대 높이를 규정<br />디폴트는 None 완벽하게 클래스 결정 값이 결정 될 때까지 깊이를 계속 키우며 분할하거나 노드가 갖는 데이터 개수가 min_samples_split보다 작아질 때까지 계속 깊이를 증가시킴.<br />깊이가 깊어지면 min_samples_split 설정대로 최대 분할하여 과적합할 수 있으므로 적절한 값으로 제어 필요. |
|  max_leaf_nodes   |                 리프 노드(Leaf)의 최대 개수                  |



##### 결정 트리 모델의 시각화

Graphviz 패키지를 사용하면 결정 트리 알고리즘이 어떤 규칙을 갖고 트리를 생성하는지 시각적으로 확인할 수 있습니다.

Graphviz는 원래 그래프 기반 dot 파일로 기술된 다양한 이미지를 쉽게 시각화하는 패키지인데,  사이킷런에서 쉽게 인터페이스 할 수 있도록 export_graphviz() API를 제공합니다.



먼저 Graphviz를 윈도우에 설치하겠습니다. 

Graphviz는 C/C++로 운영체제에 포팅된 패키지여서 파이썬과 인터페이스 할 수 있는 파이썬 래퍼(Wrapper) 모듈을 별도로 설치해야합니다.

① [Graphviz](https://graphviz.org/download/)를 설치합니다.
![image](https://user-images.githubusercontent.com/76269316/126086257-ae733f38-5506-45eb-ab96-58bfa08bce31.png)

저는 윈도우10 64bit이기 때문에 빨간색 파일을 다운 받아 설치했습니다.



![image](https://user-images.githubusercontent.com/76269316/126086295-766a4b40-6fe6-4c3c-921c-b881a3a494c8.png)

저는 윈도우, 게임만 C드라이브에 깔고 나머지는 D드라이브에 설치하기  때문에 D드라이브에 설치했습니다.



② 파이썬 래퍼 모듈을 PIP를 이용해 설치합니다.

Anaconda 콘솔이나 OS command 콘솔에서 pip install graphviz 명령어로 설치합니다. (콘솔 실행시 관리자 권한으로 실행)

![image](https://user-images.githubusercontent.com/76269316/126086404-4a67074c-78a4-486f-88dd-72eb8264171d.png)

![image](https://user-images.githubusercontent.com/76269316/126086433-36a703cd-7191-43a6-8992-1f81aac9a7c2.png)



③ Graphviz와 파이썬 래퍼를 연결하기 위해 환경 변수 설정을 해야합니다.

내 PC → 속성 → 고급 시스템 설정 → 환경변수 클릭



먼저 사용자 변수의 Path 변수 → 편집 클릭

![image](https://user-images.githubusercontent.com/76269316/126086545-7f4bedc4-ffda-417c-9f61-d5c3f25a0c16.png)

D:\Program Files\Graphviz\bin 추가

![image](https://user-images.githubusercontent.com/76269316/126086580-c972ff26-0a95-4ff9-87f5-fb563708ae25.png)



시스템 변수의 Path 변수 → 편집 클릭

![image](https://user-images.githubusercontent.com/76269316/126086605-2c81bdd6-7efc-40f9-9eff-5d28e928093b.png)

D:\Program Files\Graphviz\bin\dot.exe 추가

![image](https://user-images.githubusercontent.com/76269316/126086623-1a6a4f67-cb88-46c0-a89c-4346845ff3c1.png)

이후 주피터 노트북을 재시작합니다. (환경 변수 Path를 재로딩하기 위해)



Graphviz를 이용해 붓꽃 데이터 세트에 결정 트리를 적용할 때 어떻게 서브 트리가 생성되고 만들어지는지 시각화해 보겠습니다.

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

#DecisionTreeClassifier 생성
dt_clf = DecisionTreeClassifier(random_state=156)

#붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train, y_train)

#export_graphviz() 호출 결과로 out_file로 지정된 tree.dot 파일 생성
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)  #impurity : gini 계수 출력 여부, filled : 노드 색깔을 다르게 출력
```

```python
import graphviz

#위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북에서 시각화
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

![image](https://user-images.githubusercontent.com/76269316/126087894-7f8ef93b-ebe6-4a3f-b50a-6ce8a2e9c467.png)

출력 결과를 보면, 각 규칙에 따라 트리의 브랜치(branch) 노드와 리프(leaf) 노드가 어떻게 구성돼 있는지 한눈에 확인할 수 있게 시각화 돼 있습니다.

각 노드의 색깔은 붓꽃 데이터의 레이블 값을 의미하고 (주황색 0 : Setosa, 초록색 1 : Versicolor, 보라색 2 : Virginica) 색이 짙어질수록 지니 계수가 낮습니다. (해당 레이블에 속하는 샘플 데이터가 많다는 의미)



노드 내 지표의 의미는 다음과 같습니다.

- petal length(cm) <= 2.45 : 자식 노드를 만들기 위한 규칙 조건입니다. (조건이 없으면 리프 노드)
- gini : 다음 value=[]로 주어진 데이터 분포에서의 지니 계수
- samples : 현 규칙에 해당하는 데이터 건수
- value : 클래스 값 기반의 데이터 건수
  붓꽃 데이터 세트는 클래스 값으로 0(Setosa), 1(Versicolor), 2(Virginica) 를 갖고 있는데 value = [41, 40, 39]라면 Setosa 41개, Versicolor 40개, Virginica 39개로 데이터가 구성돼 있다는 의미입니다.



맨 윗부분부터 자세히 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126088306-547d6007-7543-45f0-ad96-c9b8c2d4d42a.png)

먼저 루트 노드인 1번 노드의 지표입니다.

**1번 노드 지표**

- samples = 120 : 전체 데이터가 120개라는 의미
- value = [41, 40, 39] : Setosa 41개, Versicolor 40개, Virginica 39개로 데이터가 구성돼 있다는 의미
- gini = 0.667 : sample 120개가 value = [41, 40, 39] 분포도로 돼 있으므로 지니 계수는 0.667
- petal length(cm) <= 2.45 규칙으로 자식 노드 생성
- class = setosa : 하위 노드를 가질 경우 setosa 개수가 41개로 가장 많다는 의미



petal legnth(cm) <= 2.45 규칙으로 True인 데이터가 2번 노드, False인 데이터가 3번 노드가 만들어집니다.

2번 노드는 petal length(cm) <= 2.45가 True인 규칙 노드로 모든 데이터가 Setosa로 결정돼 리프 노드가 되고, 2번 노드에서는 더 이상 규칙이 생성되지 않습니다.

**2번 노드 지표**

- value = [41, 0, 0] : 샘플 데이터 모두 Setosa
- gini = 0.0



3번 노드는 petal length(cm) <= 2.45가 False인 규칙 노드로 지표는 다음과 같습니다.

**3번 노드 지표**

- value = [0, 40, 39] : 79개 샘플 데이터 중 Versicolor 40개, Virginica 39개
- gini = 0.5로 여전히 높으므로 다음 자식 노드 생성
- petal width(cm) <= 1.55 규칙으로 자식 노드 생성



4번 노드는 petal width(cm) <= 1.55가 True인 규칙 노드로 지표는 다음과 같습니다.

**4번 노드 지표 설명**

- value = [0, 37, 1] : 38개의 샘플 데이터 중 Versicolor 37개, Virginica 1개로 대부분 Versicolor
- gini = 0.051로 매우 낮으나 여전히 Versicolor와 Virginica가 혼재돼 있으므로 petal length(cm) <= 5.25라는 규칙으로 자식 노드 생성



5번 노드는 petal width(cm) <= 1.55가 False인 규칙 노드로 지표는 다음과 같습니다.

**5번 노드 지표**

- value = [0, 3, 38] : 41개의 샘플 데이터 중 Versicolor 3개, Virginica 38개로 대부분 Virginica
- gini = 0.136으로 낮으나 여전히 Versicolor와 Virginica가 혼재돼 있으므로 petal length(cm) <= 1.75라는 규칙으로 자식 노드 생성

 

4번 노드를 보면 Virginica가 1개 밖에 없지만 이 1개를 완벽히 분류하기 위해 또 다른 자식 노드를 생성하는 것을 볼 수 있습니다.

이로 인해 결국 매우 복잡한 규칙 트리가 만들어져 과적합 문제가 발생합니다.



때문에 [결정 트리 하이퍼 파라미터](https://seominseok4834.github.io/machine%20learning/4.classification/#%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0) 대부분이 복잡한 트리가 생성되는 것을 막기 위한 용도로 사용됩니다.



결정 트리의 하이퍼 파라미터 변경에 따른 트리의 변화를 살펴보겠습니다.

**max_depth**

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
import graphviz
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

#DecisionTreeClassifier 생성 (max_depth=3)
dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)

#붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train, y_train)

#export_graphviz() 호출 결과로 out_file로 지정된 tree.dot 파일 생성
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)  #impurity : gini 계수 출력 여부, filled : 노드 색깔을 다르게 출력

#위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북에서 시각화
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

|                     max_depth 제약 없음                      |                        max_depth = 3                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/126087894-7f8ef93b-ebe6-4a3f-b50a-6ce8a2e9c467.png) | ![image](https://user-images.githubusercontent.com/76269316/126089242-dd31136e-63b5-45ac-9476-4e92934255b3.png) |

트리의 깊이가 설정된 max_depth에 따라 줄어들어 더 간단한 결정 트리가 생성됐습니다.



**min_samples_split**

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
import graphviz
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

#DecisionTreeClassifier 생성 (min_samples_split=4)
dt_clf = DecisionTreeClassifier(random_state=156, min_samples_split=4)

#붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train, y_train)

#export_graphviz() 호출 결과로 out_file로 지정된 tree.dot 파일 생성
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)  #impurity : gini 계수 출력 여부, filled : 노드 색깔을 다르게 출력

#위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북에서 시각화
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

![image](https://user-images.githubusercontent.com/76269316/126089748-f8baec5e-8758-48d7-a8b8-f9837263434f.png)

빨간색 네모 박스를 보면, 샘플 데이터 3개 중 1개가 다른 클래스 값이 들어가 있지만, min_samples_split=4로 지정(샘플 데이터가 최소 4개 이상 있어야지만 자식 노드 생성)해놨기 때문에 자식 노드를 생성하지 않는 것을 볼 수 있습니다.



**min_samples_leaf**

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
import graphviz
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

#DecisionTreeClassifier 생성 (min_samples_leaf=4)
dt_clf = DecisionTreeClassifier(random_state=156, min_samples_leaf=4)

#붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train, y_train)

#export_graphviz() 호출 결과로 out_file로 지정된 tree.dot 파일 생성
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)  #impurity : gini 계수 출력 여부, filled : 노드 색깔을 다르게 출력

#위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북에서 시각화
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

![image](https://user-images.githubusercontent.com/76269316/126090410-7a83d930-9861-4e76-bf82-d4a07108e496.png)

min_samples_leaf는 리프 노드가 될 수 있는 샘플 데이터 건수의 최솟값을 지정합니다. (default는 1)

위 코드에서는 min_samples_leaf를 4로 지정했기 때문에 지니 계수 값이 크더라도 (다른 클래스 값이 섞여 있더라도) 샘플 데이터가 4개 이하인 경우 리프 노드가 되게 되어 결정 트리가 간결해지게 됩니다.



결정 트리 알고리즘이 학습을 통해 규칙을 정하는데 있어 피처의 중요한 역할 지표를 feature_importances_ 속성으로 확인할 수 있습니다.

feature_importances_는 ndarray 형태로 피처 중요도가 반환됩니다.

```python
import seaborn as sns
import numpy as np
%matplotlib inline

#feature importance 추출
print("Feature importances:\n{0}".format(np.round(dt_clf.feature_importances_, 3)))

#feature별 importance 매핑
for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_):
    print('{0} : {1:.3f}'.format(name, value))
    
#feautre importance를 column 별로 시각화하기
sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names)
```

![image](https://user-images.githubusercontent.com/76269316/126091213-0fcda50c-6954-4bf2-b9c7-bc961bea4864.png)

petal_length가 피처 중요도가 가장 높은 것을 볼 수 있습니다.



##### 결정 트리 과적합(Overfitting)

결정 트리의 과적합 문제를 시각화해 알아보겠습니다.



분류를 위해 2개의 피처, 3개의 클래스 값을 갖는 임의의 데이터 세트를 생성했습니다.

```python
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
%matplotlib inline

plt.title("3 Class values with 2 Features Sample data creation")

#2차원 시각화를 위해 피처는 2개, 3가지 유형의 클래스 분류 샘플 데이터 생성
X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1, random_state=0)

#그래프 형태로 2개의 피처로 2차원 좌표 시각화 (각 클래스 값은 다른 색깔로 표시됨)
plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, edgecolor='k')  #marker : marker style, c : color for marker, s : marker size, edgecolor : marker border color
```



```python
X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1, random_state=0)
```

make_classification() 호출 시 피처 데이터 세트와 클래스 레이블 데이터 세트를 반환하기 때문에 이를 각각 X_features, y_labels로 전달 받았고,

파라미터는 다음을 의미합니다.

- n_features : 독립 변수의 수 (전체 피처의 수)
- n_redundant : 독립 변수 중 다른 독립 변수의 선형 조합으로 나타나는 성분의 수
- n_informative : 독립 변수 중 종속 변수와 상관관계가 있는 성분의 수
- n_classes : 종속 변수의 클래스 수
- n_clusters_per_class : 클래스당 클러스터 수

책에는 파라미터에 대한 언급이 없는데, 그냥 2개의 피처 3개의 클래스 값을 갖는 데이터 세트를 생성하는 코드구나하고 넘어가시면 될 것 같습니다.

![image](https://user-images.githubusercontent.com/76269316/126092088-8017a03a-5330-4a7d-a7df-8f7dbdd718a6.png)



X_features와 y_label 데이터 세트를 기반으로 별다른 제약 없이(하이퍼 파라미터를 디폴트로) 결정트리를 학습한 뒤, 결정 트리 모델이 어떠한 결정 기준을 갖고 분할하면서 데이터를 분류하는지 색상과 경계로 나타내 확인해 보겠습니다.

이를 위해 visualize_boundary 메소드를 사용했습니다.

**visualize_boundary()**

```python
import numpy as np

# Classifier의 Decision Boundary를 시각화 하는 함수
def visualize_boundary(model, X, y):
    fig,ax = plt.subplots()
    
    # 학습 데이타 scatter plot으로 나타내기
    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',
               clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim_start , xlim_end = ax.get_xlim()
    ylim_start , ylim_end = ax.get_ylim()
    
    # 호출 파라미터로 들어온 training 데이타로 model 학습 . 
    model.fit(X, y)
    # meshgrid 형태인 모든 좌표값으로 예측 수행. 
    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    
    # contourf() 를 이용하여 class boundary 를 visualization 수행. 
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3,
                           levels=np.arange(n_classes + 1) - 0.5,
                           cmap='rainbow', clim=(y.min(), y.max()),
                           zorder=1)
```

```python
from sklearn.tree import DecisionTreeClassifier

#특정한 트리 생성 제약 없이 결정 트리 학습, 결정 경계 시각화
dt_clf = DecisionTreeClassifier().fit(X_features, y_labels)
visualize_boundary(dt_clf, X_features, y_labels)
```

![image](https://user-images.githubusercontent.com/76269316/126092593-5581ccaf-0d02-49e3-b54a-9b605b02b21f.png)

일부 이상치(Outlier) 데이터까지 분류하기 위해 분할이 일어나서 결정 기준 경계가 많아졌습니다.

결정 트리의 기본 하이퍼 파라미터 설정은 리프 노드 안의 데이터가 모두 균일하거나 하나만 존재해야 하는 엄격한 분할 기준을 갖기 때문입니다.

이렇게 복잡한 모델은 학습 데이터 세트의 특성과 약간만 다른 형태의 데이터 세트를 예측하면 예측 정확도가 떨어지게 됩니다.



이번에는 min_samples_leaf = 6으로 설정해 6개 이하의 데이터는 리프 노드가 될 수 있도록 규칙을 완화해 보겠습니다.

```python
from sklearn.tree import DecisionTreeClassifier

#특정한 트리 생성 제약 없이 결정 트리 학습, 결정 경계 시각화
dt_clf = DecisionTreeClassifier(min_samples_leaf=6).fit(X_features, y_labels)
visualize_boundary(dt_clf, X_features, y_labels)
```

<img src="https://user-images.githubusercontent.com/76269316/126092720-a4865621-20d2-4f79-99f0-742356b50ee8.png" alt="image" style="zoom: 200%;" />

이상치에 크게 반응하지 않으면서 좀 더 일반화된 분류 규칙에 따라 분류됐습니다.

테스트 데이터 세트에서의 예측 성능은 min_samples_leaf=6으로 생성한 모델이 더 뛰어나게 됩니다.



##### 결정 트리 실습 - 사용자 행동 인식 데이터 세트

결정 트리를 이용해 UCI Machine Learning Repository에서 제공하는 사용자 행동 인식(Human Activity Recognition) 데이터 세트에 대한 예측 분류를 수행해 보겠습니다 .

해당 데이터는 30명에게 스마트폰 센서를 장착한 다음 사람의 동작과 관련된 여러 피처를 수집한 데이터입니다.



[Human Activity Recognition Using Smartphones Data Set](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/)

위 링크에서 UCI HAR Dataset.zip을 다운받으면 됩니다.

![image](https://user-images.githubusercontent.com/76269316/126093256-1bed4ed1-2143-4fd2-a5fe-dbc90f8f0fba.png)



위 파일은 다운받고 압축을 푼 다음 디렉토리명을 human_activity로 변경합니다.

그런 다음 해당 디렉토리를 jupyter notebook 파일 디렉토리 (C:\Users\사용자명)로 옮깁니다.

저는 챕터별로 관리하고 있어서, C:\Users\MinseokSeo\chapter4에 저장했습니다.

![image](https://user-images.githubusercontent.com/76269316/126093575-5fd5366b-2dff-41a6-b426-dad1397555f7.png)



![image](https://user-images.githubusercontent.com/76269316/126093679-68404672-8fc0-46c1-86f9-83dfb81b63ae.png)

해당 디렉토리에는 다음 파일들이 저장돼 있습니다.

README.txt와 features_info.txt에는 데이터 세트와 피처에 대한 간략한 설명이 적혀 있습니다.

features.txt에는 피처 이름이 기술돼 있습니다. 

activity_labels.txt는 동작 레이블 값에 대한 설명이 있습니다.

train, test 디렉토리에는 학습·테스트 용도의 피처 데이터 세트, 레이블 데이터 세트가 들어있습니다.



features.txt  파일을 DataFrame으로 로딩해 피처명만 확인해 보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126093878-20f592f4-1d20-4e3c-886e-f1421e66ea9f.png)



```python
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

#features.txt 파일을 DataFrame으로 로드
feature_name_df = pd.read_csv('./human_activity/features.txt', sep='\s+', header=None, names=['column_index', 'column_name'])  #sep='\s+' : 한 개 이상의 공백 구분자로 구분

#피처명 index르 ㄹ제거하고, 피처명만 리스트 객체로 생성한 뒤 10개만 출력
feature_name = feature_name_df.iloc[:, 1].values.tolist()
print('전체 피처명에서 10개만 추출:', feature_name[:10])
```

![image](https://user-images.githubusercontent.com/76269316/126094297-88bc5da2-0ecb-4248-9439-5d93d516ddb6.png)



인체의 움직임과 관련된 속성의 평균/표준편차가 X, Y, Z 값으로 돼 있음을 볼 수 있습니다.



중복된 피처명이 있는지 확인해 보겠습니다.

```python
feature_dup_df = feature_name_df.groupby('column_name').count()  #column_name으로 그룹핑한 다음 카운팅
print(feature_dup_df[feature_dup_df['column_index'] > 1].count())  #column_index가 1 이상인 것만 카운팅
feature_dup_df[feature_dup_df['column_index'] > 1].head()  #맨 위 5개만 출력
```

![image](https://user-images.githubusercontent.com/76269316/126095319-9b06fe00-ce27-44ba-b4df-81f836b7bf10.png)

총 42개의 피처명이 중복된 것을 확인할 수 있습니다.

이 중복된 피처명들은 _1, _2를 추가해 중복을 없애겠습니다.



**get_new_feature_name_df(old_feature_name_df)**

```python
def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt'])
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0] + '_' + str(x[1]) if x[1] > 0 else x[0], axis=1)
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df
```



한 줄 한 줄 보면,

```python
feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt'])
```

![image](https://user-images.githubusercontent.com/76269316/126112253-c0414fd2-3768-4893-951e-1b7f3cfb7aff.png)

'column_name'으로 그룹핑 한 다음, 카운팅한 DataFrame의 column을 dup_cnt로 변경합니다.



```python
feature_dup_df = feature_dup_df.reset_index()
```

feature_dup_df의 기존 인덱스(빨간 네모 박스)를 index라는 column으로 추가합니다.

<img src="https://user-images.githubusercontent.com/76269316/126112529-b22a887e-9967-45bb-b578-1f2021f30bd5.png" alt="image" style="zoom:67%;" />



```python
new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
```

|              old_feature_name_df.reset_index()               |                        feature_dup_df                        |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/126112922-e9d0962b-d094-4054-a976-cc22b3a49c5a.png) | ![image](https://user-images.githubusercontent.com/76269316/126112940-43a41ba9-ed67-462e-89b0-78243cb3b4af.png) |

두 DataFrame을 [outerjoin](https://seominseok4834.github.io/database/6.bags-and-extended-relational-algebra/#outerjoins)합니다.

![image](https://user-images.githubusercontent.com/76269316/126113206-127ac687-3c99-4912-bdde-d4843cc4ee97.png)

↑ outerjoin한 결과



```python
new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0] + '_' + str(x[1]) if x[1] > 0 else x[0], axis=1)
```

x[1]\(dup_cnt)가 0보다 큰 경우 (column명이 중복된 경우)에만 _1, _2를 붙이고 그렇지 않은 경우 그대로 사용합니다.



```python
new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
return new_feature_name_df
```

![image](https://user-images.githubusercontent.com/76269316/126113711-9c30a5fc-4531-46d9-975f-23b371888ba3.png)

'index' column을 drop한 뒤, return 합니다.



사용자 행동 인식 데이터 세트를 좀 더 간단하게 사용하기 위해 get_human_dataset()을 생성했습니다.

```python
import pandas as pd

def get_human_dataset():
    
    #features.txt 파일을 DataFrame으로 로드
    feature_name_df = pd.read_csv('./human_activity/features.txt', sep='\s+', header=None, names=['column_index', 'column_name'])  #sep='\s+' : 한 개 이상의 공백 구분자로 구분
    
    #중복된 피처명을 수정, 신규 피처명 DataFrame 생성
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
    
    #DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
    
    #학습 피처와 테스트 피처 데이터를 DataFrame으로 로드 (column명 feature_name 적용)
    X_train = pd.read_csv('./human_activity/train/X_train.txt', sep='\s+', names=feature_name)
    X_test = pd.read_csv('./human_activity/test/X_test.txt', sep='\s+', names=feature_name)
    
    #학습 레이블과 테스트 레이블 데이터를 DataFrame으로 로드 (column명 action으로 적용)
    y_train = pd.read_csv('./human_activity/train/y_train.txt', sep='\s', header=None, names=['action'])
    y_test = pd.read_csv('./human_activity/test/y_test.txt', sep='\s', header=None, names=['action'])
    
    #로드된 학습/테스트용 DataFrame 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()
```



학습용 피처 데이터 세트를 간략히 살펴보면,

```python
print('학습 피처 데이터셋 info()')
print(X_train.info())
```

![image](https://user-images.githubusercontent.com/76269316/126114308-d17506c4-6ac2-43e5-8970-cf4b322b3355.png)

학습 데이터 세트는 7532개의 레코드와 561개의 피처를 갖고 있고, 전부 float형이므로 별도의 카테고리 인코딩을 수행할 필요가 없습니다.



레이블 값은 1, 2, 3, 4, 5, 6의 6개 값이고 특정 값으로 왜곡되지 않고 비교적 고르게 분포돼 있습니다.

```python
print(y_train['action'].value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/126114473-63832797-33f8-4f04-8740-de7dece8e57c.png)



사이킷런 DecisionTreeClassifier를 이용해 동작 예측 분류를 수행해 보겠습니다.

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

#예제 반복 시마다 동일한 예측 결과 도출을 위해 random_state 설정
dt_clf = DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train, y_train)
pred = dt_clf.predict(X_test)
accuracy = accuracy_score(y_test, pred)
print('결정 트리 예측 정확도 : {0:.4f}'.format(accuracy))

#DecisionTreeClassifier의 하이퍼 파라미터 추출
print('DecisionTreeClassifier 기본 하이퍼 파라미터\n', dt_clf.get_params())
```

![image](https://user-images.githubusercontent.com/76269316/126116188-4297f658-a304-4d37-baf8-5e6330642b9d.png)

약 85.48%의 정확도를 갖습니다.



트리 깊이(Tree Depth)가 예측 정확도에 미치는 영향을 보기위해, GridSearchCV를 이용해 max_depth 값을 변화시키면서 예측 성능을 확인해 보겠습니다.

+[GridSearchCV](https://seominseok4834.github.io/machine%20learning/2.scikit-learn-machine-learning-in-python/#gridsearchcv---%EA%B5%90%EC%B0%A8-%EA%B2%80%EC%A6%9D%EA%B3%BC-%EC%B5%9C%EC%A0%81-%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D%EC%9D%84-%ED%95%9C-%EB%B2%88%EC%97%90) : 하이퍼 파라미터를 순차적으로 적용하면서 최적의 파라미터를 찾을 수 있게 해주는 API



```python
from sklearn.model_selection import GridSearchCV

params = {
    'max_depth' : [6, 8, 10, 12, 16, 20, 24]
}

grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1)  #verbose : GridSearchCV iteration마다 수행 결과 메시지 출력
grid_cv.fit(X_train, y_train)
print('GridSearchCV 최고 평균 정확도 수치: {0:.4f}'.format(grid_cv.best_score_))
print('GridSearchCV 최적 하이퍼 파라미터: ', grid_cv.best_params_)
```

![image](https://user-images.githubusercontent.com/76269316/126117392-bef4b39e-9a84-4199-88c8-66d51821d41d.png)

verbose=1로 설정하면 iteration 수행마다 이렇게 수행 결과 메시지가 출력됩니다.

![image](https://user-images.githubusercontent.com/76269316/126117691-9a19c6cc-363a-41a7-8790-5a241e90f870.png)

실행 결과 최고 평균 정확도가 약 85.26%가 도출됐습니다.



max_depth 값에 따라 검증용 데이터 세트(별도의 테스트 데이터 세트가 아님)의 정확도가 어떻게 변했는지 cv_results_ 속성을 통해 알아보겠습니다.

```python
#GridSearchCV 객체의 cv_results_ 속성을 DataFrame으로 생성
cv_results_df = pd.DataFrame(grid_cv.cv_results_)

#max_depth 파라미터 값과 정확도 수치 추출
cv_results_df[['param_max_depth', 'mean_test_score']]
```

![image](https://user-images.githubusercontent.com/76269316/126118297-bfeef3e8-1ab8-4584-8e11-e845465fc0fb.png)

max_depth가 8일 때 0.852로 정확도가 정점이고, 이를 넘어가면서 정확도가 계속 떨어지는 것을 볼 수 있습니다.



이번에는 별도의 테스트 데이터 세트에서 max_depth 변화에 따른 정확도 값을 확인해 보겠습니다.

```python
max_depths = [6, 8, 10, 12, 16, 20, 24]
#max_depth 값을 변화시키면서 그 때마다 학습과 테스트 세트에서의 예측 성능 측정
for depth in max_depths:
    dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156)
    dt_clf.fit(X_train, y_train)
    pred = dt_clf.predict(X_test)
    accuracy = accuracy_score(y_test, pred)
    print('max_depth = {0} 정확도: {1:.4f}'.format(depth, accuracy))
```

max_depth가 8일 때 약 87.07%로 가장 높은 정확도를 나타냈습니다.

마찬가지로 8을 넘어가면서 정확도가 계속 감소하는 것을 확인할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/126118894-87aa643a-8c29-4ef2-93f6-a32e4dae6d64.png)



이처럼 결정 트리는 깊이가 깊어질수록 학습 데이터 세트에는 올바른 예측 결과를 가져오지만, 검증 데이터 세트에서는 오히려 과적합으로 인한 성능 저하를 유발합니다.

복잡한 모델보다는 트리 깊이를 낮춘 단순한 모델이 더욱 효과적인 결과를 가져옵니다.



이번에는 max_depth와 min_samples_split을 같이 변경하면서 성능을 튜닝해 보겠습니다.

```python
params = {
    'max_depth' : [8, 12, 16, 20],
    'min_samples_split' : [16, 24],
}

grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1)
grid_cv.fit(X_train, y_train)
print('GridSearchCV 최고 평균 정확도 수치: {0:.4f}'.format(grid_cv.best_score_))
print('GridSearchCV 최적 하이퍼 파라미터: ', grid_cv.best_params_)
```

![image](https://user-images.githubusercontent.com/76269316/126119709-2eea3a4f-1692-4beb-83f3-af00fa7eb3f0.png)



```python
best_df_clf = grid_cv.best_estimator_
pred1 = best_df_clf.predict(X_test)
accuracy = accuracy_score(y_test, pred1)
print('결정 트리 예측 정확도: {0:.4f}'.format(accuracy))
```

최적 하이퍼 파라미터(max_depth=8, min_samples_split=16)로 학습된 Estimator 객체를 통해 테스트 데이터 세트 예측을 수행하면 약 87.17%가 나옵니다.

![image](https://user-images.githubusercontent.com/76269316/126120168-2b946ac5-cba0-4295-aeba-0fc605ea4aeb.png)



결정 트리에서 각 피처 중요도를 feature_importances_ 속성을 통해 확인할 수 있습니다.

```python
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

ftr_importances_values = best_df_clf.feature_importances_
#Top 중요도로 정렬을 쉽게 하고, Seaborn의 막대 그래프로 쉽게 표현하기 위해 Series로 변환
ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns)
#중요도값 순으로 Series 정렬
ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]  #내림차순 정렬
plt.figure(figsize=(8, 6))
plt.title('Feature importances Top 20')
sns.barplot(x=ftr_top20, y=ftr_top20.index)
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/126132373-c864aa10-a008-4494-902f-e611bec4e4cc.png)



### 앙상블 학습(Ensemble Learning)

앙상블 학습을 통한 분류는 여러 개의 분류기(Classifier)를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법을 의미합니다.

이미지, 영상, 음성 등의 비정형 데이터 분류는 딥러닝이 뛰어난 성능을 보이고 있고, 대부분의 정형 데이터 분류 시에는 앙상블이 뛰어난 성능을 나타내고 있습니다.



앙상블 학습의 유형은 Voting, Bagging, Boosting, Stacking등이 있습니다.

- Voting : 서로 다른 알고리즘 분류기 중 투표를 통해 최종 예측 결과를 결정하는 방식
- Bagging : 모두 같은 유형(대부분 결정 트리 알고리즘)의 알고리즘 분류기 중 투표를 통해 최종 예측 결과를 결정하는 방식

|                         Voting 방식                          |                         Bagging 방식                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/126136297-8e955c5e-47bc-43ad-9064-58465e4e8255.png) | ![image](https://user-images.githubusercontent.com/76269316/126136399-5b72fac0-cc09-4173-8d64-5da99454310b.png) |

Voting과 Bagging 방식은 학습하는 데이터 세트가 다릅니다.

Bagging 방식은 원본 학습 데이터를 샘플링해 추출한 다음, 개별 분류기에 할당해서 학습하는데 이렇게 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식을 **Bootstrapping 분할 방식**이라고 합니다.

개별 분류기가 Bootstrapping 방식으로 샘플링된 데이터 세트로 학습한 다음 예측을 수행한 결과를 Voting을 통해 최종 예측 결과를 선정하는 방식이 Bagging Ensemble 방식입니다.

교차 검증이 데이터 세트 간에 중첩을 허용하지 않는 것과 다르게 Bagging 방식은 중첩을 허용합니다. (10,000개의 데이터를 10개의 분류기가 배깅 방식으로 나누더라도 각 1000개의 데이터 내에는 중복 데이터가 있음)



- Boosting : 여러 개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서 올바르게 예측할 수 있도록 다음 분류기에게는 가중치(Weight)를 부여하면서 학습과 예측을 진행하는 것

- Stacking : 여러 가지 다른 모델의 예측 결과값을 다시 학습 데이터로 만들어서 다른 모델(메타 모델)로 재학습시켜 결과를 예측하는 방법



##### 보팅 유형 - 하드 보팅(Hard Voting)과 소프트 보팅(Soft Voting)

보팅 방법에는 하드 보팅, 소프트 보팅 두 가지가 있는데, 하드 보팅보다는 소프트 보팅이 예측 성능이 좋아서 더 많이 사용됩니다.

- 하드 보팅 : 예측한 결과값들 중 다수의 분류기가 결정한 예측값을 최종 보팅 결과값으로 선정
- 소프트 보팅 : 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결과값으로 선정

|                          하드 보팅                           |                         소프트 보팅                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/126171378-0485c939-ac16-46c2-ad9f-0fb7a803a7be.png) | ![image](https://user-images.githubusercontent.com/76269316/126171614-ab1ea346-bf18-4333-83ea-98638c3e1f70.png) |



##### 보팅 분류기 (Voting Classifier)

사이킷런은 보팅 방식의 앙상블을 구현한 VotingClassifier 클래스를 제공하고 있습니다.

보팅 방식 앙상블을 사용해 위스콘신 유방암 데이터 세트를 예측 분석해 보겠습니다.

```python
import pandas as pd
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#위스콘신 유방암 데이터 세트 생성
cancer = load_breast_cancer()

data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)

#개별 모델은 로지스틱 회귀와 KNN임
lr_clf = LogisticRegression()
knn_clf = KNeighborsClassifier(n_neighbors=8)

#개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기
vo_clf = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf)], voting='soft')

X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=156)

#VotingClassifier 학습/예측/평가
vo_clf.fit(X_train, y_train)
pred = vo_clf.predict(X_test)
print('Voting 분류기 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))

#개별 모델의 학습/예측/평가
classifiers = [lr_clf, knn_clf]
for classifier in classifiers:
    classifier.fit(X_train, y_train)
    pred = classifier.predict(X_test)
    class_name = classifier.__class__.__name__
    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test, pred)))
```

![image](https://user-images.githubusercontent.com/76269316/126141778-ff7d2214-98f4-4d33-bb8f-d759c9567f6a.png)

보팅분류기가 정확도가 조금 더 높게 나타났는데, 보팅으로 여러 개의 분류기를 결합한다고 해서 무조건 개별 분류기보다 예측 성능이 향상되지는 않습니다.



Voting과 Stacking은 서로 다른 알고리즘을 기반으로 하지만, Bagging과 Boosting은 대부분 결정 트리 알고리즘을 기반으로 합니다.

결정 트리 알고리즘은 쉽고 직관적인 분류 기준을 갖고 있지만 정확한 예측을 위해 학습 데이터에 집착하게 되면 과적합이 발생해 실제 테스트 데이터에서 예측 성능이 떨어지게 됩니다.

하지만 앙상블 학습에서는 결정 트리 알고리즘의 단점을 수십~수천 개의 많은 분류기를 결합해 다양한 상황을 학습하게 함으로써 극복합니다.

결정 트리 알고리즘의 장점은 그대로 취하고 단점은 보완하면서 편향-분산 트레이드오프 효과를 극대화 한다는 것입니다.



### 랜덤 포레스트

Bagging은 같은 알고리즘으로 여러 개의 분류기를 만들어서 Voting으로 최종 결정하는 알고리즘입니다.

Bagging의 대표적인 알고리즘이 랜덤 포레스트입니다.

랜덤 포레스트는 결정 트리 기반 알고리즘으로, 결정 트리의 쉽고 직관적인 장점을 그대로 갖고 있습니다.

랜덤 포레스트는 여러 개의 결정 트리 분류기가 전체 데이터에서 bagging 방식으로 각자의 데이터를 샘플링해 개별적으로 학습한 뒤, 최종적으로 모든 분류기가 voting을 통해 예측을 결정하게 됩니다.

<img src="https://user-images.githubusercontent.com/76269316/126171819-89bb122a-3215-4c0e-ab58-cf76a05fa2a6.png" alt="image" style="zoom: 33%;" />



개별 트리가 학습하는 데이터 세트는 전체 데이터에서 일부가 중첩되게 샘플링된 데이터 세트입니다.

+이렇게 여러 개의 데이터 세트를 중첩되게 분리하는 것을 bootstrapping 분할 방식이라고 합니다.

<img src="https://user-images.githubusercontent.com/76269316/126173252-a4e5ef2f-5df3-4740-9a95-a00eeb2c2235.png" alt="image" style="zoom:50%;" />



사이킷런은 RandomForestClassifier 클래스를 통해 랜덤 포레스트 기반 분류를 지원합니다.

사용자 행동 인식 데이터 세트를 RandomForestClassifier를 이용해 예측해 보겠습니다.

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

#결정 트리에서 사용한 get_human_dataset()을 이용해 학습/테스트용 DataFrame 반환
X_train, X_test, y_train, y_test = get_human_dataset()

#랜덤 포레스트 학습 및 별도의 테스트 세트로 예측 성능 평가
rf_clf = RandomForestClassifier(random_state=0)
rf_clf.fit(X_train, y_train)
pred = rf_clf.predict(X_test)
accuracy = accuracy_score(y_test, pred)
print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy))
```

![image](https://user-images.githubusercontent.com/76269316/126174035-76bc780a-67c7-48d2-8aca-c9eea8c40018.png)

약 92.53%의 정확도를 보여줍니다.



##### 랜덤 포레스트 하이퍼 파라미터 및 튜닝

트리 기반의 앙상블 알고리즘은 하이퍼 파라미터가 너무 많고, 그로 인해 튜닝을 위한 시간이 많이 소모됩니다.

그나마 랜덤 포레스트는 결정 트리에서 사용되는 하이퍼 파라미터와 같은 파라미터가 대부분이라 파라미터가 적은 펵에 속합니다.



**랜덤 포레스트의 하이퍼 파라미터**

- n_estimators : 랜덤 포레스트에서 결정 트리 개수를 지정합니다. (디폴트 10개)
  많이 설정할수록 좋은 성능을 기대할 수 있지만 계속 증가시킨다고 성능이 무조건 향상되는 것은 아닙니다. (늘릴수록 학습 수행 시간이 증가하는 것을 감안해야함)
- max_features : 결정 트리의 [max_features](https://seominseok4834.github.io/machine%20learning/4.classification/#%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0) 파라미터와 같지만 디폴트가 auto입니다. (sqrt)
  따라서 랜덤 포레스트를 분할하는 피처를 참조할 때 전체 피처가 아니라 <img src="https://user-images.githubusercontent.com/76269316/126085844-bc299bcc-3f39-4a67-845d-7d39a9f98854.png" alt="image" style="zoom: 50%;" />만큼 참조합니다.
- max_depth : 결정 트리와 같음
- min_samples_leaf : 결정 트리와 같음



GridSearchCV를 이용해 랜덤 포레스트의 하이퍼 파라미터를 튜닝해 보겠습니다.

n_estimators = 100, CV = 2로 설정해 최적 하이퍼 파라미터를 구해 보겠습니다.

```python
from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators' : [100],
    'max_depth' : [6, 8, 10, 12],
    'min_samples_leaf' : [8, 12, 18],
    'min_samples_split' : [8, 16, 20]
}

#RandomForestClassifier 객체 생성 후 GridSearchCV 수행
rf_clf = RandomForestClassifier(random_state=0, n_jobs=-1)  #n_jobs=-1 : 모든 CPU 코어를 이용해 학습
grid_cv = GridSearchCV(rf_clf, param_grid=params, cv=2, n_jobs=-1)
grid_cv.fit(X_train, y_train)

print('최적 하이퍼 파라미터\n', grid_cv.best_params)
print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))
```

![image](https://user-images.githubusercontent.com/76269316/126176564-b90852e9-963e-4ffc-8169-5202892ccffc.png)



위에서 구한 최적 하이퍼 파라미터를 가지고 n_estimators를 300으로 증가시켜 별도의 테스트 데이터 세트에서 예측 성능을 평가해 보겠습니다.

```python
rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_leaf=8, min_samples_split=8, random_state=0)
rf_clf1.fit(X_train, y_train)
pred = rf_clf1.predict(X_test)

print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))
```

![image](https://user-images.githubusercontent.com/76269316/126176993-5f44f472-fb77-47db-b890-70c35919a933.png)

예측 정확도 수치는 약 91.65%가 나왔습니다.



RandomForestClassifier 역시 feature_importances_ 속성을 이용해 피처 중요도를 확인할 수 있습니다.

```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

ftr_importances_values = rf_clf1.feature_importances_
ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns)
ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]  #내림차순 정렬

plt.figure(figsize=(8, 6))
plt.title('Feature importances Top 20')
sns.barplot(x=ftr_top20, y=ftr_top20.index)
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/126177784-65cfc666-692d-480a-bb63-11873440cc7e.png)





### GBM(Gradient Boosting Machine)

Boosting 알고리즘은 여러 개의 약한 학습기(weak learner)를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치를 부여해 오류를 개선해 나가는 학습 방식입니다.

Boosting의 대표적인 구현은 AdaBoost(Adaptive boosting)와 Gradient Boost가 있습니다.



![image](https://user-images.githubusercontent.com/76269316/126178465-10a3db0f-c8d5-4b90-8604-c08ea4e8a01f.png)

맨 왼쪽 그림과 같이 +, -로 된 피처 데이터 세트가 있다면

- Step 1은 첫 번쨰 약한 학습기가 분류 기준 1로 +와 -를 분류한 것입니다. 동그라미로 표시된 ⊕ 데이터는 잘못 분류된 오류 데이터입니다.
- Step 2에서는 이 오류 데이터에 대해 가중치 값을 부여합니다. 가중치가 부여된 오류 데이터 +는 다음 약한 학습기가 잘 분류할 수 있게 크기가 커졌습니다.
- Step 3는 두 번째 약한 학습기가 분류 기준 2로 +와 -를 분류했습니다. 마찬가지로 동그라미로 표시된 ⊖ 데이터는 잘못 분류된 오류 데이터입니다.
- Step 4에서 잘못 분류된 - 오류 데이터에 대해 다음 약한 학습기가 잘 분류할 수 있게 더 큰 가중치를 부여합니다. (크기가 커짐)
- Step 5에서 세 번째 약한 학습기가 분류 기준 3으로 +와 -를 분류하고 오류 데이터를 찾습니다.
- 마지막으로 맨 아래에 첫 번째, 두 번째, 세 번째 약한 학습기를 모두 결합한 예측 결과입니다. 개별 약한 학습기보다 훨씬 정확도가 높아졌음을 알 수 있습니다.



GBM(Gradient Boost Machine)도 AdaBoost와 유사하나, 가중치 업데이트를 경사 하강법(Gradient Descent)을 이용하는 것이 큰 차이입니다. (나중에 회귀 부분에서 설명)

+GBM이 [CART](https://seominseok4834.github.io/machine%20learning/4.classification/#%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0) 기반의 다른 알고리즘이라 분류, 회귀 모두 가능



사이킷런은 GBM 기반의 분류를 위해 GradientBoostingClassifier 클래스를 제공하합니다.

사이킷런 GBM을 이용해 사용자 행동 데이터 세트를 예측 분류해 보겠습니다.

```python
from sklearn.ensemble import GradientBoostingClassifier
import time
import warnings
warnings.filterwarnings('ignore')

X_train, X_test, y_train, y_test = get_human_dataset()

#GBM 수행 시간 측정을 위한 시작 시간 설정
start_time = time.time()

gb_clf = GradientBoostingClassifier(random_state=0)
gb_clf.fit(X_train, y_train)
gb_pred = gb_clf.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)

print('GBM 정확도: {0:.4f}'.format(gb_accuracy))
print('GBM 수행시간: {0:.1f}초'.format(time.time() - start_time))
```

![image](https://user-images.githubusercontent.com/76269316/126184129-bb1ce414-17ec-4efb-a5da-52d16850a121.png)

사이킷런 GradientBoostingClassifier는 약한 학습기의 순차적인 예측 오류 보정을 통해 학습을 수행하므로 멀티 CPU 코어 시스템을 사용하더라도 병렬 처리가 지원되지 않아 대용량 데이터의 경우 학습에 매우 많은 시간이 필요합니다.

반면 랜덤 포레스트의 경우 상대적으로 빠른 수행 시간을 보장해줍니다.



##### GBM 하이퍼 파라미터 튜닝

- loss : 경사 하강법에서 사용할 비용 함수 지정.
  특별한 이유가 없으면 디폴트 'deviance' 그대로 적용
- learning_rate : GBM이 학습을 진행할 때마다 적용하는 학습률. (weak learner가 순차적으로 오류 값을 보정해 나가는데 적용하는 계수)
  0~1 사이의 값을 지정할 수 있으며, 기본값은 0.1
  너무 작은 값을 적용하면 업데이트 되는 값이 작아져서 예측 성능이 높아질 가능성이 높지만 수행 시간이 오래 걸리게 됨
  너무 큰 값을 설정하면 최소 오류 값을 찾지 못하고 지나쳐 버려 예측 성능이 떨어질 가능성이 높지만 빠른 수행이 가능하게 됨
  → learning_rate는 n_estimators와 상호 보완적으로 조합해 사용해야함
- n_estimators : weak learner의 개수
  weak learner가 순차적으로 오류를 보정하므로 개수가 많을수록 예측 성능이 일정 수준까지는 좋아지나, 수행 시간이 오래 걸리게 됨. (기본값은 100)
- subsample : weak learner가 학습에 사용하는 데이터의 샘플링 비율
  기본값은 1이며 이는 전체 학습 데이터를 기반으로 학습한다는 의미입니다. (과적합이 염려되는 경우 1보다 작은 값으로 설정)



GridSearchCV를 이용해 하이퍼 파라미터를 최적화해 보겠습니다.

사용자 행동 데이터 세트가 많기 때문에 많은 시간이 걸릴 것입니다. (30분에서 1시간 정도)

```python
from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators' : [100, 500],
    'learning_rate' : [0.05, 0.1]
}

grid_cv = GridSearchCV(gb_clf, param_grid=params, cv=2, verbose=1)
grid_cv.fit(X_train, y_train)
print('최적 하이퍼 파라미터\n', grid_cv.best_params_)
print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))
```



최적 하이퍼 파라미터를 적용해 테스트 데이터 세트의 예측 정확도를 확인해 보겠습니다.

```python
#GridSearchCV를 이용해 최적으로 학습된 estimator로 예측 수행
gb_pred = grid_cv.best_estimator_predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)
print('GBM 정확도: {0:.4f}'.format(gb_accuracy))
```

GBM은 과적합에도 뛰어난 예측 성능을 가진 알고리즘이지만 수행 시간이 오래 걸린다는 단점이 있습니다.

GBM을 기반으로한 각광받고 있는 두 개의 Gradient Boosting 기반 ML 패키지가 있는데 XGBoost와 LightGBM입니다.
