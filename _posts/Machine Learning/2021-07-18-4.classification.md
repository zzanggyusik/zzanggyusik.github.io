---
title:  "4.Classification"
excerpt: "분류"
toc: true
toc_label: "Classification"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - 평가
  - 파이썬 머신러닝 완벽 가이드
last_modified_at: 2021-07-18

---

>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.



### 분류의 개요

지도학습은 명시적인 정답(label)이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식입니다.

지도학습의 대표적인 유형인 분류(Classification)는 학습 데이터로 주어진 데이터의 피처와 레이블값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 

이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것입니다.

**즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 학습한 다음 새롭게 관측된 데이터에 대한 레이블을 판별하는 것입니다.**



분류는 다양한 머신러닝 알고리즘으로 구현할 수 있습니다.

- 베이즈(Bayes) 통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)
- 독립변수와 종속변수의 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression)
- 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)
- 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine)
- 근접 거리를 기준으로 하는 최소 근접(Nearest Neighbor) 알고리즘
- 심층 연결 기반의 신경망 (Neural Network)
- 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)



### 결정 트리(Decision Tree)

결정 트리는 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 알고리즘입니다.

데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우합니다.



아래 그림은 결정 트리의 구조를 간략하게 나타낸 것입니다.

규칙 노드(Decision Node)로 표시된 노드는 규칙 조건이 되고, 새로운 규칙 조건마다 서브 트리(Sub Tree)가 생성됩니다.

리프 노드(Leaf Node)로 표시된 노드는 결정된 클래스 값입니다.

![image](https://user-images.githubusercontent.com/76269316/126070248-adec61f6-9f8e-44cb-a288-9a5abb278598.png)



데이터 세트에는 피처가 있고 피처가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어집니다.

하지만 많은 규칙이 있다는 건 분류를 결정하는 방식이 복잡해진다는 것이고, 이는 과적합으로 이어지게 됩니다.

즉, 트리의 깊이(depth)가 깊어질수록 결정 트리의 예측 성능이 저하되게 됩니다.

따라서 가능한 한 적은 결정 노드로 높은 예측 정확도를 가져야 하는데 이를 위해서는, 최대한 균일한 데이터 세트를 구성할 수 있도록  분할(Split)하는 것이 필요합니다.



균일한 데이터 세트가 갖는 의미에 대해 알아보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126084685-9a654ae8-37ed-4f7b-8a8c-06028853d29e.png)



다음 데이터 세트를 균일한 순서로 나열하면 C → B → A 입니다.

C의 경우 모두 검은 공으로 구성되므로 데이터가 모두 균일하고,

B의 경우는 일부 하얀 공을 갖고 있지만, 대부분 검은 공으로 구성되어 다음으로 균일도가 높습니다.

A의 경우는 검은 공 못지않게 많은 하얀 공을 갖고 있어 균일도가 낮습니다.



눈을 가린 채 데이터 세트 C에서 하나의 데이터를 뽑았을 때 데이터에 대한 별다른 정보 없이도 검은 공이라고 쉽게 예측할 수 있는 반면,

A의 경우 상대적으로 혼잡도가 높고 균일도가 낮기 때문에 같은 조건에서 데이터를 판단하는데 있어 많은 정보가 필요합니다.



결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만듧니다.

즉, 정보 균일도가 데이터 세트로 쪼개질 수 있는 조건을 찾아 서브 데이터 세트를 만들고, 

다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트로 쪼개는 방식으로 내려가면서 반복하는 방식으로 데이터 값을 예측합니다.



예를 들어 박스 안에 30개의 레고 블록이 있을 때, 각 레고 블록이 갖는 속성은 다음과 같습니다. 

- 형태 : 동그라미, 네모, 세모

- 색깔 : 노랑, 빨강, 파랑



이 중 노랑색 블록은 모두 동그라미이고 빨강과 파랑 블록의 경우 동그라미, 네모, 세모가 골고루 섞여있다고 하면

각 레고 블록을 형태와 색깔 속성으로 분류하고자 할 때 가장 첫 번째로 만들어지는 규칙은 **if 색깔 == '노란색'**입니다.

왜냐하면 노란색 블록이면 모두 노란 동그라미 블록으로 예측할 수 있고, 그 다음 나머지 블록에 대해 다시 균일도 조건을 찾아 분류하는 것이 가장 효율적이기 때문입니다.



정보 균일도를 측정하는 방법으로 엔트로피를 이용한 정보 이득(Information Gain) 지수와 지니 계수가 있습니다.

- 정보 이득 : 엔트로피(주어진 데이터 집합의 혼잡도) 개념을 기반으로 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮게 되는데 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값입니다.
  결정 트리는 정보 이득 지수가 높은 속성을 기준으로 분할합니다.
- 지니 계수 : 경제학에서 불평등 지수를 나타낼 때 사용하는 계수로 0이 가장 평등하고 1로 갈수록 불평등합니다.
  머신러닝에 적용될 때는 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할합니다.



결정 트리 알고리즘을 사이킷런에서 구현한 DecisionTreeClassifier에서는 지니 계수를 이용해 데이터 세트를 분할합니다.

일반적인 결정 트리 알고리즘은 데이터 세트를 분할하는데 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정합니다.

![image](https://user-images.githubusercontent.com/76269316/126085195-1a8d5692-a2be-4e23-af48-965e10d5e980.png)





##### 결정 트리 모델의 특징

결정 트리의 장점은 정보의 '균일도'라는 룰을 기반으로 하고 있어 알고리즘이 쉽고 직관적이라는 것입니다.

룰이 매우 명확하고, 이에 기반해 어떻게 규칙 노드와 리프 노드가 만들어지는지 알 수 있고 시각화까지 할 수 있습니다.

또한 정보 균일도만 신경 쓰면 되므로 특별한 경우를 제외하고는 각 피처의 스케일링과 정규화 같은 전처리 작업이 필요 없습니다.



결정 트리의 단점은 과적합으로 정확도가 떨어진다는 것입니다.

학습 데이터 기반 모델의 정확도를 높이기 위해 모든 데이터 상황을 만족하는 완벽한 규칙을 만들려고 하게되고(그럴 수 없음에도 불구하고) 결국, 트리의 깊이가 깊어지고 트리가 복잡해져서 예측 성능이 떨어지게 됩니다.



따라서 모든 데이터 상황을 만족하는 완벽한 규칙은 만들 수 없다고 인정하고 트리의 크기를 사전에 제한하는 것이 성능 향상에 도움이 됩니다.

|                             장점                             |                             단점                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| - 직관적이다.<br />- 피처의 스케일리이나 정규화 등의 사전 가공 영향도가 크지 않다. | - 과적합으로 알고리즘 성능이 떨어진다.<br />이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝이 필요 |



##### 결정 트리 파라미터

사이킷런 결정 트리 알고리즘은 **CART(Classification And Regression Trees)** 기반으로 구현한 **DecisionTreeClassifier**와 **DecisionTreeRegressor** 클래스를 제공합니다. (CART : 분류뿐만 아니라 회귀에서도 사용될 수 있는 트리 알고리즘)

DecisionTreeClassifier는 분류를 위한 클래스이고, DecisionTreeRegressor는 회귀를 위한 클래스입니다.

|    파라미터명     |                             설명                             |
| :---------------: | :----------------------------------------------------------: |
| min_samples_split | 노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는데 사용됨.<br />디폴트는 2이고 작게 설정할수록 분할되는 노드가 많아져 과적합 가능성 증가 |
| min_samples_leaf  | 말단 노드(Leaf)가 되기 위한 최소한의 샘플 데이터 수<br />min_samples_split과 유사하게 과적합 제어 용도로 사용됨.<br />비대칭적(imbalanced) 데이터의 경우 특정 클래스 데이터가 극도로 작을 수 있으므로 작게 설정 필요 |
|   max_features    | 최적의 분할을 위해 고려할 최대 피처 개수. 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할 수행.<br />int 형으로 지정하면 대상 피처의 개수, float 형으로 지정하면 전체 피처 중 대상 피처의 퍼센트<br />sqrt는 <img src="https://user-images.githubusercontent.com/76269316/126085844-bc299bcc-3f39-4a67-845d-7d39a9f98854.png" alt="image" style="zoom: 50%;" />만큼 선정<br />auto로 지정하면 sqrt와 동일<br />log는 전체 피처 중 <img src="https://user-images.githubusercontent.com/76269316/126085922-9abdd74f-e044-4079-8be5-42eea3643b88.png" alt="image" style="zoom: 50%;" />만큼 선정<br />None은 전체 피처 선정 |
|     max_depth     | 트리의 최대 높이를 규정<br />디폴트는 None 완벽하게 클래스 결정 값이 결정 될 때까지 깊이를 계속 키우며 분할하거나 노드가 갖는 데이터 개수가 min_samples_split보다 작아질 때까지 계속 깊이를 증가시킴.<br />깊이가 깊어지면 min_samples_split 설정대로 최대 분할하여 과적합할 수 있으므로 적절한 값으로 제어 필요. |
|  max_leaf_nodes   |                 리프 노드(Leaf)의 최대 개수                  |



##### 결정 트리 모델의 시각화

Graphviz 패키지를 사용하면 결정 트리 알고리즘이 어떤 규칙을 갖고 트리를 생성하는지 시각적으로 확인할 수 있습니다.

Graphviz는 원래 그래프 기반 dot 파일로 기술된 다양한 이미지를 쉽게 시각화하는 패키지인데,  사이킷런에서 쉽게 인터페이스 할 수 있도록 export_graphviz() API를 제공합니다.



먼저 Graphviz를 윈도우에 설치하겠습니다. 

Graphviz는 C/C++로 운영체제에 포팅된 패키지여서 파이썬과 인터페이스 할 수 있는 파이썬 래퍼(Wrapper) 모듈을 별도로 설치해야합니다.

① [Graphviz](https://graphviz.org/download/)를 설치합니다.
![image](https://user-images.githubusercontent.com/76269316/126086257-ae733f38-5506-45eb-ab96-58bfa08bce31.png)

저는 윈도우10 64bit이기 때문에 빨간색 파일을 다운 받아 설치했습니다.



![image](https://user-images.githubusercontent.com/76269316/126086295-766a4b40-6fe6-4c3c-921c-b881a3a494c8.png)

저는 윈도우, 게임만 C드라이브에 깔고 나머지는 D드라이브에 설치하기  때문에 D드라이브에 설치했습니다.



② 파이썬 래퍼 모듈을 PIP를 이용해 설치합니다.

Anaconda 콘솔이나 OS command 콘솔에서 pip install graphviz 명령어로 설치합니다. (콘솔 실행시 관리자 권한으로 실행)

![image](https://user-images.githubusercontent.com/76269316/126086404-4a67074c-78a4-486f-88dd-72eb8264171d.png)

![image](https://user-images.githubusercontent.com/76269316/126086433-36a703cd-7191-43a6-8992-1f81aac9a7c2.png)



③ Graphviz와 파이썬 래퍼를 연결하기 위해 환경 변수 설정을 해야합니다.

내 PC → 속성 → 고급 시스템 설정 → 환경변수 클릭



먼저 사용자 변수의 Path 변수 → 편집 클릭

![image](https://user-images.githubusercontent.com/76269316/126086545-7f4bedc4-ffda-417c-9f61-d5c3f25a0c16.png)

D:\Program Files\Graphviz\bin 추가

![image](https://user-images.githubusercontent.com/76269316/126086580-c972ff26-0a95-4ff9-87f5-fb563708ae25.png)



시스템 변수의 Path 변수 → 편집 클릭

![image](https://user-images.githubusercontent.com/76269316/126086605-2c81bdd6-7efc-40f9-9eff-5d28e928093b.png)

D:\Program Files\Graphviz\bin\dot.exe 추가

![image](https://user-images.githubusercontent.com/76269316/126086623-1a6a4f67-cb88-46c0-a89c-4346845ff3c1.png)

이후 주피터 노트북을 재시작합니다. (환경 변수 Path를 재로딩하기 위해)



Graphviz를 이용해 붓꽃 데이터 세트에 결정 트리를 적용할 때 어떻게 서브 트리가 생성되고 만들어지는지 시각화해 보겠습니다.

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

#DecisionTreeClassifier 생성
dt_clf = DecisionTreeClassifier(random_state=156)

#붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train, y_train)

#export_graphviz() 호출 결과로 out_file로 지정된 tree.dot 파일 생성
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)  #impurity : gini 계수 출력 여부, filled : 노드 색깔을 다르게 출력
```

```python
import graphviz

#위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북에서 시각화
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

![image](https://user-images.githubusercontent.com/76269316/126087894-7f8ef93b-ebe6-4a3f-b50a-6ce8a2e9c467.png)

출력 결과를 보면, 각 규칙에 따라 트리의 브랜치(branch) 노드와 리프(leaf) 노드가 어떻게 구성돼 있는지 한눈에 확인할 수 있게 시각화 돼 있습니다.



먼저 리프 노드를 보면 최종 클래스(label) 값이 결정된 노드입니다.
