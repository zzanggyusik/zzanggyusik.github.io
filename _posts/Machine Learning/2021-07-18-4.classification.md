---
title:  "4.Classification"
excerpt: "분류"
toc: true
toc_label: "Classification"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - 평가
  - 파이썬 머신러닝 완벽 가이드
last_modified_at: 2021-07-18

---

>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.



### 분류의 개요

지도학습은 명시적인 정답(label)이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식입니다.

지도학습의 대표적인 유형인 분류(Classification)는 학습 데이터로 주어진 데이터의 피처와 레이블값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 

이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것입니다.

**즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 학습한 다음 새롭게 관측된 데이터에 대한 레이블을 판별하는 것입니다.**



분류는 다양한 머신러닝 알고리즘으로 구현할 수 있습니다.

- 베이즈(Bayes) 통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)
- 독립변수와 종속변수의 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression)
- 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)
- 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine)
- 근접 거리를 기준으로 하는 최소 근접(Nearest Neighbor) 알고리즘
- 심층 연결 기반의 신경망 (Neural Network)
- 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)



### 결정 트리(Decision Tree)

결정 트리는 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 알고리즘입니다.

데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우합니다.



아래 그림은 결정 트리의 구조를 간략하게 나타낸 것입니다.

규칙 노드(Decision Node)로 표시된 노드는 규칙 조건이 되고, 새로운 규칙 조건마다 서브 트리(Sub Tree)가 생성됩니다.

리프 노드(Leaf Node)로 표시된 노드는 결정된 클래스 값입니다.

![image](https://user-images.githubusercontent.com/76269316/126070248-adec61f6-9f8e-44cb-a288-9a5abb278598.png)



데이터 세트에는 피처가 있고 피처가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어집니다.

하지만 많은 규칙이 있다는 건 분류를 결정하는 방식이 복잡해진다는 것이고, 이는 과적합으로 이어지게 됩니다.

즉, 트리의 깊이(depth)가 깊어질수록 결정 트리의 예측 성능이 저하되게 됩니다.

따라서 가능한 한 적은 결정 노드로 높은 예측 정확도를 가져야 하는데 이를 위해서는, 최대한 균일한 데이터 세트를 구성할 수 있도록  분할(Split)하는 것이 필요합니다.



균일한 데이터 세트가 갖는 의미에 대해 알아보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126084685-9a654ae8-37ed-4f7b-8a8c-06028853d29e.png)



다음 데이터 세트를 균일한 순서로 나열하면 C → B → A 입니다.

C의 경우 모두 검은 공으로 구성되므로 데이터가 모두 균일하고,

B의 경우는 일부 하얀 공을 갖고 있지만, 대부분 검은 공으로 구성되어 다음으로 균일도가 높습니다.

A의 경우는 검은 공 못지않게 많은 하얀 공을 갖고 있어 균일도가 낮습니다.



눈을 가린 채 데이터 세트 C에서 하나의 데이터를 뽑았을 때 데이터에 대한 별다른 정보 없이도 검은 공이라고 쉽게 예측할 수 있는 반면,

A의 경우 상대적으로 혼잡도가 높고 균일도가 낮기 때문에 같은 조건에서 데이터를 판단하는데 있어 많은 정보가 필요합니다.



결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만듧니다.

즉, 정보 균일도가 데이터 세트로 쪼개질 수 있는 조건을 찾아 서브 데이터 세트를 만들고, 

다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트로 쪼개는 방식으로 내려가면서 반복하는 방식으로 데이터 값을 예측합니다.



예를 들어 박스 안에 30개의 레고 블록이 있을 때, 각 레고 블록이 갖는 속성은 다음과 같습니다. 

- 형태 : 동그라미, 네모, 세모

- 색깔 : 노랑, 빨강, 파랑



이 중 노랑색 블록은 모두 동그라미이고 빨강과 파랑 블록의 경우 동그라미, 네모, 세모가 골고루 섞여있다고 하면

각 레고 블록을 형태와 색깔 속성으로 분류하고자 할 때 가장 첫 번째로 만들어지는 규칙은 **if 색깔 == '노란색'**입니다.

왜냐하면 노란색 블록이면 모두 노란 동그라미 블록으로 예측할 수 있고, 그 다음 나머지 블록에 대해 다시 균일도 조건을 찾아 분류하는 것이 가장 효율적이기 때문입니다.



정보 균일도를 측정하는 방법으로 엔트로피를 이용한 정보 이득(Information Gain) 지수와 지니 계수가 있습니다.

- 정보 이득 : 엔트로피(주어진 데이터 집합의 혼잡도) 개념을 기반으로 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮게 되는데 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값입니다.
  결정 트리는 정보 이득 지수가 높은 속성을 기준으로 분할합니다.
- 지니 계수 : 경제학에서 불평등 지수를 나타낼 때 사용하는 계수로 0이 가장 평등하고 1로 갈수록 불평등합니다.
  머신러닝에 적용될 때는 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할합니다.



결정 트리 알고리즘을 사이킷런에서 구현한 DecisionTreeClassifier에서는 지니 계수를 이용해 데이터 세트를 분할합니다.

일반적인 결정 트리 알고리즘은 데이터 세트를 분할하는데 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정합니다.

![image](https://user-images.githubusercontent.com/76269316/126085195-1a8d5692-a2be-4e23-af48-965e10d5e980.png)





##### 결정 트리 모델의 특징

결정 트리의 장점은 정보의 '균일도'라는 룰을 기반으로 하고 있어 알고리즘이 쉽고 직관적이라는 것입니다.

룰이 매우 명확하고, 이에 기반해 어떻게 규칙 노드와 리프 노드가 만들어지는지 알 수 있고 시각화까지 할 수 있습니다.

또한 정보 균일도만 신경 쓰면 되므로 특별한 경우를 제외하고는 각 피처의 스케일링과 정규화 같은 전처리 작업이 필요 없습니다.



결정 트리의 단점은 과적합으로 정확도가 떨어진다는 것입니다.

학습 데이터 기반 모델의 정확도를 높이기 위해 모든 데이터 상황을 만족하는 완벽한 규칙을 만들려고 하게되고(그럴 수 없음에도 불구하고) 결국, 트리의 깊이가 깊어지고 트리가 복잡해져서 예측 성능이 떨어지게 됩니다.



따라서 모든 데이터 상황을 만족하는 완벽한 규칙은 만들 수 없다고 인정하고 트리의 크기를 사전에 제한하는 것이 성능 향상에 도움이 됩니다.

|                             장점                             |                             단점                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| - 직관적이다.<br />- 피처의 스케일리이나 정규화 등의 사전 가공 영향도가 크지 않다. | - 과적합으로 알고리즘 성능이 떨어진다.<br />이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝이 필요 |



##### 결정 트리 파라미터

사이킷런 결정 트리 알고리즘은 **CART(Classification And Regression Trees)** 기반으로 구현한 **DecisionTreeClassifier**와 **DecisionTreeRegressor** 클래스를 제공합니다. (CART : 분류뿐만 아니라 회귀에서도 사용될 수 있는 트리 알고리즘)

DecisionTreeClassifier는 분류를 위한 클래스이고, DecisionTreeRegressor는 회귀를 위한 클래스입니다.

|    파라미터명     |                             설명                             |
| :---------------: | :----------------------------------------------------------: |
| min_samples_split | 노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는데 사용됨.<br />디폴트는 2이고 작게 설정할수록 분할되는 노드가 많아져 과적합 가능성 증가 |
| min_samples_leaf  | 말단 노드(Leaf)가 되기 위한 최소한의 샘플 데이터 수<br />min_samples_split과 유사하게 과적합 제어 용도로 사용됨.<br />비대칭적(imbalanced) 데이터의 경우 특정 클래스 데이터가 극도로 작을 수 있으므로 작게 설정 필요 |
|   max_features    | 최적의 분할을 위해 고려할 최대 피처 개수. 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할 수행.<br />int 형으로 지정하면 대상 피처의 개수, float 형으로 지정하면 전체 피처 중 대상 피처의 퍼센트<br />sqrt는 <img src="https://user-images.githubusercontent.com/76269316/126085844-bc299bcc-3f39-4a67-845d-7d39a9f98854.png" alt="image" style="zoom: 50%;" />만큼 선정<br />auto로 지정하면 sqrt와 동일<br />log는 전체 피처 중 <img src="https://user-images.githubusercontent.com/76269316/126085922-9abdd74f-e044-4079-8be5-42eea3643b88.png" alt="image" style="zoom: 50%;" />만큼 선정<br />None은 전체 피처 선정 |
|     max_depth     | 트리의 최대 높이를 규정<br />디폴트는 None 완벽하게 클래스 결정 값이 결정 될 때까지 깊이를 계속 키우며 분할하거나 노드가 갖는 데이터 개수가 min_samples_split보다 작아질 때까지 계속 깊이를 증가시킴.<br />깊이가 깊어지면 min_samples_split 설정대로 최대 분할하여 과적합할 수 있으므로 적절한 값으로 제어 필요. |
|  max_leaf_nodes   |                 리프 노드(Leaf)의 최대 개수                  |



##### 결정 트리 모델의 시각화

Graphviz 패키지를 사용하면 결정 트리 알고리즘이 어떤 규칙을 갖고 트리를 생성하는지 시각적으로 확인할 수 있습니다.

Graphviz는 원래 그래프 기반 dot 파일로 기술된 다양한 이미지를 쉽게 시각화하는 패키지인데,  사이킷런에서 쉽게 인터페이스 할 수 있도록 export_graphviz() API를 제공합니다.



먼저 Graphviz를 윈도우에 설치하겠습니다. 

Graphviz는 C/C++로 운영체제에 포팅된 패키지여서 파이썬과 인터페이스 할 수 있는 파이썬 래퍼(Wrapper) 모듈을 별도로 설치해야합니다.

① [Graphviz](https://graphviz.org/download/)를 설치합니다.
![image](https://user-images.githubusercontent.com/76269316/126086257-ae733f38-5506-45eb-ab96-58bfa08bce31.png)

저는 윈도우10 64bit이기 때문에 빨간색 파일을 다운 받아 설치했습니다.



![image](https://user-images.githubusercontent.com/76269316/126086295-766a4b40-6fe6-4c3c-921c-b881a3a494c8.png)

저는 윈도우, 게임만 C드라이브에 깔고 나머지는 D드라이브에 설치하기  때문에 D드라이브에 설치했습니다.



② 파이썬 래퍼 모듈을 PIP를 이용해 설치합니다.

Anaconda 콘솔이나 OS command 콘솔에서 pip install graphviz 명령어로 설치합니다. (콘솔 실행시 관리자 권한으로 실행)

![image](https://user-images.githubusercontent.com/76269316/126086404-4a67074c-78a4-486f-88dd-72eb8264171d.png)

![image](https://user-images.githubusercontent.com/76269316/126086433-36a703cd-7191-43a6-8992-1f81aac9a7c2.png)



③ Graphviz와 파이썬 래퍼를 연결하기 위해 환경 변수 설정을 해야합니다.

내 PC → 속성 → 고급 시스템 설정 → 환경변수 클릭



먼저 사용자 변수의 Path 변수 → 편집 클릭

![image](https://user-images.githubusercontent.com/76269316/126086545-7f4bedc4-ffda-417c-9f61-d5c3f25a0c16.png)

D:\Program Files\Graphviz\bin 추가

![image](https://user-images.githubusercontent.com/76269316/126086580-c972ff26-0a95-4ff9-87f5-fb563708ae25.png)



시스템 변수의 Path 변수 → 편집 클릭

![image](https://user-images.githubusercontent.com/76269316/126086605-2c81bdd6-7efc-40f9-9eff-5d28e928093b.png)

D:\Program Files\Graphviz\bin\dot.exe 추가

![image](https://user-images.githubusercontent.com/76269316/126086623-1a6a4f67-cb88-46c0-a89c-4346845ff3c1.png)

이후 주피터 노트북을 재시작합니다. (환경 변수 Path를 재로딩하기 위해)



Graphviz를 이용해 붓꽃 데이터 세트에 결정 트리를 적용할 때 어떻게 서브 트리가 생성되고 만들어지는지 시각화해 보겠습니다.

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

#DecisionTreeClassifier 생성
dt_clf = DecisionTreeClassifier(random_state=156)

#붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train, y_train)

#export_graphviz() 호출 결과로 out_file로 지정된 tree.dot 파일 생성
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)  #impurity : gini 계수 출력 여부, filled : 노드 색깔을 다르게 출력
```

```python
import graphviz

#위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북에서 시각화
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

![image](https://user-images.githubusercontent.com/76269316/126087894-7f8ef93b-ebe6-4a3f-b50a-6ce8a2e9c467.png)

출력 결과를 보면, 각 규칙에 따라 트리의 브랜치(branch) 노드와 리프(leaf) 노드가 어떻게 구성돼 있는지 한눈에 확인할 수 있게 시각화 돼 있습니다.

각 노드의 색깔은 붓꽃 데이터의 레이블 값을 의미하고 (주황색 0 : Setosa, 초록색 1 : Versicolor, 보라색 2 : Virginica) 색이 짙어질수록 지니 계수가 낮습니다. (해당 레이블에 속하는 샘플 데이터가 많다는 의미)



노드 내 지표의 의미는 다음과 같습니다.

- petal length(cm) <= 2.45 : 자식 노드를 만들기 위한 규칙 조건입니다. (조건이 없으면 리프 노드)
- gini : 다음 value=[]로 주어진 데이터 분포에서의 지니 계수
- samples : 현 규칙에 해당하는 데이터 건수
- value : 클래스 값 기반의 데이터 건수
  붓꽃 데이터 세트는 클래스 값으로 0(Setosa), 1(Versicolor), 2(Virginica) 를 갖고 있는데 value = [41, 40, 39]라면 Setosa 41개, Versicolor 40개, Virginica 39개로 데이터가 구성돼 있다는 의미입니다.



맨 윗부분부터 자세히 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126088306-547d6007-7543-45f0-ad96-c9b8c2d4d42a.png)

먼저 루트 노드인 1번 노드의 지표입니다.

**1번 노드 지표**

- samples = 120 : 전체 데이터가 120개라는 의미
- value = [41, 40, 39] : Setosa 41개, Versicolor 40개, Virginica 39개로 데이터가 구성돼 있다는 의미
- gini = 0.667 : sample 120개가 value = [41, 40, 39] 분포도로 돼 있으므로 지니 계수는 0.667
- petal length(cm) <= 2.45 규칙으로 자식 노드 생성
- class = setosa : 하위 노드를 가질 경우 setosa 개수가 41개로 가장 많다는 의미



petal legnth(cm) <= 2.45 규칙으로 True인 데이터가 2번 노드, False인 데이터가 3번 노드가 만들어집니다.

2번 노드는 petal length(cm) <= 2.45가 True인 규칙 노드로 모든 데이터가 Setosa로 결정돼 리프 노드가 되고, 2번 노드에서는 더 이상 규칙이 생성되지 않습니다.

**2번 노드 지표**

- value = [41, 0, 0] : 샘플 데이터 모두 Setosa
- gini = 0.0



3번 노드는 petal length(cm) <= 2.45가 False인 규칙 노드로 지표는 다음과 같습니다.

**3번 노드 지표**

- value = [0, 40, 39] : 79개 샘플 데이터 중 Versicolor 40개, Virginica 39개
- gini = 0.5로 여전히 높으므로 다음 자식 노드 생성
- petal width(cm) <= 1.55 규칙으로 자식 노드 생성



4번 노드는 petal width(cm) <= 1.55가 True인 규칙 노드로 지표는 다음과 같습니다.

**4번 노드 지표 설명**

- value = [0, 37, 1] : 38개의 샘플 데이터 중 Versicolor 37개, Virginica 1개로 대부분 Versicolor
- gini = 0.051로 매우 낮으나 여전히 Versicolor와 Virginica가 혼재돼 있으므로 petal length(cm) <= 5.25라는 규칙으로 자식 노드 생성



5번 노드는 petal width(cm) <= 1.55가 False인 규칙 노드로 지표는 다음과 같습니다.

**5번 노드 지표**

- value = [0, 3, 38] : 41개의 샘플 데이터 중 Versicolor 3개, Virginica 38개로 대부분 Virginica
- gini = 0.136으로 낮으나 여전히 Versicolor와 Virginica가 혼재돼 있으므로 petal length(cm) <= 1.75라는 규칙으로 자식 노드 생성

 

4번 노드를 보면 Virginica가 1개 밖에 없지만 이 1개를 완벽히 분류하기 위해 또 다른 자식 노드를 생성하는 것을 볼 수 있습니다.

이로 인해 결국 매우 복잡한 규칙 트리가 만들어져 과적합 문제가 발생합니다.



때문에 [결정 트리 하이퍼 파라미터](https://seominseok4834.github.io/machine%20learning/4.classification/#%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0) 대부분이 복잡한 트리가 생성되는 것을 막기 위한 용도로 사용됩니다.



결정 트리의 하이퍼 파라미터 변경에 따른 트리의 변화를 살펴보겠습니다.

**max_depth**

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
import graphviz
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

#DecisionTreeClassifier 생성 (max_depth=3)
dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)

#붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train, y_train)

#export_graphviz() 호출 결과로 out_file로 지정된 tree.dot 파일 생성
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)  #impurity : gini 계수 출력 여부, filled : 노드 색깔을 다르게 출력

#위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북에서 시각화
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

|                     max_depth 제약 없음                      |                        max_depth = 3                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/126087894-7f8ef93b-ebe6-4a3f-b50a-6ce8a2e9c467.png) | ![image](https://user-images.githubusercontent.com/76269316/126089242-dd31136e-63b5-45ac-9476-4e92934255b3.png) |

트리의 깊이가 설정된 max_depth에 따라 줄어들어 더 간단한 결정 트리가 생성됐습니다.



**min_samples_split**

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
import graphviz
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

#DecisionTreeClassifier 생성 (min_samples_split=4)
dt_clf = DecisionTreeClassifier(random_state=156, min_samples_split=4)

#붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train, y_train)

#export_graphviz() 호출 결과로 out_file로 지정된 tree.dot 파일 생성
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)  #impurity : gini 계수 출력 여부, filled : 노드 색깔을 다르게 출력

#위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북에서 시각화
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

![image](https://user-images.githubusercontent.com/76269316/126089748-f8baec5e-8758-48d7-a8b8-f9837263434f.png)

빨간색 네모 박스를 보면, 샘플 데이터 3개 중 1개가 다른 클래스 값이 들어가 있지만, min_samples_split=4로 지정(샘플 데이터가 최소 4개 이상 있어야지만 자식 노드 생성)해놨기 때문에 자식 노드를 생성하지 않는 것을 볼 수 있습니다.



**min_samples_leaf**

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
import graphviz
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

#DecisionTreeClassifier 생성 (min_samples_leaf=4)
dt_clf = DecisionTreeClassifier(random_state=156, min_samples_leaf=4)

#붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train, y_train)

#export_graphviz() 호출 결과로 out_file로 지정된 tree.dot 파일 생성
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)  #impurity : gini 계수 출력 여부, filled : 노드 색깔을 다르게 출력

#위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북에서 시각화
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

![image](https://user-images.githubusercontent.com/76269316/126090410-7a83d930-9861-4e76-bf82-d4a07108e496.png)

min_samples_leaf는 리프 노드가 될 수 있는 샘플 데이터 건수의 최솟값을 지정합니다. (default는 1)

위 코드에서는 min_samples_leaf를 4로 지정했기 때문에 지니 계수 값이 크더라도 (다른 클래스 값이 섞여 있더라도) 샘플 데이터가 4개 이하인 경우 리프 노드가 되게 되어 결정 트리가 간결해지게 됩니다.



결정 트리 알고리즘이 학습을 통해 규칙을 정하는데 있어 피처의 중요한 역할 지표를 feature_importances_ 속성으로 확인할 수 있습니다.

feature_importances_는 ndarray 형태로 피처 중요도가 반환됩니다.

```python
import seaborn as sns
import numpy as np
%matplotlib inline

#feature importance 추출
print("Feature importances:\n{0}".format(np.round(dt_clf.feature_importances_, 3)))

#feature별 importance 매핑
for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_):
    print('{0} : {1:.3f}'.format(name, value))
    
#feautre importance를 column 별로 시각화하기
sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names)
```

![image](https://user-images.githubusercontent.com/76269316/126091213-0fcda50c-6954-4bf2-b9c7-bc961bea4864.png)

petal_length가 피처 중요도가 가장 높은 것을 볼 수 있습니다.



##### 결정 트리 과적합(Overfitting)

결정 트리의 과적합 문제를 시각화해 알아보겠습니다.



분류를 위해 2개의 피처, 3개의 클래스 값을 갖는 임의의 데이터 세트를 생성했습니다.

```python
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
%matplotlib inline

plt.title("3 Class values with 2 Features Sample data creation")

#2차원 시각화를 위해 피처는 2개, 3가지 유형의 클래스 분류 샘플 데이터 생성
X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1, random_state=0)

#그래프 형태로 2개의 피처로 2차원 좌표 시각화 (각 클래스 값은 다른 색깔로 표시됨)
plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, edgecolor='k')  #marker : marker style, c : color for marker, s : marker size, edgecolor : marker border color
```



```python
X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1, random_state=0)
```

make_classification() 호출 시 피처 데이터 세트와 클래스 레이블 데이터 세트를 반환하기 때문에 이를 각각 X_features, y_labels로 전달 받았고,

파라미터는 다음을 의미합니다.

- n_features : 독립 변수의 수 (전체 피처의 수)
- n_redundant : 독립 변수 중 다른 독립 변수의 선형 조합으로 나타나는 성분의 수
- n_informative : 독립 변수 중 종속 변수와 상관관계가 있는 성분의 수
- n_classes : 종속 변수의 클래스 수
- n_clusters_per_class : 클래스당 클러스터 수

책에는 파라미터에 대한 언급이 없는데, 그냥 2개의 피처 3개의 클래스 값을 갖는 데이터 세트를 생성하는 코드구나하고 넘어가시면 될 것 같습니다.

![image](https://user-images.githubusercontent.com/76269316/126092088-8017a03a-5330-4a7d-a7df-8f7dbdd718a6.png)



X_features와 y_label 데이터 세트를 기반으로 별다른 제약 없이(하이퍼 파라미터를 디폴트로) 결정트리를 학습한 뒤, 결정 트리 모델이 어떠한 결정 기준을 갖고 분할하면서 데이터를 분류하는지 색상과 경계로 나타내 확인해보겠습니다.

이를 위해 visualize_boundary 메소드를 사용했습니다.

**visualize_boundary()**

```python
import numpy as np

# Classifier의 Decision Boundary를 시각화 하는 함수
def visualize_boundary(model, X, y):
    fig,ax = plt.subplots()
    
    # 학습 데이타 scatter plot으로 나타내기
    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',
               clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim_start , xlim_end = ax.get_xlim()
    ylim_start , ylim_end = ax.get_ylim()
    
    # 호출 파라미터로 들어온 training 데이타로 model 학습 . 
    model.fit(X, y)
    # meshgrid 형태인 모든 좌표값으로 예측 수행. 
    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    
    # contourf() 를 이용하여 class boundary 를 visualization 수행. 
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3,
                           levels=np.arange(n_classes + 1) - 0.5,
                           cmap='rainbow', clim=(y.min(), y.max()),
                           zorder=1)
```

```python
from sklearn.tree import DecisionTreeClassifier

#특정한 트리 생성 제약 없이 결정 트리 학습, 결정 경계 시각화
dt_clf = DecisionTreeClassifier().fit(X_features, y_labels)
visualize_boundary(dt_clf, X_features, y_labels)
```

![image](https://user-images.githubusercontent.com/76269316/126092593-5581ccaf-0d02-49e3-b54a-9b605b02b21f.png)

일부 이상치(Outlier) 데이터까지 분류하기 위해 분할이 일어나서 결정 기준 경계가 많아졌습니다.

결정 트리의 기본 하이퍼 파라미터 설정은 리프 노드 안의 데이터가 모두 균일하거나 하나만 존재해야 하는 엄격한 분할 기준을 갖기 때문입니다.

이렇게 복잡한 모델은 학습 데이터 세트의 특성과 약간만 다른 형태의 데이터 세트를 예측하면 예측 정확도가 떨어지게 됩니다.



이번에는 min_samples_leaf = 6으로 설정해 6개 이하의 데이터는 리프 노드가 될 수 있도록 규칙을 완화해보겠습니다.

```python
from sklearn.tree import DecisionTreeClassifier

#특정한 트리 생성 제약 없이 결정 트리 학습, 결정 경계 시각화
dt_clf = DecisionTreeClassifier(min_samples_leaf=6).fit(X_features, y_labels)
visualize_boundary(dt_clf, X_features, y_labels)
```

<img src="https://user-images.githubusercontent.com/76269316/126092720-a4865621-20d2-4f79-99f0-742356b50ee8.png" alt="image" style="zoom: 200%;" />

이상치에 크게 반응하지 않으면서 좀 더 일반화된 분류 규칙에 따라 분류됐습니다.

테스트 데이터 세트에서의 예측 성능은 min_samples_leaf=6으로 생성한 모델이 더 뛰어나게 됩니다.



##### 결정 트리 실습 - 사용자 행동 인식 데이터 세트

결정 트리를 이용해 UCI Machine Learning Repository에서 제공하는 사용자 행동 인식(Human Activity Recognition) 데이터 세트에 대한 예측 분류를 수행해보겠습니다.

해당 데이터는 30명에게 스마트폰 센서를 장착한 다음 사람의 동작과 관련된 여러 피처를 수집한 데이터입니다.



[Human Activity Recognition Using Smartphones Data Set](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/)

위 링크에서 UCI HAR Dataset.zip을 다운받으면 됩니다.

![image](https://user-images.githubusercontent.com/76269316/126093256-1bed4ed1-2143-4fd2-a5fe-dbc90f8f0fba.png)



위 파일은 다운받고 압축을 푼 다음 디렉토리명을 human_activity로 변경합니다.

그런 다음 해당 디렉토리를 jupyter notebook 파일 디렉토리 (C:\Users\사용자명)로 옮깁니다.

저는 챕터별로 관리하고 있어서, C:\Users\MinseokSeo\chapter4에 저장했습니다.

![image](https://user-images.githubusercontent.com/76269316/126093575-5fd5366b-2dff-41a6-b426-dad1397555f7.png)



![image](https://user-images.githubusercontent.com/76269316/126093679-68404672-8fc0-46c1-86f9-83dfb81b63ae.png)

해당 디렉토리에는 다음 파일들이 저장돼 있습니다.

README.txt와 features_info.txt에는 데이터 세트와 피처에 대한 간략한 설명이 적혀 있습니다.

features.txt에는 피처 이름이 기술돼 있습니다. 

activity_labels.txt는 동작 레이블 값에 대한 설명이 있습니다.

train, test 디렉토리에는 학습·테스트 용도의 피처 데이터 세트, 레이블 데이터 세트가 들어있습니다.



features.txt  파일을 DataFrame으로 로딩해 피처명만 확인해보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126093878-20f592f4-1d20-4e3c-886e-f1421e66ea9f.png)



```python
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

#features.txt 파일을 DataFrame으로 로드
feature_name_df = pd.read_csv('./human_activity/features.txt', sep='\s+', header=None, names=['column_index', 'column_name'])  #sep='\s+' : 한 개 이상의 공백 구분자로 구분

#피처명 index르 ㄹ제거하고, 피처명만 리스트 객체로 생성한 뒤 10개만 출력
feature_name = feature_name_df.iloc[:, 1].values.tolist()
print('전체 피처명에서 10개만 추출:', feature_name[:10])
```

![image](https://user-images.githubusercontent.com/76269316/126094297-88bc5da2-0ecb-4248-9439-5d93d516ddb6.png)



인체의 움직임과 관련된 속성의 평균/표준편차가 X, Y, Z 값으로 돼 있음을 볼 수 있습니다.



중복된 피처명이 있는지 확인해보겠습니다.

```python
feature_dup_df = feature_name_df.groupby('column_name').count()  #column_name으로 그룹핑한 다음 카운팅
print(feature_dup_df[feature_dup_df['column_index'] > 1].count())  #column_index가 1 이상인 것만 카운팅
feature_dup_df[feature_dup_df['column_index'] > 1].head()  #맨 위 5개만 출력
```

![image](https://user-images.githubusercontent.com/76269316/126095319-9b06fe00-ce27-44ba-b4df-81f836b7bf10.png)

총 42개의 피처명이 중복된 것을 확인할 수 있습니다.

이 중복된 피처명들은 _1, _2를 추가해 중복을 없애겠습니다.



**get_new_feature_name_df(old_feature_name_df)**

```python
def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt'])
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0] + '_' + str(x[1])
                                                                                              if x[1] > 0 else x[0], axis=1)
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df
```

```python
import pandas as pd

def get_human_dataset():
    
    #features.txt 파일을 DataFrame으로 로드
    feature_name_df = pd.read_csv('./human_activity/features.txt', sep='\s+', header=None, names=['column_index', 'column_name'])  #sep='\s+' : 한 개 이상의 공백 구분자로 구분
    
    #중복된 피처명을 수정, 신규 피처명 DataFrame 생성
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
    
    #DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
    
    #학습 피처와 테스트 피처 데이터를 DataFrame으로 로드 (column명 feature_name 적용)
    X_train = pd.read_csv('./human_activity/train/X_train.txt', sep='\s+', names=feature_name)
    X_test = pd.read_csv('./human_activity/test/X_test.txt', sep='\s+', names=feature_name)
    
    #학습 레이블과 테스트 레이블 데이터를 DataFrame으로 로드 (column명 action으로 적용)
    y_train = pd.read_csv('./human_activity/train/y_train.txt', sep='\s', header=None, names=['action'])
    y_test = pd.read_csv('./human_activity/test/y_test.txt', sep='\s', header=None, names=['action'])
    
    #로드된 학습/테스트용 DataFrame 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()
```
