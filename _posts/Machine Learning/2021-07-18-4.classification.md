---
title:  "4.Classification"
excerpt: "분류"
toc: true
toc_label: "Classification"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - 분류
  - 파이썬 머신러닝 완벽 가이드
last_modified_at: 2021-07-24

---

>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.


### 분류의 개요

지도학습은 명시적인 정답(label)이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식입니다.

지도학습의 대표적인 유형인 분류(Classification)는 학습 데이터로 주어진 데이터의 피처와 레이블값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 

이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것입니다.

**즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 학습한 다음 새롭게 관측된 데이터에 대한 레이블을 판별하는 것입니다.**



분류는 다양한 머신러닝 알고리즘으로 구현할 수 있습니다.

- 베이즈(Bayes) 통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)
- 독립변수와 종속변수의 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression)
- 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)
- 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine)
- 근접 거리를 기준으로 하는 최소 근접(Nearest Neighbor) 알고리즘
- 심층 연결 기반의 신경망 (Neural Network)
- 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)



### 결정 트리(Decision Tree)

결정 트리는 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 알고리즘입니다.

데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우합니다.



아래 그림은 결정 트리의 구조를 간략하게 나타낸 것입니다.

규칙 노드(Decision Node)로 표시된 노드는 규칙 조건이 되고, 새로운 규칙 조건마다 서브 트리(Sub Tree)가 생성됩니다.

리프 노드(Leaf Node)로 표시된 노드는 결정된 클래스 값입니다.

![image](https://user-images.githubusercontent.com/76269316/126070248-adec61f6-9f8e-44cb-a288-9a5abb278598.png)



데이터 세트에는 피처가 있고 피처가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어집니다.

하지만 많은 규칙이 있다는 건 분류를 결정하는 방식이 복잡해진다는 것이고, 이는 과적합으로 이어지게 됩니다.

즉, 트리의 깊이(depth)가 깊어질수록 결정 트리의 예측 성능이 저하되게 됩니다.

따라서 가능한 한 적은 결정 노드로 높은 예측 정확도를 가져야 하는데 이를 위해서는, 최대한 균일한 데이터 세트를 구성할 수 있도록  분할(Split)하는 것이 필요합니다.



균일한 데이터 세트가 갖는 의미에 대해 알아보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126084685-9a654ae8-37ed-4f7b-8a8c-06028853d29e.png)



다음 데이터 세트를 균일한 순서로 나열하면 C → B → A 입니다.

C의 경우 모두 검은 공으로 구성되므로 데이터가 모두 균일하고,

B의 경우는 일부 하얀 공을 갖고 있지만, 대부분 검은 공으로 구성되어 다음으로 균일도가 높습니다.

A의 경우는 검은 공 못지않게 많은 하얀 공을 갖고 있어 균일도가 낮습니다.



눈을 가린 채 데이터 세트 C에서 하나의 데이터를 뽑았을 때 데이터에 대한 별다른 정보 없이도 검은 공이라고 쉽게 예측할 수 있는 반면,

A의 경우 상대적으로 혼잡도가 높고 균일도가 낮기 때문에 같은 조건에서 데이터를 판단하는데 있어 많은 정보가 필요합니다.



결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만들고, 다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트로 쪼개는 방식으로 내려가면서 반복하는 방식으로 데이터 값을 예측합니다.



예를 들어 박스 안에 30개의 레고 블록이 있을 때, 각 레고 블록이 갖는 속성은 다음과 같습니다. 

- 형태 : 동그라미, 네모, 세모

- 색깔 : 노랑, 빨강, 파랑



이 중 노랑색 블록은 모두 동그라미이고 빨강과 파랑 블록의 경우 동그라미, 네모, 세모가 골고루 섞여있다고 하면

각 레고 블록을 형태와 색깔 속성으로 분류하고자 할 때 가장 첫 번째로 만들어지는 규칙은 **if 색깔 == '노란색'**입니다.

왜냐하면 노란색 블록이면 모두 노란 동그라미 블록으로 예측할 수 있고, 그 다음 나머지 블록에 대해 다시 균일도 조건을 찾아 분류하는 것이 가장 효율적이기 때문입니다.



정보 균일도를 측정하는 방법으로 엔트로피를 이용한 정보 이득(Information Gain) 지수와 지니 계수가 있습니다.

- 정보 이득 : 엔트로피(주어진 데이터 집합의 혼잡도) 개념을 기반으로 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮게 되는데 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값입니다.
  결정 트리는 정보 이득 지수가 높은 속성을 기준으로 분할합니다.
- 지니 계수 : 경제학에서 불평등 지수를 나타낼 때 사용하는 계수로 0이 가장 평등하고 1로 갈수록 불평등합니다.
  머신러닝에 적용될 때는 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할합니다.



결정 트리 알고리즘을 사이킷런에서 구현한 DecisionTreeClassifier에서는 지니 계수를 이용해 데이터 세트를 분할합니다.

일반적인 결정 트리 알고리즘은 데이터 세트를 분할하는데 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정합니다.

![image](https://user-images.githubusercontent.com/76269316/126085195-1a8d5692-a2be-4e23-af48-965e10d5e980.png)





##### 결정 트리 모델의 특징

결정 트리의 장점은 정보의 '균일도'라는 룰을 기반으로 하고 있어 알고리즘이 쉽고 직관적이라는 것입니다.

룰이 매우 명확하고, 이에 기반해 어떻게 규칙 노드와 리프 노드가 만들어지는지 알 수 있고 시각화까지 할 수 있습니다.

또한 정보 균일도만 신경 쓰면 되므로 특별한 경우를 제외하고는 각 피처의 스케일링과 정규화 같은 전처리 작업이 필요 없습니다.



결정 트리의 단점은 과적합으로 정확도가 떨어진다는 것입니다.

학습 데이터 기반 모델의 정확도를 높이기 위해 모든 데이터 상황을 만족하는 완벽한 규칙을 만들려고 하게되고(그럴 수 없음에도 불구하고) 결국, 트리의 깊이가 깊어지고 트리가 복잡해져서 예측 성능이 떨어지게 됩니다.



따라서 모든 데이터 상황을 만족하는 완벽한 규칙은 만들 수 없다고 인정하고 트리의 크기를 사전에 제한하는 것이 성능 향상에 도움이 됩니다.

|                             장점                             |                             단점                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| - 직관적이다.<br />- 피처의 스케일리이나 정규화 등의 사전 가공 영향도가 크지 않다. | - 과적합으로 알고리즘 성능이 떨어진다.<br />이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝이 필요 |



##### 결정 트리 파라미터

사이킷런 결정 트리 알고리즘은 **CART(Classification And Regression Trees)** 기반으로 구현한 **DecisionTreeClassifier**와 **DecisionTreeRegressor** 클래스를 제공합니다. (CART : 분류뿐만 아니라 회귀에서도 사용될 수 있는 트리 알고리즘)

DecisionTreeClassifier는 분류를 위한 클래스이고, DecisionTreeRegressor는 회귀를 위한 클래스입니다.

|    파라미터명     |                             설명                             |
| :---------------: | :----------------------------------------------------------: |
| min_samples_split | 노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는데 사용됨.<br />디폴트는 2이고 작게 설정할수록 분할되는 노드가 많아져 과적합 가능성 증가 |
| min_samples_leaf  | 말단 노드(Leaf)가 되기 위한 최소한의 샘플 데이터 수<br />min_samples_split과 유사하게 과적합 제어 용도로 사용됨.<br />비대칭적(imbalanced) 데이터의 경우 특정 클래스 데이터가 극도로 작을 수 있으므로 작게 설정 필요 |
|   max_features    | 최적의 분할을 위해 고려할 최대 피처 개수. 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할 수행.<br />int 형으로 지정하면 대상 피처의 개수, float 형으로 지정하면 전체 피처 중 대상 피처의 퍼센트<br />sqrt는 <img src="https://user-images.githubusercontent.com/76269316/126085844-bc299bcc-3f39-4a67-845d-7d39a9f98854.png" alt="image" style="zoom: 50%;" />만큼 선정<br />auto로 지정하면 sqrt와 동일<br />log는 전체 피처 중 <img src="https://user-images.githubusercontent.com/76269316/126085922-9abdd74f-e044-4079-8be5-42eea3643b88.png" alt="image" style="zoom: 50%;" />만큼 선정<br />None은 전체 피처 선정 |
|     max_depth     | 트리의 최대 높이를 규정<br />디폴트는 None 완벽하게 클래스 결정 값이 결정 될 때까지 깊이를 계속 키우며 분할하거나 노드가 갖는 데이터 개수가 min_samples_split보다 작아질 때까지 계속 깊이를 증가시킴.<br />깊이가 깊어지면 min_samples_split 설정대로 최대 분할하여 과적합할 수 있으므로 적절한 값으로 제어 필요. |
|  max_leaf_nodes   |                 리프 노드(Leaf)의 최대 개수                  |



##### 결정 트리 모델의 시각화

Graphviz 패키지를 사용하면 결정 트리 알고리즘이 어떤 규칙을 갖고 트리를 생성하는지 시각적으로 확인할 수 있습니다.

Graphviz는 원래 그래프 기반 dot 파일로 기술된 다양한 이미지를 쉽게 시각화하는 패키지인데,  사이킷런에서 쉽게 인터페이스 할 수 있도록 export_graphviz() API를 제공합니다.



먼저 Graphviz를 윈도우에 설치하겠습니다. 

Graphviz는 C/C++로 운영체제에 포팅된 패키지여서 파이썬과 인터페이스 할 수 있는 파이썬 래퍼(Wrapper) 모듈을 별도로 설치해야합니다.

① [Graphviz](https://graphviz.org/download/)를 설치합니다.
![image](https://user-images.githubusercontent.com/76269316/126086257-ae733f38-5506-45eb-ab96-58bfa08bce31.png)

저는 윈도우10 64bit이기 때문에 빨간색 파일을 다운 받아 설치했습니다.



![image](https://user-images.githubusercontent.com/76269316/126086295-766a4b40-6fe6-4c3c-921c-b881a3a494c8.png)

저는 윈도우, 게임만 C드라이브에 깔고 나머지는 D드라이브에 설치하기  때문에 D드라이브에 설치했습니다.



② 파이썬 래퍼 모듈을 PIP를 이용해 설치합니다.

Anaconda 콘솔이나 OS command 콘솔에서 pip install graphviz 명령어로 설치합니다. (콘솔 실행시 관리자 권한으로 실행)

![image](https://user-images.githubusercontent.com/76269316/126086404-4a67074c-78a4-486f-88dd-72eb8264171d.png)

![image](https://user-images.githubusercontent.com/76269316/126086433-36a703cd-7191-43a6-8992-1f81aac9a7c2.png)



③ Graphviz와 파이썬 래퍼를 연결하기 위해 환경 변수 설정을 해야합니다.

내 PC → 속성 → 고급 시스템 설정 → 환경변수 클릭



먼저 사용자 변수의 Path 변수 → 편집 클릭

![image](https://user-images.githubusercontent.com/76269316/126086545-7f4bedc4-ffda-417c-9f61-d5c3f25a0c16.png)

D:\Program Files\Graphviz\bin 추가

![image](https://user-images.githubusercontent.com/76269316/126086580-c972ff26-0a95-4ff9-87f5-fb563708ae25.png)



시스템 변수의 Path 변수 → 편집 클릭

![image](https://user-images.githubusercontent.com/76269316/126086605-2c81bdd6-7efc-40f9-9eff-5d28e928093b.png)

D:\Program Files\Graphviz\bin\dot.exe 추가

![image](https://user-images.githubusercontent.com/76269316/126086623-1a6a4f67-cb88-46c0-a89c-4346845ff3c1.png)

이후 주피터 노트북을 재시작합니다. (환경 변수 Path를 재로딩하기 위해)



Graphviz를 이용해 붓꽃 데이터 세트에 결정 트리를 적용할 때 어떻게 서브 트리가 생성되고 만들어지는지 시각화해 보겠습니다.

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

#DecisionTreeClassifier 생성
dt_clf = DecisionTreeClassifier(random_state=156)

#붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train, y_train)

#export_graphviz() 호출 결과로 out_file로 지정된 tree.dot 파일 생성
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)  #impurity : gini 계수 출력 여부, filled : 노드 색깔을 다르게 출력
```

```python
import graphviz

#위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북에서 시각화
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

![image](https://user-images.githubusercontent.com/76269316/126087894-7f8ef93b-ebe6-4a3f-b50a-6ce8a2e9c467.png)

출력 결과를 보면, 각 규칙에 따라 트리의 브랜치(branch) 노드와 리프(leaf) 노드가 어떻게 구성돼 있는지 한눈에 확인할 수 있게 시각화 돼 있습니다.

각 노드의 색깔은 붓꽃 데이터의 레이블 값을 의미하고 (주황색 0 : Setosa, 초록색 1 : Versicolor, 보라색 2 : Virginica) 색이 짙어질수록 지니 계수가 낮습니다. (해당 레이블에 속하는 샘플 데이터가 많다는 의미)



노드 내 지표의 의미는 다음과 같습니다.

- petal length(cm) <= 2.45 : 자식 노드를 만들기 위한 규칙 조건입니다. (조건이 없으면 리프 노드)
- gini : 다음 value=[]로 주어진 데이터 분포에서의 지니 계수
- samples : 현 규칙에 해당하는 데이터 건수
- value : 클래스 값 기반의 데이터 건수
  붓꽃 데이터 세트는 클래스 값으로 0(Setosa), 1(Versicolor), 2(Virginica) 를 갖고 있는데 value = [41, 40, 39]라면 Setosa 41개, Versicolor 40개, Virginica 39개로 데이터가 구성돼 있다는 의미입니다.



맨 윗부분부터 자세히 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126088306-547d6007-7543-45f0-ad96-c9b8c2d4d42a.png)

먼저 루트 노드인 1번 노드의 지표입니다.

**1번 노드 지표**

- samples = 120 : 전체 데이터가 120개라는 의미
- value = [41, 40, 39] : Setosa 41개, Versicolor 40개, Virginica 39개로 데이터가 구성돼 있다는 의미
- gini = 0.667 : sample 120개가 value = [41, 40, 39] 분포도로 돼 있으므로 지니 계수는 0.667
- petal length(cm) <= 2.45 규칙으로 자식 노드 생성
- class = setosa : 하위 노드를 가질 경우 setosa 개수가 41개로 가장 많다는 의미



petal legnth(cm) <= 2.45 규칙으로 True인 데이터가 2번 노드, False인 데이터가 3번 노드가 만들어집니다.

2번 노드는 petal length(cm) <= 2.45가 True인 규칙 노드로 모든 데이터가 Setosa로 결정돼 리프 노드가 되고, 2번 노드에서는 더 이상 규칙이 생성되지 않습니다.

**2번 노드 지표**

- value = [41, 0, 0] : 샘플 데이터 모두 Setosa
- gini = 0.0



3번 노드는 petal length(cm) <= 2.45가 False인 규칙 노드로 지표는 다음과 같습니다.

**3번 노드 지표**

- value = [0, 40, 39] : 79개 샘플 데이터 중 Versicolor 40개, Virginica 39개
- gini = 0.5로 여전히 높으므로 다음 자식 노드 생성
- petal width(cm) <= 1.55 규칙으로 자식 노드 생성



4번 노드는 petal width(cm) <= 1.55가 True인 규칙 노드로 지표는 다음과 같습니다.

**4번 노드 지표 설명**

- value = [0, 37, 1] : 38개의 샘플 데이터 중 Versicolor 37개, Virginica 1개로 대부분 Versicolor
- gini = 0.051로 매우 낮으나 여전히 Versicolor와 Virginica가 혼재돼 있으므로 petal length(cm) <= 5.25라는 규칙으로 자식 노드 생성



5번 노드는 petal width(cm) <= 1.55가 False인 규칙 노드로 지표는 다음과 같습니다.

**5번 노드 지표**

- value = [0, 3, 38] : 41개의 샘플 데이터 중 Versicolor 3개, Virginica 38개로 대부분 Virginica
- gini = 0.136으로 낮으나 여전히 Versicolor와 Virginica가 혼재돼 있으므로 petal length(cm) <= 1.75라는 규칙으로 자식 노드 생성

 

4번 노드를 보면 Virginica가 1개 밖에 없지만 이 1개를 완벽히 분류하기 위해 또 다른 자식 노드를 생성하는 것을 볼 수 있습니다.

이로 인해 결국 매우 복잡한 규칙 트리가 만들어져 과적합 문제가 발생합니다.



때문에 [결정 트리 하이퍼 파라미터](https://seominseok4834.github.io/machine%20learning/4.classification/#%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0) 대부분이 복잡한 트리가 생성되는 것을 막기 위한 용도로 사용됩니다.



결정 트리의 하이퍼 파라미터 변경에 따른 트리의 변화를 살펴보겠습니다.

**max_depth**

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
import graphviz
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

#DecisionTreeClassifier 생성 (max_depth=3)
dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)

#붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train, y_train)

#export_graphviz() 호출 결과로 out_file로 지정된 tree.dot 파일 생성
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)  #impurity : gini 계수 출력 여부, filled : 노드 색깔을 다르게 출력

#위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북에서 시각화
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

|                     max_depth 제약 없음                      |                        max_depth = 3                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/126087894-7f8ef93b-ebe6-4a3f-b50a-6ce8a2e9c467.png) | ![image](https://user-images.githubusercontent.com/76269316/126089242-dd31136e-63b5-45ac-9476-4e92934255b3.png) |

트리의 깊이가 설정된 max_depth에 따라 줄어들어 더 간단한 결정 트리가 생성됐습니다.



**min_samples_split**

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
import graphviz
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

#DecisionTreeClassifier 생성 (min_samples_split=4)
dt_clf = DecisionTreeClassifier(random_state=156, min_samples_split=4)

#붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train, y_train)

#export_graphviz() 호출 결과로 out_file로 지정된 tree.dot 파일 생성
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)  #impurity : gini 계수 출력 여부, filled : 노드 색깔을 다르게 출력

#위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북에서 시각화
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

![image](https://user-images.githubusercontent.com/76269316/126089748-f8baec5e-8758-48d7-a8b8-f9837263434f.png)

빨간색 네모 박스를 보면, 샘플 데이터 3개 중 1개가 다른 클래스 값이 들어가 있지만, min_samples_split=4로 지정(샘플 데이터가 최소 4개 이상 있어야지만 자식 노드 생성)해놨기 때문에 자식 노드를 생성하지 않는 것을 볼 수 있습니다.



**min_samples_leaf**

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import export_graphviz
import graphviz
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

#DecisionTreeClassifier 생성 (min_samples_leaf=4)
dt_clf = DecisionTreeClassifier(random_state=156, min_samples_leaf=4)

#붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리
iris_data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)

#DecisionTreeClassifier 학습
dt_clf.fit(X_train, y_train)

#export_graphviz() 호출 결과로 out_file로 지정된 tree.dot 파일 생성
export_graphviz(dt_clf, out_file="tree.dot", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)  #impurity : gini 계수 출력 여부, filled : 노드 색깔을 다르게 출력

#위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북에서 시각화
with open("tree.dot") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

![image](https://user-images.githubusercontent.com/76269316/126090410-7a83d930-9861-4e76-bf82-d4a07108e496.png)

min_samples_leaf는 리프 노드가 될 수 있는 샘플 데이터 건수의 최솟값을 지정합니다. (default는 1)

위 코드에서는 min_samples_leaf를 4로 지정했기 때문에 지니 계수 값이 크더라도 (다른 클래스 값이 섞여 있더라도) 샘플 데이터가 4개 이하인 경우 리프 노드가 되게 되어 결정 트리가 간결해지게 됩니다.



결정 트리 알고리즘이 학습을 통해 규칙을 정하는데 있어 피처의 중요한 역할 지표를 feature_importances_ 속성으로 확인할 수 있습니다.

feature_importances_는 ndarray 형태로 피처 중요도가 반환됩니다.

```python
import seaborn as sns
import numpy as np
%matplotlib inline

#feature importance 추출
print("Feature importances:\n{0}".format(np.round(dt_clf.feature_importances_, 3)))

#feature별 importance 매핑
for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_):
    print('{0} : {1:.3f}'.format(name, value))
    
#feautre importance를 column 별로 시각화하기
sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names)
```

![image](https://user-images.githubusercontent.com/76269316/126091213-0fcda50c-6954-4bf2-b9c7-bc961bea4864.png)

petal_length가 피처 중요도가 가장 높은 것을 볼 수 있습니다.



##### 결정 트리 과적합(Overfitting)

결정 트리의 과적합 문제를 시각화해 알아보겠습니다.



분류를 위해 2개의 피처, 3개의 클래스 값을 갖는 임의의 데이터 세트를 생성했습니다.

```python
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
%matplotlib inline

plt.title("3 Class values with 2 Features Sample data creation")

#2차원 시각화를 위해 피처는 2개, 3가지 유형의 클래스 분류 샘플 데이터 생성
X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1, random_state=0)

#그래프 형태로 2개의 피처로 2차원 좌표 시각화 (각 클래스 값은 다른 색깔로 표시됨)
plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, edgecolor='k')  #marker : marker style, c : color for marker, s : marker size, edgecolor : marker border color
```



```python
X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1, random_state=0)
```

make_classification() 호출 시 피처 데이터 세트와 클래스 레이블 데이터 세트를 반환하기 때문에 이를 각각 X_features, y_labels로 전달 받았고,

파라미터는 다음을 의미합니다.

- n_features : 독립 변수의 수 (전체 피처의 수)
- n_redundant : 독립 변수 중 다른 독립 변수의 선형 조합으로 나타나는 성분의 수
- n_informative : 독립 변수 중 종속 변수와 상관관계가 있는 성분의 수
- n_classes : 종속 변수의 클래스 수
- n_clusters_per_class : 클래스당 클러스터 수

책에는 파라미터에 대한 언급이 없는데, 그냥 2개의 피처 3개의 클래스 값을 갖는 데이터 세트를 생성하는 코드구나하고 넘어가시면 될 것 같습니다.

![image](https://user-images.githubusercontent.com/76269316/126092088-8017a03a-5330-4a7d-a7df-8f7dbdd718a6.png)



X_features와 y_label 데이터 세트를 기반으로 별다른 제약 없이(하이퍼 파라미터를 디폴트로) 결정트리를 학습한 뒤, 결정 트리 모델이 어떠한 결정 기준을 갖고 분할하면서 데이터를 분류하는지 색상과 경계로 나타내 확인해 보겠습니다.

이를 위해 visualize_boundary 메소드를 사용했습니다.

**visualize_boundary()**

```python
import numpy as np

# Classifier의 Decision Boundary를 시각화 하는 함수
def visualize_boundary(model, X, y):
    fig,ax = plt.subplots()
    
    # 학습 데이타 scatter plot으로 나타내기
    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',
               clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim_start , xlim_end = ax.get_xlim()
    ylim_start , ylim_end = ax.get_ylim()
    
    # 호출 파라미터로 들어온 training 데이타로 model 학습 . 
    model.fit(X, y)
    # meshgrid 형태인 모든 좌표값으로 예측 수행. 
    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    
    # contourf() 를 이용하여 class boundary 를 visualization 수행. 
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3,
                           levels=np.arange(n_classes + 1) - 0.5,
                           cmap='rainbow', clim=(y.min(), y.max()),
                           zorder=1)
```

```python
from sklearn.tree import DecisionTreeClassifier#특정한 트리 생성 제약 없이 결정 트리 학습, 결정 경계 시각화dt_clf = DecisionTreeClassifier().fit(X_features, y_labels)visualize_boundary(dt_clf, X_features, y_labels)
```

![image](https://user-images.githubusercontent.com/76269316/126092593-5581ccaf-0d02-49e3-b54a-9b605b02b21f.png)

일부 이상치(Outlier) 데이터까지 분류하기 위해 분할이 일어나서 결정 기준 경계가 많아졌습니다.

결정 트리의 기본 하이퍼 파라미터 설정은 리프 노드 안의 데이터가 모두 균일하거나 하나만 존재해야 하는 엄격한 분할 기준을 갖기 때문입니다.

이렇게 복잡한 모델은 학습 데이터 세트의 특성과 약간만 다른 형태의 데이터 세트를 예측하면 예측 정확도가 떨어지게 됩니다.



이번에는 min_samples_leaf = 6으로 설정해 6개 이하의 데이터는 리프 노드가 될 수 있도록 규칙을 완화해 보겠습니다.

```python
from sklearn.tree import DecisionTreeClassifier#특정한 트리 생성 제약 없이 결정 트리 학습, 결정 경계 시각화dt_clf = DecisionTreeClassifier(min_samples_leaf=6).fit(X_features, y_labels)visualize_boundary(dt_clf, X_features, y_labels)
```

<img src="https://user-images.githubusercontent.com/76269316/126092720-a4865621-20d2-4f79-99f0-742356b50ee8.png" alt="image" style="zoom: 200%;" />

이상치에 크게 반응하지 않으면서 좀 더 일반화된 분류 규칙에 따라 분류됐습니다.

테스트 데이터 세트에서의 예측 성능은 min_samples_leaf=6으로 생성한 모델이 더 뛰어나게 됩니다.



##### 결정 트리 실습 - 사용자 행동 인식 데이터 세트

결정 트리를 이용해 UCI Machine Learning Repository에서 제공하는 사용자 행동 인식(Human Activity Recognition) 데이터 세트에 대한 예측 분류를 수행해 보겠습니다 .

해당 데이터는 30명에게 스마트폰 센서를 장착한 다음 사람의 동작과 관련된 여러 피처를 수집한 데이터입니다.



[Human Activity Recognition Using Smartphones Data Set](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/)

위 링크에서 UCI HAR Dataset.zip을 다운받으면 됩니다.

![image](https://user-images.githubusercontent.com/76269316/126093256-1bed4ed1-2143-4fd2-a5fe-dbc90f8f0fba.png)



위 파일은 다운받고 압축을 푼 다음 디렉토리명을 human_activity로 변경합니다.

그런 다음 해당 디렉토리를 jupyter notebook 파일 디렉토리 (C:\Users\사용자명)로 옮깁니다.

저는 챕터별로 관리하고 있어서, C:\Users\MinseokSeo\chapter4에 저장했습니다.

![image](https://user-images.githubusercontent.com/76269316/126093575-5fd5366b-2dff-41a6-b426-dad1397555f7.png)



![image](https://user-images.githubusercontent.com/76269316/126093679-68404672-8fc0-46c1-86f9-83dfb81b63ae.png)

해당 디렉토리에는 다음 파일들이 저장돼 있습니다.

README.txt와 features_info.txt에는 데이터 세트와 피처에 대한 간략한 설명이 적혀 있습니다.

features.txt에는 피처 이름이 기술돼 있습니다. 

activity_labels.txt는 동작 레이블 값에 대한 설명이 있습니다.

train, test 디렉토리에는 학습·테스트 용도의 피처 데이터 세트, 레이블 데이터 세트가 들어있습니다.



features.txt  파일을 DataFrame으로 로딩해 피처명만 확인해 보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126093878-20f592f4-1d20-4e3c-886e-f1421e66ea9f.png)



```python
import pandas as pdimport matplotlib.pyplot as plt
%matplotlib inline

#features.txt 파일을 DataFrame으로 로드
feature_name_df = pd.read_csv('./human_activity/features.txt', sep='\s+', header=None, names=['column_index', 'column_name'])  #sep='\s+' : 한 개 이상의 공백 구분자로 구분
#피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 10개만 출력
feature_name = feature_name_df.iloc[:, 1].values.tolist()print('전체 피처명에서 10개만 추출:', feature_name[:10])
```

![image](https://user-images.githubusercontent.com/76269316/126094297-88bc5da2-0ecb-4248-9439-5d93d516ddb6.png)



인체의 움직임과 관련된 속성의 평균/표준편차가 X, Y, Z 값으로 돼 있음을 볼 수 있습니다.



중복된 피처명이 있는지 확인해 보겠습니다.

```python
feature_dup_df = feature_name_df.groupby('column_name').count()  #column_name으로 그룹핑한 다음 카운팅
print(feature_dup_df[feature_dup_df['column_index'] > 1].count())  #column_index가 1 이상인 것만 카운팅
feature_dup_df[feature_dup_df['column_index'] > 1].head()  #맨 위 5개만 출력
```

![image](https://user-images.githubusercontent.com/76269316/126095319-9b06fe00-ce27-44ba-b4df-81f836b7bf10.png)

총 42개의 피처명이 중복된 것을 확인할 수 있습니다.

이 중복된 피처명들은 _1, _2를 추가해 중복을 없애겠습니다.



**get_new_feature_name_df(old_feature_name_df)**

```python
def get_new_feature_name_df(old_feature_name_df):
    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt'])
    feature_dup_df = feature_dup_df.reset_index()
    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0] + '_' + str(x[1]) if x[1] > 0 else x[0], axis=1)
    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)
    return new_feature_name_df
```



한 줄 한 줄 보면,

```python
feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt'])
```

![image](https://user-images.githubusercontent.com/76269316/126112253-c0414fd2-3768-4893-951e-1b7f3cfb7aff.png)

'column_name'으로 그룹핑 한 다음, 카운팅한 DataFrame의 column을 dup_cnt로 변경합니다.



```python
feature_dup_df = feature_dup_df.reset_index()
```

feature_dup_df의 기존 인덱스(빨간 네모 박스)를 index라는 column으로 추가합니다.

<img src="https://user-images.githubusercontent.com/76269316/126112529-b22a887e-9967-45bb-b578-1f2021f30bd5.png" alt="image" style="zoom:67%;" />



```python
new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')
```

|              old_feature_name_df.reset_index()               |                        feature_dup_df                        |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/126112922-e9d0962b-d094-4054-a976-cc22b3a49c5a.png) | ![image](https://user-images.githubusercontent.com/76269316/126112940-43a41ba9-ed67-462e-89b0-78243cb3b4af.png) |

두 DataFrame을 [outerjoin](https://seominseok4834.github.io/database/6.bags-and-extended-relational-algebra/#outerjoins)합니다.

![image](https://user-images.githubusercontent.com/76269316/126113206-127ac687-3c99-4912-bdde-d4843cc4ee97.png)

↑ outerjoin한 결과



```python
new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0] + '_' + str(x[1]) if x[1] > 0 else x[0], axis=1)
```

x[1]\(dup_cnt)가 0보다 큰 경우 (column명이 중복된 경우)에만 _1, _2를 붙이고 그렇지 않은 경우 그대로 사용합니다.



```python
new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)return new_feature_name_df
```

![image](https://user-images.githubusercontent.com/76269316/126113711-9c30a5fc-4531-46d9-975f-23b371888ba3.png)

'index' column을 drop한 뒤, return 합니다.



사용자 행동 인식 데이터 세트를 좀 더 간단하게 사용하기 위해 get_human_dataset()을 생성했습니다.

```python
import pandas as pd

def get_human_dataset():
    
    #features.txt 파일을 DataFrame으로 로드
    feature_name_df = pd.read_csv('./human_activity/features.txt', sep='\s+', header=None, names=['column_index', 'column_name'])  #sep='\s+' : 한 개 이상의 공백 구분자로 구분
    
    #중복된 피처명을 수정, 신규 피처명 DataFrame 생성
    new_feature_name_df = get_new_feature_name_df(feature_name_df)
    
    #DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 변환
    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()
    
    #학습 피처와 테스트 피처 데이터를 DataFrame으로 로드 (column명 feature_name 적용)
    X_train = pd.read_csv('./human_activity/train/X_train.txt', sep='\s+', names=feature_name)
    X_test = pd.read_csv('./human_activity/test/X_test.txt', sep='\s+', names=feature_name)
    
    #학습 레이블과 테스트 레이블 데이터를 DataFrame으로 로드 (column명 action으로 적용)
    y_train = pd.read_csv('./human_activity/train/y_train.txt', sep='\s+', header=None, names=['action'])
    y_test = pd.read_csv('./human_activity/test/y_test.txt', sep='\s+', header=None, names=['action'])
    
    #로드된 학습/테스트용 DataFrame 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_human_dataset()
```



학습용 피처 데이터 세트를 간략히 살펴보면,

```python
print('학습 피처 데이터셋 info()')print(X_train.info())
```

![image](https://user-images.githubusercontent.com/76269316/126114308-d17506c4-6ac2-43e5-8970-cf4b322b3355.png)

학습 데이터 세트는 7532개의 레코드와 561개의 피처를 갖고 있고, 전부 float형이므로 별도의 카테고리 인코딩을 수행할 필요가 없습니다.



레이블 값은 1, 2, 3, 4, 5, 6의 6개 값이고 특정 값으로 왜곡되지 않고 비교적 고르게 분포돼 있습니다.

```python
print(y_train['action'].value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/126114473-63832797-33f8-4f04-8740-de7dece8e57c.png)



사이킷런 DecisionTreeClassifier를 이용해 동작 예측 분류를 수행해 보겠습니다.

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

#예제 반복 시마다 동일한 예측 결과 도출을 위해 random_state 설정
dt_clf = DecisionTreeClassifier(random_state=156)
dt_clf.fit(X_train, y_train)
pred = dt_clf.predict(X_test)
accuracy = accuracy_score(y_test, pred)
print('결정 트리 예측 정확도 : {0:.4f}'.format(accuracy))

#DecisionTreeClassifier의 하이퍼 파라미터 추출
print('DecisionTreeClassifier 기본 하이퍼 파라미터\n', dt_clf.get_params())
```

![image](https://user-images.githubusercontent.com/76269316/126116188-4297f658-a304-4d37-baf8-5e6330642b9d.png)

약 85.48%의 정확도를 갖습니다.



트리 깊이(Tree Depth)가 예측 정확도에 미치는 영향을 보기위해, GridSearchCV를 이용해 max_depth 값을 변화시키면서 예측 성능을 확인해 보겠습니다.

+[GridSearchCV](https://seominseok4834.github.io/machine%20learning/2.scikit-learn-machine-learning-in-python/#gridsearchcv---%EA%B5%90%EC%B0%A8-%EA%B2%80%EC%A6%9D%EA%B3%BC-%EC%B5%9C%EC%A0%81-%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%ED%8A%9C%EB%8B%9D%EC%9D%84-%ED%95%9C-%EB%B2%88%EC%97%90) : 하이퍼 파라미터를 순차적으로 적용하면서 최적의 파라미터를 찾을 수 있게 해주는 API



```python
from sklearn.model_selection import GridSearchCV

params = {
    'max_depth' : [6, 8, 10, 12, 16, 20, 24]
}

grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1)  #verbose : GridSearchCV iteration마다 수행 결과 메시지 출력
grid_cv.fit(X_train, y_train)
print('GridSearchCV 최고 평균 정확도 수치: {0:.4f}'.format(grid_cv.best_score_))
print('GridSearchCV 최적 하이퍼 파라미터: ', grid_cv.best_params_)
```

![image](https://user-images.githubusercontent.com/76269316/126117392-bef4b39e-9a84-4199-88c8-66d51821d41d.png)

verbose=1로 설정하면 iteration 수행마다 이렇게 수행 결과 메시지가 출력됩니다.

![image](https://user-images.githubusercontent.com/76269316/126117691-9a19c6cc-363a-41a7-8790-5a241e90f870.png)

실행 결과 최고 평균 정확도가 약 85.26%가 도출됐습니다.



max_depth 값에 따라 검증용 데이터 세트(별도의 테스트 데이터 세트가 아님)의 정확도가 어떻게 변했는지 cv_results_ 속성을 통해 알아보겠습니다.

```python
#GridSearchCV 객체의 cv_results_ 속성을 DataFrame으로 생성
cv_results_df = pd.DataFrame(grid_cv.cv_results_)

#max_depth 파라미터 값과 정확도 수치 추출
cv_results_df[['param_max_depth', 'mean_test_score']]
```

![image](https://user-images.githubusercontent.com/76269316/126118297-bfeef3e8-1ab8-4584-8e11-e845465fc0fb.png)

max_depth가 8일 때 0.852로 정확도가 정점이고, 이를 넘어가면서 정확도가 계속 떨어지는 것을 볼 수 있습니다.



이번에는 별도의 테스트 데이터 세트에서 max_depth 변화에 따른 정확도 값을 확인해 보겠습니다.

```python
max_depths = [6, 8, 10, 12, 16, 20, 24]

#max_depth 값을 변화시키면서 그 때마다 학습과 테스트 세트에서의 예측 성능 측정
for depth in max_depths:    
    dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156)    
    dt_clf.fit(X_train, y_train)    
    pred = dt_clf.predict(X_test)    
    accuracy = accuracy_score(y_test, pred)    
    print('max_depth = {0} 정확도: {1:.4f}'.format(depth, accuracy))
```

max_depth가 8일 때 약 87.07%로 가장 높은 정확도를 나타냈습니다.

마찬가지로 8을 넘어가면서 정확도가 계속 감소하는 것을 확인할 수 있습니다.

![image](https://user-images.githubusercontent.com/76269316/126118894-87aa643a-8c29-4ef2-93f6-a32e4dae6d64.png)



이처럼 결정 트리는 깊이가 깊어질수록 학습 데이터 세트에는 올바른 예측 결과를 가져오지만, 검증 데이터 세트에서는 오히려 과적합으로 인한 성능 저하를 유발합니다.

복잡한 모델보다는 트리 깊이를 낮춘 단순한 모델이 더욱 효과적인 결과를 가져옵니다.



이번에는 max_depth와 min_samples_split을 같이 변경하면서 성능을 튜닝해 보겠습니다.

```python
params = {
    'max_depth' : [8, 12, 16, 20],
    'min_samples_split' : [16, 24],
}

grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1)
grid_cv.fit(X_train, y_train)
print('GridSearchCV 최고 평균 정확도 수치: {0:.4f}'.format(grid_cv.best_score_))
print('GridSearchCV 최적 하이퍼 파라미터: ', grid_cv.best_params_)
```

![image](https://user-images.githubusercontent.com/76269316/126119709-2eea3a4f-1692-4beb-83f3-af00fa7eb3f0.png)



```python
best_df_clf = grid_cv.best_estimator_
pred1 = best_df_clf.predict(X_test)
accuracy = accuracy_score(y_test, pred1)
print('결정 트리 예측 정확도: {0:.4f}'.format(accuracy))
```

최적 하이퍼 파라미터(max_depth=8, min_samples_split=16)로 학습된 Estimator 객체를 통해 테스트 데이터 세트 예측을 수행하면 약 87.17%가 나옵니다.

![image](https://user-images.githubusercontent.com/76269316/126120168-2b946ac5-cba0-4295-aeba-0fc605ea4aeb.png)



결정 트리에서 각 피처 중요도를 feature_importances_ 속성을 통해 확인할 수 있습니다.

```python
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

ftr_importances_values = best_df_clf.feature_importances_
#Top 중요도로 정렬을 쉽게 하고, Seaborn의 막대 그래프로 쉽게 표현하기 위해 Series로 변환
ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns)
#중요도값 순으로 Series 정렬
ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]  #내림차순 정렬
plt.figure(figsize=(8, 6))
plt.title('Feature importances Top 20')
sns.barplot(x=ftr_top20, y=ftr_top20.index)
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/126132373-c864aa10-a008-4494-902f-e611bec4e4cc.png)



### 앙상블 학습(Ensemble Learning)

앙상블 학습을 통한 분류는 여러 개의 분류기(Classifier)를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법을 의미합니다.

이미지, 영상, 음성 등의 비정형 데이터 분류는 딥러닝이 뛰어난 성능을 보이고 있고, 대부분의 정형 데이터 분류 시에는 앙상블이 뛰어난 성능을 나타내고 있습니다.



앙상블 학습의 유형은 Voting, Bagging, Boosting, Stacking등이 있습니다.

- Voting : 서로 다른 알고리즘 분류기 중 투표를 통해 최종 예측 결과를 결정하는 방식
- Bagging : 모두 같은 유형(대부분 결정 트리 알고리즘)의 알고리즘 분류기 중 투표를 통해 최종 예측 결과를 결정하는 방식

|                         Voting 방식                          |                         Bagging 방식                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/126136297-8e955c5e-47bc-43ad-9064-58465e4e8255.png) | ![image](https://user-images.githubusercontent.com/76269316/126136399-5b72fac0-cc09-4173-8d64-5da99454310b.png) |

Voting과 Bagging 방식은 학습하는 데이터 세트가 다릅니다.

Bagging 방식은 원본 학습 데이터를 샘플링해 추출한 다음, 개별 분류기에 할당해서 학습하는데 이렇게 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식을 **Bootstrapping 분할 방식**이라고 합니다.

개별 분류기가 Bootstrapping 방식으로 샘플링된 데이터 세트로 학습한 다음 예측을 수행한 결과를 Voting을 통해 최종 예측 결과를 선정하는 방식이 Bagging Ensemble 방식입니다.

교차 검증이 데이터 세트 간에 중첩을 허용하지 않는 것과 다르게 Bagging 방식은 중첩을 허용합니다. (10,000개의 데이터를 10개의 분류기가 배깅 방식으로 나누더라도 각 1000개의 데이터 내에는 중복 데이터가 있음)



- Boosting : 여러 개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서 올바르게 예측할 수 있도록 다음 분류기에게는 가중치(Weight)를 부여하면서 학습과 예측을 진행하는 것

- Stacking : 여러 가지 다른 모델의 예측 결과값을 다시 학습 데이터로 만들어서 다른 모델(메타 모델)로 재학습시켜 결과를 예측하는 방법



##### 보팅 유형 - 하드 보팅(Hard Voting)과 소프트 보팅(Soft Voting)

보팅 방법에는 하드 보팅, 소프트 보팅 두 가지가 있는데, 하드 보팅보다는 소프트 보팅이 예측 성능이 좋아서 더 많이 사용됩니다.

- 하드 보팅 : 예측한 결과값들 중 다수의 분류기가 결정한 예측값을 최종 보팅 결과값으로 선정
- 소프트 보팅 : 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결과값으로 선정

|                          하드 보팅                           |                         소프트 보팅                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/126171378-0485c939-ac16-46c2-ad9f-0fb7a803a7be.png) | ![image](https://user-images.githubusercontent.com/76269316/126171614-ab1ea346-bf18-4333-83ea-98638c3e1f70.png) |



##### 보팅 분류기 (Voting Classifier)

사이킷런은 보팅 방식의 앙상블을 구현한 VotingClassifier 클래스를 제공하고 있습니다.

보팅 방식 앙상블을 사용해 위스콘신 유방암 데이터 세트를 예측 분석해 보겠습니다.

```python
import pandas as pd
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

#위스콘신 유방암 데이터 세트 생성
cancer = load_breast_cancer()

data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)

#개별 모델은 로지스틱 회귀와 KNN임
lr_clf = LogisticRegression()
knn_clf = KNeighborsClassifier(n_neighbors=8)

#개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기
vo_clf = VotingClassifier(estimators=[('LR', lr_clf), ('KNN', knn_clf)], voting='soft')

X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=156)

#VotingClassifier 학습/예측/평가
vo_clf.fit(X_train, y_train)
pred = vo_clf.predict(X_test)
print('Voting 분류기 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))

#개별 모델의 학습/예측/평가
classifiers = [lr_clf, knn_clf]
for classifier in classifiers:
    classifier.fit(X_train, y_train)
    pred = classifier.predict(X_test)
    class_name = classifier.__class__.__name__
    print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test, pred)))
```

![image](https://user-images.githubusercontent.com/76269316/126141778-ff7d2214-98f4-4d33-bb8f-d759c9567f6a.png)

보팅분류기가 정확도가 조금 더 높게 나타났는데, 보팅으로 여러 개의 분류기를 결합한다고 해서 무조건 개별 분류기보다 예측 성능이 향상되지는 않습니다.



Voting과 Stacking은 서로 다른 알고리즘을 기반으로 하지만, Bagging과 Boosting은 대부분 결정 트리 알고리즘을 기반으로 합니다.

결정 트리 알고리즘은 쉽고 직관적인 분류 기준을 갖고 있지만 정확한 예측을 위해 학습 데이터에 집착하게 되면 과적합이 발생해 실제 테스트 데이터에서 예측 성능이 떨어지게 됩니다.

하지만 앙상블 학습에서는 결정 트리 알고리즘의 단점을 수십~수천 개의 많은 분류기를 결합해 다양한 상황을 학습하게 함으로써 극복합니다.

결정 트리 알고리즘의 장점은 그대로 취하고 단점은 보완하면서 편향-분산 트레이드오프 효과를 극대화 한다는 것입니다.



### 랜덤 포레스트

Bagging은 같은 알고리즘으로 여러 개의 분류기를 만들어서 Voting으로 최종 결정하는 알고리즘입니다.

Bagging의 대표적인 알고리즘이 랜덤 포레스트입니다.

랜덤 포레스트는 결정 트리 기반 알고리즘으로, 결정 트리의 쉽고 직관적인 장점을 그대로 갖고 있습니다.

랜덤 포레스트는 여러 개의 결정 트리 분류기가 전체 데이터에서 bagging 방식으로 각자의 데이터를 샘플링해 개별적으로 학습한 뒤, 최종적으로 모든 분류기가 voting을 통해 예측을 결정하게 됩니다.

<img src="https://user-images.githubusercontent.com/76269316/126171819-89bb122a-3215-4c0e-ab58-cf76a05fa2a6.png" alt="image" style="zoom: 33%;" />



개별 트리가 학습하는 데이터 세트는 전체 데이터에서 일부가 중첩되게 샘플링된 데이터 세트입니다.

+이렇게 여러 개의 데이터 세트를 중첩되게 분리하는 것을 bootstrapping 분할 방식이라고 합니다.

<img src="https://user-images.githubusercontent.com/76269316/126173252-a4e5ef2f-5df3-4740-9a95-a00eeb2c2235.png" alt="image" style="zoom:50%;" />



사이킷런은 RandomForestClassifier 클래스를 통해 랜덤 포레스트 기반 분류를 지원합니다.

사용자 행동 인식 데이터 세트를 RandomForestClassifier를 이용해 예측해 보겠습니다.

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

#결정 트리에서 사용한 get_human_dataset()을 이용해 학습/테스트용 DataFrame 반환
X_train, X_test, y_train, y_test = get_human_dataset()

#랜덤 포레스트 학습 및 별도의 테스트 세트로 예측 성능 평가
rf_clf = RandomForestClassifier(random_state=0)
rf_clf.fit(X_train, y_train)
pred = rf_clf.predict(X_test)
accuracy = accuracy_score(y_test, pred)
print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy))
```

![image](https://user-images.githubusercontent.com/76269316/126174035-76bc780a-67c7-48d2-8aca-c9eea8c40018.png)

약 92.53%의 정확도를 보여줍니다.



##### 랜덤 포레스트 하이퍼 파라미터 및 튜닝

트리 기반의 앙상블 알고리즘은 하이퍼 파라미터가 너무 많고, 그로 인해 튜닝을 위한 시간이 많이 소모됩니다.

그나마 랜덤 포레스트는 결정 트리에서 사용되는 하이퍼 파라미터와 같은 파라미터가 대부분이라 파라미터가 적은 펵에 속합니다.



**랜덤 포레스트의 하이퍼 파라미터**

- n_estimators : 랜덤 포레스트에서 결정 트리 개수를 지정합니다. (디폴트 10개)
  많이 설정할수록 좋은 성능을 기대할 수 있지만 계속 증가시킨다고 성능이 무조건 향상되는 것은 아닙니다. (늘릴수록 학습 수행 시간이 증가하는 것을 감안해야함)
- max_features : 결정 트리의 [max_features](https://seominseok4834.github.io/machine%20learning/4.classification/#%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0) 파라미터와 같지만 디폴트가 auto입니다. (sqrt)
  따라서 랜덤 포레스트를 분할하는 피처를 참조할 때 전체 피처가 아니라 <img src="https://user-images.githubusercontent.com/76269316/126085844-bc299bcc-3f39-4a67-845d-7d39a9f98854.png" alt="image" style="zoom: 50%;" />만큼 참조합니다.
- max_depth : 결정 트리와 같음
- min_samples_leaf : 결정 트리와 같음



GridSearchCV를 이용해 랜덤 포레스트의 하이퍼 파라미터를 튜닝해 보겠습니다.

n_estimators = 100, CV = 2로 설정해 최적 하이퍼 파라미터를 구해 보겠습니다.

```python
from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators' : [100],
    'max_depth' : [6, 8, 10, 12],
    'min_samples_leaf' : [8, 12, 18],
    'min_samples_split' : [8, 16, 20]
}

#RandomForestClassifier 객체 생성 후 GridSearchCV 수행
rf_clf = RandomForestClassifier(random_state=0, n_jobs=-1)  #n_jobs=-1 : 모든 CPU 코어를 이용해 학습
grid_cv = GridSearchCV(rf_clf, param_grid=params, cv=2, n_jobs=-1)
grid_cv.fit(X_train, y_train)

print('최적 하이퍼 파라미터\n', grid_cv.best_params)
print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))
```

![image](https://user-images.githubusercontent.com/76269316/126176564-b90852e9-963e-4ffc-8169-5202892ccffc.png)



위에서 구한 최적 하이퍼 파라미터를 가지고 n_estimators를 300으로 증가시켜 별도의 테스트 데이터 세트에서 예측 성능을 평가해 보겠습니다.

```python
rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_leaf=8, min_samples_split=8, random_state=0)
rf_clf1.fit(X_train, y_train)
pred = rf_clf1.predict(X_test)
print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred)))
```

![image](https://user-images.githubusercontent.com/76269316/126176993-5f44f472-fb77-47db-b890-70c35919a933.png)

예측 정확도 수치는 약 91.65%가 나왔습니다.



RandomForestClassifier 역시 feature_importances_ 속성을 이용해 피처 중요도를 확인할 수 있습니다.

```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

ftr_importances_values = rf_clf1.feature_importances_
ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns)
ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]  #내림차순 정렬

plt.figure(figsize=(8, 6))
plt.title('Feature importances Top 20')
sns.barplot(x=ftr_top20, y=ftr_top20.index)
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/126177784-65cfc666-692d-480a-bb63-11873440cc7e.png)





### GBM(Gradient Boosting Machine)

Boosting 알고리즘은 여러 개의 약한 학습기(weak learner)를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치를 부여해 오류를 개선해 나가는 학습 방식입니다.

Boosting의 대표적인 구현은 AdaBoost(Adaptive boosting)와 Gradient Boost가 있습니다.



먼저 AdaBoost가 어떻게 학습을 진행하는지 보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126178465-10a3db0f-c8d5-4b90-8604-c08ea4e8a01f.png)

맨 왼쪽 그림과 같이 +, -로 된 피처 데이터 세트가 있다면

- Step 1은 첫 번째 약한 학습기가 분류 기준 1로 +와 -를 분류한 것입니다. 동그라미로 표시된 ⊕ 데이터는 잘못 분류된 오류 데이터입니다.
- Step 2에서는 이 오류 데이터에 대해 가중치 값을 부여합니다. 가중치가 부여된 오류 데이터 +는 다음 약한 학습기가 잘 분류할 수 있게 크기가 커졌습니다.
- Step 3는 두 번째 약한 학습기가 분류 기준 2로 +와 -를 분류했습니다. 마찬가지로 동그라미로 표시된 ⊖ 데이터는 잘못 분류된 오류 데이터입니다.
- Step 4에서 잘못 분류된 - 오류 데이터에 대해 다음 약한 학습기가 잘 분류할 수 있게 더 큰 가중치를 부여합니다. (크기가 커짐)
- Step 5에서 세 번째 약한 학습기가 분류 기준 3으로 +와 -를 분류하고 오류 데이터를 찾습니다.
- 마지막으로 맨 아래에 첫 번째, 두 번째, 세 번째 약한 학습기를 모두 결합한 예측 결과입니다. 개별 약한 학습기보다 훨씬 정확도가 높아졌음을 알 수 있습니다.



GBM(Gradient Boost Machine)도 AdaBoost와 유사하나, 가중치 업데이트를 경사 하강법(Gradient Descent)을 이용하는 것이 큰 차이입니다. (나중에 회귀 부분에서 설명)

+GBM이 [CART](https://seominseok4834.github.io/machine%20learning/4.classification/#%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0) 기반의 다른 알고리즘이라 분류, 회귀 모두 가능



사이킷런은 GBM 기반의 분류를 위해 GradientBoostingClassifier 클래스를 제공합니다.

사이킷런 GBM을 이용해 사용자 행동 데이터 세트를 예측 분류해 보겠습니다.

```python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
import time
import warnings
warnings.filterwarnings('ignore')

X_train, X_test, y_train, y_test = get_human_dataset()

#GBM 수행 시간 측정을 위한 시작 시간 설정
start_time = time.time()

gb_clf = GradientBoostingClassifier(random_state=0)
gb_clf.fit(X_train, y_train)
gb_pred = gb_clf.predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)

print('GBM 정확도: {0:.4f}'.format(gb_accuracy))
print('GBM 수행시간: {0:.1f}초'.format(time.time() - start_time))
```

![image](https://user-images.githubusercontent.com/76269316/126184129-bb1ce414-17ec-4efb-a5da-52d16850a121.png)

사이킷런 GradientBoostingClassifier는 약한 학습기의 순차적인 예측 오류 보정을 통해 학습을 수행하므로 멀티 CPU 코어 시스템을 사용하더라도 병렬 처리가 지원되지 않아 대용량 데이터의 경우 학습에 매우 많은 시간이 필요합니다.

반면 랜덤 포레스트의 경우 상대적으로 빠른 수행 시간을 보장해줍니다.



##### GBM 하이퍼 파라미터 튜닝

- loss : 경사 하강법에서 사용할 비용 함수 지정.
  특별한 이유가 없으면 디폴트 'deviance' 그대로 적용
- learning_rate : GBM이 학습을 진행할 때마다 적용하는 학습률. (weak learner가 순차적으로 오류 값을 보정해 나가는데 적용하는 계수)
  0~1 사이의 값을 지정할 수 있으며, 기본값은 0.1
  너무 작은 값을 적용하면 업데이트 되는 값이 작아져서 예측 성능이 높아질 가능성이 높지만 수행 시간이 오래 걸리게 됨
  너무 큰 값을 설정하면 최소 오류 값을 찾지 못하고 지나쳐 버려 예측 성능이 떨어질 가능성이 높지만 빠른 수행이 가능하게 됨
  → learning_rate는 n_estimators와 상호 보완적으로 조합해 사용해야함
- n_estimators : weak learner의 개수
  weak learner가 순차적으로 오류를 보정하므로 개수가 많을수록 예측 성능이 일정 수준까지는 좋아지나, 수행 시간이 오래 걸리게 됨. (기본값은 100)
- subsample : weak learner가 학습에 사용하는 데이터의 샘플링 비율
  기본값은 1이며 이는 전체 학습 데이터를 기반으로 학습한다는 의미입니다. (과적합이 염려되는 경우 1보다 작은 값으로 설정)



GridSearchCV를 이용해 하이퍼 파라미터를 최적화해 보겠습니다.

사용자 행동 데이터 세트가 많기 때문에 많은 시간이 걸릴 것입니다.

```python
from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators' : [100, 500],
    'learning_rate' : [0.05, 0.1]
}

grid_cv = GridSearchCV(gb_clf, param_grid=params, cv=2, verbose=1)
grid_cv.fit(X_train, y_train)
print('최적 하이퍼 파라미터\n', grid_cv.best_params_)
print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))
```

![image](https://user-images.githubusercontent.com/76269316/126864666-72dff075-74cc-4fc8-becb-bf4e89e6337c.png)

jupyter로 돌리니까 2시간 40분 정도 걸렸습니다.

컴퓨터 사양 ↓

![image](https://user-images.githubusercontent.com/76269316/126864785-ed7c0daf-0e60-4035-9e80-db34feaf284d.png)



![image](https://user-images.githubusercontent.com/76269316/126999143-f3036e7d-63d1-4771-81e4-3a635dc1af48.png)

학교 GPU 서버에서 jupyter로 돌렸을 때는 2시간 35분 정도 걸렸습니다.



![image](https://user-images.githubusercontent.com/76269316/126864769-8ab1be98-062f-4820-bc1e-8880c78b2325.png)

colab으로 돌리니까 2시간 10분 정도 걸렸네요.



최적 하이퍼 파라미터를 적용해 테스트 데이터 세트의 예측 정확도를 확인해 보겠습니다.

```python
#GridSearchCV를 이용해 최적으로 학습된 estimator로 예측 수행
gb_pred = grid_cv.best_estimator_predict(X_test)
gb_accuracy = accuracy_score(y_test, gb_pred)
print('GBM 정확도: {0:.4f}'.format(gb_accuracy))
```



GBM은 과적합에도 뛰어난 예측 성능을 가진 알고리즘이지만 수행 시간이 오래 걸린다는 단점이 있습니다.

GBM을 기반으로한 각광받고 있는 두 개의 Gradient Boosting 기반 ML 패키지가 있는데 XGBoost와 LightGBM입니다.



### XGBoost(eXtra Gradient Boost)

##### XGBoost 개요

XGBoost는 트리 기반 앙상블 학습에서 가장 각광받고 있는 알고리즘 중 하나입니다.

압도적인 수치차는 아니지만 분류에 있어 다른 머신러닝보다 뛰어난 예측 성능을 나타냅니다.

XGBoost는 GBM에 기반하고 있지만 GBM의 단점인 느린 수행 시간 및 과적합 규제(Regularization) 부재 등의 문제를 해결해서 매우 각광 받고 있습니다.

병렬 CPU 환경에서 병렬 학습이 가능하게 만듦으로써 기존 GBM보다 빠르게 학습할 수 있습니다.



XGBoost의 장점

| 항목                              | 설명                                                         |
| :-------------------------------- | :----------------------------------------------------------- |
| 뛰어난 예측 성능                  | 일반적으로 분류와 회귀 영역에서 뛰어난 예측 성능을 발휘합니다. |
| GBM 대비 빠른 수행 시간           | 일반적인 GBM은 순차적으로 weak learner가 가중치를 증감하는 방법으로 학습하기 때문에 속도가 느림<br />하지만 XGBoost는 병렬 수행 및 다양한 기능으로 GBM에 비해 빠른 수행 성능을 보장<br />일반적인 GBM에 비해 수행 시간이 빠른 것이지 다른 머신러닝 알고리즘(ex 랜덤 포레스트)보다 빠르지는 않습니다. |
| 과적합 규제<br />(Regularization) | 표준 GBM의 경우 과적합 규제 기능이 없으나 XGBoost는 자체 과적합 규제 기능으로 과적합에 좀 더 강한 내구성을 가질 수 있습니다. |
| 나무 가지치기<br />(Tree pruning) | GBM은 분할 시 부정 손실이 발생하면 분할을 더 이상 수행하지 않지만, 이런 방식도 지나치게 많은 분할을 발생시킬 수 있습니다.<br />다른 GBM과 마찬가지로 XGBoost도 max_depth 파라미터로 분할 깊이를 조정하지만, tree pruning으로 더 이상 긍정 이득이 없는 분할을 가지치기 해서 분할 수를 더 줄일 수 있습니다. |
| 자체 내장된 교차 검증             | XGBoost는 반복 수행 시마다 내부적으로 학습 데이터 세트와 평가 데이터 세트에 대한 교차 검증을 수행해 최적화된 반복 수행 횟수를 가질 수 있습니다.<br />지정된 반복 횟수가 아니라 교차 검증을 통해 평가 데이터 세트의 평가 값이 최적화되면 반복을 중간에 멈출 수 있는 조기 중단 기능이 있습니다. |
| 결손값 자체 처리                  | XGBoost는 결손값을 자체 처리할 수 있는 기능을 갖고 있습니다. |



XGBoost의 핵심 라이브러리는 C/C++로 작성돼 있는데, 파이썬에서도 XGBoost를 구동할 수 있도록 파이썬 패키지를 제공합니다. (파이썬 패키지의 역할은 대부분 C/C++ 핵심 라이브러리를 호출하는 것)



XGBoost는 고유의 프레임워크를 파이썬 언어 기반에서 구현한 별도의 API와 사이킷런과 연동할 수 있는 래퍼 클래스(Wrapper Class) 두 가지를 제공합니다.

구분을 위해 독자적인 XGBoost 프레임워크 기반의 XGBoost를 **파이썬 래퍼 XGBoost 모듈**, 사이킷런과 연동되는 모듈을 **사이킷런 래퍼 XGBoost 모듈**이라고 지지칭하겠습니다.



차이점은 뒤에서 설명하도록 하겠습니다.



##### XGBoost 설치하기

아나콘다 Command 창을 연 뒤 다음 명령어를 입력합니다.

conda install -c anaconda py-xgboost

![image](https://user-images.githubusercontent.com/76269316/126246976-5c1cb820-e7c2-42c2-a996-df3378ccbb44.png)

Proceed([y]/n)>에서 y를 입력하면 설치가 진행됩니다.



새로운 주피터 노트북을 생성한 후 xgboost 모듈이 정상적으로 import되면 설치가 정상적으로 된 것입니다.

```python
import xgboost as xgb
from xgboost import XGBClassifier
```

'no module named xgboost'와 같이 에러메시지가 나오면 정상적으로 설치가 되지 않은 것입니다.



##### 파이썬 래퍼 XGBoost 하이퍼 파라미터

XGBoost는 GBM과 유사한 하이퍼 파라미터를 동일하게 갖고 있으며, 여기에 조기 중단(early stopping), 과적합을 규제하기 위한 하이퍼 파라미터등이 추가 됐습니다.

**파이썬 래퍼 XGBoost 모듈과 사이킷런 래퍼 XGBoost 모듈의 일부 하이퍼 파라미터가 동일한 기능을 하지만, 파라미터명이 다르므로 주의가 필요합니다.**



파이썬 래퍼 XGBoost 하이퍼 파라미터를 먼저 살펴보겠습니다.



파이썬 래퍼 XGBoost 하이퍼 파라미터는 세 가지 유형으로 나뉩니다.

- 일반 파라미터 : 일반적으로 실행 시 스레드의 개수나 silent 모드 등의 선택을 위한 파라미터로써 디폴트 파라미터 값을 바꾸는 경우는 거의 없습니다.
- 부스트 파라미터 : 트리 최적화, 부스팅, regularization 등과 관련 파라미터 등을 지칭합니다.
- 학습 태스크 파라미터 : 학습 수행 시의 객체 함수, 평가를 위한 지표 등을 설정하는 파라미터입니다.

대부분의 하이퍼 파라미터는 부스터 파라미터에 속합니다.



**주요 일반 파라미터**

- booster : gbtree(tree based model) 또는 gblinear(linear model) 선택 (디폴트는 gbtree)
- silent : 출력 메시지를 나타내고 싶지 않을 경우 1로 설정 (디폴트는 0)
- nthread : CPU 실행 스레드 개수를 조정 (디폴트는 CPU 전체 스레드를 다 사용)
  멀티 코어/스레드 CPU 시스템에서 전체 CPU를 사용하지 않고 일부 CPU만 사용해 ML 어플리케이션을 구동하는 경우 변경



**주요 부스트 파라미터**

- eta [default=0.3, alias: learning_rate] : GBM의 학습률(learning rate)과 같은 파라미터
  0에서 1사이의 값을 지정하며 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률 값
  파이썬 래퍼 기반 xgboost 디폴트 : 0.3, 사이킷런 래퍼 클래스에서는 파라미터명이 learning_rate이며, 디폴트는 0.1 (보통 0.01~0.2 사이 값 선호)
- num_boost_rounds : GBM의 n_estimators와 같은 파라미터 (결정 트리의 개수)
- min_child_weight[default=1] : 트리에서 추가적으로 가지를 나눌지 결정하기 위해 필요한 데이터들의 weight 총합 (과적합을 조절하기 위해 사용)
  min_child_weight가 클수록 분할을 자제함
- gamma[default=0, alias: min_split_loss] : 트리의 리프 노드를 추가적으로 나눌지를 결정할 최소 손실 감소 값
  해당 값보다 큰 손실(loss)이 감소된 경우 리프 노드를 분리 (값이 클수록 과적합 감소 효과)
- max_depth[default=6] : 트리 기반 알고리즘의 max_depth와 같은 파라미터
  0 지정시 깊이 제한 X, max_depth가 높으면 특정 피처 조건에 특화되어 과적합 가능성이 높아짐 (보통 3~10 사이 값 적용)
- sub_sample[default=1] : GBM의 subsample과 동일
  트리가 커져서 과적합되는 것을 제어하기 위해 데이터를 샘플링하는 비율 지정 (sub_sample=0.5로 지정시 전체 데이터의 절반을 트리를 생성하는데 사용, 일반적으로 0.5~1 사이 값 사용)
- colsample_bytree[default=1] : GBM의 max_features와 유사
  트리 생성에 필요한 피처(컬럼)를 임의로 샘플링하는데 사용됨 (매우 많은 피처가 있는 경우 과적합을 조정하는데 적용)
- lambda[default=1, alias: reg_lambda] : L2 Regularization 적용 값
  피처 개수가 많을 경우 적용을 검토하며, 클수록 과적합 감소 효과가 있음
- alpha[default=0, alias: reg_alpha] : L1 Regularization 적용 값
  피처 개수가 많을 경우 적용을 검토하며, 클수록 과적합 감소 효과가 있음
- scale_pos_weight[default=1] : 특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 세트의 균형을 유지하기 위한 파라미터



**학습 태스크 파라미터**

- objective : 최솟값을 가져야할 손실 함수를 정의
  XGBoost는 많은 유형의 손실 함수를 사용할 수 있음 (이진 분류인지 다중 분류인지에 따라 사용하는게 다름)

-binary:logistic : 이진 분류일 때 적용

-multi:softmax : 다중 분류일 때 적용 (손실 함수가 multi:softmax일 경우 레이블 클래스 개수인 num_class 파라미터를 지정해야 함)

-multi:softprob: multi:softmax와 유사하나 개별 레이블 클래스의 해당되는 예측 확률을 반환함

- eval_metric : 검증에 사용되는 함수 정의
  기본값은 회귀인 경우 rmse, 분류일 경우 error

-rmse : Root Mean Square Error

-mae : Mean Absolute Error

-logloss : Negative log-likelihood

-error : Binary classification error rate (0.5 threshold)

-merror : Multiclass classification error rate

-mlogloss : Multiclass logloss

-auc : Area under the curve



과적합 문제가 심각하다면 다음을 고려할 수 있습니다.

- eta 값을 낮춥니다. (0.01~0.1)
  eta 값을 낮출 경우 num_round(n_estimators)는 반대로 높여줘야 합니다.
- max_depth 값을 낮춥니다.
- min_child_weight 값을 높입니다.
- gamma 값을 높입니다.
- subsample과 colsample_bytree를 조정합니다.



XGBoost는 자체적으로 교차 검증, 성능 평가, 피처 중요도 등의 시각화 기능을 갖고 있습니다.

또한 수행 속도를 향상시키기 위해 조기 중단(Early Stopping) 기능을 갖고 있습니다.

기본 GBM의 경우 n_estimators(num_boost_rounds - XGBM)에 지정된 횟수만큼 반복적으로 학습 오류를 감소시키며 학습을 진행하는데, 중간에 반복을 멈출 수 없고 n_estimators에 지정된 횟수를 다 완료해야 합니다.

XGBoost, 뒤에 소개할 LightGBM은 모두 조기 중단 기능이 있어 지정한 부스팅 반복 횟수에 도달하지 않더라도 예측 오류가 더 이상 개선되지 않으면 중지해 수행 시간을 개선할 수 있습니다.

num_estimators = 200으로 설정하고 early_stopping_rounds = 50 (조기 중단 파라미터)으로 설정하면 1부터 200회까지 부스팅을 반복하다가 50회를 반복하는 동안 학습 오류가 감소하지 않는 경우 진행을 종료합니다.



##### 파이썬 래퍼 XGBoost 적용 - 위스콘신 유방암 예측

위스콘신 유방암 데이터 세트는 종양의 크기, 모양 등의 다양한 속성값을 기반으로 악성 종양(malignant)인지 양성 종양(benign)인지 분류한 데이터 세트입니다.

위스콘신 유방암 데이터 세트를 가지고 XGBoost를 이용해 악성 종양인지 양성 종양인지 예측해보겠습니다.



먼저 데이터를 로딩한 다음 DataFrame으로 변환했습니다.

```python
import xgboost as xgb
from xgboost import plot_importance  #피처 중요도 시각화 모듈
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

dataset = load_breast_cancer()
X_features = dataset.data
y_label = dataset.target

cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names)
cancer_df['target'] = y_label
cancer_df
```

![image](https://user-images.githubusercontent.com/76269316/126778680-f30effaa-2834-4f25-9565-49753888d108.png)



레이블 값의 분포를 확인해보겠습니다. (악성 : 0, 양성 : 1)

```python
print(dataset.target_names)
print(cancer_df['target'].value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/126778835-9d84e8b2-1814-4371-8145-0ba05a5e2d2e.png)



이후 학습용 데이터 세트로 80%, 테스트용 데이터 세트로 20% 분할해줬습니다.

파이썬 래퍼 XGBoost는 사이킷런 래퍼 XGBoost와 차이점 중 하나로, 학습용·테스트용 데이터 세트를 DMatrix 객체로 생성합니다.

DMatrix는 주로 넘파이를 입력받아 만드는 XGBoost만의 전용 데이터 세트입니다.

주요 파라미터로 data(피처 데이터 세트), label(분류 - 레이블 데이터 세트, 회귀 - 숫자형인 종속값 데이터 세트)을 입력 받습니다.

```python
#전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터
X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size=0.2, random_state=156)

#학습 데이터 세트와 테스트 데이터 세트를 DMatrix로 변환
dtrain = xgb.DMatrix(data=X_train, label=y_train)
dtest = xgb.DMatrix(data=X_test, label=y_test)
```



학습을 수행하기 전에 마지막으로 XGBoost의 하이퍼 파라미터를 설정했습니다.

하이퍼 파라미터는 딕셔너리 형태로 입력합니다.

```python
params = {
    'max_depth' : 3,
    'eta' : 0.1,
    'objective' : 'binary:logistic',
    'eval_metric' : 'logloss',
    'early_stoppings' : 100
}
num_rounds = 400
```



이제 지정된 하이퍼 파라미터를 가지고 XGBoost 모델을 학습시키겠습니다.

조기 중단을 위해 train() 함수의 early_stopping_rounds 파라미터를 설정했습니다. (early_stopping_rounds 파라미터를 설정해 조기 중단을 수행하기 위해서는 eval_set, eval_metric을 지정해줘야 합니다. → evals 파라미터로 지정)

- eval_set : 성능 평가를 수행할 평가용 데이터 세트
- eval_metric : 평가 세트에 적용할 성능 평가 방법
  -분류일 경우 'error'(분류 오류), 'logloss' 적용

```python
#train 데이터 세트는 'train', evaluation(test) 데이터 세트는 'eval'로 명기
wlist = [(dtrain, 'train'), (dtest, 'eval')]

#하이퍼 파라미터와 early stopping  파라미터를 train() 함수 파라미터로 전달
#train() 함수는 학습이 완료된 모델 객체 반환
xgb_model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_rounds, early_stopping_rounds=100, evals=wlist)
```

![image](https://user-images.githubusercontent.com/76269316/126780827-62c0fce2-2df1-480f-8417-5fc6678d8174.png)

train()으로 학습을 수행하면 반복 시 train-error와 eval-logloss가 지속적으로 감소합니다.

학습이 완료된 모델을 가지고 테스트 데이터 세트 예측을 수행해 보겠습니다.

파이썬 래퍼 XGBoost의 경우 predict() 메소드가 예측 확률 값을 반환한다는 점에 주의해야 합니다.

```python
pred_probs = xgb_model.predict(dtest)
print('predict() 수행 결과값을 10개만 표시, 예측 확률값으로 표시됨')
print(np.round(pred_probs[:10], 3))

#예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측값 결정해 preds 리스트에 저장
preds = [1 if x > 0.5 else 0 for x in pred_probs]
print('예측값 10개만 표시:', preds[:10])
```

![image](https://user-images.githubusercontent.com/76269316/126781382-5e578b98-cce9-472b-9832-4c17a96c406e.png)



분류 포스팅에서 사용한 get_clf_eval() 함수를 사용해 XGBoost 모델의 예측 성능을 평가해봤습니다.



**get_clf_eval**

오차 행렬, 정확도, 정밀도, 재현율, F1 score, AUC를 한 번에 계산하기 위한 메소드

```python
import sklearn
from sklearn.metrics import accuracy_score, precision_score,recall_score, confusion_matrix, roc_auc_score, f1_score

def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    #ROC-AUC 추가
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차 행렬')
    print(confusion)
    #ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC: {4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))
    
get_clf_eval(y_test, preds, pred_probs)
```

![image](https://user-images.githubusercontent.com/76269316/126782486-27ee8c6b-a3bf-46f6-b7fd-10bb671d1b6c.png)



마지막으로 xgboost 패키지에 plot_importance() API로 피처의 중요도를 막대그래프 형식으로 나타내보겠습니다. (기본 평가 지표로 f1 스코어 사용)

```python
from xgboost import plot_importance
import matplotlib.pyplot as plt
%matplotlib inline

fig, ax = plt.subplots(figsize=(10, 12))
plot_importance(xgb_model, ax=ax)
```

xgboost는 넘파이 기반 피처 데이터로 학습하기 때문에(넘파이 ndarray를 DMatrix로 변환) 피처명을 제대로 알 수 없습니다.

따라서 plot_importance()로 시각화하면 f0, f1과 같이 피처 순서별로 피처명을 나열합니다. (f0은 첫 번째 피처, f1은 두 번째 피처)

![image](https://user-images.githubusercontent.com/76269316/126783034-25bce82c-55af-4621-944d-769b3fc6c175.png)



결정 트리에서 보여준 트리 기반 규칙 구조도 xgboost 모듈의 to_graphviz() API를 사용해 시각화 할 수 있습니다. (Graphiz 프로그램, 패키지가 설치돼 있어야 함)

또한, 사이킷런 GridSearchCV와 유사하게 데이터 세트에 대한 교차 검증 수행 후 최적 파라미터를 구할 수 있는 cv() API를 제공합니다.

cv() API와 파라미터에 대한 설명입니다.

```python
xgboost.cv(params, dtrain, num_boost_round=10, nfold=3, stratified=False, folds=None, metrics=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, fpreproc=None, as_pandas=True, verbose_eval=None, show_stdv=True, seed=0, callbacks=None, shuffle=True)
```

- pramas (dict) : 부스트 파라미터
- dtrain (DMatrix) : 학습 데이터
- num_boost_round (int) : 부스팅 반복 횟수
- nfold (int) : CV 폴드 개수
- stratified (bool) : CV 수행 시 층화 표본 추출(stratified sampling) 수행 여부
- metrics (string or list of strings) : CV 수행 시 모니터링할 성능 평가 지표
- early_stopping_rounds (int) : 조기 중단 활성화, 반복 횟수 지정



xgboost.cv의 반환값은 DataFrame 형태입니다.



##### 사이킷런 래퍼 XGBoost의 개요 및 적용

파이썬 래퍼 XGBoost와 다르게 사이킷런의 기본 estimator를 그대로 상속해 만들었기 때문에 다른 Estimator와 동일하게 fit()과 predict()만으로 학습·예측이 가능하고, GridSearchCV, pipeline등 다른 사이킷런의 유틸리티를 그대로 사용할 수 있습니다.



위에서 파이썬 래퍼 XGBoost와 사이킷런 래퍼 XGBoost의 하이퍼 파라미터에 차이가 있다고 했는데, 정리하면 다음과 같습니다.

- num_boost_round → n_estimaotrs
- eta → learning_rate
- sub_sample → subsample
- lambda → reg_lambda
- alpah → reg_alpha



위스콘신 유방암 데이터 세트를 사이킷런 래퍼 XGBoost(XGBClassifier)를 이용해 예측해보겠습니다.

위 예제와 동일하게 하이퍼 파라미터를 적용하겠습니다.

**get_clf_eval**

오차 행렬, 정확도, 정밀도, 재현율, F1 score, AUC를 한 번에 계산하기 위한 메소드

```python
import sklearn
from sklearn.metrics import accuracy_score, precision_score,recall_score, confusion_matrix, roc_auc_score, f1_score

def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    #ROC-AUC 추가
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차 행렬')
    print(confusion)
    #ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC: {4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))
```

```python
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier  #사이킷런 래퍼 XGBoost 클래스인 XGBClassifier import
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

dataset = load_breast_cancer()
X_features = dataset.data
y_label = dataset.target

cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names)
cancer_df['target'] = y_label

#전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터
X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size=0.2, random_state=156)

xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)
xgb_wrapper.fit(X_train, y_train)
w_preds = xgb_wrapper.predict(X_test)
w_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]

get_clf_eval(y_test, w_preds, w_pred_proba)
```

![image](https://user-images.githubusercontent.com/76269316/126865110-40ff4c88-b46f-4a98-af32-47a2d6159ef4.png)

[파이썬 래퍼 XGBoost](https://seominseok4834.github.io/machine%20learning/4.classification/#%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%9E%98%ED%8D%BC-xgboost-%EC%A0%81%EC%9A%A9---%EC%9C%84%EC%8A%A4%EC%BD%98%EC%8B%A0-%EC%9C%A0%EB%B0%A9%EC%95%94-%EC%98%88%EC%B8%A1)와 동일한 평가 결과가 나온 것을 확인할 수 있습니다.



사이킷런 래퍼 XGBoost에도 조기 중단을 수행할 수 있습니다.

조기 중단 관련 파라미터는 다음과 같습니다.

- 평가 지표가 향상될 수 있는 반복 횟수 : early_stopping_rounds
- 조기 중단을 위한 평가 지표 : eval_metric
- 성능 평가를 수행할 데이터 세트 : eval_set

**성능 평가를 수행할 데이터 세트는 학습 데이터가 아니라 별도의 데이터 세트여야합니다.**

아래 코드에서는 데이터 세트의 크기가 작아 평가를 위한 데이터 세트로 **테스트 데이터 세트**를 사용했지만, 테스트 데이터 세트는 학습 시에는 완전히 알려지지 않은 데이터 세트를 사용해야하므로(학습 시 참고가 되어 과적합이 발생할 수 있으므로) 좋은 코드는 아닙니다.

```python
#사이킷런 래퍼 XGBoost 클래스인 XGBClassifier import
from xgboost import XGBClassifier

import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')  #경고 메시지 무시

dataset = load_breast_cancer()
X_features = dataset.data
y_label = dataset.target

cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names)
cancer_df['target'] = y_label

#전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터
X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size=0.2, random_state=156)

xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)
evals = [(X_test, y_test)]  #성능 평가를 위한 데이터 세트로 테스트 데이터 세트 사용 (좋지 않음)
xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="logloss", eval_set=evals, verbose=True)
ws100_preds = xgb_wrapper.predict(X_test)
ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]
get_clf_eval(y_test, ws100_preds, ws100_pred_proba)
```

![image](https://user-images.githubusercontent.com/76269316/126866826-6a109d48-d3e3-4c73-a41a-687f53a7687e.png)

실행 결과 n_estimators를 400으로 설정했지만 310번 반복한 후 학습을 완료한 것을 볼 수 있습니다.

210번 반복 시 logloss가 <u>[210]	validation_0-logloss:0.08581</u>이고 310번 반복 시 <u>[311]	validation_0-logloss:0.08592</u>인데 210번에서 310번까지 100번(early_stopping_rounds) 반복 동안 성능 평가 지수가 향상되지 않았기 때문입니다.



조기 중단값을 너무 급격하게 줄이면 성능이 향상될 여지가 있음에도 불구하고 충분한 학습 전에 멈춰버리기 때문에 예측 성능이 나빠질 수 있습니다.





```python
#early_stopping_rounds를 10으로 설정하고 재학습
xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=10, eval_metric="logloss", eval_set=evals, verbose=True)
ws10_preds = xgb_wrapper.predict(X_test)
ws10_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]
get_clf_eval(y_test,ws10_preds, ws10_pred_proba)
```

![image](https://user-images.githubusercontent.com/76269316/126866793-4de6c0d8-9190-4b9d-a011-ddd4946171ed.png)

early_stopping_rounds를 10으로 변경하고 실행한 결과, 61번만 수행된 후 학습이 종료됐는데, 51번 반복으로부터 10번의 반복 동안 성능 평가 지수가 향상되지 못했기 때문입니다.

early_stopping_rounds=100인 코드보다 정확도가 약간 낮은 것을 확인할 수 있습니다.



피처의 중요도를 시각화하는 모듈인 plot_importance() API를 사용하면 파이썬 래퍼 클래스와 동일하게 피처 중요도를 시각화 할 수 있습니다.

```python
from xgboost import plot_importance
import matplotlib.pyplot as plt
%matplotlib inline

fig, ax = plt.subplots(figsize=(10, 12))
#사이킷런 래퍼 클래스를 입력해도 동일하게 동작
plot_importance(xgb_wrapper, ax=ax)
```

![image](https://user-images.githubusercontent.com/76269316/126866873-f89999c8-92d4-4572-bdcc-6f73c82c59b1.png)



### LightGBM

LightGBM은 XGBoost보다 학습에 걸리는 시간이 훨씬 적습니다. 또한 메모리 사용량도 적습니다.

이러한 장점 때문에 예측 성능이 상대적으로 떨어진다든가 기능상의 부족함이 있을 것 같다고 생각할 수 있는데 실상은 그렇지 않습니다.

XGBoost와 예측 성능은 별다른 차이가 없을 뿐더러, 기능상 다양성은 LightGBM이 약간 더 많습니다. (XGBoost보다 2년 후에 만들어졌기 때문에)

LightGBM의 한 가지 단점은 적은 데이터 세트(일반적으로 10,000건 이하)에 사용할 경우 과적합이 발생하기 쉽다는 것입니다.



대부분 트리 기반 알고리즘은 트리의 깊이를 효과적으로 줄이기 위한 균형 트리 분할 (Level Wise) 방식을 사용합니다.

![image](https://user-images.githubusercontent.com/76269316/126867394-bc4ba9ec-fda1-47b0-aded-f0df18331e0e.png)

이렇게 균형 잡힌 트리를 생성하는 이유는 과적합에 보다 더 강한 구조를 가질 수 있다고 알려져 있기 때문입니다.

하지만 균형을 맞추기 위한 시간이 필요하다는 단점이 있습니다.



LightGBM은 일반 GBM 계열 트리 분할 방법과 다르게 리프 중심 트리 분할 (Leaf Wise) 방식을 사용합니다.

![image](https://user-images.githubusercontent.com/76269316/126867502-4b3bdde6-95aa-446c-a918-6bc6d0834b1f.png)

리프 중심 트리 분할 방식은 트리의 균형을 맞추지 않고, 최대 손실 값(max delta loss)을 갖는 리프 노드를 지속적으로 분할하면서 트리의 깊이가 깊어지고 비대칭적인 규칙 트리를 생성합니다.

이렇게 생성된 규칙 트리는 학습을 반복할수록 균형 트리 분할 방식보다 예측 오류 손실을 최소화 할 수 있다는 것이 LightGBM의 구현 사상입니다.



**LightGBM의 XGBoost 대비 장점**

- 더 빠른 학습, 예측 수행 시간
- 더 작은 메모리 사용량
- 카테고리형 피처의 자동 변환, 최적 변환(원-핫 인코딩등을 사용하지 않고도 카테고리형 피처를 최적으로 변환하고 이에 따른 노드 분할 수행)



LightGBM도 파이썬 래퍼 클래스와 사이킷런 래퍼 클래스를 제공합니다.



##### LightGBM 설치

아나콘다 Command 창을 연 뒤 다음 명령어를 입력합니다.

conda install -c conda-forge lightgbm

![image](https://user-images.githubusercontent.com/76269316/126867634-586dc8db-95c7-4e5f-8b77-4bab1b678fb6.png)

Proceed([y]/n)>에서 y를 입력하면 설치가 진행됩니다.



##### LightGBM 하이퍼 파라미터

LightGBM은 XGBoost와 유사한 하이퍼 파라미터를 갖습니다.

하지만 XGBoost와 다르게 리프 노드가 계속 분할되면서 트리의 깊이가 깊어지므로 이러한 트리 특성에 맞는 하이퍼 파라미터 설정이 필요합니다. (예:max_depth를 매우 크게 가짐)



**주요 파라미터**

- num_iterations[default=100, alias: n_estimators] : 반복 수행하려는 트리의 개수
  크게 지정할수록 예측 성능이 높아질 수 있으나, 너무 크게 지정하면 과적합으로 성능이 저하될 수 있음. (사이킷런 래퍼 클래스에서는 n_estimators로 이름이 변경됨)
- learning_rate[default=0.1] : 0에서 1사이의 값을 지정하며 부스팅 스텝을 반복적으로 수행할 때마다 업데이트되는 학습률 값 (GBM, XGBoost와 같은 파라미터)
  일반적으로 num_iterations를 크게하고 learning_rate를 작게해서 예측 성능을 향상시킬 수 있으나 과적합 이슈, 학습 시간이 길어지는 것을 고려해야 함
- max_depth[default=-1] : 트리 기반 알고리즘의 max_depth와 같은 의미
  0보다 작은 값으로 지정시 깊이 제한 X
- min_data_in_leaf[default=20, alias: min_child_samples] : 최종 결정 클래스인 리프 노드가 되기 위해 최소한으로 필요한 레코드 수  (사이킷런 래퍼 클래스에서는 min_child_samples로 이름 변경됨)
- num_leaves[default=31] : 하나의 트리가 가질 수 있는 최대 리프 노드 개수
- boosting[default=gbdt] : 부스팅 트리를 생성하는 알고리즘을 기술
  -gbdt : 일반적인 그래디언트 부스팅 결정 트리
  -rf : 랜덤 포레스트
- bagging_fraction[default=1.0, alias: subsample] : 트리가 커져 과적합되는 것을 제어하기 위해 데이터를 샘플링하는 비율
- feature_fraction[default=1.0, alias: colsample_bytree] : 개별 트리를 학습할 때마다 무작위로 선택하는 피처 비율
- lambda_l2[default=0.0. alias: reg_lambda] : L2 regulation 제어를 위한 값, 피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있음
- lambda_l1[default=0.0, alias: reg_alpha] : L1 regulation 제어를 위한 값, 피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있음



**Learning Task 파라미터**

- objective : 최솟값을 가져야 할 손실함수 정의
  XGBoost의 objective 파라미터와 동일 (회귀, 다중 클래스 분류, 이진 분류인지에 따라 objective인 손실함수 지정)



##### 하이퍼 파라미터 튜닝 방안

- num_leaves(개별 트리가 가질 수 있는 최대 리프 개수) 개수를 높이면 정확도가 높아지지만, 트리의 깊이가 깊어지고 모델 복잡도가 증가하여 과적합 영향도 커짐
- min_data_in_leaf(최종 결정 클래스인 리프 노드가 되기 위해 필요한 최소 레코드 수)는 num_leaves와 학습 데이터 크기에 따라 달라지지만, 보통 큰 값으로 설정하면 트리가 깊어지는 것을 방지
- max_depth는 트리의 깊이를 제한하여 과적합을 개선하는데 사용

또한 learning_rate를 작게하면서 num_iterations를 크게 하는 것은 부스팅 계열 튜닝에서 가장 기본적인 튜닝 방안입니다.



##### 파이썬 래퍼 LightGBM과 사이킷런 래퍼 XGBoost, LightGBM 하이퍼 파라미터 비교

<img src="https://user-images.githubusercontent.com/76269316/126868190-2051f625-9233-44d9-a225-3398125fe76e.png" alt="image" style="zoom:80%;" />



##### LightGBM 적용 - 위스콘신 유방암 예측

LightGBM 사이킷런 래퍼 클래스(LGBMClassifier)를 사용해 위스콘신 유방암 데이터 세트를 예측해 보겠습니다.



**get_clf_eval**

오차 행렬, 정확도, 정밀도, 재현율, F1 score, AUC를 한 번에 계산하기 위한 메소드

```python
import sklearn
from sklearn.metrics import accuracy_score, precision_score,recall_score, confusion_matrix, roc_auc_score, f1_score

def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    #ROC-AUC 추가
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차 행렬')
    print(confusion)
    #ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC: {4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))
```

```python
#LightGBM 파이썬 패키지인 lightGBM에서 LGBMClassifier import
from lightgbm import LGBMClassifier

import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

dataset = load_breast_cancer()
ftr = dataset.data
target = dataset.target

#전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터로 분리
X_train, X_test, y_train, y_test = train_test_split(ftr, target, test_size=0.2, random_state=156)

#앞서 XGBoost와 동일하게 n_estimators=400으로 설정
lgbm_wrapper = LGBMClassifier(n_estimators=400)

#조기 중단 기능 설정
evals = [(X_test, y_test)]
lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="logloss", eval_set=evals, verbose=True)
preds = lgbm_wrapper.predict(X_test)
pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1]

get_clf_eval(y_test, preds, pred_proba)
```

![image](https://user-images.githubusercontent.com/76269316/126868487-a7441083-d7b4-4de7-a1e4-d090121f99b5.png)

조기 중단으로 145번까지만 반복하고 학습을 종료했습니다. 



LightGBM도 plot_importance()를 통해 피처 중요도를 시각화 할 수 있습니다.

```python
from lightgbm import plot_importance
import matplotlib.pyplot as plt
%matplotlib inline

fig, ax = plt.subplots(figsize=(10, 12))
#사이킷런 래퍼 클래스를 입력해도 동일하게 동작
plot_importance(lgbm_wrapper, ax=ax)
```

![image](https://user-images.githubusercontent.com/76269316/126868545-c89d8dc6-5ef9-46eb-ad5a-a0471ea84836.png)



### 분류 실습 - 캐글 산탄데르 고객 만족 예측

캐글 산탄데르 고객 만족(Santander Customer Satisfaction) 데이터 세트에 대해 XGBoost와 LightGBM을 활용해 예측해보겠습니다.

산탄데르 은행이 캐글에 경연을 의뢰한 데이터로써 피처 이름은 모두 익명 처리돼 이름만을 가지고 어떤 속성인지는 추정할 수 없고,

클래스 레이블명은 TARGET이며 1이면 불만을 가진 고객, 0이면 만족한 고객입니다.

모델 성능 평가는 ROC-AUC로 평가합니다. (대부분이 만족이고 불만족인 데이터는 일부이기 때문에 정확도보다 ROC-AUC가 적합)

[Santander Customer Satisfaction](https://www.kaggle.com/c/santander-customer-satisfaction/data)에서 train.csv를 train_santander.csv로 저장하여 사용하겠습니다.



![image](https://user-images.githubusercontent.com/76269316/126868675-79f1bbde-c859-45ca-947e-03a94196605b.png)

캐글 경연 규칙 준수에 동의하고 다운로드하면 됩니다.



##### 데이터 전처리

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib

cust_df = pd.read_csv("./train_santander.csv", encoding='latin-1')
print('dataset shape:', cust_df.shape)
cust_df.head()
```

![image](https://user-images.githubusercontent.com/76269316/126868769-7e99c651-cc1e-4a4f-8484-5344fbf97bd2.png)

클래스 값 column을 포함한 371개의 피처가 있습니다.



피처 타입과 Null 값을 확인해보겠습니다.

```python
cust_df.info()
```

![image](https://user-images.githubusercontent.com/76269316/126868802-8839161d-ac1c-4593-9095-4d21f73992f1.png)

111개 피처가 float형, 260개 피처가 int형으로 모든 피처가 숫자형이고 null 값은 없습니다.



이번에는 전체 데이터에서 만족과 불만족 비율을 확인해보겠습니다.

```python
print(cust_df['TARGET'].value_counts())
unsatisfied_cnt = cust_df[cust_df['TARGET'] == 1].TARGET.count()
total_cnt = cust_df.TARGET.count()
print('unsatisfied 비율은 {0:.2f}'.format(unsatisfied_cnt / total_cnt))
```

![image](https://user-images.githubusercontent.com/76269316/126868872-18143e51-b15c-4f74-a85a-f75629266b36.png)

대부분이 만족이며 불만족인 고객은 4%에 불과한 것을 확인할 수 있습니다.



DataFrame의 describe() 메소드를 사용해 각 피처 값 분포를 확인해 보겠습니다.

```python
cust_df.describe()
```

![image](https://user-images.githubusercontent.com/76269316/126868956-e8fab5a2-aa87-408e-a231-2669e4348250.png)

var3 column의 경우 min 값이 -999999입니다.



```python
print(cust_df.var3.value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/126868983-84293137-aa40-4c9f-a5a9-efe9c2a6d872.png)

var3 column의 값을 조사해보니까 -999999 값이 116개나 있습니다.

다른 값에 비해 편차가 너무 심하므로 가장 값이 많은 2로 모두 변환해 주었습니다.

또한 ID 피처는 단순 식별자에 불과하므로 드롭했습니다.

```python
cust_df['var3'].replace(-999999, 2, inplace=True)
cust_df.drop('ID', axis=1, inplace=True)
```



이후 학습과 성능 평가를 위해 원본 데이터 세트에서 학습 데이터 세트와 테스트 데이터 세트로 분리했습니다.

비대칭 데이터 세트이므로 학습 데이터와 테스트 데이터의 TARGET 값 분포도가 비슷하게 추출됐는지 확인했습니다.

```python
from sklearn.model_selection import train_test_split

#피처 데이터 세트와 레이블 데이터 세트 분리
X_features = cust_df.iloc[:, :-1]
y_labels = cust_df.iloc[:, -1]

X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=0)

train_cnt = y_train.count()
test_cnt = y_test.count()
print('학습 세트 Shape:{0}, 테스트 세트 Shape:{1}'.format(X_train.shape, X_test.shape))

print('학습 세트 레이블 값 분포 비율')
print(y_train.value_counts() / train_cnt)
print('\n테스트 세트 레이블 값 분포 비율')
print(y_test.value_counts() / test_cnt)
```

![image](https://user-images.githubusercontent.com/76269316/126869124-1dec5694-8ff0-4d7f-9e1c-238536f4fbfd.png)

학습 데이터 세트와 테스트 데이터 세트 모두 TARGET 값 분포가 원본 데이터와 유사하게 분리된 것을 확인할 수 있습니다.



##### XGBoost 모델 학습과 하이퍼 파라미터 튜닝

```python
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score

#n_estimators=500, random_state는 예제 수행 시마다 동일 예측 결과를 위해 설정
xgb_clf = XGBClassifier(n_estimators=500, random_state=156)

#성능 평가 지표를 auc로, 조기 중단 파라미터는 100으로 설정하고 학습 수행
xgb_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="auc", eval_set=[(X_train, y_train), (X_test,y_test)])

xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1], average='macro')
print('ROC AUC: {0:.4f}'.format(xgb_roc_score))
```

![image](https://user-images.githubusercontent.com/76269316/126869334-8002a370-3887-4ebd-8d35-c32bec5f0c58.png)

ROC AUC가 약 0.8359가 나왔습니다.



컬럼 개수가 많으므로 과적합 가능성을 가정하고, max_depth, min_child_weight, colsample_byree 하이퍼 파라미터만 튜닝하겠습니다.



학습 시간이 많이 필요한 ML 모델의 경우 하이퍼 파라미터 튜닝을 수령하는 요령 중 첫 번째는 먼저 2~3개 정도의 파라미터를 결합해 최적 파라미터를 찾아낸 뒤, 이 최적 파라미터를 기반으로 다시 1~2개 파라미터를 결합해 파라미터 튜닝을 수행하는 것입니다.



아래 코드는 8개의 하이퍼 파라미터 경우의 수를 갖는데, 수행 시간이 오래 걸리므로 n_estimaotrs는 100으로 줄이고 early_stopping_rounds도 30으로 줄여서 테스트 한 뒤, 하이퍼 파라미터 튜닝이 완료된 뒤 다시 증가 시키겠습니다.

```python
from sklearn.model_selection import GridSearchCV

#하이퍼 파라미터 테스트 수행 속도를 향상시키기 위해 n_estimators를 100으로 감소
xgb_clf = XGBClassifier(n_estimators=100)

params = {'max_depth': [5,7], 'min_child_weight': [1, 3], 'colsample_bytree':[0.5, 0.75]}

#cv는 3으로 지정
gridcv = GridSearchCV(xgb_clf, param_grid=params, cv=3)
gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric="auc", eval_set=[(X_train, y_train), (X_test, y_test)])

print('GridSearchCV 최적 파라미터:', gridcv.best_params_)

xgb_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:, 1], average='macro')
print('ROC AUC: {0:.4f}'.format(xgb_roc_score))
```

![image](https://user-images.githubusercontent.com/76269316/126869625-314a6b42-a446-43af-9223-ddaf99e84d05.png)

이전보다 조금 개선됐습니다.



위에서 구한 최적화 하이퍼 파라미터를 기반으로 다른 하이퍼 파라미터 또한 변경해 최적화를 진행해 보겠습니다.

```python
#n_estimators는 1000으로 증가시키고, learning_rate는 0.02로 감소, reg_alpha=0.03 추가
xgb_clf = XGBClassifier(n_estimators=1000, random_state=156, learning_rate=0.02, max_depth=5, min_child_weight=3, colsample_bytree=0.75, reg_alpha=0.03)

#성능 평가 지표를 auc로, 조기 중단 파라미터 값은 200으로 설정하고 학습 수행
xgb_clf.fit(X_train, y_train, early_stopping_rounds=200, eval_metric="auc", eval_set=[(X_train, y_train), (X_test, y_test)])

xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:, 1], average='macro')
print('ROC AUC: {0:.4f}'.format(xgb_roc_score))
```

![image](https://user-images.githubusercontent.com/76269316/126869752-380d9669-5cbe-4e16-b207-d91d2f0a1f24.png)

이전 테스트보다 살짝 향상된 결과가 나왔습니다.



튜닝된 모델에서 각 피처 중요도를 그래프로 나타내보겠습니다.

```python
from xgboost import plot_importance
import matplotlib.pyplot as plt
%matplotlib inline

fig, ax = plt.subplots(1, 1, figsize=(10, 8))
plot_importance(xgb_clf, ax=ax, max_num_features=20, height=0.4)
```

![image](https://user-images.githubusercontent.com/76269316/126869794-76b66c97-ae1b-4618-90ed-e64c8a179dd2.png)



##### LightGBM 모델 학습과 하이퍼 파라미터 튜닝

위의 XGBoost 예제 코드에서 만들어진 데이터 세트를 기반으로 LightGBM으로 학습을 수행하고 ROC-AUC를 측정해 봤습니다.

```python
from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(n_estimators=500)

evals = [(X_test, y_test)]
lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="auc", eval_set=evals, verbose=True)

lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, 1], average='macro')
print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))
```

![image](https://user-images.githubusercontent.com/76269316/126869873-30dab74b-06fd-4225-965c-867defc9f5a6.png)



num_leaves, max_depth, min_child_samples, subsample 하이퍼 파라미터를 튜닝해보겠습니다.

```python
from sklearn.model_selection import GridSearchCV

#하이퍼 파라미터 테스트 수행 속도를 향상시키기 위해 n_estimators를 200으로 감소
lgbm_clf = LGBMClassifier(n_estimators=200)

params = {'num_leaves' : [32, 64],
          'max_depth' : [128, 160],
          'min_child_samples' : [60, 100],
          'subsample' : [0.8, 1]}

#cv는 3으로 지정
gridcv = GridSearchCV(lgbm_clf, param_grid=params, cv=3)
gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric="auc", eval_set=[(X_train, y_train), (X_test, y_test)])

print('GridSearchCV 최적 파라미터:', gridcv.best_params_)
lgbm_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:, 1], average='macro')
print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))
```

![image](https://user-images.githubusercontent.com/76269316/126870035-1a9adb1b-77c1-441f-afde-a2a40012911b.png)

최적 하이퍼 파라미터를 LightGBM에 적용해 다시 학습한 다음 ROC-AUC 측정 결과를 도출해보겠습니다.



```python
lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=32, sumsample=0.8, min_child_samples=60, max_depth=128)

evals = [(X_test, y_test)]
lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="auc", eval_set=evals, verbose=True)

lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:, 1], average='macro')
print('ROC AUC: {0:.4f}'.format(lgbm_roc_score))
```

![image](https://user-images.githubusercontent.com/76269316/126870100-a8f67f85-a439-48b3-be67-fc414c797995.png)



### 분류 실습 - 캐글 신용카드 사기 검출

이번에는 캐글 신용카드 데이터 세트를 이용해 사기 검출 분류를 수행해보겠습니다.

[Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)에서 creditcard.csv를 다운받으면 됩니다.



해당 데이터 세트의 레이블인 Class 피처는 매우 불균형한 분포를 갖고 있습니다.

0이 사기가 아닌 정상적인 신용카드 트랜잭션 데이터, 1은 신용카드 사기 트랜잭션을 의미합니다.

전체 데이터의 약 0.172%만이 사기 트랜잭션입니다.



##### 언더 샘플링과 오버 샘플링의 이해

레이블이 불균형한 분포를 가진 데이터 세트를 학습시킬 때 이상 레이블을 갖는 데이터 건수가 정상 레이블을 가진 데이터 건수에 비해 너무 적기 때문에 예측 성능에 문제가 발생할 수 있습니다.

(이상 레이블을 갖는 데이터 건수는 적고, 정상 레이블을 갖는 데이터 건수는 많아 정상 레이블로 치우친 학습을 수행하기 때문)

지도 학습에서의 극도로 불균형한 레이블 값 분포로 인한 문제점을 해결하기 위해 적절한 학습 데이터를 확보하는 대표적인 방법으로, 오버 샘플링(**Oversampling**)과 언더 샘플링(**Undersampling**)이 있습니다.

![image](https://user-images.githubusercontent.com/76269316/126884026-fd6fe3f0-1ec4-4875-bc14-79b1dbf87086.png)

- 언더 샘플링 : 많은 데이터 세트를 적은 데이터 세트 수준으로 감소시키는 방식

  정상 레이블 데이터가 10,000건, 이상 레이블을 가진 데이터가 100건이 있으면 정상 레이블 데이터를 100건으로 줄여버리는 방식

  과도하게 정상 레이블로 학습/예측하는 부작용을 개선할 수 있지만, 너무 많은 정상 레이블 데이터를 감소시키기 때문에 정상 레이블의 경우 오히려 제대로 된 학습을 수행할 수 없다는 단점이 있어 잘 적용하지 않습니다.
  
- 오버 샘플링 : 이상 데이터와 같은 적은 데이터 세트를 증식하여 학습을 위한 충분한 데이터를 확보하는 방식

  동일한 데이터를 단순히 증식시키는 방법은 과적합이 되기 때문에 원본 데이터의 피처 값들을 아주 약간만 변경하여 증식

  대표적으로 SMOTE(Synthetic Minority Over-sampling Technique) 방법이 있습니다.

  SMOTE는 적은 데이터 세트에 개별 데이터들의 K 최근접 이웃 (K Nearest Neighbor)을 찾아 이 데이터와 K개 이웃들의 차이를 일정 값으로 만들어서 기존 데이터와 약간 차이가 나는 새로운 데이터들을 생성하는 방식

![image](https://user-images.githubusercontent.com/76269316/126884203-28c5786c-7f09-4d1d-9cdb-e78c3fc324fa.png)

SMOTE를 구현한 파이썬 패키지는 imbalanced-learn입니다.

아나콘다 프롬프트를 관리자 권한으로 실행하고 아래 명령어를 입력하여 설치할 수 있습니다.

conda install -c conda-forge imbalanced-learn



##### 데이터 일차 가공 및 모델 학습/예측/평가

먼저 데이터 세트를 로딩해보겠습니다.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
%matplotlib inline

card_df = pd.read_csv('./creditcard.csv')
card_df.head()
```

![image](https://user-images.githubusercontent.com/76269316/126884320-51a026a8-a0df-4f5c-8bf8-7fe0ec6f98b8.png)



산탄데르 데이터처럼 피처의 의미는 알 수 없고, Time 피처의 경우 데이터 생성 관련 작업용 속성으로써 크게 의미가 없기 때문에 드롭했습니다.

Amount 피처는 신용카드 트랜잭션 금액(카드 사용 금액)을 의미하고, Class는 레이블로써 0의 경우 정상, 1의 경우 사기 트랜잭션입니다.



```python
card_df.info()
```

![image](https://user-images.githubusercontent.com/76269316/126884480-00f11713-a64b-4c8e-b303-4ae200757b47.png)

결측치(Missing Value)는 없으며, Class 레이블만 int형이고 나머지 피처들은 모두 float형인 것을 확인할 수 있습니다.



**get_preprocessed_df()**

인자로 입력된 DataFrame을 복사한 뒤, 이를 가공하여 반환하는 메소드

일단은 Time 피처만 삭제하는 역할을 합니다.

```python
#인자로 입력받은 DataFrame을 복사한 뒤 Time 컬럼만 삭제하고 복사된 DataFrame 반환
def get_preprocessed_df(df=None):
    df_copy = df.copy()
    df_copy.drop('Time', axis=1, inplace=True)
    return df_copy
```



**get_train_test_dataset()**

get_preprocessed_df()를 호출한 뒤 학습 데이터 세트, 테스트 데이터 세트를 반환합니다.

테스트 데이터 세트를 전체의 30%인 Stratified 방식으로 추출해 학습 데이터 세트와 테스트 데이터 세트의 레이블 값 분포도를 서로 동일하게 만듧니다.

```python
from sklearn.model_selection import train_test_split

#사전 데이터 가공 후 학습과 테스트 데이터 세트를 반환하는 함수
def get_train_test_dataset(df=None):
    #인자로 입력된 DataFramae의 사전 데이터 가공이 완료된 복사 DataFrame 반환
    df_copy = get_preprocessed_df(df)
    #DataFrame의 맨 마지막 컬럼이 레이블, 나머지는 피처
    X_features = df_copy.iloc[:, :-1]
    y_target = df_copy.iloc[:, -1]
    #train_test_split()으로 학습과 테스트 데이터 분할 (Stratify=y_traget으로 Stratified 기반 분할)
    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target)
    #학습과 테스트 데이터 세트 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)
```



생성한 학습 데이터 세트와 테스트 데이터 세트의 레이블 값 비율을 백분율로 환산해 비슷하게 분할됐는지 확인해 봤습니다.

```python
print('학습 데이터 레이블 값 비율')
print(y_train.value_counts() / y_train.shape[0] * 100)
print('테스트 데이터 레이블 값 비율')
print(y_test.value_counts() / y_test.shape[0] * 100)
```

![image](https://user-images.githubusercontent.com/76269316/126884688-b1c14c49-6709-4616-a0ec-def2a5d1f9dc.png)

큰 차이가 없이 잘 분할된 것을 확인할 수 있습니다.



먼저 로지스틱 회귀를 이용해 신용 카드 사기 여부를 예측해 보겠습니다.

**get_clf_eval**

오차 행렬, 정확도, 정밀도, 재현율, F1 score, AUC를 한 번에 계산하기 위한 메소드

```python
import sklearn
from sklearn.metrics import accuracy_score, precision_score,recall_score, confusion_matrix, roc_auc_score, f1_score

def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    #ROC-AUC 추가
    roc_auc = roc_auc_score(y_test, pred_proba)
    print('오차 행렬')
    print(confusion)
    #ROC-AUC print 추가
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC: {4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))
```

```python
from sklearn.linear_model import LogisticRegression

lr_clf = LogisticRegression()
lr_clf.fit(X_train, y_train)
lr_pred = lr_clf.predict(X_test)
lr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]

get_clf_eval(y_test, lr_pred, lr_pred_proba)
```

![image](https://user-images.githubusercontent.com/76269316/126884738-beaca358-cb70-4589-8a4c-0ad79d095f2d.png)



모델을 쉽게 변경해 학습/예측/평가할 수 있도록 get_model_train_eval() 메소드를 만들어 사용하도록 하겠습니다.

**get_model_train_eval()**

```python
#인자로 사이킷런 Estimator 객체와 학습/테스트 데이터 세트를 입력 받아 학습/예측/평가 수행
def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):
    model.fit(ftr_train, tgt_train)
    pred = model.predict(ftr_test)
    pred_proba = model.predict_proba(ftr_test)[:, 1]
    get_clf_eval(tgt_test, pred, pred_proba)
```



get_model_train_eval를 사용해서 LightGBM으로 학습한 뒤, 별도의 테스트 데이터 세트에서 예측 평가를 수행해 보겠습니다.

데이터 세트가 불균형한 레이블 값 분포도를 갖고 있으므로 LGBMClassifier 객체 생성 시 boost_from_average=False로 설정해야 합니다.

(레이블 값이 불균형한 분포를 이루는 경우 boost_from_average=True로 설정 시 ROC-AUC 성능을 매우 크게 저하시킴)

```python
from lightgbm import LGBMClassifier

lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```

![image](https://user-images.githubusercontent.com/76269316/126885469-3cbd6ea5-677f-4982-8004-d83b8d343813.png)

로지스틱 회귀보다는 높은 성능이 나왔습니다.



##### 데이터 분포도 변환 후 모델 학습/예측/평가

왜곡된 분포도를 갖는 데이터를 재가공한 뒤 모델을 다시 테스트하기 위해 정상/사기 트랜잭션을 결정하는 매우 중요한 속성인 Amount 피처 분포도를 확인해 보겠습니다.

```python
import seaborn as sns
plt.figure(figsize=(8,4))
plt.xticks(range(0, 30000, 1000),rotation=60)
sns.distplot(card_df['Amount'])
```

![image](https://user-images.githubusercontent.com/76269316/126887126-0e550633-9c75-4b53-9fa9-655de19ec3c2.png)

Amount 피처는 신용 카드 사용 금액으로, 카드 사용 금액이 1000불 이하인 데이터가 대부분이고 27,000불까지 드물지만 많은 금액을 사용한 경우가 발생하면서 꼬리가 긴 형태의 분포 곡선을 갖고 있습니다.



선형 모델(로지스틱 회귀)은 중요 피처들의 값이 정규 분포 형태를 유지하는 것을 선호하므로 StandardScaler 클래스를 이용해 Amount 피처를 정규 분포 형태로 변환해보겠습니다.

이를 위해 정규 분포로 변환하는 코드를 get_preprocessed_df 함수에 추가하겠습니다.



**get_preprocessed_df()**

데이터 전처리 함수

```python
from sklearn.preprocessing import StandardScaler

#사이킷런 StandardScaler를 이용해 정규 분포 형태로 Amount 피처값을 변환하는 로직 추가
def get_preprocessed_df(df=None):
    df_copy = df.copy()
    scaler = StandardScaler()
    amount_n = scaler.fit_transform(df_copy['Amount'].values.reshape(-1, 1))
    #변환된 Amount를 Amount_Scaled로 피처명 변경 후 DataFrame 맨 앞 컬럼으로 입력
    df_copy.insert(0, 'Amount_Scaled', amount_n)
    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)
    return df_copy
```



**get_train_test_dataset()**

학습 데이터 세트, 테스트 데이터 세트로 분리하는 함수

```python
from sklearn.model_selection import train_test_split

#사전 데이터 가공 후 학습과 테스트 데이터 세트를 반환하는 함수
def get_train_test_dataset(df=None):
    #인자로 입력된 DataFramae의 사전 데이터 가공이 완료된 복사 DataFrame 반환
    df_copy = get_preprocessed_df(df)
    #DataFrame의 맨 마지막 컬럼이 레이블, 나머지는 피처
    X_features = df_copy.iloc[:, :-1]
    y_target = df_copy.iloc[:, -1]
    #train_test_split()으로 학습과 테스트 데이터 분할 (Stratify=y_traget으로 Stratified 기반 분할)
    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target)
    #학습과 테스트 데이터 세트 반환
    return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)
```



**get_model_train_eval()**

모델별로 학습/예측/평가를 수행하는 함수

```python
#인자로 사이킷런 Estimator 객체와 학습/테스트 데이터 세트를 입력 받아 학습/예측/평가 수행
def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):
    model.fit(ftr_train, tgt_train)
    pred = model.predict(ftr_test)
    pred_proba = model.predict_proba(ftr_test)[:, 1]
    get_clf_eval(tgt_test, pred, pred_proba)
```



로지스틱 회귀와 LightGBM 모델을 각각 학습/예측/평가 해보겠습니다.

```python
from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier

#Amount를 정규 분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 실행
X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)

print('### 로지스틱 회귀 예측 성능###')
lr_clf = LogisticRegression()
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)

print('### LightGBM 예측 성능###')
lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```



|                       정규분포 변환 전                       |                       정규분포 변환 후                       |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/126884738-beaca358-cb70-4589-8a4c-0ad79d095f2d.png)<br />![image](https://user-images.githubusercontent.com/76269316/126885469-3cbd6ea5-677f-4982-8004-d83b8d343813.png) | ![image](https://user-images.githubusercontent.com/76269316/126887687-5c943779-178a-45ce-a14e-d0a55eec3f11.png) |

정규분포로 변환해도 성능이 크게 개선되지 않았습니다.



이번에는 로그 변환을 수행해 보겠습니다.

로그 변환은 데이터 분포도가 심하게 왜곡되어 있을 경우 적용하는 기법 중 하나입니다. (원래 값을 log 값으로 변환해 원래 큰 값을 상대적으로 작은 값으로 바꿔주므로 데이터 분포도의 왜곡을 상당 수준 개선해 줌 - 회귀 부분에서 설명하겠습니다.)



**get_preprocessed_df()**

```python
import numpy as np

#넘파이 log1p를 이용해 Amount 피처값을 로그 변환하는 로직 추가
def get_preprocessed_df(df=None):
    df_copy = df.copy()
    amount_n = np.log1p(df_copy['Amount'])
    #변환된 Amount를 Amount_Scaled로 피처명 변경 후 DataFrame 맨 앞 컬럼으로 입력
    df_copy.insert(0, 'Amount_Scaled', amount_n)
    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)
    return df_copy
```



```python
from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier

#Amount를 정규 분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 실행
X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)

print('### 로지스틱 회귀 예측 성능###')
lr_clf = LogisticRegression()
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)

print('### LightGBM 예측 성능###')
lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```

|                         로그 변환 전                         |                         로그 변환 후                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![image](https://user-images.githubusercontent.com/76269316/126884738-beaca358-cb70-4589-8a4c-0ad79d095f2d.png)<br />![image](https://user-images.githubusercontent.com/76269316/126885469-3cbd6ea5-677f-4982-8004-d83b8d343813.png) | ![image](https://user-images.githubusercontent.com/76269316/126887700-85265ea4-cc22-46f3-8b06-9a60d64849ea.png) |

두 모델 모두 정밀도, 재현율, ROC-AUC에서 약간씩 성능이 개선 됐습니다.



##### 이상치 데이터 제거 후 모델 학습/예측/평가

이상치 데이터(Outlier)는 전체 데이터 패턴에서 벗어난 이상 값을 갖는 데이터입니다.

이상치로 인해 머신러닝 모델 성능에 영향을 주는 경우가 발생하기 쉽습니다.



이상치를 찾는 방법은 여러가지가 있지만, IQR(Inter Quantile Range) 방식을 적용해보겠습니다.

IQR은 사분위(Quantile) 값의 편차를 이용하는 방법입니다.



사분위는 전체 데이터를 값이 높은 순으로 정렬하고, 이를 1/4(25%)씩 구간을 분할하는 것을 지칭합니다.

<img src="https://user-images.githubusercontent.com/76269316/126889429-ea011513-d909-4cab-99f0-99a1dcc61fd3.png" alt="image" style="zoom: 67%;" />

이들 중 25% 구간 ~ 75% 구간 (Q1 ~ Q3) 범위를 IQR이라고 합니다.

IQR을 이용해 이상치 데이터를 검출하는 방식은 보통 IQR에 1.5를 곱해서 생성된 범위를 이용해 최댓값(Q3에 IQR * 1.5를 더한 값)과 최솟값(Q1에 IQR * 1.5를 뺀 값을 결정한 뒤 최댓값을 초과하거나 최솟값에 미달하는 데이터를 이상치로 간주하는 것입니다.

<img src="https://user-images.githubusercontent.com/76269316/126889622-81cd2704-1e63-4141-9b8e-fad3ed1c869e.png" alt="image" style="zoom: 50%;" />



모든 피처들의 이상치를 검출하는 것은 시간이 많이 소모되고, 또한 결정값과 상관성이 높지 않은 피처들의 경우 이상치를 제거하더라도 크게 성능 향상에 기여하지 않기 때문에 먼저, 어떤 피처가 결정값(레이블)과 가장 상관성이 높은지 heatmap을 통해 시각화해보겠습니다.

```python
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(9, 9))
corr = card_df.corr()
sns.heatmap(corr, cmap='RdBu')  #양의 상관관계가 높을수록 진한 파란색, 음의 상관관계가 높을 수록 진한 빨간색으로 표현
```

![image](https://user-images.githubusercontent.com/76269316/126889696-87c85c70-a80a-48f8-93b0-340d1c29b057.png)

음의 상관관계가 높은 피처는 V14와 V17입니다.

이 중 V14에 대해서만 이상치를 찾아 제거해보겠습니다.



**get_outlier()**

이상치 데이터의 인덱스를 찾는 함수

```python
import numpy as np

def get_outlier(df=None, column=None, weight=1.5):
    #fraud에 해당하는 column 데이터만 추출, 1/4분위와 3/4 분위 지점을 np.percentile로 구함
    fraud = df[df['Class'] == 1][column]
    quantile_25 = np.percentile(fraud.values, 25)
    quantile_75 = np.percentile(fraud.values, 75)
    #IQR을 구하고, IQR에 1.5를 곱해 최댓값과 최솟값 지점을 구함
    iqr = quantile_75 - quantile_25
    iqr_weight = iqr * weight
    lowest_val = quantile_25 - iqr_weight
    highest_val = quantile_75 + iqr_weight
    #최댓값보다 크거나, 최솟값보다 작은 값을 이상치 데이터로 설정하고 DataFrame index 반환
    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index
    return outlier_index
```



```python
outlier_index = get_outlier(df=card_df, column='V14', weight=1.5)
print('이상치 데이터 인덱스:', outlier_index)
```

![image](https://user-images.githubusercontent.com/76269316/126889842-7c0e1a98-bcaf-4bae-910b-3250c9bd589a.png)

총 4개의 데이터 8296, 8615, 9035, 9252번 Index가 이상치로 추출됐습니다.

get_outlier()를 이용해 이상치를 추출, 삭제하는 로직을 get_processed_df() 함수에 추가하고, 로지스틱 회귀와 LightGBM 모델에 다시 적용해 보겠습니다.



**get_preprocessed_df()**

```python
import numpy as np

#넘파이 log1p를 이용해 Amount 피처값을 로그 변환하는 로직 추가
def get_preprocessed_df(df=None):
    df_copy = df.copy()
    amount_n = np.log1p(df_copy['Amount'])
    #변환된 Amount를 Amount_Scaled로 피처명 변경 후 DataFrame 맨 앞 컬럼으로 입력
    df_copy.insert(0, 'Amount_Scaled', amount_n)
    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)
    #이상치 데이터 삭제하는 로직 추가
    outlier_index = get_outlier(df=card_df, column='V14', weight=1.5)
    df_copy.drop(outlier_index, axis=0, inplace=True)
    return df_copy
```



```python
from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier

#Amount를 정규 분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 실행
X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)

print('### 로지스틱 회귀 예측 성능###')
lr_clf = LogisticRegression()
get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)

print('### LightGBM 예측 성능###')
lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)
get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)
```

| 이상치 데이터 제거 전                                        | 이상치 데이터 제거 후                                        |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image](https://user-images.githubusercontent.com/76269316/126887700-85265ea4-cc22-46f3-8b06-9a60d64849ea.png) | ![image](https://user-images.githubusercontent.com/76269316/126890006-1697b3e7-1362-469f-b6d1-c94a3aa58f31.png) |

이상치 데이터를 제거한 뒤 재현율, F1 스코어, AUC에서 예측 성능이 크게 증가한 것을 확인할 수 있습니다.



##### SMOTE 오버 샘플링 적용 후 모델 학습/예측/평가

[SMOTE 기법](https://seominseok4834.github.io/machine%20learning/4.classification/#%EC%96%B8%EB%8D%94-%EC%83%98%ED%94%8C%EB%A7%81%EA%B3%BC-%EC%98%A4%EB%B2%84-%EC%83%98%ED%94%8C%EB%A7%81%EC%9D%98-%EC%9D%B4%ED%95%B4)으로 오버 샘플링을 적용한 뒤 로지스틱 회귀와 LightGBM 모델의 예측 성능을 평가해 보겠습니다.

SMOTE를 적용할 때는 반드시 **학습 데이터 세트만** 오버 샘플링을 해야 합니다.

검증 데이터 세트나 테스트 데이터 세트를 오버 샘플링 할 경우 결국 원본 데이터 세트가 아닌 데이터 세트에서 검증, 테스트를 수행하기 때문에 올바른 검증/테스트가 될 수 없기 때문입니다.



```python
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=0)
X_train_over, y_train_over = smote.fit_resample(X_train, y_train)
print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트:', X_train.shape, y_train.shape)
print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트:', X_train_over.shape, y_train_over.shape)
print('SMOTE 적용 후 레이블 값 분포\n', pd.Series(y_train_over).value_counts())
```

![image-20210725154639751](https://user-images.githubusercontent.com/76269316/126891820-8db52b25-e484-4403-b90a-1110d3737e9f.png)



SMOTE 적용 후 학습 데이터 세트는 398,040건으로 증식 됐고, 레이블 값이 0과 1 분포가 동일하게 199,020건으로 생성됐습니다.

이렇게 생성된 학습 데이터 세트를 기반으로 먼저 로지스틱 회귀 모델을 학습한 뒤 성능을 평가해보겠습니다.

```python
from sklearn.linear_model import LogisticRegression

print('### 로지스틱 회귀 예측 성능###')
lr_clf = LogisticRegression()
#ftr_train과 tgt_train 인자값이 SMOTE로 증식된 X_train_over와 y_train_over로 변경됨
get_model_train_eval(lr_clf, ftr_train=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test)
```

![image](https://user-images.githubusercontent.com/76269316/126890419-04537c6d-c315-4a1a-9f2e-6e6715d8d83a.png)

재현율이 크게 증가했지만 정밀도가 6.3%로 급격하게 저하됐습니다.

이는 로지스틱 회귀 모델이 오버 샘플링으로 인해 실제 원본 데이터 유형보다 너무 많은 Class=1 데이터로 학습하면서 실제 테스트 데이터 세트에서 예측을 지나치게 Class=1로 적용했기 때문입니다.



분류 결정 임계값에 따른 정밀도와 재현율 곡선을 확인해 보겠습니다.

**precision_recall_curve_plot**

임계값에 따른 정밀도와 재현율을 시각화해주는 메소드

```python
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
%matplotlib inline

def precision_recall_curve_plot(y_test, pred_proba_c1):
    #threshold ndarray와 threshold에 따른 정밀도, 재현율 ndarray 추출
    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1)
    
    #X축을 threshold 값으로, Y축은 정밀도, 재현율 값으로 각각 plot 실행. (정밀도는 점선으로 표시_
    plt.figure(figsize=(8, 6))
    threshold_boundary = thresholds.shape[0]
    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')
    plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')
    
    #threshold 값 X축의 scale을 0.1 단위로 변경
    start, end = plt.xlim()  #X축 범위 반환
    plt.xticks(np.round(np.arange(start, end, 0.1), 2))  #X축에 눈금 표시
    
    #X축, Y축 label과 legend, grid 설정
    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')
    plt.legend(); plt.grid()  #범례, 격자 표시
    plt.show()
```

```python
precision_recall_curve_plot(y_test, lr_clf.predict_proba(X_test)[:, 1])
```

![image](https://user-images.githubusercontent.com/76269316/126890601-0f1bd724-353d-409a-a9c1-546fec7a5cff.png)

임계값이 0.99 이하에서는 재현율이 매우 좋고 정밀도가 극단적으로 낮다가 0.99 이상에서는 반대로 재현율이 대폭 떨어지고 정밀도가 높아집니다.

분류 결정 임계값을 조정하더라도 임계값의 민감도가 너무 심해 로지스틱 회귀 모델의 경우 SMOTE 적용 후 올바른 예측 모델이 생서되지 못했습니다.



이번에는 LightGBM 모델로 학습/예측/평가를 수행해보겠습니다.

```python
from lightgbm import LGBMClassifier

print('### LightGBM 예측 성능###')
lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)
#ftr_train과 tgt_train 인자값이 SMOTE로 증식된 X_train_over와 y_train_over로 변경됨
get_model_train_eval(lgbm_clf, ftr_train=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test)
```

| 이상치만 제거한 경우                                         | SMOTE 적용한 경우                                            |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image](https://user-images.githubusercontent.com/76269316/126890006-1697b3e7-1362-469f-b6d1-c94a3aa58f31.png) | ![image](https://user-images.githubusercontent.com/76269316/126890685-cc3e9247-b486-4530-b667-57c2d95aca8d.png) |

SMOTE를 적용하면 재현율이 높아지고 정밀도는 낮아지는 게 일반적인데, 저는 실행 결과가 조금 이상하게 나왔습니다,,



### 스태킹 앙상블

스태킹(Stacking)은 개별적인 여러 알고리즘을 결합해 예측 결과를 도출한다는 점에서 배깅(Bagging), 부스팅(Boosting)과 공통점을 갖고 있습니다.

차이점은 개별 알고리즘으로 예측한 데이터를 기반으로 다시 예측을 수행한다는 것입니다.



스태킹은 개별 알고리즘의 예측 결과 데이터 세트를 최종적인 메타 데이터 세트로 만들어 별도의 ML 알고리즘으로 최종 학습을 수행하고 테스트 데이터를 기반으로 다시 최종 예측을 수행하는 방식입니다.

따라서 **개별적인 기반 모델**과 개별 기반 모델의 예측 데이터를 학습 데이터로 만들어서 학습하는 최종 **메타 모델** 두 종류 모델이 필요합니다.



M개의 row, N개의 피처(column)을 갖는 데이터 세트에 스태킹 앙상블을 적용한다고 하면,

모델별로 각각 학습을 시킨 뒤 예측을 수행하면 각각 M개의 row를 가진 1개의 레이블 값을 도출할 것입니다.

모델별로 도출된 이 예측 레이블 값들을 다시 합해 (Stacking) 새로운 데이터 세트를 만들고 이렇게 스태킹된 데이터 세트에 대해 최종 모델을 적용해 최종 예측 하는 것이 스태킹 앙상블 모델입니다.

<img src="https://user-images.githubusercontent.com/76269316/126891646-103cc4a0-2680-4e95-b3da-4d6a7f388379.png" alt="image" style="zoom: 50%;" />





##### 기본 스태킹 모델

기본 스태킹 모델을 위스콘신 암 데이터 세트에 적용해 보겠습니다.



KNN, 랜덤 포레스트, 결정 트리, 에이다부스트로 개별 예측을 진행한 뒤, 예측 결과를 합한 데이터 세트를 로지스틱 회귀로 학습/예측 하겠습니다.

```python
import numpy as np

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

cancer_data = load_breast_cancer()

X_data = cancer_data.data
y_label = cancer_data.target

X_train, X_test, y_train, y_test = train_test_split(X_data, y_label, test_size=0.2, random_state=0)

#스태킹에 사용될 머신러닝 알고리즘 클래스 생성 (KNN, 랜덤 포레스트, 결정 트리, 에이다부스트)
knn_clf = KNeighborsClassifier(n_neighbors=4)
rf_clf = RandomForestClassifier(n_estimators=100, random_state=0)
dt_clf = DecisionTreeClassifier()
ada_clf = AdaBoostClassifier(n_estimators=100)

#스태킹으로 만들어진 테이터 세트를 학습, 예측할 최종 모델
lr_final = LogisticRegression(C=10)

#개별 모델들을 학습
knn_clf.fit(X_train, y_train)
rf_clf.fit(X_train, y_train)
dt_clf.fit(X_train, y_train)
ada_clf.fit(X_train, y_train)

#학습된 개별 모델들이 각자 반환하는 예측 데이터 세트를 생성하고 개별 모델의 정확도 측정
knn_pred = knn_clf.predict(X_test)
rf_pred = rf_clf.predict(X_test)
dt_pred = dt_clf.predict(X_test)
ada_pred = ada_clf.predict(X_test)

print('KNN 정확도: {0:.4f}'.format(accuracy_score(y_test, knn_pred)))
print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))
print('결정 트리 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))
print('에이다부스트 정확도: {0:.4f}'.format(accuracy_score(y_test, ada_pred)))
```

![image](https://user-images.githubusercontent.com/76269316/126894863-47340505-8532-4420-ac4a-63e3bddfd5d2.png)



개별 알고리즘으로부터 예측된 예측값을 컬럼 레벨로 옆으로 붙여 피처 값으로 만든 뒤, 최종 메타 모델(로지스틱 회귀)에서 학습 데이터로 다시 사용하겠습니다.



```python
pred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])
print(pred.shape)

#반환된 예측 데이터 세트(knn_pred, rf_pred, dt_pred, ada_pred)는 모두 ndarray이므로 행 형태로 붙임
#transpose를 이용해 행과 열의 위치 교환. 컬럼 레벨로 각 알고리즘의 예측 결과를 피처로 만듦
pred = np.transpose(pred)  
print(pred.shape)  #행과 열 위치를 바꿈
```

![image](https://user-images.githubusercontent.com/76269316/126894933-c849c930-6738-4d23-ae11-92b0268c1af4.png)



```python
lr_final.fit(pred, y_test)
final = lr_final.predict(pred)

print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test, final)))
```

![image](https://user-images.githubusercontent.com/76269316/126894948-36caa183-5a89-4f67-9ff8-0ec7fb17ac8a.png)

개별 모델의 예측 데이터를 스태킹으로 재구성해 최종 메타 모델에서 학습하고 예측한 결과, 정확도가 97.37%로 개별 모델 정확도보다 향상됐습니다. (항상 좋다는 보장은 없습니다.)

최종 학습할 때 레이블 데이터 세트로 학습 데이터가 아닌 테스트용 레이블 데이터 세트를 기반으로 학습했기 때문에 과적합이 발생할 수 있습니다.

이런 과적합을 개선하기 위한 CV 세트 기반 스태킹 모델을 살펴보겠습니다.



##### CV 세트 기반의 스태킹

CV 세트 기반의 스태킹 모델은 과적합을 개선하기 위해 최종 메타 모델을 위한 데이터 세트를 만들 때 교차 검증 기반으로 예측된 결과 데이터 세트를 이용합니다.

다음과 같은 2단계 스텝으로 수행합니다.

- Step1 : 각 모델별로 원본 학습/테스트 데이터를 예측한 결과 값을 기반으로 메타 모델을 위한 학습용/테스트용 데이터를 생성
- Step2 : Step1에서 개별 모델들이 생성한 학습용 데이터를 모두 스태킹 형태로 합쳐 메타 모델이 학습할 최종 학습용 데이터 세트를 생성
  마찬가지로 각 모델들이 생성한 테스트용 데이터를 모두 스태킹 형태로 합쳐 메타 모델이 예측할 최종 테스트 데이터 세트를 생성
  메타 모델은 최종적으로 생성된 학습 데이터 세트와 원본 학습 데이터의 레이블 데이터를 기반으로 학습한 뒤, 최종적으로 생성된 테스트 데이터 세트를 예측하고, 원본 테스트 데이터와 레이블 데이터를 기반으로 평가



아래 그림을 통해 자세히 설명하도록 하겠습니다.

**Step1**

먼저 학습용 데이터를 N개의 폴드(Fold)로 나눕니다. 여기서는 3개로 나누겠습니다.

이 중 2개는 학습을 위한 데이터 폴드로, 나머지 1개는 검증을 위한 데이터 폴드입니다.

이렇게 두 개의 폴드로 나뉜 학습 데이터를 기반으로 개별 모델을 학습시키면 개별 모델은 검증 폴드 1개의 데이터를 예측하고 그 결과를 저장합니다.

이런 로직을 학습 데이터와 검증 데이터를 변경해가면서 3번 반복합니다.

<img src="https://user-images.githubusercontent.com/76269316/126895245-16840ac4-db28-4b8c-bf46-96ec4f125e49.png" alt="image" style="zoom:50%;" />



<img src="https://user-images.githubusercontent.com/76269316/126895325-c761e27d-ce2c-4e43-b7a9-d86f8f4cbd6e.png" alt="image" style="zoom:50%;" />

<img src="https://user-images.githubusercontent.com/76269316/126895359-15272477-d68e-4d7a-a7f5-520095bffd06.png" alt="image" style="zoom:50%;" />

이렇게 만들어진 예측 데이터는 메타 모델을 학습시키는 학습 데이터로 사용됩니다.



2개의 학습 폴드 데이터로 학습된 개별 모델은 원본 테스트 데이터를 예측하여 예측값을 생성하고 이러한 로직을 3번 반복합니다.

이 예측값의 평균으로 최종 결과값을 생성하고 이를 메타 모델을 위한 테스트 데이터로 사용합니다.



**Step2**

![image](https://user-images.githubusercontent.com/76269316/126895419-1b9e4fac-bb4d-47c1-a955-dca50829d23a.png)

각 모델들이 Step1으로 생성한 학습과 테스트 데이터를 모두 합쳐 최종적으로 메타 모델이 사용할 학습 데이터와 테스트 데이터를 생성합니다.

최종 학습 데이터와 원본 데이터의 레이블 데이터를 합쳐 메타 모델을 학습한 뒤, 최종 테스트 데이터로 예측을 수행합니다.

이후 최종 예측 결과를 원본 테스트 데이터의 레이블 데이터와 비교해 평가합니다.



**Step1**

**get_stacking_base_datasets**

개별 모델의 Classifier 객체, 원본인 학습용 피처 데이터, 원본 학습용 레이블 데이터, 원본 테스트 피처 데이터, K 폴드를 몇 개로 할지를 파라미터로 입력 받아

메타 모델을 위한 학습용 데이터와 테스트용 데이터를 반환

```python
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error

#개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수
def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):
    #지정된 n_folds 값으로 KFold 생성
    kf = KFold(n_splits=n_folds, shuffle=False)
    #추후 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화
    train_fold_pred = np.zeros((X_train_n.shape[0], 1))
    test_pred = np.zeros((X_test_n.shape[0], n_folds))
    print(model.__class__.__name__, 'model 시작')
    
    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n)):
        #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 세트 추출
        print('\t 폴드 세트: ', folder_counter, ' 시작')
        X_tr = X_train_n[train_index]
        y_tr = y_train_n[train_index]
        X_te = X_train_n[valid_index]
        
        #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행
        model.fit(X_tr, y_tr)
        #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장
        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1, 1)
        #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측후 데이터 저장
        test_pred[:, folder_counter] = model.predict(X_test_n)
        
        #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델 학습 수행
        model.fit(X_tr, y_tr)
        #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장
        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1, 1)
        #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장
        test_pred[:, folder_counter] = model.predict(X_test_n)
    
    #폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성
    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)
    
    #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터
    return train_fold_pred, test_pred_mean
```

```python
knn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)
rf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)
dt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7)
ada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)
```

![image](https://user-images.githubusercontent.com/76269316/126895831-4f4ea1c2-b053-4107-9c9c-a17cef6fdfa9.png)



**Step2**

Step1에서 get_stacking_base_datasets() 호출로 반환된 각 모델별 학습 데이터와 테스트 데이터를 넘파이 concatenate()를 이용해 합쳤습니다.

```python
Stack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)
Stack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)
print('원본 학습 피처 데이터 Shape:', X_train.shape, '원본 테스트 피처 Shape:', X_test.shape)
print('스태킹 학습 피처 데이터 Shape:', Stack_final_X_train.shape, '스태킹 테스트 피처 데이터 Shape:', Stack_final_X_test.shape)
```

![image](https://user-images.githubusercontent.com/76269316/126895916-42f4e8ce-34c7-442a-8e69-43d62bb466e9.png)



이렇게 만들어진 스태킹된 학습용 피처 데이터 세트와 원본 학습 레이블 데이터로 학습한 뒤,

스태킹된 테스트 데이터 세트로 예측하고, 예측 결과를 원본 테스트 레이블 데이터와 비교해 정확도를 측정했습니다.

```python
lr_final.fit(Stack_final_X_train, y_train)
stack_final = lr_final.predict(Stack_final_X_test)

print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test, stack_final)))
```

![image](https://user-images.githubusercontent.com/76269316/126895979-a36d0854-f308-45c1-8980-f198866239d6.png)



개별 모델의 알고리즘에서 파라미터 튜닝을 하지 않았지만, 최적 파라미터를 튜닝한 상태에서 스태킹 모델을 만드는 것이 일반적입니다.
