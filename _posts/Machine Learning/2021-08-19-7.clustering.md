---
title: "7.Clustering"
excerpt: "군집화"
toc: true
toc_label: "Clustering"
toc_sticky: true

categories:
  - Machine Learning
tags:
  - 군집화 
  - 파이썬 머신러닝 완벽 가이드
last_modified_at: 2021-08-21
---

>![파이썬 머신러닝 완벽 가이드](https://user-images.githubusercontent.com/76269316/122906446-1fa9c000-d38d-11eb-9cab-1eb7e347a1e6.png)
>
>파이썬 머신러닝 완벽 가이드를 읽고 공부한 내용을 정리한 포스팅입니다.



군집화(Clustering)란 **비지도 학습**의 대표적인 기술로 피처에 대한 레이블이 지정되어 있지 않은 데이터를 그룹핑 하는 분석 알고리즘입니다.

<br>

### K-평균 알고리즘

K-평균은 군집화에서 가장 일반적으로 사용되는 알고리즘으로, 군집 중심점(centroid)을 선택해 해당 중심에서 가장 가까운 포인트들을 선택하는 군집화 기법입니다.

<br>

아래는 K-평균이 어떻게 동작하는지를 시각적으로 표현한 그림입니다.

![image](https://user-images.githubusercontent.com/76269316/130090558-aff2b98e-3be6-4e2c-a94b-e6285af29694.png)

1. 먼저 군집화의 기준이 되는 중심을 구성하려는 군집화 개수만큼 적합한 위치에 갖다 놓습니다. (전체 데이터를 2개로 군집화하려면 2개의 중심을 임의의 위치에 갖다 놓습니다.)
2. 각 데이터는 가장 가까운 곳에 위치한 중심점에 소속됩니다.
   위 그림에서는 A, B 데이터가 같은 중심점에 소속되며 C, E, F 데이터가 같은 중심점에 소속됩니다.
3. 소속이 결정되면 군집 중심점을 소속된 데이터의 평균 중심으로 이동시킵니다.
   위 그림에서는 A, B 데이터 포인트의 평군 위치로 중심점이 이동했고 다른 중심점 역시 C, E, F 데이터의 평균 위치로 이동했습니다.
4. 중심점이 이동했기 때문에 각 데이터는 기존에 속한 중심점보다 더 가까운 중심점이 있다면 해당 중심점으로 소속을 변경합니다.
   위 그림에서는 C 데이터가 기존의 중심점보다 가까운 중심점으로 변경됐습니다.
5. 다시 중심을 소속된 데이터의 평균 중심으로 이동합니다.
   위 그림에서는 C가 중심 소속이 변경되면서 두 개의 중심이 모두 이동합니다.
6. 중심점을 이동했는데 데이터의 중심점 소속 변경이 없으면 군집화를 종료합니다.
   그렇지 않을 경우 4-5번 과정을 반복합니다.



K-평균 알고리즘의 장단점

|                             장점                             |                             단점                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| 일반적인 군집화에서 가장 많이 활용되는 알고리즘이고, 쉽고 간결함 | 거리 기반 알고리즘으로 속성 개수가 매우 많을 경우 군집화 정확도가 떨어짐 (따라서 PCA로 차원 감소를 적용해야 할 수도 있음)<br />반복을 수행하는데, 반복 횟수가 많을 경우 수행 시간이 매우 느려짐<br />몇 개의 군집(cluster)을 선택해야 할지 가이드하기 어려움 |

<br><br>

##### 사이킷런 KMeans 클래스 소개

사이킷런 패키지는 K-평균을 구현하기 위해 KMeans 클래스를 제공합니다.

 KMeans 클래스는 다음과 같은 초기화 파라미터를 갖고 있습니다.

```python
class sklearn.cluster.KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')
```

 이 중 중요한 파라미터 다음과 같습니다.

- n_clusters : 군집 개수, 즉 군집 중심점의 개수를 의미합니다.
- init : 군십 중심점의 좌표 설정 방식, 보통 임의로 설정하지 않고 k-means++ 방식으로 설정합니다.
- max_iter : 최대 반복 횟수, 이 횟수 이전에 모든 데이터의 중심점 이동이 없으면 종료합니다.

<br>

사이킷런 KMeans 클래스는 fit, fit_transform 메소드를 이용해 군집화를 수행하면 됩니다.

군집화 수행이 완료되면 군집화와 관련된 주요 속성을 알 수 있습니다. 다음은 이 주요 속성 정보입니다.

- labels_ : 각 데이터 포인트가 속한 군집 중심점 레이블
- cluster_centers_ : 각 군집 중심점 좌표(Shape는 [군집 개수, 피처 개수])로 이를 이용하면 군집 중심점 좌표가 어딘지 시각화 할 수 있습니다.

<br><br>

##### K-평균을 이용한 붓꽃 데이터 세트 군집화

붓꽃 데이터(붓꽃의 꽃받침(sepal)과 꽃잎(petal) 길이와 너비에 따라 품종을 분류하는 데이터 세트)를 이용해 K-평균 군집화를 수행해 보겠습니다.

3개 그룹으로 군집화해 보고, 분류 값과 비교해 보겠습니다.

```python
from sklearn.preprocessing import scale
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
%matplotlib inline

iris = load_iris()
#더 편리한 데이터 핸들링을 위해 DataFrame으로 변환
irisDF = pd.DataFrame(data=iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])
irisDF['target'] = iris.target  #붓꽃 데이터 세트 target 값을 target으로 저장

#3개의 군집으로 군집화, 초기 중심 설정 방식은 k-means++, 최대 반복 횟수 300회
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, random_state=0)
kmeans.fit(irisDF)

#각 데이터 포인트가 속한 군집 중심점 레이블 출력
print(kmeans.labels_)
```

![image](https://user-images.githubusercontent.com/76269316/130238520-b43844bd-6dfd-4c5c-ac86-90a96dae457c.png)

labels_ 값이 0, 1, 2로 돼 있는데 이는 각 레코드가 첫 번째 군집, 두 번째 군집, 세 번째 군집에 속함을 의미합니다.

실제 붓꽃 품종 분류 값과 비교해보면서 군집화가 효과적으로 됐는지 확인해 보겠습니다.

<br>

```python
irisDF['cluster'] = kmeans.labels_  #군집화 분류값을 cluster로 저장
iris_result = irisDF.groupby('target')['cluster'].value_counts()
print(iris_result)
```

![image](https://user-images.githubusercontent.com/76269316/130337648-3c71a721-3e48-44a9-87b7-12100bf68155.png)

분류 target이 0 값인 데이터와 1인 데이터는 각각 1번 군집, 0번 군집으로 잘 그룹핑 됐습니다.

target이 2인 데이터의 경우 0번 군집 1개, 2번 군집 49개로 크게 분산되지 않고 잘 군집화 됐습니다.

<br>

붓꽃 데이터 세트 군집화를 시각화해 보겠습니다.

붓꽃 데이터 세트는 속성이 4개 이므로 2차원 평면에 적합하지 않습니다.

따라서 PCA를 이용해 4개의 속성을 2개로 차원 축소한 다음 X 좌표, Y 좌표로 개별 데이터를 표현 했습니다.

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca_transformed = pca.fit_transform(iris.data)

irisDF['pca_x'] = pca_transformed[:, 0]
irisDF['pca_y'] = pca_transformed[:, 1]

#군집 값 0, 1, 2 인덱스 추출
marker0_ind = irisDF[irisDF['cluster'] == 0].index
marker1_ind = irisDF[irisDF['cluster'] == 1].index
marker2_ind = irisDF[irisDF['cluster'] == 2].index

#군집 값 0, 1, 2에 해당하는 인덱스로 각 군집 레벨의 pca_x, pca_y 값 추출 (o, s, ^로 마커 표시)
plt.scatter(x=irisDF.loc[marker0_ind, 'pca_x'], y=irisDF.loc[marker0_ind, 'pca_y'], marker='o')
plt.scatter(x=irisDF.loc[marker1_ind, 'pca_x'], y=irisDF.loc[marker1_ind, 'pca_y'], marker='s')
plt.scatter(x=irisDF.loc[marker2_ind, 'pca_x'], y=irisDF.loc[marker2_ind, 'pca_y'], marker='^')

plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('3 Clusters Visualization by 2 PCA Components')
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/130240244-3f6c5ba1-c21b-4ae1-b764-4f552941fde3.png)

Cluster 1을 나타내는 네모('s')는 명확히 다른 군집과 잘 분리돼 있습니다.

Cluster 0을 나타내는 동그라미('o')와 Cluster 2를 나타내는 세모('^')는 상당 수준 분리돼 있지만, 네모만큼 명확하게 분리돼 있지는 않는 것을 확인할 수 있습니다.

<br><br>

##### 군집화 알고리즘 테스트를 위한 데이터 생성

사이킷런은 다양한 유형의 군집화 알고리즘을 테스트해 보기 위한 간단한 데이터 생성기를 제공합니다.

대표적인 군집화용 데이터 생성기로 make_blobs(), make_classification() API가 있습니다.

두 API는 여러 개의 클래스에 해당하는 데이터 세트를 만드는데, 하나의 클래스에 여러 개의 군집이 분포될 수 있게 데이터를 생성할 수 있습니다.

둘 중 어떤 것을 사용해도 큰 차이는 없지만, make_blobs()는 개별 군집의 중심점과 표준 편차 제어 기능이 있고, make_classification()은 노이즈를 포함한 데이터를 만들 수 있습니다. (둘 다 분류 용도의 데이터 세트 생성이 가능)

make_circle(), make_moon() API는 중심 기반 군집화로 해결하기 어려운 데이터 세트를 만드는 데 사용됩니다.

<br>

**make_blobs()**

make_blobs()를 호출하면 피처 데이터 세트와 타깃 데이터 세트가 튜플로 반환됩니다.

make_blobs() 호출 파라미터는 다음과 같습니다.

- n_samples : 생성할 총 데이터 개수 (default=100)
- n_features : 데이터 피처 개수, 시각화를 목표로 할 경우 2개로 설정해 첫 번째 피처 - x 좌표, 두 번째 피처 - y 좌표로 표현합니다.
- centers : int 값으로 설정시 군집의 개수를 의미, ndarray로 설정시 개별 군집 중심점의 좌표를 의미
- cluster_std : float 값으로 설정시 생성될 군집 데이터의 표준 편차를 의미, [0.8, 1.2, 0.6]과 같은 형태로 표현시 3개의 군집에서 첫 번째 군집내 데이터의 표준 편차 0.8, 두 번째 군집내 데이터의 표준 편차 1.2, 세 번째 군집내 데이터 표준 편차 0.6을 의미 (군집별로 다른 표준 편차를 가진 데이터 세트를 만들 때 사용)

<br>

![image](https://user-images.githubusercontent.com/76269316/130246535-68b8c4cf-917e-4303-9d21-21d55ab2d649.png)

![image](https://user-images.githubusercontent.com/76269316/130246610-b608dfd1-ece9-4fd3-aae2-d1cfa54a6595.png)

![image](https://user-images.githubusercontent.com/76269316/130246647-02f0369b-e4c7-44e0-a8e6-34704ad04b6a.png)

![image](https://user-images.githubusercontent.com/76269316/130246694-26030a1c-7987-45ad-b317-c348513893c0.png)

make_blobs()의 cluster_std 파라미터를 0.4, 0.8, 1.2 ,1.6으로 했을 때의 데이터를 시각화한 것입니다.

cluster_std가 작을수록 군집 중심에 데이터가 모여 있고, 클수록 데이터가 퍼져 있는 것을 확인할 수 있습니다.

<br>

X, y = make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=0.8, random_state=0)를 호출하면

총 200개의 레코드와 2개의 피처가 3개의 군집화 기반 분포도를 갖는 피처 데이터 세트 X와 3개의 군집화 값을 갖는 타깃 데이터 세트 y가 반환됩니다.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
%matplotlib inline

X, y = make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=0.8, random_state=0)
print(X.shape, y.shape)

#y target 값 분포 확인
unique, counts = np.unique(y, return_counts=True)  #return_counts=True : 고유한 원소별 개수 구하기
print(unique, counts)
```

![image](https://user-images.githubusercontent.com/76269316/130241888-7b4d4bb3-0d24-4d96-9a39-93ce049771bf.png)

피처 데이터 세트 X는 200개 레코드와 2개의 피처를 가지므로 (200, 2), 군집 타깃 데이터 세트 y는 (200, ), 3개의 cluster값은 [0, 1, 2]이고 각각 67, 67, 66개로 균일하게 구성돼 있습니다.



데이터 가공을 편하게 하기 위해 DataFrame으로 변환한 뒤, 어떠한 군집화 분포를 가졌는지 확인해 보기 위해 마커를 다르게 해서 산점도를 그려보겠습니다.

```python
import pandas as pd

#더 편리한 데이터 핸들링을 위해 DataFrame으로 변환
clusterDF = pd.DataFrame(data=X, columns=['ftr1', 'ftr2'])
clusterDF['target'] = y

#3개의 군집 영역으로 구분한 데이터 세트를 생성했으므로 target_list는 [0, 1, 2]
target_list = np.unique(y)
#각 타깃별 산점도 마커 값
markers = ['o', 's', '^', 'P', 'D', 'H', 'x']

for target in target_list:
    target_cluster = clusterDF[clusterDF['target'] == target]
    plt.scatter(x=target_cluster['ftr1'], y=target_cluster['ftr2'], edgecolor='k', marker=markers[target])

plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/130243117-f2e80901-52f5-4a88-9aab-c38292b5e6c9.png)

<br>

이렇게 만들어진 데이터 세트에 KMeans 군집화를 수행한 뒤 군집별로 시각화해 보겠습니다.

KMeans 객체에 fit_predict를 수행해 make_blobs() 피처 데이터 세트인 X 데이터를 군집화합니다.

이를 clusterDF 'kmeans_label' 컬럼으로 저장하고, KMeans 객체 cluster_centers_ 속성으로 개별 군집의 중심 위치 좌표를 표시하겠습니다.

```python
#KMeans 객체를 이용해 X 데이터를 K-Means 클러스터링 수행
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=200, random_state=0)
cluster_labels = kmeans.fit_predict(X)
clusterDF['kmeans_label'] = cluster_labels  #clusterDF에 kmeans_label 컬럼으로 저장

#개별 클러스터의 중심 위치 좌표 시각화를 위해 cluster_centers_ 추출
centers = kmeans.cluster_centers_

unique_labels = np.unique(cluster_labels)
markers = ['o', 's', '^', 'P', 'D', 'H', 'x']

#군집된 label 유형별로 iteration 하면서 marker별 scatter plot 수행
for label in unique_labels:
    label_cluster = clusterDF[clusterDF['kmeans_label'] == label]  #군집별 인덱스 추출
    center_x_y = centers[label]  #군집별 중심 좌표 추출
    plt.scatter(x=label_cluster['ftr1'], y=label_cluster['ftr2'], edgecolor='k', marker=markers[label])
    
    #군집별 중심 위치 좌표 시각화
    plt.scatter(x=center_x_y[0], y=center_x_y[1], s=200, color='white', alpha=0.9, edgecolor='k', marker=markers[label])
    plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color='k', edgecolor='k', marker='$%d$' % label)
    
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/130245218-e33906b0-3456-4c59-8e2c-bcff21b5f623.png)

```python
print(clusterDF.groupby('target')['kmeans_label'].value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/130245336-76fe885c-bb33-42b1-8667-d22008695910.png)

kmeans_label은 군집 번호를 의미하므로 make_blobs() 타깃과 다른 값으로 매핑될 수 있습니다. (그래서 산점도의 마커가 다름)

Target 0이 cluster label 0으로, target 1이 cluster label2로, target 2가 cluster label 1로 거의 대부분 잘 매핑된 것을 확인할 수 있습니다.

<br><br><br>

### 군집 평가(Cluster Evaluation)

위에서 다룬 붓꽃 데이터 세트의 경우 결괏값에 품종을 뜻하는 타깃 레이블이 있었기 때문에 군집화 결과를 레이블과 비교해 군집화가 효율적으로 됐는지 짐작할 수 있었습니다.

하지만 대부분의 군집화 데이터 세트는 이렇게 비교할 만한 타깃 레이블을 갖고 있지 않기 때문에 타깃 레이블과 비교하는 방식으로는 성능을 평가하기 어렵습니다.

군집화의 성능을 평가하는 대표적인 방법인 실루엣 분석에 대해 설명하겠습니다.

<br>

##### 실루엣 분석의 개요

실루엣 분석(silhouette analysis)은 간 군집 간의 거리가 얼마나 효율적으로 분리돼 있는지를 나타냅니다.

효율적으로 잘 분리돼다는 것은 다른 군집과의 거리는 떨어져 있고 동일 군집끼리의 데이터는 서로 가깝게 잘 뭉쳐 있다는 의미입니다.

실루엣 분석은 실루엣 계수(silhouette coefficient)를 기반으로 하는데, 실루엣 계수는 해당 데이터가 같은 군집 내의 데이터와 얼마나 가깝게 군집화돼 있고, 다른 군집에 있는 데이터와는 얼마나 멀리 분리돼 있는지를 나타내는 지표입니다.

![image](https://user-images.githubusercontent.com/76269316/130302511-132aedf9-777c-4de7-828e-8cb0211d44eb.png)

- a(i) : 해당 데이터 포인트와 같은 군집 내에 있는 다른 데이터 포인트와의 거리를 평균한 값
- b(i) : 해당 데이터 포인트가 속하지 않은 가장 가까운 군집과의 평균 거리

두 군집간의 거리가 얼마나 떨어져 있는지 나타내는 값은 b(i) - a(i)이고 이 값을 정규화하기 위해 MAX(a(i), b(i)) 값으로 나누는데 이것이 실루엣 계수의 정의입니다.



**i번째 데이터 포인트의 실루엣 계수 s(i)**

![image](https://user-images.githubusercontent.com/76269316/130302905-973466ac-9e91-4b47-a2ce-fb3f8a521215.png)

실루엣 계수는 -1에서 1 사이의 값을 가지며, 1로 가까워질수록 근처의 군집과 더 멀리 떨어져 있다는 것이고 0에 가까울수록 근처 군집과 가깝다는 의미입니다.

\- 값은 아예 다른 군집에 데이터 포인트가 할당됐음을 의미합니다.

<br>

사이킷런은 실루엣 분석을 위해 다음과 같은 메소드를 제공합니다.

- sklearn.metrics.silhouette_samples(X, labels, metric='euclidean', \**kwds): 인자로 X feature 데이터 세트와 각 피처 데이터 세트가 속한 군집 레이블 값인 labels 데이터를 입력하면 각 데이터 포인트의 실루엣 계수를 계산해 반환합니다.
- sklearn.metrics.silhouette_score(X, labels, metric='euclidean', sample_size=None, \**kwds):  인자로 X feature 데이터 세트와 각 피처 데이터 세트가 속한 군집 레이블 값인 labels 데이터를 입력해주면 전체 데이터의 실루엣 계수 값을 평균해 반환합니다. (즉 np.mean(silhouette_samples())입니다)
  일반적으로 이 값이 높을수록 군집화가 어느정도 잘 됐다고 판단할 수 있지만, 무조건 이 값이 높다고 군집화가 잘 됐다고 판단할 수는 없습니다.

<br>

좋은 군집화가 되려면 다음 조건을 만족해야 합니다.

1. 전체 실루엣 계수의 평균값(silhouette_score())이 0 ~ 1 사이의 값을 가지며, 1에 가까울수록 좋습니다.
2. 전체 실루엣 계수의 평균값과 더불어 개별 군집의 평균값의 편차가 크지 않아야 합니다. (개별 군집의 실루엣 계수값이 전체 실루엣 계수의 평균값에서 크게 벗어나지 않는 것이 중요)
   만약 전체 실루엣 계수 평균값은 높지만 특정 군집의 실루엣 계수값만 유난히 높고 다른 군집의 실루엣 계수값은 낮으면 좋은 군집화가 아닙니다.

<br><br>

##### 붓꽃 데이터 세트를 이용한 평가

붓꽃 데이터 세트 군집화 결과를 실루엣 분석으로 평가해 보겠습니다.

```python
from sklearn.preprocessing import scale
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score  #실루엣 분석 평가 지표 값을 구하기 위한 API 추가
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
%matplotlib inline

iris = load_iris()
feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
irisDF = pd.DataFrame(data=iris.data, columns=feature_names)
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, random_state=0).fit(irisDF)
irisDF['cluster'] = kmeans.labels_

#iris 데이터 세트의 모든 개별 데이터 실루엣 계수 값을 구함
score_samples = silhouette_samples(iris.data, irisDF['cluster'])
print('silhouette_samples() return 값의 shape: ', score_samples.shape)

#irisDF에 실루엣 계수 컬럼 추가
irisDF['silhouette_coeff'] = score_samples

#모든 데이터의 평균 실루엣 계수 값을 구함
average_score = silhouette_score(iris.data, irisDF['cluster'])
print('붓꽃 데이터 세트 Silhouette Analysis Score: {0:.3f}'.format(average_score))
irisDF.head(3)
```

![image](https://user-images.githubusercontent.com/76269316/130303988-c12455d0-4f3d-4fdf-8ec6-6987d1b16b45.png)

head로 출력한 1번 군집의 실루엣 계수 값은 모두 0.8 이상으로 높은데 평균이 0.553인 것은 다른 군집의 실루엣 계수 값이 평균보다 낮기 때문일 것입니다.

<br>

군집별로 group by하여 군집별 실루엣 계수 평균값을 구해보겠습니다.

```python
irisDF.groupby('cluster')['silhouette_coeff'].mean()
```

![image](https://user-images.githubusercontent.com/76269316/130304040-ee7a5fc7-b2e1-4fc4-baf2-5352ddabb54f.png)

1번 군집은 실루엣 계수 평균 값이 0.79인데 반해, 0번은 0.41, 2번은 0.45로 상대적으로 평균값이 낮습니다.

<br><br>

##### 군집별 평균 실루엣 계수의 시각화를 통한 군집 개수 최적화 방법

전체 데이터의 평균 실루엣 계수 값이 높다고 반드시 최적의 군집 개수로 군집화가 잘 됐다고는 볼 수 없습니다.

특정 군집 내의 실루엣 계수 값만 높고 다른 군집은 내부 데이터끼리의 거리가 너무 떨어져 있어 실루엣 계수 값이 낮아져도 평균적으로 높은 값을 가질 수 있습니다.

[여러 개의 군집 개수가 주어졌을 때 이를 분석한 도표](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)를 참고해 평균 실루엣 계수로 군집 개수를 최적화하는 방법을 알아보겠습니다.

<br>

**군집 개수가 2개인 경우**

![image](https://user-images.githubusercontent.com/76269316/130311631-7fc32b2e-a601-4302-b012-7ac3d54df735.png)

이때 평균 실루엣 계수(silhouette_score)는 약 0.704로 매우 높게 나타났습니다.

왼쪽 그림은 개별 군집에 속하는 데이터의 실루엣 계수를 2차원으로 나타낸 것입니다.

X축은 실루엣 계수 값이고, Y축은 개별 군집과 이에 속하는 데이터입니다. 이에 해당하는 데이터는 숫자값으로 표시되지는 않았지만 Y축 높이로 추측할 수 있습니다.

점선으로 표시된 선은 전체 평균 실루엣 계수 값을 나타냅니다.

이로 판단해 볼 때 1번 군집의 모든 데이터는 평균 실루엣 계수 값 이상이지만, 2번 군집의 경우 평균보다 적은 데이터가 매우 많은 것을 확인할 수 있습니다.

오른쪽 그림으로 보충해서 설명하면 1번 군집의 경우 0번 군집과 멀리 떨어져 있고 내부 데이터끼리도 잘 뭉쳐 있는 반면, 0번 군집은 내부 데이터끼리 많이 떨어져 있습니다.

<br>

**군집 개수가 3개인 경우**

![image](https://user-images.githubusercontent.com/76269316/130312382-d6218304-6cf6-4a06-b1a2-d240db74f0e4.png)

전체 데이터 평균 실루엣 계수 값은 약 0.588입니다.

1번, 2번 군집의 경우 평균보다 높은 실루엣 계수 값을 갖고 있지만, 0번의 경우 모두 평균보다 낮습니다.

오른쪽 그림을 보면 0번의 경우 내부 데이터 간 거리도 멀지만, 2번 군집과도 가깝게 위치하고 있는 것을 확인할 수 있습니다.

<br>

**군집 개수가 4개인 경우**

![image](https://user-images.githubusercontent.com/76269316/130312407-e69b7c35-ef58-40b6-80b3-18126f3ea3b6.png)

전체 데이터 평균 실루엣 계수 값은 약 0.65입니다.

왼쪽 그림에서 보듯 개별 군집의 평균 실루엣 계수 값이 비교적 균일하게 위치하고 있습니다.

1번 군집의 경우 모든 데이터가 평균보다 높은 계수 값을 갖고 있고, 0번, 2번의 경우 절반 이상이 평균보다 높은 계수 값을, 3번 군집의 경우만 1/3 정도가 평균보다 높은 계수 값을 갖고 있습니다.

군집이 2개인 경우보다 평균 실루엣 계수 값이 작지만 4개인 경우가 가장 이상적인 군집화 개수로 판단할 수 있습니다.

<br>

위 사이트에 있는 소스 코드 중 왼쪽 그림의 군집별 평균 실루엣 계수 값을 구하는 부분을 **visualize_silhouette(cluster_lists, X_features)** 별도의 함수로 만들었습니다.

개별 군집별 평균 실루엣 계수 값을 시각화해 군집의 개수를 정하는데  사용하겠습니다.

**visualize_silhouette(cluster_lists, X_features**

군집 개수에 따라 K-평균 군집을 수행했을 때 개별 군집별 평균 실루엣 계수 값을 시각화해주는 함수

```python
### 여러개의 클러스터링 갯수를 List로 입력 받아 각각의 실루엣 계수를 면적으로 시각화한 함수 작성
def visualize_silhouette(cluster_lists, X_features): 
    
    from sklearn.datasets import make_blobs
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_samples, silhouette_score

    import matplotlib.pyplot as plt
    import matplotlib.cm as cm
    import math
    
    # 입력값으로 클러스터링 갯수들을 리스트로 받아서, 각 갯수별로 클러스터링을 적용하고 실루엣 개수를 구함
    n_cols = len(cluster_lists)
    
    # plt.subplots()으로 리스트에 기재된 클러스터링 수만큼의 sub figures를 가지는 axs 생성 
    fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols)
    
    # 리스트에 기재된 클러스터링 갯수들을 차례로 iteration 수행하면서 실루엣 개수 시각화
    for ind, n_cluster in enumerate(cluster_lists):
        
        # KMeans 클러스터링 수행하고, 실루엣 스코어와 개별 데이터의 실루엣 값 계산. 
        clusterer = KMeans(n_clusters = n_cluster, max_iter=500, random_state=0)
        cluster_labels = clusterer.fit_predict(X_features)
        
        sil_avg = silhouette_score(X_features, cluster_labels)
        sil_values = silhouette_samples(X_features, cluster_labels)
        
        y_lower = 10
        axs[ind].set_title('Number of Cluster : '+ str(n_cluster)+'\n' \
                          'Silhouette Score :' + str(round(sil_avg,3)) )
        axs[ind].set_xlabel("The silhouette coefficient values")
        axs[ind].set_ylabel("Cluster label")
        axs[ind].set_xlim([-0.1, 1])
        axs[ind].set_ylim([0, len(X_features) + (n_cluster + 1) * 10])
        axs[ind].set_yticks([])  # Clear the yaxis labels / ticks
        axs[ind].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])
        
        # 클러스터링 갯수별로 fill_betweenx( )형태의 막대 그래프 표현. 
        for i in range(n_cluster):
            ith_cluster_sil_values = sil_values[cluster_labels==i]
            ith_cluster_sil_values.sort()
            
            size_cluster_i = ith_cluster_sil_values.shape[0]
            y_upper = y_lower + size_cluster_i
            
            color = cm.nipy_spectral(float(i) / n_cluster)
            axs[ind].fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_values, \
                                facecolor=color, edgecolor=color, alpha=0.7)
            axs[ind].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
            y_lower = y_upper + 10
            
        axs[ind].axvline(x=sil_avg, color="red", linestyle="--")
```

<br>

make_blobs()로 4개 군집 중심의 500개 2차원 데이터 세트를 만들고 이를 K-평균으로 군집화 할 때 2개, 3개, 4개, 5개 중 최적의 군집 개수를 시각화로 알아보겠습니다.

```python
#make_blobs를 통해 군집화를 위한 4개의 군집 중심의 500개 2차원 데이터 세트 생성
from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1, center_box=(-10.0, 10.0), shuffle=True, random_state=1)

visualize_silhouette([2, 3, 4, 5], X)
```

![image](https://user-images.githubusercontent.com/76269316/130312590-58abec0b-1759-4ddc-bcd3-ebbb319e8634.png)

4개의 군집일 때 가장 최적이 됨을 알 수 있습니다.

<br>

붓꽃 데이터를 이용해 K-평균 수행 시 최적의 군집 개수를 알아보겠습니다.

```python
from sklearn.datasets import load_iris
iris=load_iris()
visualize_silhouette([2, 3, 4, 5], iris.data)
```

![image](https://user-images.githubusercontent.com/76269316/130312708-bc0e92f3-59f3-44a2-81bd-57589c8cf680.png)

붓꽃 데이터를 K-평균으로 군집화할 경우 군집 개수를 2개로 하는 것이 가장 좋아 보입니다.



실루엣 계수를 통한 K-평균 군집 평가 방법은 직관적으로 이해하기 쉽지만, 각 데이터별로 다른 데이터와의 거리를 반복적으로 계산해야 하므로 데이터 양이 늘어나면 수행 시간이 크게 늘어납니다.

특히 몇 만 건 이상의 데이터에 대해 사이킷런의 실루엣 계수 평가 API를 개인용 PC에서 수행할 경우 메모리 부족 등의 에러가 발생하기 쉽습니다.

이 경우 군집별로 임의의 데이터를 샘플링해 실루엣 계수를 평가하는 방법을 고민해야 합니다.

<br><br><br>

### 평균 이동(Mean Shift)

평균 이동 군집화는 데이터를 반경 내의 데이터 분포 확률 밀도가 가장 높은 곳으로 이동하기 위해 주변 데이터와의 거리 값을 KDE(Kernel Density Estimation) 함수 값으로 입력한 뒤 그 반환 값을 현재 위치에서 업데이트 하면서 이동시킵니다.

지정된 반복(Iteration) 횟수만큼 전체 데이터에 대해서 KDE 기반으로 데이터를 이동시키면서 군집화 시킨 뒤, 집중적으로 데이터가 모여있어 확률 밀도 함수(probability denstiy function)가 피크인 점을 군집 중심점으로 선정합니다.

![image](https://user-images.githubusercontent.com/76269316/130313187-efa6018d-70d0-406a-b617-64fb124de4ae.png)



KDE는 커널(Kernel) 함수를 통해 어떤 변수의 확률 밀도 함수를 추정하는 대표적인 방법으로, 대표적인 커널 함수로써 가우시안 분포 함수가 사용됩니다.

관측된 데이터 각각에 커널 함수를 적용한 값을 모두 더한 뒤 데이터 건수로 나눠 확률 밀도 함수를 추정합니다.

확률 밀도 함수를 알면 특정 변수가 어떤 값을 갖게 될지에 대한 확률을 알게 되므로 이를 통해 변수의 특성(정규 분포의 경우 평균, 분산), 확률 분포 등 변수의 많은 요소를 알 수 있습니다.

<br>

다음 그림의 왼쪽은 개별 관측 데이터에 가우시안 커널 함수를 적용한 것이고, 오른쪽 그림은 적용 값을 모두 더한 KDE 결과입니다.

<img src="https://user-images.githubusercontent.com/76269316/130314838-a24a8225-c317-4871-927c-78516bb159ef.png" alt="image" style="zoom: 50%;" />

KDE는 다음과 같은 커널 함수식으로 표현됩니다.

K는 커널 함수, x는 확률 변수 값, xi는 관측값, h는 대역폭(bandwidth)입니다.

<img src="https://user-images.githubusercontent.com/76269316/130315018-a13fd258-5514-46fd-8c2a-f8599723e2c0.png" alt="image" style="zoom: 67%;" />

<br>

대역폭 h는 KDE 형태를 부드로운(또는 뾰족한) 형태로 평활화(Smoothing)하는데 적용하며, h를 어떻게 설정하느냐에 따라 확률 밀도 추정 성능을 크게 좌우할 수 있습니다.

다음 그림은 h를 증가시키면서 변화되는 KDE를 나타냅니다.

![image](https://user-images.githubusercontent.com/76269316/130316455-a404a698-ddc1-4f69-a6e5-f2297ff9e063.png)

h가 작은 경우(h=1.0) 좁고 뾰족한 KDE를 갖게 되며, 이는 변동성이 큰 방식으로 확률 밀도 함수를 추정하므로 과적합(over-fitting)하기 쉽습니다.

반대로 h가 큰 경우(h=10) 과도하게 평활화된 KDE로 인해 지나치게 단순화된 방식으로 확률 밀도 함수를 추정해 결과적으로 과소적합(under-fitting)하기 쉽습니다.

따라서 적절한 KDE의 대역폭 h를 계산하는 것은 KDE 기반 평균 이동 군집화에서 매우 중요합니다.

또한 평균 이동 군집화는 군집의 개수를 지정하지 않으며, 오직 대역폭의 크기에 따라 군집화를 수행합니다. (대역폭이 클수록 평활화된 KDE로 인해 적은 수의 군집 중심점을 갖고, 대역폭이 적을수록 많은 수의 군집 중심점을 가짐)



사이킷런은 평균 이동 군집화를 위해 MeanShift 클래스를 제공합니다.

가장 중요한 초기 파라미터는 bandwidth이며 이 파라미터는 KDE의 대역폭 h와 동일합니다.

대역폭 크기 설정이 군집화 품질에 큰 영향을 미치기 때문에 사이킷런은 최적의 대역폭 계산을 위해 estimate_bandwidth() 함수를 제공합니다.

<br>

make_blobs()의 cluster_std를 0.7로 정한 3개의 군집 데이터에 대해  평균 이동 군집화 알고리즘을 적용해 보겠습니다.

```python
import numpy as np
from sklearn.datasets import make_blobs
from sklearn.cluster import MeanShift

X, y = make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=0.7, random_state=0)

meanshift = MeanShift(bandwidth=0.8)
cluster_labels = meanshift.fit_predict(X)
print('cluster labels 유형: ', np.unique(cluster_labels))
```

![image](https://user-images.githubusercontent.com/76269316/130315559-a89404a4-e666-4fee-8371-d96dc5ea2782.png)

군집이 0부터 5까지 6개로 지나치게 세분화돼 군집화됐습니다.



bandwidth를 살짝 높인 1.0으로 MeanShift를 다시 수행해 보겠습니다.

```python
meanshift = MeanShift(bandwidth=1)
cluster_labels = meanshift.fit_predict(X)
print('cluster labels 유형: ', np.unique(cluster_labels))
```

![image](https://user-images.githubusercontent.com/76269316/130315597-09dd474d-dd16-44e9-841d-3b64ee82a444.png)

3개의 군집으로 적절히 군집화 됐습니다.

데이터 분포 유형에 따라 bandwidth 값의 변화는 군집화 개수에 큰 영향을 미칠 수 있기 때문에 최적의 bandwidth로 설정하는 것이 매우 중요합니다.

estimate_bandwidth() 파라미터로 피처 데이터 세트를 입력해주면 최적화된 bandwidth 값을 반환해줍니다.



```python
from sklearn.cluster import estimate_bandwidth

bandwidth = estimate_bandwidth(X)
print('bandwidth 값: ', round(bandwidth, 3))
```

![image](https://user-images.githubusercontent.com/76269316/130315648-54627df7-aae6-49c3-aff5-bbced0ea7c60.png)

estimate_bandwidth()로 측정된 bandwidth를 적용해 군집화를 수행해 보겠습니다.



```python
import pandas as pd

clusterDF = pd.DataFrame(data=X, columns=['ftr1', 'ftr2'])
clusterDF['target'] = y

#estimate_bandwidth()로 최적의 bandwidth 계산
best_bandwidth = estimate_bandwidth(X)

meanshift = MeanShift(bandwidth=best_bandwidth)
cluster_labels = meanshift.fit_predict(X)
print('cluster labels 유형: ', np.unique(cluster_labels))
```

![image](https://user-images.githubusercontent.com/76269316/130315815-a2bf42d8-a0c6-40fc-a013-9b60574607f2.png)

3개의 군집으로 구성됐습니다.

구성된 3개의 군집을 시각화해 보겠습니다.

평균 이동도 K-평균과 유사하게 중심을 갖고 있으므로 cluster_centers_ 속성으로 군집 중심 좌표를 표시하겠습니다.

```python
import matplotlib.pyplot as plt
%matplotlib inline

clusterDF['meanshift_label'] = cluster_labels
centers = meanshift.cluster_centers_
unique_labels = np.unique(cluster_labels)
markers = ['o', 's', '^', 'x', '*']

for label in unique_labels:
    label_cluster = clusterDF[clusterDF['meanshift_label'] == label]
    center_x_y = centers[label]
    
    #군집별로 다른 마커로 산점도 적용
    plt.scatter(x=label_cluster['ftr1'], y=label_cluster['ftr2'], edgecolor='k', marker=markers[label])
    
    #군집별 중심 표현
    plt.scatter(x=center_x_y[0], y=center_x_y[1], s=200, color='gray', alpha=0.9, marker=markers[label])
    plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color='k', edgecolor='k', marker='$%d$' % label)
    
plt.show()
```

![image](https://user-images.githubusercontent.com/76269316/130315980-88535c96-d057-45d5-af2b-8f263bb94d37.png)

Target 값과 군집 label 값을 비교해 보겠습니다.

```python
print(clusterDF.groupby('target')['meanshift_label'].value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/130316000-dfab1b4c-686f-448c-86b0-0579a5b3fbfe.png)

Target 값과 군집 label 값이 1:1로 잘 매칭됐습니다.

<br>

평균 이동의 장점은 데이터 세트의 형태를 특정 형태로 가정한다든가, 특정 분포도 기반의 모델로 가정하지 않기 때문에 좀 더 유연한 군집화가 가능합니다.

또한 이상치의 영향력도 크지 않고, 미리 군집의 개수를 정할 필요도 없습니다.

하지만 알고리즘 수행 시간이 오래 걸리고 무엇보다 band-width 크기에 따른 군집화 영향도가 큽니다.

이 같은 특징 때문에 일반적으로 평균 이동 군집화 기법은 분석 업무 기반 데이터 세트보다 컴퓨터 비전 영역에서 이미지나 영상 데이터의 특정 개체를 구분하거나 움직임을 추적하는 데 사용됩니다.

<br><br><br>

### GMM(Gaussian Mixture Model)

GMM 군집화는 군집화를 적용하고자 하는 데이터가 여러 개의 가우시안 분포(GaussianDistribution)를 갖는 데이터 집합들이 섞여 생성된 것이라는 가정하에 군집화를 수행하는 방식입니다.

정규 분포(Normal Distribution)로도 알려진 가우시안 분포는 좌우 대칭형의 종(Bell) 형태를 가진 통계학에서 가장 잘 알려진 연속 확률 함수입니다.

![image](https://user-images.githubusercontent.com/76269316/130316210-9ca714a2-eda0-4cb2-8f23-86bcf063d063.png)

정규 분포는 평균 μ를 중심으로 높은 데이터 분포도를 갖고 있으며, 좌우 표준편차 1에 전체 데이터의 68.27%, 좌우 표준편차 2에 95.45%의 전체 데이터를 갖고 있습니다. (평균이 0이고 표준편차가 1인 정규 분포를 표준 정규 분포라고 합니다)



GMM은 데이터를 여러 개의 가우시안 분포로 섞인 것으로 간주하고 섞인 데이터 분포에서 개별 유형의 가우시안 분포를 추출합니다.

다음과 같이 세 개의 가우시안 분포 A, B, C를 가진 데이터 세트가 있다고 할 때,

<img src="https://user-images.githubusercontent.com/76269316/130316533-9d6188c1-c6ff-4510-a8bb-7c59bf64eaa5.png" alt="image" style="zoom:50%;" />

이 세개의 정규 분포를 합치면 다음 형태가 됩니다.

<img src="https://user-images.githubusercontent.com/76269316/130316574-13f488de-47c9-495c-bed7-72359aac9556.png" alt="image" style="zoom:50%;" />

![image](https://user-images.githubusercontent.com/76269316/130316619-adf039b8-970a-4fc1-9ace-e511e6eed0c1.png)

군집화를 수행하려는 실제 데이터 세트의 데이터 분포도가 다음과 같다면 쉽게 이 데이터 세트가 정규 분포 A, B, C가 합쳐져 만들어진 데이터 분포도임을 알 수 있습니다.



전체 데이터 세트는 서로 다른 정규 분포 형태를 가진 여러 가지 확률 분포 곡선으로 구성될 수 있으며, 이런 서로 다른 정규 분포에 기반해 군집화를 수행하는 것이 GMM 군집화 방식입니다.



가령 1,000개의 데이터 세트가 있다면 이를 구성하는 여러 개의 정규 분포 곡선을 추출하고 개별 데이터가 이 중 어떤 정규 분포에 속하는지 결정하는 방식입니다.

![image](https://user-images.githubusercontent.com/76269316/130316665-8e782873-f8f3-4d5a-8015-7c888b590e3a.png)

이와 같은 방식을  GMM에서는 모수 추정이라고 합니다.

모수 추정은 대표적으로 2가지를 추정합니다.

- 개별 정규 분포의 평균과 분산
- 각 데이터가 어떤 정규 분포에 해당되는지의 확률



이러한 모수 추정을 위해 GMM은 EM(Expectation and Maximization) 방법을 적용합니다.

사이킷런은 GMM의 EM 방식을 통한 모수 추정 군집화를 지원하기 위해 GaussianMixture 클래스를 제공합니다.

<br><br>

##### GMM을 이용한 붓꽃 데이터 세트 군집화

붓꽃 데이터 세트로 GMM(확률 기반 군집화)과 K-평균(거리 기반 군집화)을 이용해 군집화를 수행한 뒤 양쪽 방식을 비교해 보겠습니다.

```python
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans

import matplotlib.pyplot as plt
import numpy as np
import pandas  as pd
%matplotlib inline

iris = load_iris()
feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']

#좀 더 편리한 데이터 Handling을 위해 DataFrame으로 변환
irisDF = pd.DataFrame(data=iris.data, columns=feature_names)
irisDF['target'] = iris.target
```

<br>

**GMM**

GaussianMixture 객체의 가장 중요한 파라미터는 n_components입니다.

n_components는 gaussian mixture 모델의 총 개수로 K-평균의 n_clusters와 같이 군집 개수를 정하는 데 중요한 역할을 수행합니다.



```python
from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=3, random_state=0).fit(iris.data)
gmm_cluster_labels = gmm.predict(iris.data)

#군집화 결과를 irisDF의 'gmm_cluster' 컬럼으로 저장
irisDF['gmm_cluster'] = gmm_cluster_labels
irisDF['target'] = iris.target

#target 값에 따라 gmm_cluster 값이 어떻게 매핑됐는지 확인
iris_result = irisDF.groupby('target')['gmm_cluster'].value_counts()
print(iris_result)
```

![image](https://user-images.githubusercontent.com/76269316/130316981-71d7ff17-432b-4d83-ba9f-533b0d2e1a7e.png)

Target 0은 cluster 0으로, Target 2는 cluster 1로 모두 잘 매핑됐습니다.

Target 1만 cluster 2로 45개(90%), cluster 1로 5개(10%) 매핑됐습니다.

이번에는 K-평균 군집화를 수행한 결과를 보겠습니다.

<br>

**K-평균**

```python
kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, random_state=0).fit(iris.data)
kmeans_cluster_labels = kmeans.predict(iris.data)
irisDF['kmeans_cluster'] = kmeans_cluster_labels
iris_result = irisDF.groupby('target')['kmeans_cluster'].value_counts()
print(iris_result)
```

![image](https://user-images.githubusercontent.com/76269316/130317169-0059bf46-f49b-4874-a64e-2662a9c4a026.png)

이는 어떤 알고리즘이 더 뛰어나다는 의미가 아니라 붓꽃 데이터 세트가 GMM 군집화에 더 효과적이라는 의미입니다.

K-평균은 평균 거리 중심으로(원형의 범위에서) 중심을 이동하면서 군집화를 수행하는 방식이므로 개별 군집 내의 데이터가 원형으로 흩어져 있는 경우 효과적입니다.

<br><br>

##### GMM과 K-평균의 비교

하지만 데이터가 원형으로 퍼져 있지 않고, 길쭉한 타원형으로 늘어선 경우에는 군집화를 잘 수행하지 못합니다.

make_blobs()로 길쭉한 타원형 데이터 세트를 생성한 뒤 KMeans 군집화를 수행해 보겠습니다.

군집 시각화를 위한 별도의 함수 visualize_cluster_plot(clusterobj, dataframe, label_name, iscluster=True)를 생성해 사용했습니다.



**visualize_cluster_plot**

군집 수행 객체를 입력 받아 시각화하는 함수

- clusterobj: 사이킷런의 군집 수행 객체(KMeans, GaussianMixture의 fit(), predict()로 군집화를 완료한 객체)
  군집화 결과 시각화가 아닌 make_blobs()로 생성한 데이터의 시각화인 경우 None 입력
- dataframe: 피처 데이터 세트와 label 값을 가진 DataFrame
- label_name: 군집화 결과 시각화일 경우 dataframe 내의 군집화 label 컬럼명, make_blobs() 결과 시각화일 경우 dataframe 내의 target 컬럼명
- iscenter: 사이킷런 cluster 객체가 군집 중심 좌표를 제공하면 True, 아니면 False

```python
### 클러스터 결과를 담은 DataFrame과 사이킷런의 Cluster 객체등을 인자로 받아 클러스터링 결과를 시각화하는 함수  
def visualize_cluster_plot(clusterobj, dataframe, label_name, iscenter=True):
    if iscenter :
        centers = clusterobj.cluster_centers_
        
    unique_labels = np.unique(dataframe[label_name].values)
    markers=['o', 's', '^', 'x', '*']
    isNoise=False

    for label in unique_labels:
        label_cluster = dataframe[dataframe[label_name]==label]
        if label == -1:
            cluster_legend = 'Noise'
            isNoise=True
        else :
            cluster_legend = 'Cluster '+str(label)
        
        plt.scatter(x=label_cluster['ftr1'], y=label_cluster['ftr2'], s=70,\
                    edgecolor='k', marker=markers[label], label=cluster_legend)
        
        if iscenter:
            center_x_y = centers[label]
            plt.scatter(x=center_x_y[0], y=center_x_y[1], s=250, color='white',
                        alpha=0.9, edgecolor='k', marker=markers[label]) 
            plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color='k',\
                        edgecolor='k', marker='$%d$' % label)
    if isNoise:
        legend_loc='upper center'
    else: legend_loc='upper right'
    
    plt.legend(loc=legend_loc)
    plt.show()
```



먼저 visualize_cluster_plot() 함수를 사용해 make_blobs로 만든 타원형 데이터 세트를 시각화해 보겠습니다.

```python
from sklearn.datasets import make_blobs

#make_blobs()로 3개의 군집 세트, cluster_std=0.5인 300개의 데이터 세트를 만듦
X, y = make_blobs(n_samples=300, n_features=2, centers=3, cluster_std=0.5, random_state=0)

#길게 늘어난 타원형 데이터 세트를 생성하기 위해 변환
transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]

X_aniso = np.dot(X, transformation)
#feature 데이터 세트와 make_blobs()의 y 결괏값을 DataFrame으로 저장
clusterDF = pd.DataFrame(data=X_aniso, columns=['ftr1', 'ftr2'])
clusterDF['target'] = y

#생성된 데이터 세트를 target별로 다른 마커로 표시해 시각화
visualize_cluster_plot(None, clusterDF, 'target', iscenter=False)
```

![image](https://user-images.githubusercontent.com/76269316/130320924-127700e6-5020-42e8-82c4-abdf78514b31.png)

<br>

위 데이터 세트에 KMeans 군집화를 적용해 보겠습니다.

```python
from sklearn.cluster import KMeans

#3개 군집 기반 KMeans를 X_aniso 데이터 세트에 적용
kmeans = KMeans(3, random_state=0)
kmeans_label = kmeans.fit_predict(X_aniso)
clusterDF['kmeans_label'] = kmeans_label

visualize_cluster_plot(kmeans, clusterDF, 'kmeans_label', iscenter=True)
```

![image](https://user-images.githubusercontent.com/76269316/130321127-936d5fa9-0100-4d12-b2ac-d4c1dca0db01.png)

KMeans로 군집화할 경우 원형 영역 위치로 개별 군집화가 되면서 원본 군집과 다르게 구성된 것을 볼 수 있습니다.

KMeans가 평균 거리 기반으로 군집화를 수행하므로 같은 거리상 원형으로 군집을 구성하기 때문에 위와 같이 길쭉한 방향으로 데이터가 밀접한 경우 최적의 군집화가 어렵습니다.

<br>

이번에는 GMM으로 군집화를 수행해 보겠습니다.

```python
from sklearn.mixture import GaussianMixture

#3개의 n_components 기반 GMM을 X_aniso 데이터 세트에 적용
gmm = GaussianMixture(n_components=3, random_state=0)
gmm_label = gmm.fit(X_aniso).predict(X_aniso)
clusterDF['gmm_label'] = gmm_label

#GaussianMixture는 cluster_centers_ 속성이 없으므로 iscenter를 False로 설정
visualize_cluster_plot(gmm, clusterDF, 'gmm_label', iscenter=False)
```

![image](https://user-images.githubusercontent.com/76269316/130322775-d8bdd053-b947-4bb9-88c7-d3f66cbd2bfd.png)

데이터가 분포된 방향에 따라 정확하게 군집화됐음을 알 수 있습니다.

GMM은 K-평균과 다르게 군집의 중심 좌표를 구현할 수 없기 때문에 군집 중심점이 표현되지 않습니다.

<br>

KMeans, GMM 군집 label 값을 비교해 얼만큼의 군집화 효율 차이가 발생하는지 확인해 보겠습니다.

```python
print('### KMeans Clustering ###')
print(clusterDF.groupby('target')['kmeans_label'].value_counts())
print('\n### Gaussian Mixture Clustering ###')
print(clusterDF.groupby('target')['gmm_label'].value_counts())
```

![image](https://user-images.githubusercontent.com/76269316/130322870-1748f4ff-7143-4ffc-a5d8-2d2334b5fbc8.png)

KMeans의 경우 군집 1번만 정확히 매핑됐지만 나머지 군집의 경우 target 값과 어긋나는 경우가 발생했습니다.

GMM의 경우는 target 값과 잘 매핑돼 있습니다.

GMM의 KMeans보다 유연하게 다양한 데이터 세트에 잘 적용할 수 있지만 군집화를 위한 수행 시간이 오래 걸린다는 단점이 있습니다.

<br><br><br>

### DBSCAN(Density Based Spatial Clustering of Applications with Noise)

![image](https://user-images.githubusercontent.com/76269316/130323172-a7f785b6-0ef7-4fa1-bacd-2daef11f6626.png)

다음과 같이 내부의 원과 외부의 원으로 이루어진 형태의 분포를 가진 데이터 세트를 K-평균이나, 평균이동, GMM으로 군집화를 한다면 효과적으로 군집화를 수행하기 어렵습니다.

DBSCAN은 특정 공간 내에 데이터 밀도 차이를 기반 알고리즘으로 하고 있어 복잡한 기하학적 분포도를 가진 데이터 세트에 대해서도 군집화를 잘 수행합니다.

DBSCAN을 구성하는 가장 중요한 두 파라미터는 다음과 같습니다.

- 입실론 주변 영역(epsilon): 개별 데이터를 중심으로 입실론 반경을 갖는 원형의 영역
- 최소 데이터 개수(min points): 개별 데이터의 입실론 주변 영역에 포함되는 타 데이터 개수



입실론 주변 영역 내에 포함되는 최소 데이터 개수를 충족시키는가 아닌가에 따라 데이터 포인트를 다음과 같이 정의합니다.

- 핵심 포인트(Core Point): 주변 영역 내에 최소 데이터 개수 이상의 타 데이터를 갖고 있는 데이터
- 이웃 포인트(Neighbor Point): 주변 영역 내에 위치한 타 데이터
- 경계 포인트(Border Point): 주변 영역 내에 최소 데이터 개수 이상의 이웃 포인트를 갖고 있지 않지만 핵심 포인트를 이웃 포인트로 갖고 있는 데이터
- 잡음 포인트(Noise Point): 최소 데이터 개수 이상의 이웃 포인트를 갖고 있지 않고, 핵심 포인트도 이웃 포인트로 갖고 있지 않는 데이터



아래의 P1에서 P12까지 12개의 데이터 세트에 대해 DBSCAN 군집화를 적용하면서 주요 개념을 설명하겠습니다.

특정 입실론 반경 내에 포함될 최소 데이터 세트를 6개(자기 자신 포함)로 가정하겠습니다.

<img src="https://user-images.githubusercontent.com/76269316/130323918-fe7b7a24-f806-4820-bbe7-df659dbc3d9e.png" alt="image" style="zoom:50%;" />

<img src="https://user-images.githubusercontent.com/76269316/130323973-6243dc4f-b802-49fc-8817-72116d89332a.png" alt="image" style="zoom:50%;" />

1.P1 데이터를 기준으로 입실론 반경 내에 포함된 데이터가 7개 (자신은 P1, 이웃은 P2, P6, P7, P8, P9, P11)로 최소 데이터 5개 이상을 만족하므로 P1 데이터는 핵심 포인트(Core Point)입니다.

<img src="https://user-images.githubusercontent.com/76269316/130323985-e74bb58b-38f6-4200-a64a-83fec8b66fdb.png" alt="image" style="zoom:50%;" />

2.P2 데이터 역시 반경 내에 6개의 데이터(자신은 P2, 이웃 데이터 P1, P3, P4, P9, P10)를 갖고 있으므로 핵심 포인트입니다.

<img src="https://user-images.githubusercontent.com/76269316/130324005-ab280117-d906-4b96-b9ef-4bb8af5edd0e.png" alt="image" style="zoom:50%;" />

3.핵심 포인트 P1의 이웃 데이터 포인트 P2 역시 핵심 포인트이므로 P1에서 P2로 연결해 직접 접근이 가능합니다.

<img src="https://user-images.githubusercontent.com/76269316/130324119-f23722be-8dcc-4421-ad94-b476b52eab65.png" alt="image" style="zoom:50%;" />

4.특정 핵심 포인트에서 직접 접근이 가능한 다른 핵심 포인트를 서로 연결하면서 군집화를 구성합니다.

이러한 방식으로 점차적으로 군집(Cluster) 영역을 확장해 나가는 것이 DBSCAN 군집화 방식입니다.

<img src="https://user-images.githubusercontent.com/76269316/130324138-fb87b1a0-dc09-409b-9472-42f5edbaf6d4.png" alt="image" style="zoom:50%;" />

5.P3 데이터의 경우 반경 내에 포함되는 이웃 데이터는 P2, P4 2개 뿐이므로 군집으로 구분할 수 있는 핵심 포인트가 될 수 없지만 이웃 데이터 중 핵심 포인트인 P2를 갖고 있습니다.

이처럼 자신은 핵심 포인트가 아니지만, 이웃 데이터로 핵심 포인트를 갖고 있는 데이터를 경계 포인트(Border Point)라고 합니다. 경계 포인트는 군집 외곽을 형성합니다.

<img src="https://user-images.githubusercontent.com/76269316/130324197-f05b7d74-84e3-4de6-a212-77f99f5fb95e.png" alt="image" style="zoom:50%;" />

6.P5와 같이 반경 내에 최소 데이터를 갖고 있지도 않고, 핵심 포인트 또한 이웃 데이터로 갖고 있지 않는 데이터를 잡음 포인트(Noise Point)라고 합니다.

<br>

사이킷런은 DBSCAN 클래스를 통해 DBSCAN 알고리즘을 지원합니다.

DBSCAN 클래스는 다음과 같은 주요 파라미터를 갖고 있습니다.

- eps: 입실론 주변 영역의 반경
- min_samples: 핵심 포인트가 되기 위해 입실론 주변 영역 내에 포함돼야 할 데이터의 최소 개수 (자신의 데이터를 포함)

<br><br>

##### DBSCAN 적용하기 - 붓꽃 데이터 세트

DBSCAN 알고리즘으로 붓꽃 데이터 세트를 군집화해 보겠습니다.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.cluster import DBSCAN
%matplotlib inline

iris = load_iris()
#더 편리한 데이터 핸들링을 위해 DataFrame으로 변환
irisDF = pd.DataFrame(data=iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])
irisDF['target'] = iris.target  #붓꽃 데이터 세트 target 값을 target으로 저장

dbscan = DBSCAN(eps=0.6, min_samples=8)
dbscan_labels = dbscan.fit_predict(iris.data)

irisDF['dbscan_cluster'] = dbscan_labels

iris_result = irisDF.groupby('target')['dbscan_cluster'].value_counts()
print(iris_result)
```

![image](https://user-images.githubusercontent.com/76269316/130337352-256f1cfa-9d0b-46e4-be35-c6e2c43a237f.png)

dbscan_cluster 값을 보면 특이하게 -1이 군집 레이블로 있습니다.

-1은 노이즈에 속하는 군집을 의미합니다. 따라서 붓꽃 데이터 세트는 0과 1 두 개의 군집으로 군집화 됐습니다.

Target 값 유형이 3개인데 군집이 2개라고 군집화 효율이 떨어지는 것은 아닙니다.

<br>

군집화 데이터 세트를 2차원 평면에서 표현하기 위해 PCA를 이용해 2개의 피처로 압축 변환한 뒤, 앞 예제에서 사용한 visualize_cluster_plot() 함수를 이용해 시각화해 보겠습니다.

<br>

**visualize_cluster_plot**

군집 수행 객체를 입력 받아 시각화하는 함수

- clusterobj: 사이킷런의 군집 수행 객체(KMeans, GaussianMixture의 fit(), predict()로 군집화를 완료한 객체) 군집화 결과 시각화가 아닌 make_blobs()로 생성한 데이터의 시각화인 경우 None 입력
- dataframe: 피처 데이터 세트와 label 값을 가진 DataFrame
- label_name: 군집화 결과 시각화일 경우 dataframe 내의 군집화 label 컬럼명, make_blobs() 결과 시각화일 경우 dataframe 내의 target 컬럼명
- iscenter: 사이킷런 cluster 객체가 군집 중심 좌표를 제공하면 True, 아니면 False

```python
### 클러스터 결과를 담은 DataFrame과 사이킷런의 Cluster 객체등을 인자로 받아 클러스터링 결과를 시각화하는 함수  
def visualize_cluster_plot(clusterobj, dataframe, label_name, iscenter=True):
    if iscenter :
        centers = clusterobj.cluster_centers_
        
    unique_labels = np.unique(dataframe[label_name].values)
    markers=['o', 's', '^', 'x', '*']
    isNoise=False

    for label in unique_labels:
        label_cluster = dataframe[dataframe[label_name]==label]
        if label == -1:
            cluster_legend = 'Noise'
            isNoise=True
        else :
            cluster_legend = 'Cluster '+str(label)
        
        plt.scatter(x=label_cluster['ftr1'], y=label_cluster['ftr2'], s=70,\
                    edgecolor='k', marker=markers[label], label=cluster_legend)
        
        if iscenter:
            center_x_y = centers[label]
            plt.scatter(x=center_x_y[0], y=center_x_y[1], s=250, color='white',
                        alpha=0.9, edgecolor='k', marker=markers[label]) 
            plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color='k',\
                        edgecolor='k', marker='$%d$' % label)
    if isNoise:
        legend_loc='upper center'
    else: legend_loc='upper right'
    
    plt.legend(loc=legend_loc)
    plt.show()
```

```python
from sklearn.decomposition import PCA

#2차원으로 시각화하기 위해 PCA n_components=2로 피처 데이터 세트 변환
pca = PCA(n_components=2, random_state=0)
pca_transformed = pca.fit_transform(iris.data)

#visualize_cluster_2d()
irisDF['ftr1'] = pca_transformed[:, 0]
irisDF['ftr2'] = pca_transformed[:, 1]

visualize_cluster_plot(dbscan, irisDF, 'dbscan_cluster', iscenter=False)
```

![image](https://user-images.githubusercontent.com/76269316/130337476-61f53b5e-7165-4bde-b0d6-0284d4557f4d.png)

★로 표현된 값은 모두 노이즈입니다.

DBSCAN은 군집 개수를 알고리즘에 따라 자동 지정하므로, DBSCAN 적용 시 특정 군집 개수로 군집을 강제하는 것보다 eps와 min_samples 파라미터를 통해 최적의 군집을 찾는게 중요합니다.

일반적으로 eps 값을 크게 하면 반경이 커져 포함하는 데이터가 많아지므로 노이즈 데이터 개수가 적어지고, min_samples를 크게 하면 주어진 반경 내에 더 많은 데이터를 포함시켜야 하므로 노이즈 데이터 개수가 커지게 됩니다.

<br>

eps를 0.6에서 0.8로 증가시켜서 확인해 보겠습니다.

```python
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.8, min_samples=8)
dbscan_labels = dbscan.fit_predict(iris.data)

irisDF['dbscan_cluster'] = dbscan_labels
irisDF['target'] = iris.target

iris_result = irisDF.groupby('target')['dbscan_cluster'].value_counts()
print(iris_result)

visualize_cluster_plot(dbscan, irisDF, 'dbscan_cluster', iscenter=False)
```

![image](https://user-images.githubusercontent.com/76269316/130337581-724a9ccb-6c42-4244-908d-b9295a5e260c.png)

노이즈 군집이 3개 밖에 없는 것을 확인할 수 있습니다.

eps가 0.6일 때 노이즈로 분류된 데이터는 eps 반경이 커지면서 Cluster 1에 소속됐습니다.

<br>

이번에는 eps는 0.6으로 유지하고 min_samples를 16으로 늘려보겠습니다.

```python
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.6, min_samples=16)
dbscan_labels = dbscan.fit_predict(iris.data)

irisDF['dbscan_cluster'] = dbscan_labels
irisDF['target'] = iris.target

iris_result = irisDF.groupby('target')['dbscan_cluster'].value_counts()
print(iris_result)

visualize_cluster_plot(dbscan, irisDF, 'dbscan_cluster', iscenter=False)
```

![image](https://user-images.githubusercontent.com/76269316/130337687-a86a637e-8e45-4804-b8c3-23a83ac5da41.png)

노이즈 데이터가 기존보다 많이 증가한 것을 확인할 수 있습니다.

<br><br>

##### DBSCAN 적용하기 - make_circles() 데이터 세트

이번에는 복잡한 기하학적 분포를 갖는 데이터 세트에서 DBSCAN과 타 알고리즘을 비교해 보겠습니다.

make_circles() 함수를 이용해 내부 원과 외부 원 형태로 돼 있는 2차원 데이터 세트를 만들어 사용하겠습니다.

make_circles()는 오직 2개의 피처만을 생성하므로 별도로 피처 개수를 지정하지 않아도 됩니다.

파라미터 noise는 노이저 데이트 세트의 비율, factor는 외부원과 내부 원의 scale 비율입니다.

또한 마찬가지로 시각화를 위해 visualize_cluster_plot 함수를 사용했습니다.

<br>

**visualize_cluster_plot**

군집 수행 객체를 입력 받아 시각화하는 함수

- clusterobj: 사이킷런의 군집 수행 객체(KMeans, GaussianMixture의 fit(), predict()로 군집화를 완료한 객체) 군집화 결과 시각화가 아닌 make_blobs()로 생성한 데이터의 시각화인 경우 None 입력
- dataframe: 피처 데이터 세트와 label 값을 가진 DataFrame
- label_name: 군집화 결과 시각화일 경우 dataframe 내의 군집화 label 컬럼명, make_blobs() 결과 시각화일 경우 dataframe 내의 target 컬럼명
- iscenter: 사이킷런 cluster 객체가 군집 중심 좌표를 제공하면 True, 아니면 False

```python
### 클러스터 결과를 담은 DataFrame과 사이킷런의 Cluster 객체등을 인자로 받아 클러스터링 결과를 시각화하는 함수  
def visualize_cluster_plot(clusterobj, dataframe, label_name, iscenter=True):
    if iscenter :
        centers = clusterobj.cluster_centers_
        
    unique_labels = np.unique(dataframe[label_name].values)
    markers=['o', 's', '^', 'x', '*']
    isNoise=False

    for label in unique_labels:
        label_cluster = dataframe[dataframe[label_name]==label]
        if label == -1:
            cluster_legend = 'Noise'
            isNoise=True
        else :
            cluster_legend = 'Cluster '+str(label)
        
        plt.scatter(x=label_cluster['ftr1'], y=label_cluster['ftr2'], s=70,\
                    edgecolor='k', marker=markers[label], label=cluster_legend)
        
        if iscenter:
            center_x_y = centers[label]
            plt.scatter(x=center_x_y[0], y=center_x_y[1], s=250, color='white',
                        alpha=0.9, edgecolor='k', marker=markers[label]) 
            plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color='k',\
                        edgecolor='k', marker='$%d$' % label)
    if isNoise:
        legend_loc='upper center'
    else: legend_loc='upper right'
    
    plt.legend(loc=legend_loc)
    plt.show()
```

```python
from sklearn.datasets import make_circles

X, y = make_circles(n_samples=1000, shuffle=True, noise=0.05, random_state=0, factor=0.5)
clusterDF = pd.DataFrame(data=X, columns=['ftr1', 'ftr2'])
clusterDF['target'] = y

visualize_cluster_plot(None, clusterDF, 'target', iscenter=False)
```

![image](https://user-images.githubusercontent.com/76269316/130337730-14a5e61a-d944-4b2b-9728-5f8798a0dc0b.png)

<br>

먼저 K-평균으로 데이터 세트를 군집화해 보겠습니다.

```python
#KMeans로 make_circles() 데이터 세트 군집화 수행
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=2, max_iter=1000, random_state=0)
kmeans_labels = kmeans.fit_predict(X)
clusterDF['kmeans_cluster'] = kmeans_labels

visualize_cluster_plot(kmeans, clusterDF, 'kmeans_cluster', iscenter=True)
```

![image](https://user-images.githubusercontent.com/76269316/130337772-7369eb7f-da0c-400e-b4dc-9578ec6ff0b1.png)

위, 아래 군집 중심을 기반으로 위와 아래 절반으로 군집화됐습니다.

거리 기반 군집화로는 위와 같이 데이터가 특정한 형태로 지속해서 이어지는 부분을 찾아내기 어렵습니다.

<br>

이번에는 GMM을 적용해 보겠습니다.

```python
#GMM으로 make_circles() 데이터 세트 군집화 수행
from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=2, random_state=0)
gmm_label = gmm.fit(X).predict(X)
clusterDF['gmm_cluster'] = gmm_label

visualize_cluster_plot(gmm, clusterDF, 'gmm_cluster', iscenter=False)
```

![image](https://user-images.githubusercontent.com/76269316/130337795-7acf1097-da7b-4d19-9ca5-afd00362915c.png)

GMM도 [일렬로 늘어선 데이터 세트](https://seominseok4834.github.io/machine%20learning/7.clustering/#gmm%EA%B3%BC-k-%ED%8F%89%EA%B7%A0%EC%9D%98-%EB%B9%84%EA%B5%90)에서는 효과적으로 군집화 적용이 가능했으나, 내부와 외부 원형으로 구성된 복잡한 형태의 데이터 세트에서는 군집화가 원하는 방향으로 되지 않았습니다.

<br>

DBSCAN으로 군집화를 적용해 보겠습니다.

```python
#DBSCAN으로 make_circles() 데이터 세트 군집화 수행
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.2, min_samples=10)
dbscan_labels = dbscan.fit_predict(X)
clusterDF['dbscan_cluster'] = dbscan_labels
visualize_cluster_plot(dbscan, clusterDF, 'dbscan_cluster', iscenter=False)
```

![image](https://user-images.githubusercontent.com/76269316/130337843-902934bb-9b4e-4a07-ae47-8c571af326ac.png)

DBSCAN으로 군집화를 적용하니 원하는 방향으로 군집화가 됐습니다.

이렇듯 DBSCAN은 기하학적으로 복잡한 데이터 세트에도 효과적인 군집화가 가능합니다.

<br><br><br>

### 군집화 실습 - 고객 세그먼테이션

##### 고객 세그먼테이션의 정의와 기법

고객 세그먼테이션(Customer Segmentation)은 다양한 기준으로 고객을 분류하는 기법을 지칭합니다.

고객 세그먼테이션의 주요 목표는 타깃 마케팅입니다.

타깃 마케팅이란 고객의 여러 특성에 맞게 세분화해서 그 유형에 따라 맞춤형 마케팅이나 서비스를 제공하는 것입니다.

고객 세그먼테이션은 고객의 어떤 요소를 기반으로 군집화할 것인가를 결정하는 것이 중요한데, 저희는 기본적인 고객 분석 요소인 RFM 기법을 사용하겠습니다.

RFM기법은 Recency(R), Frequency(F), Monetary Value(M)의 앞글자를 합친 것으로 각 단어의 의미는 다음과 같습니다.

- Recency: 가장 최근 상품 구입 일에서 오늘까지의 기간
- Frequency : 상품 구매 횟수
- Monetary Value: 총 구매 금액

<br>

[온라인 판매 데이터](http://archive.ics.uci.edu/ml/datasets/online+retail)를 RFM 기반으로 고객 세그먼테이션을 군집화 기반으로 수행해 보겠습니다.

<br>

##### 데이터 세트 로딩과 데이터 클랜징

```python
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
%matplotlib inline

retail_df = pd.read_excel(io='Online Retail.xlsx')
retail_df.head()
```

![image](https://user-images.githubusercontent.com/76269316/130341302-0b302b80-7798-47e0-9473-e4588b839344.png)

이 데이터 세트는 주문 데이터 세트로 Invoice(주문번호) + StockCode(제품코드)를 기반으로 주문량, 주문 일자, 제품 단가, 주문 고객 번호, 주문 고객 국가 등의 컬럼으로 구성돼 있습니다.

데이터 세트의 각 컬럼은 다음과 같습니다.

- InvoiceNo: 주문번호 ('C'로 시작하는 것은 취소 주문)
- StockCode: 제품 코드(Item Code)
- Description: 제품 설명
- Quantity: 주문 제품 건수
- InvocieDate: 주문 일자
- UnitPrice: 제품 단가
- CustomerID: 고객 번호
- Country: 국가명(주문 고객의 국적)

<br>

데이터 세트의 전체 건수, 컬럼 타입, Null 개수를 확인해 보겠습니다.

```python
retail_df.info()
```

![image](https://user-images.githubusercontent.com/76269316/130341383-6994c0ac-1811-4647-b133-b31b67a11f38.png)

전체 데이터는 541,909개인데 CustomerID의 Null 값이 13만 5천개로 너무 많습니다.

또한 다른 컬럼의 경우도 오류 데이터가 존재합니다. 따라서 이 데이터는 두 가지 사전 정제 작업이 필요합니다.

- Null 데이터 제거: CustomerID가 Null 데이터가 많은데, 고객 세그먼테이션을 수행하므로 고객 식별 번호가 없는 데이터는 삭제합니다.
- 오류 데이터 삭제: Quantity, UnitPrice가 0보다 작은 경우인데 이는 사실 오류 데이터라기보다 반환(반품)을 뜻하는 값입니다.
  분석의 효율성을 위해 이 데이터는 모두 삭제하겠습니다.

<br>

불린 인덱싱을 적용해 Quantity > 0 , UnitPrice > 0이고 Not Null인 값만 다시 필터링하겠습니다.

```python
retail_df = retail_df[retail_df['Quantity'] > 0]
retail_df = retail_df[retail_df['UnitPrice'] > 0]
retail_df = retail_df[retail_df['CustomerID'].notnull()]
print(retail_df.shape)
retail_df.isnull().sum()
```

![image](https://user-images.githubusercontent.com/76269316/130341683-9b5e15d6-b9f1-46b5-ab95-f387874692b9.png)

전체 데이터가 397,884개로 줄었습니다. 이제 Null 값은 존재하지 않습니다.

<br>

Country 컬럼은 주문 고객 국가인데 다음과 같이 여러나라들이 포함돼 있습니다.

```python
retail_df['Country'].value_counts()[:5]
```

![image](https://user-images.githubusercontent.com/76269316/130341713-8e0770f3-3e36-4f27-8b3e-c7f7dcd17014.png)

영국이 대다수를 차지하므로 다른 국가의 데이터는 모두 제외하겠습니다.

```python
retail_df = retail_df[retail_df['Country'] == 'United Kingdom']
print(retail_df.shape)
```

![image](https://user-images.githubusercontent.com/76269316/130341745-90880bec-bea7-492c-9147-11abe24e0274.png)

사전 정제는 이 정도로 하고 정제된 데이터를 기반으로 고객 세그먼테이션 군집화를 RFM 기반으로 수행하겠습니다.

<br><br>

##### RFM 기반 데이터 가공

RFM 기반 군집화를 위해 'UnitPrice'와 'Quantity'를 곱해 주문 금액 데이터(Monetary Value, 'sale_amount')를 만들었습니다.

또한 CustomerNo도 편리한 식별성을 위해 int형으로 형 변환 했습니다.

```python
retail_df['sale_amount'] = retail_df['Quantity'] * retail_df['UnitPrice']
retail_df['CustomerID'] = retail_df['CustomerID'].astype(int)
```

<br>

해당 온라인 판매 데이터 세트는 주문 횟수와 주문 금액이 압도적으로 높은 특정 고객에게 많은 특성을 갖고 있습니다. (개인 고객의 주문과 소매점의 주문이 함께 포함돼 있어서)

Top-5 주문 건수와 주문 금액을 가진 고객 데이터를 추출해 보겠습니다.

```python
print(retail_df['CustomerID'].value_counts().head(5))
print(retail_df.groupby('CustomerID')['sale_amount'].sum().sort_values(ascending=False)[:5])
```

![image](https://user-images.githubusercontent.com/76269316/130342292-8192eac8-849c-4f46-af78-72b4900e73cf.png)

위의 결과에서 볼 수 있듯이 몇몇 특정 고객이 많은 주문 건수와 주문 금액을 갖고 있습니다.

주어진 온라인 판매 데이터 세트는 전형적인 판매 데이터 세트와 같이 주문번호(InvoiceNo) + 상품코드(StockCode) 레벨의 식별자로 돼 있습니다.

따라서 InvoiceNo + StockCode로 Group by를 수행하면 거의 1에 가깝게 유일한 식별자 레벨이 되게 됩니다.

```python
retail_df.groupby(['InvoiceNo', 'StockCode'])['InvocieNo'].count().mean
```

<br>

하지만 RFM 기반 고객 세그먼테이션은 고객 레벨로 주문 기간, 주문 횟수, 주문 금액 데이터를 기반으로 세그먼테이션을 수행하는 것입니다.

지금은 데이터가 주문번호 + 상품코드 기준으로 돼 있기 때문에 이를 Recency, Frequency, Monentary Value 데이터로 변경하겠습니다.

<br>

주문번호 기준의  retail_df에 groupby('CustomerID')를 적용해 CustomerID 기준으로 DataFrame을 새로 생성하겠습니다.

DataFrame의 groupby()만 사용해서는 여러 개의 컬럼에 서로 다른 aggregation 연산(count(), max() 등)을 한 번에 수행하기 어렵기 때문에 groupby를 호출해 반환된 DataFrameGroupby 객체에 agg()를 이용해 aggregation 연산을 적용하겠습니다.

Frequency는 고객별 주문 건수이므로 'CustomerID'로 groupby()한 다음, 'InvoiceNo'를 count() aggregation해서 구하고

Monetary Value는 고객별 주문 금액이므로 'CustomerID'로 groupby()한 다음, 'sale_amount'의 sum() aggregation으로 구합니다.

Recency 는 개별 고객당 가장 최근의 주문인데, 데이터 값의 특성으로 인해 두 번의 가공 작업을 수행해야 합니다.

우선 'CustomerID'로 groupby한 뒤 'InvoiceDate' 컬럼에 max() aggregation을 적용해 가장 최근 주문 일자를 먼저 구하겠습니다.

```python
#DataFrame의 groupby()에 multiple 연산을 위해 agg() 이용
#Recency는 InvoiceDate 컬럼의 max()에서 데이터 가공
#Frequency는 InoviceNo 컬럼의 count(), Monetary Value는 sale_amount 컬럼의 sum()
aggregations = {
    'InvoiceDate' : 'max',
    'InvoiceNo' : 'count',
    'sale_amount' : 'sum'
}

cust_df = retail_df.groupby('CustomerID').agg(aggregations)

#groupby된 결과 컬럼 값을 Recency, Frequency, Monetary로 변경
cust_df = cust_df.rename(columns = {'InvoiceDate' : 'Recency',
                                    'InvoiceNo' : 'Frequency',
                                    'sale_amount' : 'Monetary'})
cust_df = cust_df.reset_index()
cust_df.head()
```

![image](https://user-images.githubusercontent.com/76269316/130343245-0a2155e3-51e7-40fc-b11b-f80903aa623c.png)

<br>

Recency는 고객이 가장 최근에 주문한 날짜를 기반으로 하는데, 오늘 날짜를 기준으로 가장 최근 주문 일자를 뺀 날자입니다.

주의할 점은 오늘 날짜를 현재 날짜로 해서는 안 된다는 것입니다.

온라인 판매 데이터가 2010년 12월 1일에서 2011년 12월 9일까지의 데이터이므로 오늘 날짜를 2011년 12월 10일로 해야 합니다.

2011년 12월 10일에서 가장 최근의 가장 최근의 주문 일자를 뺀 데이터에서 일(days)만 추출해 Recency로 생성하겠습니다.

```python
import datetime as dt

cust_df['Recency'] = dt.datetime(2011, 12, 10) - cust_df['Recency']
cust_df['Recency'] = cust_df['Recency'].apply(lambda x: x.days+1)
print('cust_df row와 column 수: ', cust_df.shape)
cust_df.head()
```

![image](https://user-images.githubusercontent.com/76269316/130343257-49defd10-c043-427f-9c44-d6d4dfb92d5e.png)

드디어 고객별 RFM 분석에 필요한 Recency, Frequency, Monetary 컬럼을 모두 생성했습니다.

<br><br>

##### RFM 기반 고객 세그먼테이션

생성된 고객 RFM 데이터 세트는 소매업체의 대규모 주문을 포함하고 있기 때문에 주문 횟수, 주문 금액에서 개인 고객 주문과 매우 큰 차이를 나타내고 있습니다.

이로 인해 매우 왜곡된 데이터 분포도를 갖게 돼 군집화가 한쪽 군집에 집중되는 현상이 발생하게 됩니다.

먼저 온라인 판매 데이터 세트의 컬럼별 히스토그램을 확인해 보겠습니다.

```python
fig, (ax1, ax2, ax3) = plt.subplots(figsize=(12, 4), nrows=1, ncols=3)
ax1.set_title('Recency Histogram')
ax1.hist(cust_df['Recency'])

ax2.set_title('Frequency Histogram')
ax2.hist(cust_df['Frequency'])

ax3.set_title('Monetary Histogram')
ax3.hist(cust_df['Monetary'])
```

![image](https://user-images.githubusercontent.com/76269316/130343273-a6b704cc-c916-48ab-82fa-45e6f47b586d.png)

Recency, Frequency, Monetary 모두 왜곡된 데이터 값 분포도를 갖고 있으며, 특히 Frequency, Monetary의 경우 특정 범위에 값이 몰려 있어 왜곡 정도가 심합니다.

각 컬럼의 데이터 값이 어떻게 분포돼 있는지 확인해 보겠습니다.

```python
cust_df[['Recency', 'Frequency', 'Monetary']].describe()
```

![image](https://user-images.githubusercontent.com/76269316/130343307-5f9f2aa1-f72e-4cca-af9d-f1e7f660d375.png)

Recency는 평균이 91.7인데 50%(중위값 2/4 분위)인 50보다  50보다 크게 높습니다. max 값은 373으로 85%(중우값 3/4 분위)인 142보다 훨씬 커서 왜곡 정도가 높음을 알 수 있습니다.

Frequency도 마찬가지로 평균이 90.3인데 75%인 99.25에 가깝습니다. 이는 max 값 7847을 포함한 상위 몇 개의 큰 값으로 인한 것입니다.

Monetary도 평균 1864.3이 75%인 1578.5보다 매우 큽니다.

이렇게 왜곡 정도가 높은 데이터 세트에 K-평균 군집을 적용하면 중심의 개수를 증가시키더라도 변별력이 떨어지게 됩니다.

StandardScaler로 평균과 표준편차를 재조정한 뒤 K-평균을 수행했습니다.

```python
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples

X_features = cust_df[['Recency', 'Frequency', 'Monetary']].values  #DataFrame을 ndarray로 변환
X_features_scaled = StandardScaler().fit_transform(X_features)

kmeans = KMeans(n_clusters=3, random_state=0)
labels = kmeans.fit_predict(X_features_scaled)
cust_df['cluster_label'] = labels

print('실루엣 스코어: {0:.3f}'.format(silhouette_score(X_features_scaled, labels)))
```

![image](https://user-images.githubusercontent.com/76269316/130343533-adb5aa98-5f61-4a64-89a0-22de8d7a3b02.png)

<br>

[군집별 평균 실루엣 계수의 시각화를 통한 군집 개수 최적화 방법](https://seominseok4834.github.io/machine%20learning/7.clustering/#%EA%B5%B0%EC%A7%91%EB%B3%84-%ED%8F%89%EA%B7%A0-%EC%8B%A4%EB%A3%A8%EC%97%A3-%EA%B3%84%EC%88%98%EC%9D%98-%EC%8B%9C%EA%B0%81%ED%99%94%EB%A5%BC-%ED%86%B5%ED%95%9C-%EA%B5%B0%EC%A7%91-%EA%B0%9C%EC%88%98-%EC%B5%9C%EC%A0%81%ED%99%94-%EB%B0%A9%EB%B2%95)에서 사용한 **visualize_silhouette()** 함수와 군집 개수별로 군집화 구성을시각화하는 **visualize_kmeans_plot_multi()** 함수를 사용해 군집 개수에 따른 실루엣 계수와 군집화 구성을 시각화 했습니다.

<br>

**visualize_silhouette(cluster_lists, X_features**

군집 개수에 따라 K-평균 군집을 수행했을 때 개별 군집별 평균 실루엣 계수 값을 시각화해주는 함수

```python
### 여러개의 클러스터링 갯수를 List로 입력 받아 각각의 실루엣 계수를 면적으로 시각화한 함수 작성
def visualize_silhouette(cluster_lists, X_features): 
    
    from sklearn.datasets import make_blobs
    from sklearn.cluster import KMeans
    from sklearn.metrics import silhouette_samples, silhouette_score

    import matplotlib.pyplot as plt
    import matplotlib.cm as cm
    import math
    
    # 입력값으로 클러스터링 갯수들을 리스트로 받아서, 각 갯수별로 클러스터링을 적용하고 실루엣 개수를 구함
    n_cols = len(cluster_lists)
    
    # plt.subplots()으로 리스트에 기재된 클러스터링 수만큼의 sub figures를 가지는 axs 생성 
    fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols)
    
    # 리스트에 기재된 클러스터링 갯수들을 차례로 iteration 수행하면서 실루엣 개수 시각화
    for ind, n_cluster in enumerate(cluster_lists):
        
        # KMeans 클러스터링 수행하고, 실루엣 스코어와 개별 데이터의 실루엣 값 계산. 
        clusterer = KMeans(n_clusters = n_cluster, max_iter=500, random_state=0)
        cluster_labels = clusterer.fit_predict(X_features)
        
        sil_avg = silhouette_score(X_features, cluster_labels)
        sil_values = silhouette_samples(X_features, cluster_labels)
        
        y_lower = 10
        axs[ind].set_title('Number of Cluster : '+ str(n_cluster)+'\n' \
                          'Silhouette Score :' + str(round(sil_avg,3)) )
        axs[ind].set_xlabel("The silhouette coefficient values")
        axs[ind].set_ylabel("Cluster label")
        axs[ind].set_xlim([-0.1, 1])
        axs[ind].set_ylim([0, len(X_features) + (n_cluster + 1) * 10])
        axs[ind].set_yticks([])  # Clear the yaxis labels / ticks
        axs[ind].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])
        
        # 클러스터링 갯수별로 fill_betweenx( )형태의 막대 그래프 표현. 
        for i in range(n_cluster):
            ith_cluster_sil_values = sil_values[cluster_labels==i]
            ith_cluster_sil_values.sort()
            
            size_cluster_i = ith_cluster_sil_values.shape[0]
            y_upper = y_lower + size_cluster_i
            
            color = cm.nipy_spectral(float(i) / n_cluster)
            axs[ind].fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_values, \
                                facecolor=color, edgecolor=color, alpha=0.7)
            axs[ind].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
            y_lower = y_upper + 10
            
        axs[ind].axvline(x=sil_avg, color="red", linestyle="--")
```

<br>

**visualize_kmeans_plot_multi**

군집 개수에 따른 군집화 구성을 시각화하는 함수

```
### 여러개의 클러스터링 갯수를 List로 입력 받아 각각의 클러스터링 결과를 시각화 
def visualize_kmeans_plot_multi(cluster_lists, X_features):
    
    from sklearn.cluster import KMeans
    from sklearn.decomposition import PCA
    import pandas as pd
    import numpy as np
    
    # plt.subplots()으로 리스트에 기재된 클러스터링 만큼의 sub figures를 가지는 axs 생성 
    n_cols = len(cluster_lists)
    fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols)
    
    # 입력 데이터의 FEATURE가 여러개일 경우 2차원 데이터 시각화가 어려우므로 PCA 변환하여 2차원 시각화
    pca = PCA(n_components=2)
    pca_transformed = pca.fit_transform(X_features)
    dataframe = pd.DataFrame(pca_transformed, columns=['PCA1','PCA2'])
    
     # 리스트에 기재된 클러스터링 갯수들을 차례로 iteration 수행하면서 KMeans 클러스터링 수행하고 시각화
    for ind, n_cluster in enumerate(cluster_lists):
        
        # KMeans 클러스터링으로 클러스터링 결과를 dataframe에 저장. 
        clusterer = KMeans(n_clusters = n_cluster, max_iter=500, random_state=0)
        cluster_labels = clusterer.fit_predict(pca_transformed)
        dataframe['cluster']=cluster_labels
        
        unique_labels = np.unique(clusterer.labels_)
        markers=['o', 's', '^', 'x', '*']
       
        # 클러스터링 결과값 별로 scatter plot 으로 시각화
        for label in unique_labels:
            label_df = dataframe[dataframe['cluster']==label]
            if label == -1:
                cluster_legend = 'Noise'
            else :
                cluster_legend = 'Cluster '+str(label)           
            axs[ind].scatter(x=label_df['PCA1'], y=label_df['PCA2'], s=70,\
                        edgecolor='k', marker=markers[label], label=cluster_legend)

        axs[ind].set_title('Number of Cluster : '+ str(n_cluster))    
        axs[ind].legend(loc='upper right')
    
    plt.show()
```



```python
visualize_silhouette([2, 3, 4, 5], X_features_scaled)
visualize_kmeans_plot_multi([2, 3, 4, 5], X_features_scaled)
```

![image](https://user-images.githubusercontent.com/76269316/130343700-d89e954d-2a9c-43b4-a45a-6185b71e7bf9.png)

군집이 2개일 경우 0번 군집과 1번 군집이 너무 개괄적으로 군집화 됐습니다.

군집 수를 증가시키면 개선이 가능할 것 같았는데 군집이 3개 이상일 때부터 데이터 세트 개수가 너무 작은 군집이 만들어집니다.

이 군집에 속한 데이터는 개수가 작을뿐더러 실루엣 계수 역시 상대적으로 매우 작습니다. 또한 군집 내부에서도 데이터가 광범위하게 퍼져있습니다.

이 소수의 데이터 세트는 왜곡된 데이터 값인 특정 소매점의 대량 주문 데이터입니다. 군집 수를 계속 늘려봐야 이 군집만 지속적으로 분리해 의미 없는 군집화 결과로 이어지게 됩니다.

<br>

데이터 세트의 왜곡 정도를 낮추기 위해 로그 변환을 적용하고 다시 K-평균 알고리즘을 적용해 보겠습니다.

```python
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples

#Recency, Frequency, Monetary 컬럼에 np.log1p()로 Log Transformation
cust_df['Recency_log'] = np.log1p(cust_df['Recency'])
cust_df['Frequency_log'] = np.log1p(cust_df['Frequency'])
cust_df['Monetary_log'] = np.log1p(cust_df['Monetary'])

#Log Transformation 데이터에 StandardScaler 적용
X_features = cust_df[['Recency_log', 'Frequency_log', 'Monetary_log']].values  #DataFrame을 ndarray로 변환
X_features_scaled = StandardScaler().fit_transform(X_features)

kmeans = KMeans(n_clusters=3, random_state=0)
labels = kmeans.fit_predict(X_features_scaled)
cust_df['cluster_label'] = labels;

print('실루엣 스코어: {0:.3f}'.format(silhouette_score(X_features_scaled, labels)))
```

![image](https://user-images.githubusercontent.com/76269316/130344006-8a42c64c-6a03-459c-8dcf-2182a30acfaf.png)

실루엣 스코어는 로그 변환 전보다 떨어집니다.

로그 변환한 데이터 세트를 기반으로 실루엣 계수와 군집화 구성을 시각화해 보겠습니다.

```python
visualize_silhouette([2,3,4,5],X_features_scaled)
visualize_kmeans_plot_multi([2,3,4,5],X_features_scaled)
```

![image](https://user-images.githubusercontent.com/76269316/130344023-f2453914-24d2-4c97-9769-effdf838e49c.png)

실루엣 스코어는 로그 변환하기 전보다 떨어지지만 더 균일하게 군집화가 구성됐음을 확인할 수 있습니다.

이처럼 왜곡된 데이터 세트에 대해 로그 변환으로 데이터를 일차 변환한 후에 군집화를 수행하면 더 나은 결과를 도출할 수 있습니다.

<br><br><br>

### 정리

각 군집화 기법은 나름의 장/단점을 갖고 있으며, 군집화하려는 데이터 특성에 맞게 선택해야 합니다.

K-평균의 경우 거리 기반으로 군집 중심점을 이동시키면서 군집화를 수행합니다. 

매우 쉽고 직관적이지만 복잡한 구조를 갖는 데이터 세트에 적용하기는 한계가 있으며, 군집의 개수를 최적화하기 어렵습니다.

군집이 잘 되었는지 평가를 위해서는 실루엣 계수를 이용합니다.

<br>

평균 이동(Mean Shift)은 데이터가 모여 있는 밀도가 가장 높은 쪽으로 군집 중심점을 이동하면서 군집화를 수행합니다.

일반 업무 기반의 정형 데이터 세트보다 컴퓨터 비전 영역에서 이미지나 영상 데이터에서 특정 개체를 구분하거나 움직임을 추적하는 데 뛰어난 역할을 수행합니다.

<br>

GMM(Gaussian Mixture Model) 군집화는 군집화를 적용하고자 하는 데이터가 여러 개의 가우시안 분포(Gaussian Distribution)를 섞어 생성된 모델이라 가정하고 수행하는 방식입니다.

전체 데이터 세트에서 서로 다른 정규 분포 형태를 추출해 다른 정규 분포를 가진 데이터 세트를 각각 군집화하는 것입니다.

GMM의 경우 K-평균보다 유연하게 다양한 데이터 세트에 잘 적용될 수 있지만 수행 시간이 오래 걸린다는 단점이 있습니다.

<br>

DBSCAN(Density Based Spatial Clustering of Applications with Noise)은 밀도 기반 군집화의 대표적인 알고리즘으로 입실론 주변 영역 내에 포함되는 최소 데이터 개수 충족 여부에 따라 핵심 포인트, 이웃 포인트, 경계 포인트, 잡음 포인트로 구분하고 특정 핵심 포인트에서 직접 접근이 가능한 다른 핵심 포인트를 서로 연결하면서 군집화를 구성하는 방식입니다.

데이터 분포가 기하학적으로 복잡한 데이터 세트에도 효과적인 군집화가 가능합니다.
