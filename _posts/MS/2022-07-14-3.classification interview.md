---
title: "[대학원 면접 준비] ML Classification 정리"
excerpt: "대학원 면접용 분류 정리"
toc: true
toc_label: "ML Classification 정리"
toc_sticky: true

categories:
  - MS
tags:
  - 대학원
last_modified_at: 2022-07-14
---

### Classification

분류는 지도학습의 대표적인 태스크 중 하나로, **정답(label)이 있는 데이터를 가지고 데이터가 어떤 클래스에 속하는지를 학습한 다음 새롭게 관측된 데이터에 대한 클래스를 판별하는 것**입니다.

분류를 위한 학습 알고리즘에는 아래와 같이 매우 다양한 알고리즘들이 있습니다.

- 베이즈(Bayes) 통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)
- 독립변수와 종속변수의 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression)
- 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)
- 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine)
- 근접 거리를 기준으로 하는 최소 근접(Nearest Neighbor) 알고리즘
- 심층 연결 기반의 신경망 (Neural Network)
- 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)

먼저 Decision Tree(결정 트리)에 대해 알아보겠습니다.

<br>

### Decision Tree (결정 트리)

결정 트리는 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 알고리즘입니다.

아래 그림은 결정 트리의 구조를 간략하게 나타낸 것으로, 규칙 노드(Decision Node)로 표시된 노드는 규칙 조건이 되고, 새로운 규칙 조건마다 서브 트리(Sub Tree)가 생성됩니다.

리프 노드(Leaf Node)로 표시된 노드는 결정된 클래스 값입니다.

<img width="1512" alt="스크린샷 2022-07-14 오후 8 32 12" src="https://user-images.githubusercontent.com/76269316/178973105-34f4043f-3391-4c10-bcd6-0439d4ec5d73.png">

규칙 노드는 데이터의 특징을 나타내는 피처가 결합해 새로운 규칙 조건을 만들 때마다 생성됩니다. 하지만 많은 규칙이 있다는 건 분류를 결정하는 방식이 복잡해진다는 것이고, 이는 과적합(학습데이터를 과하게 학습하는 것)으로 이어지게 됩니다.

즉, 트리의 깊이(depth)가 깊어질수록 결정 트리의 예측 성능이 저하되게 됩니다.

따라서 가능한 한 적은 규칙 노드로 높은 예측 정확도를 가져야 하는데 이를 위해서는, 최대한 균일한 데이터 세트를 구성할 수 있도록 분할(Split)하는 것이 필요합니다.

<br>

**균일한 데이터세트란?**

<img src="https://user-images.githubusercontent.com/76269316/126084685-9a654ae8-37ed-4f7b-8a8c-06028853d29e.png" alt="image" style="zoom:67%;" />

다음과 같이 흑백으로 된 공이 있다고 할 때, 균일한 순서대로 나열하면 C -> B -> A입니다.

C는 모든 공이 검은 공이므로 가장 균일하고, B는 일부 하얀 공을 갖고 있지만 대부분은 검은 공으로 구성돼 있어 다음으로 균일도가 높습니다. A는 검은 공과 하얀 공이 비슷하게  있어 균일도가 가장 낮습니다.

<br>

결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만들고, 이후 나머지 데이터 세트에 대해서도 균일도가 높게 자식 데이터 세트를 쪼개는 것을 반복하면서 데이터 값을 예측합니다.

<br>

- 형태 : 동그라미, 네모, 세모

- 색깔 : 노랑, 빨강, 파랑

각각의 형태와 색깔을 갖는 레고 블록이 다음과 같이 섞여있다고 하면, 결정 트리에서 가장 첫 번째로 만들어지는 규칙 노드는 **if 색깔 == '노란색'**입니다.

색깔이 노란색인지 아닌지만 비교함으로써 모든 노란 동그라미 블록을 예측할 수 있고, 그 다음 나머지 블록에 대해 다시 균일도가 높은 조건을 찾아 분류하는 것이 가장 효율적이기 때문입니다.

<img width="318" alt="스크린샷 2022-07-14 오후 10 32 51" src="https://user-images.githubusercontent.com/76269316/178994530-f50bee6e-d203-40ba-97ec-bb57bfdf7fc8.png">

<br>

정보 균일도를 수학적으로 측정하는 방법은 **정보 이득 (Information Gain)**과 **지니 계수 (Gini Coefficient)**가 있습니다.

- 정보 이득 : 엔트로피는 데이터 집합의 혼잡도를 나타내는데 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮게 됩니다. 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값입니다. (결정 트리에서는 정보 이득 지수가 높은 속성을 기준으로 분할합니다)
- 지니 계수 : 경제학에서 불평등 지수를 나타낼 때 사용하는 계수로 0이 가장 평등하고 1로 갈수록 불평등합니다. (결정 트리에서는 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할합니다)

결론적으로 결정 트리 알고리즘은 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추게 됩니다.

<br>

##### Decision Tree 특징

|                             장점                             |                             단점                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| - 직관적이다.<br />- 피처의 스케일리이나 정규화 등의 사전 가공 영향도가 크지 않다. | - 모든 데이터 상황을 만족하는 완벽한 규칙을 만들려고 하게되고(그럴 수 없음에도 불구하고) 결국, 트리의 깊이가 깊어지고 트리가 복잡해져서 예측 성능이 떨어지게 됨<br />➡️ 이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝이 필요 |

<br><br>

### Ensemble Learning (앙상블 학습)

앙상블 학습은 여러 개의 분류기(Classifier)를 생성해 결합함으로써 보다 정확한 최종 예측을 도출하는 기법으로 Bagging, Voting, Boosting, Stacking 등이 있습니다.

- Bagging : 모두 같은 유형(대부분 결정 트리 알고리즘)의 알고리즘 분류기 중 투표를 통해 최종 예측 결과를 결정하는 방식
- Voting : 서로 다른 알고리즘 분류기 중 투표를 통해 최종 예측 결과를 결정하는 방식

Bagging과 Voting은 투표 방식이라는 점에서 유사하지만, Bagging은 같은 종류의 모델들을 사용하고 Voting은 다른 종류의 알고리즘 모델들을  사용한다는 차이점이 있습니다.

|                           Bagging                            |                            Voting                            |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img width="537" alt="스크린샷 2022-07-15 오후 4 35 42" src="https://user-images.githubusercontent.com/76269316/179175377-c1248e8d-fe90-40cf-b773-376f2ea747c2.png"> | <img width="506" alt="스크린샷 2022-07-15 오후 4 36 17" src="https://user-images.githubusercontent.com/76269316/179175479-298f8a6e-4ad1-4770-9a96-5661d5a06e6b.png"> |

Voting과 Bagging은 학습하는 데이터 세트가 다른데,

Bagging 방식은 원본 학습 데이터를 샘플링해 추출한 다음, 개별 분류기에 할당해서 학습하는데 이렇게 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식을 **Bootstrapping 분할 방식**이라고 합니다.

<img src="https://user-images.githubusercontent.com/76269316/179231715-7b50826c-c640-4c1c-8d39-b5dec152a58f.png" alt="image" style="zoom: 67%;" />

출처: [A Trip to Random Forest](https://medium.com/greyatom/a-trip-to-random-forest-5c30d8250d6a)

개별 분류기가 Bootstrapping 방식으로 샘플링된 데이터 세트로 학습한 다음 예측을 수행한 결과를 Voting을 통해 최종 예측 결과를 선정하는 방식이 Bagging Ensemble 방식입니다.

교차 검증이 데이터 세트 간에 중첩을 허용하지 않는 것과 다르게 Bagging 방식은 중첩을 허용합니다. (10,000개의 데이터를 10개의 분류기가 배깅 방식으로 나누더라도 각 1000개의 데이터 내에는 중복 데이터가 있음)

<br>

### Voting

Voting 방법에는 Hard Voting, Soft Voting 두 가지가 있는데, 하드 보팅보다는 소프트 보팅이 예측 성능이 좋아서 더 많이 사용됩니다.

- Hard Voting : 예측한 결과값들 중 다수의 분류기가 결정한 예측값을 최종 보팅 결과값으로 선정
- Soft Voting : 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결과값으로 선정

|                         Hard Voting                          |                         Soft Voting                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img width="556" alt="스크린샷 2022-07-15 오후 9 48 21" src="https://user-images.githubusercontent.com/76269316/179226031-e5313c75-1d6d-473f-9708-b9ec6c02f767.png"> | <img width="611" alt="스크린샷 2022-07-15 오후 10 10 57" src="https://user-images.githubusercontent.com/76269316/179229768-dff28fa2-56ef-40eb-a494-e4cf2f1f7969.png"> |

앞서 설명한 결정 트리 알고리즘은 훈련 데이터의 정확한 예측을 위해 트리의 깊이를 깊게하면, 과적합이 발생해 실제 테스트 데이터에서 예측 성능이 떨어지게 됩니다.

하지만 앙상블 학습에서는 결정 트리 알고리즘의 단점을 수십~수천 개의 많은 분류기를 결합해 다양한 상황을 학습하게 함으로써 극복합니다.

결정 트리 알고리즘의 장점은 그대로 취하고 단점은 보완하면서 편향-분산 트레이드오프 효과를 극대화 한다는 것입니다.

<br>

##### Random Forest

랜덤 포레스트는 Bagging Ensemble 방식을 사용한 알고리즘으로 여러 개의 Decision Tree를 만들어, 각 트리에서 분류한 결과를 투표해 가장 많이 득표한 결과를 최종 분류 결과로 선택하는 알고리즘입니다.

랜덤 포레스트가 생성한 일부 트리는 overfitting 될 수 있지만, 많은 수의 트리를 생성함으로써 예측하는데 있어 과적합이 큰 영향을 미치지 못 하도록 합니다.

<img width="1038" alt="스크린샷 2022-07-15 오후 10 15 22" src="https://user-images.githubusercontent.com/76269316/179230516-0cf5127c-d4a0-4228-b03b-4a561c54c723.png">

*나무가(Tree) 여러 개 있는 모습이 숲(Forest) 같아서 이름을 이렇게 지은건가..?*

<br><br>

### Boosting

Boosting 알고리즘은 여러 개의 약한 학습기(weak learner)를 결합해, 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치를 부여해 오류를 개선해 나가는 학습 방식입니다.

Boosting 계열의 대표적인 모델로는 AdaBoost(Adaptive boosting)와 Gradient Boost가 있습니다.

<br>

##### AdaBoost

AdaBoost에 대해서는 유튜브에 잘 정리돼 있어서 해당 링크를 참고하시면 될 것 같습니다.

- AdaBoost 정리 유튜브 👉  [AdaBoost, Clearly Explained](https://www.youtube.com/watch?v=LsK-xG1cLYA)

- 해당 유튜브 내용을 한글로 정리한 블로그 글 👉 [머신러닝 - 14. 에이다 부스트(AdaBoost)](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-14-AdaBoost)

저는 AdaBoost가 어떻게 학습을 진행하는지 대략적으로만 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126178465-10a3db0f-c8d5-4b90-8604-c08ea4e8a01f.png)

맨 왼쪽 그림과 같이 +, -로 된 피처 데이터 세트가 있다면

- Step 1은 첫 번째 약한 학습기가 분류 기준 1로 +와 -를 분류한 것입니다. 동그라미로 표시된 ⊕ 데이터는 잘못 분류된 오류 데이터입니다.
- Step 2에서는 이 오류 데이터에 대해 가중치 값을 부여합니다. 가중치가 부여된 오류 데이터 +는 다음 약한 학습기가 잘 분류할 수 있게 크기가 커졌습니다.
- Step 3는 두 번째 약한 학습기가 분류 기준 2로 +와 -를 분류했습니다. 마찬가지로 동그라미로 표시된 ⊖ 데이터는 잘못 분류된 오류 데이터입니다.
- Step 4에서 잘못 분류된 - 오류 데이터에 대해 다음 약한 학습기가 잘 분류할 수 있게 더 큰 가중치를 부여합니다. (크기가 커짐)
- Step 5에서 세 번째 약한 학습기가 분류 기준 3으로 +와 -를 분류하고 오류 데이터를 찾습니다.
- 마지막으로 맨 아래에 첫 번째, 두 번째, 세 번째 약한 학습기를 모두 결합한 예측 결과입니다. 개별 약한 학습기보다 훨씬 정확도가 높아졌음을 알 수 있습니다.

<br><br>

### 분류 관련 면접 질문

**1. 50개의 작은 의사결정 나무는 큰 의사결정 나무보다 괜찮을까요? 왜 그렇게 생각하나요?**

<details markdown="1">
<summary>답변</summary>
50개의 작은 의사결정 나무는 앙상블 Bagging 기법을 사용한 모델로 볼 수 있다. 이 문제는 즉, Bagging의 대표적인 방법인 RandomForest가 왜 좋은지 설명하는 문제이다. <br><br>
큰 트리는 작은 편향(bias)와 큰 분산(variance)을 갖기 때문에, 깊이가 깊은 트리는 훈련데이터에 대해 과적합(overfitting)하게 된다. Random Forest 방식으로 학습하면, 트리들의 편향은 그대로 유지하면서 여러 데이터셋/여러 경우에 대해 학습하기 떄문에 분산을 감소시킬 수 있다. <br><br>
또한 한 개의 결정트리의 경우, 훈련 데이터에 있는 노이즈에 대해 매우 민감하지만, 여러 트리들을 만들면서 평균을 내면, 노이즈에 대해 강인해질 수 있다. 따라서 하나의 깊은/큰 의사결정 나무보다 50개의 작은 의사결정 나무가 더 좋은 모델을 완성시킨다고 할 수 있다.
<br>

**2. 앙상블 방법엔 어떤 것들이 있나요?**

<details markdown="1">
<summary>답변</summary>
앙상블(Ensemble)은 여러 개의 모델을 조합해서 그 결과를 뽑아내는 방법이다. <br><br> "정확도가 높은 강한 모델을 하나 사용하는 것보다, 정확도가 낮은 약한 모델을 여러개 조합 하는 방식의 정확도가 높다"는 개념에서 비롯한 방법으로 Bagging, Boosting, Stacking 등이 있다. <br><br>
배깅(Bagging, Bootstrap Aggregation)이란 샘플을 여러번 뽑아(Bootstrap = 복원 랜덤 샘플링) 각 모델을 학습시켜 결과물을 집계(Aggregation)하는 방법이다. <br><br> 카테고리 데이터는 투표 방식(Votinig)으로 결과를 집계하며, 연속형 데이터는 평균으로 집계한다. Bagging을 사용한 대표적인 기법에는 Random Forest 방법이 있다. <br><br> 학습 데이터가 충분하지 않더라도 충분한 학습효과를 주어 높은 bias의 underfitting 문제나, 높은 variance로 인한 overfitting 문제를 해결하는데 도움을 준다. <br><br>
부스팅(Boosting)이란 이전 모델의 오답에 가중치를 높게 부여하여 다음 모델을 학습하는 방법으로, 오답을 정답으로 맞추기 위해 오답에 더 집중하여 학습시킨다. 일반적으로 배깅에 비해 정확도가 높다. <br><br> 그러나 틀렸던 부분에 대해 반복적으로 학습하므로 과적합이 발생할 우려가 있으며, outlier에 취약하고, 속도가 느리다는 단점을 가지고 있다. GBM(Gradient Boosting) 방법이 대표적이고, AdaBoost, GradientBoost, XGBoost 등의 알고리즘이 존재한다.
스태킹(Stacking)이란 여러 개별 모델이 예측한 결과값을 다시 학습 데이터셋으로 사용해서 모델을 만드는 방법이다. <br><br> 그러나 기본적인 스태킹 방법은 같은 데이터셋을 통해 예측한 결과를 기반으로 다시 학습하므로 overfitting 문제점이 있다. 따라서 스태킹에 Cross Validation 방식을 도입하여 이 문제를 해결할 수 있다. 데이터를 쪼개고 이들 중 일부만을 가지고 학습한 모델을 여러개 만들어, 그 결과들을 메타 학습 데이터셋(meta train dataset)으로 사용하여 다시 학습하는 것이다. 이 방법은 많은 개별 모델의 결과를 결합하여 예측 성능을 높일 수 있다는 장점이 있다.

<br>

### Reference

**면접 질문**

- [AI Tech Interview](https://boostdevs.gitbook.io/ai-tech-interview/)