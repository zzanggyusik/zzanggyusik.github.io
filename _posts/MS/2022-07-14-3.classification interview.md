---
title: "[대학원 면접 준비] ML Classification 정리"
excerpt: "대학원 면접용 분류 정리"
toc: true
toc_label: "ML Classification 정리"
toc_sticky: true

categories:
  - MS
tags:
  - 대학원
last_modified_at: 2022-07-14
---

### Classification

분류는 지도학습의 대표적인 태스크 중 하나로, **정답(label)이 있는 데이터를 가지고 데이터가 어떤 클래스에 속하는지를 학습한 다음 새롭게 관측된 데이터에 대한 클래스를 판별하는 것**입니다.

분류를 위한 학습 알고리즘에는 아래와 같이 매우 다양한 알고리즘들이 있습니다.

- 베이즈(Bayes) 통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)
- 독립변수와 종속변수의 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression)
- 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)
- 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine)
- 근접 거리를 기준으로 하는 최소 근접(Nearest Neighbor) 알고리즘
- 심층 연결 기반의 신경망 (Neural Network)
- 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)

먼저 Decision Tree(결정 트리)에 대해 알아보겠습니다.

<br>

### Decision Tree (결정 트리)

결정 트리는 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 알고리즘입니다.

아래 그림은 결정 트리의 구조를 간략하게 나타낸 것으로, 규칙 노드(Decision Node)로 표시된 노드는 규칙 조건이 되고, 새로운 규칙 조건마다 서브 트리(Sub Tree)가 생성됩니다.

리프 노드(Leaf Node)로 표시된 노드는 결정된 클래스 값입니다.

<img width="1512" alt="스크린샷 2022-07-14 오후 8 32 12" src="https://user-images.githubusercontent.com/76269316/178973105-34f4043f-3391-4c10-bcd6-0439d4ec5d73.png">

규칙 노드는 데이터의 특징을 나타내는 피처가 결합해 새로운 규칙 조건을 만들 때마다 생성됩니다. 하지만 많은 규칙이 있다는 건 분류를 결정하는 방식이 복잡해진다는 것이고, 이는 과적합(학습데이터를 과하게 학습하는 것)으로 이어지게 됩니다.

즉, 트리의 깊이(depth)가 깊어질수록 결정 트리의 예측 성능이 저하되게 됩니다.

따라서 가능한 한 적은 규칙 노드로 높은 예측 정확도를 가져야 하는데 이를 위해서는, 최대한 균일한 데이터 세트를 구성할 수 있도록 분할(Split)하는 것이 필요합니다.

<br>

**균일한 데이터세트란?**

<img src="https://user-images.githubusercontent.com/76269316/126084685-9a654ae8-37ed-4f7b-8a8c-06028853d29e.png" alt="image" style="zoom:67%;" />

다음과 같이 흑백으로 된 공이 있다고 할 때, 균일한 순서대로 나열하면 C -> B -> A입니다.

C는 모든 공이 검은 공이므로 가장 균일하고, B는 일부 하얀 공을 갖고 있지만 대부분은 검은 공으로 구성돼 있어 다음으로 균일도가 높습니다. A는 검은 공과 하얀 공이 비슷하게  있어 균일도가 가장 낮습니다.

<br>

결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만들고, 이후 나머지 데이터 세트에 대해서도 균일도가 높게 자식 데이터 세트를 쪼개는 것을 반복하면서 데이터 값을 예측합니다.

<br>

- 형태 : 동그라미, 네모, 세모

- 색깔 : 노랑, 빨강, 파랑

각각의 형태와 색깔을 갖는 레고 블록이 다음과 같이 섞여있다고 하면, 결정 트리에서 가장 첫 번째로 만들어지는 규칙 노드는 **if 색깔 == '노란색'**입니다.

색깔이 노란색인지 아닌지만 비교함으로써 모든 노란 동그라미 블록을 예측할 수 있고, 그 다음 나머지 블록에 대해 다시 균일도가 높은 조건을 찾아 분류하는 것이 가장 효율적이기 때문입니다.

<img width="318" alt="스크린샷 2022-07-14 오후 10 32 51" src="https://user-images.githubusercontent.com/76269316/178994530-f50bee6e-d203-40ba-97ec-bb57bfdf7fc8.png">

<br>

정보 균일도를 수학적으로 측정하는 방법은 **정보 이득 (Information Gain)**과 **지니 계수 (Gini Coefficient)**가 있습니다.

- 정보 이득 : 엔트로피는 데이터 집합의 혼잡도를 나타내는데 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮게 됩니다. 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값입니다. (결정 트리에서는 정보 이득 지수가 높은 속성을 기준으로 분할합니다)
- 지니 계수 : 경제학에서 불평등 지수를 나타낼 때 사용하는 계수로 0이 가장 평등하고 1로 갈수록 불평등합니다. (결정 트리에서는 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할합니다)

결론적으로 결정 트리 알고리즘은 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추게 됩니다.

<br>

##### Decision Tree 특징

|                             장점                             |                             단점                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| - 직관적이다.<br />- 피처의 스케일리이나 정규화 등의 사전 가공 영향도가 크지 않다. | - 모든 데이터 상황을 만족하는 완벽한 규칙을 만들려고 하게되고(그럴 수 없음에도 불구하고) 결국, 트리의 깊이가 깊어지고 트리가 복잡해져서 예측 성능이 떨어지게 됨<br />➡️ 이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝이 필요 |

<br><br>

### 분류 관련 면접 질문

**1. 50개의 작은 의사결정 나무는 큰 의사결정 나무보다 괜찮을까요? 왜 그렇게 생각하나요?**

<details markdown="1">
<summary>답변</summary>
50개의 작은 의사결정 나무는 앙상블 Bagging 기법을 사용한 모델로 볼 수 있다. 이 문제는 즉, Bagging의 대표적인 방법인 RandomForest가 왜 좋은지 설명하는 문제이다. <br><br>
큰 트리는 작은 편향(bias)와 큰 분산(variance)을 갖기 때문에, 깊이가 깊은 트리는 훈련데이터에 대해 과적합(overfitting)하게 된다. Random Forest 방식으로 학습하면, 트리들의 편향은 그대로 유지하면서 여러 데이터셋/여러 경우에 대해 학습하기 떄문에 분산을 감소시킬 수 있다. <br><br>
또한 한 개의 결정트리의 경우, 훈련 데이터에 있는 노이즈에 대해 매우 민감하지만, 여러 트리들을 만들면서 평균을 내면, 노이즈에 대해 강인해질 수 있다. 따라서 하나의 깊은/큰 의사결정 나무보다 50개의 작은 의사결정 나무가 더 좋은 모델을 완성시킨다고 할 수 있다.

<br><br>

### Reference

**면접 질문**

- [AI Tech Interview](https://boostdevs.gitbook.io/ai-tech-interview/)