---
title: "[대학원 면접 준비] ML Classification 정리"
excerpt: "대학원 면접용 분류 정리"
toc: true
toc_label: "ML Classification 정리"
toc_sticky: true

categories:
  - MS
tags:
  - 대학원
last_modified_at: 2022-07-14
---

### Classification

분류는 지도학습의 대표적인 태스크 중 하나로, **정답(label)이 있는 데이터를 가지고 데이터가 어떤 클래스에 속하는지를 학습한 다음 새롭게 관측된 데이터에 대한 클래스를 판별하는 것**입니다.

분류를 위한 학습 알고리즘에는 아래와 같이 매우 다양한 알고리즘들이 있습니다.

- 베이즈(Bayes) 통계와 생성 모델에 기반한 나이브 베이즈(Naive Bayes)
- 독립변수와 종속변수의 선형 관계성에 기반한 로지스틱 회귀(Logistic Regression)
- 데이터 균일도에 따른 규칙 기반의 결정 트리(Decision Tree)
- 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신(Support Vector Machine)
- 근접 거리를 기준으로 하는 최소 근접(Nearest Neighbor) 알고리즘
- 심층 연결 기반의 신경망 (Neural Network)
- 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블(Ensemble)

먼저 Decision Tree(결정 트리)에 대해 알아보겠습니다.

<br>

### Decision Tree (결정 트리)

결정 트리는 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 알고리즘입니다.

아래 그림은 결정 트리의 구조를 간략하게 나타낸 것으로, 규칙 노드(Decision Node)로 표시된 노드는 규칙 조건이 되고, 새로운 규칙 조건마다 서브 트리(Sub Tree)가 생성됩니다.

리프 노드(Leaf Node)로 표시된 노드는 결정된 클래스 값입니다.

<img width="1512" alt="스크린샷 2022-07-14 오후 8 32 12" src="https://user-images.githubusercontent.com/76269316/178973105-34f4043f-3391-4c10-bcd6-0439d4ec5d73.png">

규칙 노드는 데이터의 특징을 나타내는 피처가 결합해 새로운 규칙 조건을 만들 때마다 생성됩니다. 하지만 많은 규칙이 있다는 건 분류를 결정하는 방식이 복잡해진다는 것이고, 이는 과적합(학습데이터를 과하게 학습하는 것)으로 이어지게 됩니다.

즉, 트리의 깊이(depth)가 깊어질수록 결정 트리의 예측 성능이 저하되게 됩니다.

따라서 가능한 한 적은 규칙 노드로 높은 예측 정확도를 가져야 하는데 이를 위해서는, 최대한 균일한 데이터 세트를 구성할 수 있도록 분할(Split)하는 것이 필요합니다.

<br>

**균일한 데이터세트란?**

<img src="https://user-images.githubusercontent.com/76269316/126084685-9a654ae8-37ed-4f7b-8a8c-06028853d29e.png" alt="image" style="zoom:67%;" />

다음과 같이 흑백으로 된 공이 있다고 할 때, 균일한 순서대로 나열하면 C -> B -> A입니다.

C는 모든 공이 검은 공이므로 가장 균일하고, B는 일부 하얀 공을 갖고 있지만 대부분은 검은 공으로 구성돼 있어 다음으로 균일도가 높습니다. A는 검은 공과 하얀 공이 비슷하게  있어 균일도가 가장 낮습니다.

<br>

결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만들고, 이후 나머지 데이터 세트에 대해서도 균일도가 높게 자식 데이터 세트를 쪼개는 것을 반복하면서 데이터 값을 예측합니다.

<br>

- 형태 : 동그라미, 네모, 세모

- 색깔 : 노랑, 빨강, 파랑

각각의 형태와 색깔을 갖는 레고 블록이 다음과 같이 섞여있다고 하면, 결정 트리에서 가장 첫 번째로 만들어지는 규칙 노드는 **if 색깔 == '노란색'**입니다.

색깔이 노란색인지 아닌지만 비교함으로써 모든 노란 동그라미 블록을 예측할 수 있고, 그 다음 나머지 블록에 대해 다시 균일도가 높은 조건을 찾아 분류하는 것이 가장 효율적이기 때문입니다.

<img width="318" alt="스크린샷 2022-07-14 오후 10 32 51" src="https://user-images.githubusercontent.com/76269316/178994530-f50bee6e-d203-40ba-97ec-bb57bfdf7fc8.png">

<br>

정보 균일도를 수학적으로 측정하는 방법은 **정보 이득 (Information Gain)**과 **지니 계수 (Gini Coefficient)**가 있습니다.

- 정보 이득 : 엔트로피는 데이터 집합의 혼잡도를 나타내는데 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮게 됩니다. 정보 이득 지수는 1에서 엔트로피 지수를 뺀 값입니다. (결정 트리에서는 정보 이득 지수가 높은 속성을 기준으로 분할합니다)
- 지니 계수 : 경제학에서 불평등 지수를 나타낼 때 사용하는 계수로 0이 가장 평등하고 1로 갈수록 불평등합니다. (결정 트리에서는 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석해 지니 계수가 낮은 속성을 기준으로 분할합니다)

결론적으로 결정 트리 알고리즘은 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추게 됩니다.

<br>

##### Decision Tree 특징

|                             장점                             |                             단점                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| - 직관적이다.<br />- 피처의 스케일리이나 정규화 등의 사전 가공 영향도가 크지 않다. | - 모든 데이터 상황을 만족하는 완벽한 규칙을 만들려고 하게되고(그럴 수 없음에도 불구하고) 결국, 트리의 깊이가 깊어지고 트리가 복잡해져서 예측 성능이 떨어지게 됨<br />➡️ 이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝이 필요 |

<br><br>

### Ensemble Learning (앙상블 학습)

앙상블 학습은 여러 개의 분류기(Classifier)를 생성해 결합함으로써 보다 정확한 최종 예측을 도출하는 기법으로 Bagging, Voting, Boosting, Stacking 등이 있습니다.

- Bagging : 모두 같은 유형(대부분 결정 트리 알고리즘)의 알고리즘 분류기 중 투표를 통해 최종 예측 결과를 결정하는 방식
- Voting : 서로 다른 알고리즘 분류기 중 투표를 통해 최종 예측 결과를 결정하는 방식

Bagging과 Voting은 투표 방식이라는 점에서 유사하지만, Bagging은 같은 종류의 모델들을 사용하고 Voting은 다른 종류의 알고리즘 모델들을  사용한다는 차이점이 있습니다.

|                           Bagging                            |                            Voting                            |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img width="373" alt="스크린샷 2022-07-16 오후 10 35 56" src="https://user-images.githubusercontent.com/76269316/179357218-55fa3f03-538a-4e2a-9d62-346d765313c1.png"> | <img width="376" alt="스크린샷 2022-07-16 오후 10 35 20" src="https://user-images.githubusercontent.com/76269316/179357196-2e0f2fc5-ec35-479f-8766-0f718b62cf45.png"> |

Voting과 Bagging은 학습하는 데이터 세트가 다른데,

Bagging 방식은 원본 학습 데이터를 샘플링해 추출한 다음, 개별 분류기에 할당해서 학습하는데 이렇게 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식을 **Bootstrapping 분할 방식**이라고 합니다.

<img src="https://user-images.githubusercontent.com/76269316/179231715-7b50826c-c640-4c1c-8d39-b5dec152a58f.png" alt="image" style="zoom: 67%;" />

출처: [A Trip to Random Forest](https://medium.com/greyatom/a-trip-to-random-forest-5c30d8250d6a)

개별 분류기가 Bootstrapping 방식으로 샘플링된 데이터 세트로 학습한 다음 예측을 수행한 결과를 Voting을 통해 최종 예측 결과를 선정하는 방식이 Bagging Ensemble 방식입니다.

교차 검증이 데이터 세트 간에 중첩을 허용하지 않는 것과 다르게 Bagging 방식은 중첩을 허용합니다. (10,000개의 데이터를 10개의 분류기가 배깅 방식으로 나누더라도 각 1000개의 데이터 내에는 중복 데이터가 있음)

<br>

### Voting

Voting 방법에는 Hard Voting, Soft Voting 두 가지가 있는데, 하드 보팅보다는 소프트 보팅이 예측 성능이 좋아서 더 많이 사용됩니다.

- Hard Voting : 예측한 결과값들 중 다수의 분류기가 결정한 예측값을 최종 보팅 결과값으로 선정
- Soft Voting : 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결과값으로 선정

|                         Hard Voting                          |                         Soft Voting                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img width="613" alt="스크린샷 2022-07-16 오후 10 36 40" src="https://user-images.githubusercontent.com/76269316/179357234-cb108af4-bdb5-4c11-a6de-548b55331850.png"> | <img width="609" alt="스크린샷 2022-07-16 오후 10 39 15" src="https://user-images.githubusercontent.com/76269316/179357340-0594d382-4b0b-47d0-92e6-a121d4fd37a1.png"> |

앞서 설명한 결정 트리 알고리즘은 훈련 데이터의 정확한 예측을 위해 트리의 깊이를 깊게하면, 과적합이 발생해 실제 테스트 데이터에서 예측 성능이 떨어지게 됩니다.

하지만 앙상블 학습에서는 결정 트리 알고리즘의 단점을 수십~수천 개의 많은 분류기를 결합해 다양한 상황을 학습하게 함으로써 극복합니다.

결정 트리 알고리즘의 장점은 그대로 취하고 단점은 보완하면서 편향-분산 트레이드오프 효과를 극대화 한다는 것입니다.

<br>

##### Random Forest

랜덤 포레스트는 Bagging Ensemble 방식을 사용한 알고리즘으로 여러 개의 Decision Tree를 만들어, 각 트리에서 분류한 결과를 투표해 가장 많이 득표한 결과를 최종 분류 결과로 선택하는 알고리즘입니다.

랜덤 포레스트가 생성한 일부 트리는 overfitting 될 수 있지만, 많은 수의 트리를 생성함으로써 예측하는데 있어 과적합이 큰 영향을 미치지 못 하도록 합니다.

<img width="1038" alt="스크린샷 2022-07-15 오후 10 15 22" src="https://user-images.githubusercontent.com/76269316/179230516-0cf5127c-d4a0-4228-b03b-4a561c54c723.png">

*나무가(Tree) 여러 개 있는 모습이 숲(Forest) 같아서 이름을 이렇게 지은건가..?*

<br><br>

### Boosting

Boosting 알고리즘은 여러 개의 약한 학습기(weak learner)를 결합해, 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치를 부여해 오류를 개선해 나가는 학습 방식입니다.

Boosting 계열의 대표적인 모델로는 AdaBoost(Adaptive boosting)와 Gradient Boost가 있습니다.

<br>

##### AdaBoost

AdaBoost에 대해서는 유튜브에 잘 정리돼 있어서 해당 링크를 참고하시면 될 것 같습니다.

- AdaBoost 정리 유튜브 👉  [AdaBoost, Clearly Explained](https://www.youtube.com/watch?v=LsK-xG1cLYA)

- 해당 유튜브 내용을 한글로 정리한 블로그 글 👉 [머신러닝 - 14. 에이다 부스트(AdaBoost)](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-14-AdaBoost)

저는 AdaBoost가 어떻게 학습을 진행하는지 대략적으로만 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/76269316/126178465-10a3db0f-c8d5-4b90-8604-c08ea4e8a01f.png)

맨 왼쪽 그림과 같이 +, -로 된 피처 데이터 세트가 있다면

- Step 1은 첫 번째 약한 학습기가 분류 기준 1로 +와 -를 분류한 것입니다. 동그라미로 표시된 ⊕ 데이터는 잘못 분류된 오류 데이터입니다.
- Step 2에서는 이 오류 데이터에 대해 가중치 값을 부여합니다. 가중치가 부여된 오류 데이터 +는 다음 약한 학습기가 잘 분류할 수 있게 크기가 커졌습니다.
- Step 3는 두 번째 약한 학습기가 분류 기준 2로 +와 -를 분류했습니다. 마찬가지로 동그라미로 표시된 ⊖ 데이터는 잘못 분류된 오류 데이터입니다.
- Step 4에서 잘못 분류된 - 오류 데이터에 대해 다음 약한 학습기가 잘 분류할 수 있게 더 큰 가중치를 부여합니다. (크기가 커짐)
- Step 5에서 세 번째 약한 학습기가 분류 기준 3으로 +와 -를 분류하고 오류 데이터를 찾습니다.
- 마지막으로 맨 아래에 첫 번째, 두 번째, 세 번째 약한 학습기를 모두 결합한 예측 결과입니다. 개별 약한 학습기보다 훨씬 정확도가 높아졌음을 알 수 있습니다.

<br>

##### Gradient Boosting Algorithm

GBM(Gradient Boost Machine)은 AdaBoost와 유사하나, 가중치 업데이트를 경사 하강법(Gradient Descent)을 이용하는 것이 큰 차이점입니다.

GBM은 과적합에도 뛰어난 예측 성능을 가진 알고리즘이지만 수행 시간이 오래 걸린다는 단점이 있어, GBM에 기반한 XGBoost, LightGBM 등의 모델이 많이 사용되는 추세입니다.

<br>

**XGBoost (eXtra Gradient Boost)**

<img src="https://user-images.githubusercontent.com/76269316/179359086-9ac10a5d-f8f0-4cc4-9a48-e4d16a2e162f.png" alt="image" style="zoom:67%;" />

XGBoost는 GBM에 기반하고 있지만 GBM의 단점인 느린 수행 시간 및 과적합 규제(Regularization) 부재 등의 문제를 해결해서 매우 각광 받고 있습니다.

cf) 규제(Regularization)란? 머신러닝에서 모델이 가질 수 있는 파라미터의 값에 제약을 부여해 과적합을 방지하고 모델의 강건함을 높이는 방법론

<br>

**XGBoost의 장점**

| 항목                              | 설명                                                         |
| :-------------------------------- | :----------------------------------------------------------- |
| 뛰어난 예측 성능                  | 일반적으로 분류와 회귀 영역에서 뛰어난 예측 성능을 발휘합니다. |
| GBM 대비 빠른 수행 시간           | 일반적인 GBM은 순차적으로 weak learner가 가중치를 증감하는 방법으로 학습하기 때문에 속도가 느림<br />하지만 XGBoost는 병렬 수행 및 다양한 기능으로 GBM에 비해 빠른 수행 성능을 보장<br />일반적인 GBM에 비해 수행 시간이 빠른 것이지 다른 머신러닝 알고리즘(ex 랜덤 포레스트)보다 빠르지는 않습니다. |
| 과적합 규제<br />(Regularization) | 표준 GBM의 경우 과적합 규제 기능이 없으나 XGBoost는 자체 과적합 규제 기능으로 과적합에 좀 더 강한 내구성을 가질 수 있습니다. |
| 나무 가지치기<br />(Tree pruning) | GBM은 분할 시 부정 손실이 발생하면 분할을 더 이상 수행하지 않지만, 이런 방식도 지나치게 많은 분할을 발생시킬 수 있습니다.<br />다른 GBM과 마찬가지로 XGBoost도 max_depth 파라미터로 분할 깊이를 조정하지만, tree pruning으로 더 이상 긍정 이득이 없는 분할을 가지치기 해서 분할 수를 더 줄일 수 있습니다. |
| 자체 내장된 교차 검증             | XGBoost는 반복 수행 시마다 내부적으로 학습 데이터 세트와 평가 데이터 세트에 대한 교차 검증을 수행해 최적화된 반복 수행 횟수를 가질 수 있습니다.<br />지정된 반복 횟수가 아니라 교차 검증을 통해 평가 데이터 세트의 평가 값이 최적화되면 반복을 중간에 멈출 수 있는 조기 중단 기능이 있습니다. |
| 결손값 자체 처리                  | XGBoost는 결손값을 자체 처리할 수 있는 기능을 갖고 있습니다. |

<br>

**LightGBM**

<img src="https://user-images.githubusercontent.com/76269316/179359473-d82a934f-3a11-4d91-9efd-e7adb5e619dd.png" alt="스크린샷 2022-07-16 오후 11 38 46" style="zoom:67%;" />

대부분 트리 기반 알고리즘은 트리의 깊이를 효과적으로 줄이고, 과적합에 더욱더 강한 구조를 갖는다고 알려진 균형 트리 분할 (Level Wise) 방식을 사용합니다.

<img width="672" alt="스크린샷 2022-07-16 오후 11 18 20" src="https://user-images.githubusercontent.com/76269316/179358690-25466406-27d5-4dfc-9164-18a113a8c100.png">

LightGBM은 일반 GBM 계열 트리 분할 방법과 다르게 리프 중심 트리 분할 (Leaf Wise) 방식을 사용합니다.

<img width="1077" alt="스크린샷 2022-07-16 오후 11 19 42" src="https://user-images.githubusercontent.com/76269316/179358740-97a933e7-061a-4414-a067-b9d35915b3f8.png">

리프 중심 트리 분할 방식은 트리의 균형을 맞추지 않고, 최대 손실 값(max delta loss)을 갖는 리프 노드를 지속적으로 분할하면서 트리의 깊이가 깊어지고 비대칭적인 규칙 트리를 생성합니다.

이렇게 생성된 규칙 트리는 학습을 반복할수록 균형 트리 분할 방식보다 예측 오류 손실을 최소화 할 수 있다는 것이 LightGBM의 구현 개념입니다.

<br>

**LightGBM의 XGBoost 대비 장점**

- 더 빠른 학습, 예측 수행 시간
- 더 작은 메모리 사용량
- 카테고리형 피처의 자동 변환, 최적 변환(원-핫 인코딩등을 사용하지 않고도 카테고리형 피처를 최적으로 변환하고 이에 따른 노드 분할 수행)

LightGBM의 단점은 적은 데이터 세트(일반적으로 10,000건 이하)에 사용할 경우 과적합이 발생하기 쉽다고 합니다.

<br><br>

### Stacking

스태킹(Stacking)은 개별적인 여러 알고리즘을 결합해 예측 결과를 도출한다는 점에서 배깅(Bagging), 부스팅(Boosting)과 공통점을 갖고 있습니다.

차이점은 개별 알고리즘의 예측 결과를 최종적인 메타 데이터 세트로 만들어 별도의 ML 알고리즘으로 최종 학습을 수행하고 테스트 데이터를 기반으로 다시 최종 예측을 수행하는 방식입니다.

따라서 **개별적인 기반 모델**과 개별 기반 모델의 예측 데이터를 학습 데이터로 만들어서 학습하는 최종 **메타 모델** 두 종류 모델이 필요합니다.

<br>

M개의 row, N개의 피처(column)을 갖는 데이터 세트에 스태킹 앙상블을 적용한다고 하면, 모델별로 각각 학습을 시킨 뒤 예측을 수행하면 각각 M개의 row를 가진 1개의 레이블 값을 도출할 것입니다.

모델별로 도출된 이 예측 레이블 값들을 다시 합해 (Stacking) 새로운 데이터 세트를 만들고 이렇게 스태킹된 데이터 세트에 대해 최종 모델을 적용해 최종 예측 하는 것이 스태킹 앙상블 모델입니다.

<img width="1512" alt="스크린샷 2022-07-16 오후 10 42 33" src="https://user-images.githubusercontent.com/76269316/179357446-3761ed9e-49da-4a3f-9200-35180be773aa.png">

<br>

스태킹 앙상블은 학습시 테스트 데이터셋에 대한 예측 레이블을 기반으로 학습하기 때문에, 과적합이 발생할 수 있습니다.

과적합을 개선하기 위해 Cross Validation 기반 예측 데이터 셋을 사용합니다.

<br>

##### Stacking with cross-validation

CV 세트 기반의 스태킹 모델은 과적합을 개선하기 위해 최종 메타 모델을 위한 데이터 세트를 만들 때 교차 검증 기반으로 예측된 결과 데이터 세트를 이용합니다.

다음과 같은 두가지 스텝으로 수행됩니다.

- Step1 : 각 모델별로 원본 학습/테스트 데이터를 예측한 결과 값을 기반으로 메타 모델을 위한 학습용/테스트용 데이터를 생성
- Step2 : Step1에서 개별 모델들이 생성한 학습용 데이터를 모두 스태킹 형태로 합쳐 메타 모델이 학습할 최종 학습용 데이터 세트를 생성
  마찬가지로 각 모델들이 생성한 테스트용 데이터를 모두 스태킹 형태로 합쳐 메타 모델이 예측할 최종 테스트 데이터 세트를 생성
  메타 모델은 최종적으로 생성된 학습 데이터 세트와 원본 학습 데이터의 레이블 데이터를 기반으로 학습한 뒤, 최종적으로 생성된 테스트 데이터 세트를 예측하고, 원본 테스트 데이터와 레이블 데이터를 기반으로 평가

아래 그림을 통해 자세히 설명하도록 하겠습니다.

<br>

**Step1**

먼저 학습용 데이터를 N개의 폴드(Fold)로 나눕니다. 여기서는 3개로 나누겠습니다.

이 중 2개는 학습을 위한 데이터 폴드로, 나머지 1개는 검증을 위한 데이터 폴드입니다.

이렇게 두 개의 폴드로 나뉜 학습 데이터를 기반으로 개별 모델을 학습시키면 개별 모델은 검증 폴드 1개의 데이터를 예측하고 그 결과를 저장합니다.

이런 로직을 학습 데이터와 검증 데이터를 변경해가면서 3번 반복합니다.

<img width="1388" alt="스크린샷 2022-07-16 오후 10 57 04" src="https://user-images.githubusercontent.com/76269316/179357930-1a740c19-3690-4e95-a0ec-34e3e2f53178.png">

<img width="1387" alt="스크린샷 2022-07-16 오후 10 57 45" src="https://user-images.githubusercontent.com/76269316/179357955-a7d5646f-80db-4a14-85ec-eb54baaca73b.png">

<img width="1381" alt="스크린샷 2022-07-16 오후 10 58 14" src="https://user-images.githubusercontent.com/76269316/179357972-a0b26fa4-ca98-402d-9b5d-7c5d6ea5d57e.png">

이렇게 만들어진 예측 데이터(오른쪽 주황색 박스)는 메타 모델을 학습시키는 학습 데이터로 사용됩니다.

2개의 학습 폴드 데이터로 학습된 개별 모델은 원본 테스트 데이터를 예측하여 예측값을 생성하고 이러한 로직을 3번 반복합니다.

이 예측값의 평균으로 최종 결과값을 생성하고 이를(노란색) 메타 모델을 위한 테스트 데이터로 사용합니다.

<br>

**Step2**

<img width="1512" alt="스크린샷 2022-07-16 오후 11 02 24" src="https://user-images.githubusercontent.com/76269316/179358106-5ddc947b-90bb-42e8-89b0-1b850128b8b0.png">

각 모델들이 Step1에서 생성한 학습, 테스트 데이터를 모두 합쳐 최종적으로 메타 모델이 사용할 학습 데이터와 테스트 데이터를 생성합니다.

최종 학습 데이터와 원본 데이터의 레이블 데이터를 합쳐 메타 모델을 학습한 뒤, 최종 테스트 데이터로 예측을 수행합니다.

이후 최종 예측 결과를 원본 테스트 데이터의 레이블 데이터와 비교해 평가합니다.

<br>

2개의 학습 폴드 데이터로 학습된 개별 모델은 원본 테스트 데이터를 예측하여 예측값을 생성하고 이러한 로직을 3번 반복합니다.

이 예측값의 평균으로 최종 결과값을 생성하고 이를 메타 모델을 위한 테스트 데이터로 사용합니다.



<br><br>

### 분류 관련 면접 질문

**1. 50개의 작은 의사결정 나무는 큰 의사결정 나무보다 괜찮을까요? 왜 그렇게 생각하나요?**

50개의 작은 의사결정 나무는 앙상블 Bagging 기법을 사용한 모델로 볼 수 있다. 이 문제는 즉, Bagging의 대표적인 방법인 RandomForest가 왜 좋은지 설명하는 문제이다.

큰 트리는 작은 편향(bias)와 큰 분산(variance)을 갖기 때문에, 깊이가 깊은 트리는 훈련데이터에 대해 과적합(overfitting)하게 된다. Random Forest 방식으로 학습하면, 트리들의 편향은 그대로 유지하면서 여러 데이터셋/여러 경우에 대해 학습하기 떄문에 분산을 감소시킬 수 있다. 

또한 한 개의 결정트리의 경우, 훈련 데이터에 있는 노이즈에 대해 매우 민감하지만, 여러 트리들을 만들면서 평균을 내면, 노이즈에 대해 강인해질 수 있다. 따라서 하나의 깊은/큰 의사결정 나무보다 50개의 작은 의사결정 나무가 더 좋은 모델을 완성시킨다고 할 수 있다

<br>

**2. 앙상블 방법엔 어떤 것들이 있나요?**

앙상블(Ensemble)은 여러 개의 모델을 조합해서 그 결과를 뽑아내는 방법이다.

"정확도가 높은 강한 모델을 하나 사용하는 것보다, 정확도가 낮은 약한 모델을 여러개 조합 하는 방식의 정확도가 높다"는 개념에서 비롯한 방법으로 Bagging, Boosting, Stacking 등이 있다.

배깅(Bagging, Bootstrap Aggregation)이란 샘플을 여러번 뽑아(Bootstrap = 복원 랜덤 샘플링) 각 모델을 학습시켜 결과물을 집계(Aggregation)하는 방법이다.

카테고리 데이터는 투표 방식(Votinig)으로 결과를 집계하며, 연속형 데이터는 평균으로 집계한다. Bagging을 사용한 대표적인 기법에는 Random Forest 방법이 있다.

학습 데이터가 충분하지 않더라도 충분한 학습효과를 주어 높은 bias의 underfitting 문제나, 높은 variance로 인한 overfitting 문제를 해결하는데 도움을 준다.

부스팅(Boosting)이란 이전 모델의 오답에 가중치를 높게 부여하여 다음 모델을 학습하는 방법으로, 오답을 정답으로 맞추기 위해 오답에 더 집중하여 학습시킨다.

일반적으로 배깅에 비해 정확도가 높다. 그러나 틀렸던 부분에 대해 반복적으로 학습하므로 과적합이 발생할 우려가 있으며, outlier에 취약하고, 속도가 느리다는 단점을 가지고 있다.

GBM(Gradient Boosting) 방법이 대표적이고, AdaBoost, GradientBoost, XGBoost 등의 알고리즘이 존재한다.

스태킹(Stacking)이란 여러 개별 모델이 예측한 결과값을 다시 학습 데이터셋으로 사용해서 모델을 만드는 방법이다.

그러나 기본적인 스태킹 방법은 같은 데이터셋을 통해 예측한 결과를 기반으로 다시 학습하므로 overfitting 문제점이 있다. 

따라서 스태킹에 Cross Validation 방식을 도입하여 이 문제를 해결할 수 있다. 데이터를 쪼개고 이들 중 일부만을 가지고 학습한 모델을 여러개 만들어, 그 결과들을 메타 학습 데이터셋(meta train dataset)으로 사용하여 다시 학습하는 것이다.

이 방법은 많은 개별 모델의 결과를 결합하여 예측 성능을 높일 수 있다는 장점이 있다.

##### Reference

**면접 질문**

- [AI Tech Interview](https://boostdevs.gitbook.io/ai-tech-interview/)