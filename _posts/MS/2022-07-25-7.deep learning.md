---
title: "[대학원 면접] 딥러닝 정리"
excerpt: "딥러닝 정리"
toc: true
toc_label: "딥러닝 정리"
toc_sticky: true

categories:
  - MS
tags:
  - 대학원
last_modified_at: 2022-07-25
---

##### 인공신경망의 장점?

전통적인 머신러닝은 모델이 학습을 잘 할 수 있도록, 사람이 데이터를 가공하여 좋은 변수를 만들어야 함

이러한 과정을 피처 엔지니어링이라고 하는데 딥러닝에서는 별도의 가공 없이 데이터를 주더라도 스스로 피처를 학습하는 feature learning 개념을 갖고 있음

사용자가 다양한 구조를 만들 수 있기 때문에 다양한 분야에서 사용 가능

<br>

##### 딥러닝이란?

인공 신경망을 깊게 만들어 모델을 학습시키는 방법

<br>

##### 인공 신경망 (Artificial Neural Network)

![스크린샷 2022-07-25 오후 9 18 12](https://user-images.githubusercontent.com/76269316/180775846-282bf12d-0d90-436a-a5b8-0b45b99ae426.png)

각 층 사이의 계산은 행렬 연산을 통해서 이루어짐

![스크린샷 2022-07-25 오후 9 24 52](https://user-images.githubusercontent.com/76269316/180776845-961b72b3-d910-4ebb-a332-c2e2ea27a107.png)

<br>

##### 활성화 함수 (Activation Function)

<img src="https://user-images.githubusercontent.com/76269316/180777937-c5bc5a7d-bd2b-4d7f-b1d7-5e8c5b291003.png" alt="스크린샷 2022-07-25 오후 9 31 36" style="zoom:67%;" />

인공 신경망은 1차 결합 형태로 돼 있음

👉 미분이 쉬움

👉 계산이 쉬움

하지만 실제 현실 세계에서는 선형적인 관계만 존재하는게 아님 ➡️ **비선형적인 관계를 표현하기 위해 활성화 함수를 사용!**

<img src="https://user-images.githubusercontent.com/76269316/180778606-5a562409-9905-44b8-a10b-4bc6b896e03a.png" alt="스크린샷 2022-07-25 오후 9 35 21" style="zoom:67%;" />

<br>

- 시그모이드 함수

다층의 관계를 비선형적으로 만들어줌, 미분이 가능, 모든 값을 0 ~ 1 사이로 변환

![스크린샷 2022-07-25 오후 9 37 03](https://user-images.githubusercontent.com/76269316/180778894-90136d2c-7c7a-4b6e-8c87-01ac2f15796b.png)

- 하이퍼볼릭 탄젠트

-1 ~ 1 사이의 값을 가짐. 미분 가능, 시그모이드 보다 가파른 기울기를 갖고 있음

![스크린샷 2022-07-25 오후 9 37 27](https://user-images.githubusercontent.com/76269316/180778957-3a740039-a7b4-4a90-8916-f44d29559c3b.png)

- ReLU

음수 값이 들어오면 0으로, 양수 값이 들어오면 그대로 내보냄. (직선 두 개를 합쳤기 때문에 비선형 함수이지만, 선형식의 성질을 갖고 있음 - 계산·미분이 쉬움)

음수 값은 미분시 0, 양수 값은 1, 0일 때는? **미분 불가능**

![스크린샷 2022-07-25 오후 9 38 16](https://user-images.githubusercontent.com/76269316/180779102-e513c7f1-6eb9-4c4c-b528-c918bd44d9f0.png)

❗️Subgradient

0 기준 왼쪽은 미분값이 0, 오른쪽은 1이므로 0에서는 0 ~ 1 사이의 기울기 값을 가짐 👉 미분이 불가능한 지점에서도 구간을 가지고 있다면 일부의 값을 미분값으로 취할 수 있음

<br>

- Leaky ReLU

음수 값도 기울기가 있는 직선으로 대체

![스크린샷 2022-07-25 오후 9 46 21](https://user-images.githubusercontent.com/76269316/180780601-c23217bf-79a7-4b42-944c-0f269191e5f6.png)

- ELU

음수 값을 곡선으로 대체

![스크린샷 2022-07-25 오후 9 47 23](https://user-images.githubusercontent.com/76269316/180780809-be28dda8-a1fb-4b64-b8df-dcc20dabfc4a.png)

- Softmax

모든 입력 값에 대해 0 ~ 1 사이의 값을 가짐. 모든 성분의 합은 항상 1

![스크린샷 2022-07-25 오후 9 50 17](https://user-images.githubusercontent.com/76269316/180781354-9fef2f58-dfe7-45c4-a6a1-5c8f3561d837.png)

분자에 지수 함수를 사용한 이유는 음수 값이 입력으로 들어왔을 때 이를 그대로 사용하면 확률값으로 만들 수 없기 때문 (입력값이 양수든 음수든 항상 0 ~ 1 사이의 값이 나오도록 하는 장치)

<br>

##### XOR 문제 (XOR Problem)

XOR 연산을 퍼셉트론을 가지고 해결할 수 있는가?에 대한 문제

![스크린샷 2022-07-25 오후 9 53 02](https://user-images.githubusercontent.com/76269316/180781847-d1be5ca7-fb2e-4fe1-9570-ad46135014da.png)

<br>

선형적인 관계만 표현할 수 있던 퍼셉트론으로는 해결할 수 없었음

![스크린샷 2022-07-25 오후 10 00 46](https://user-images.githubusercontent.com/76269316/180783299-bc936092-54a5-4fd0-88a6-ae463fb8ccec.png)

<br>

은닉층을 만들어서 해결 👉 MLP 

![스크린샷 2022-07-25 오후 10 02 26](https://user-images.githubusercontent.com/76269316/180783617-5b40abae-9a57-42f1-89df-722e9136757c.png)

<br>

여러 입력값을 전치 행렬(Transpose)을 통해 한 번에 계산할 수 있음

![스크린샷 2022-07-25 오후 10 09 03](https://user-images.githubusercontent.com/76269316/180784893-b2d11090-be98-4b80-a9aa-33b366cb76c0.png)

<br>

##### 손실 함수 (Loss Function)

실제값과 예측값의 차이, 예측이 얼마나 정확한지를 나타내는 척도

<br>

**회귀(Regression)**

결과값이 연속적인 변수인 것을 예측하는 문제

- 평균 절대 오차 (Mean Absolute Error)

실제값과 예측값의 차이(잔차-residual)의 절댓값을 기반으로 한 손실 함수

![스크린샷 2022-07-25 오후 10 17 05](https://user-images.githubusercontent.com/76269316/180786329-422e1111-9584-478b-9a79-b9e705a994f0.png)

절댓값이기 때문에 미분 불가능한 지점이 존재

- 평균 제곱 오차(Mean Square Error)

실제값과 예측값의 수직 거리의 제곱의 평균으로 표현한 손실 함수

![스크린샷 2022-07-25 오후 10 18 03](https://user-images.githubusercontent.com/76269316/180786525-d83d2d88-2f36-42c3-b8ff-4980437bd5e7.png)

모든 점에서 미분 가능, 제곱식이 들어가기 때문에 MAE 보다는 큰 loss 값이 나옴, 이상치가 많은 데이터의 경우 MAE보다 성능이 안 좋을 수도 있음

<img src="https://user-images.githubusercontent.com/76269316/180786924-e0f49bdc-892d-48de-86a1-bbc5da900ac6.png" alt="스크린샷 2022-07-25 오후 10 19 53" style="zoom:67%;" />

<br>

- 평균 제곱근 오차 (Root Mean Square Error)

MSE가 원래 단위를 제곱한 것이기 때문에 loss의 크기를 원래 데이터의 단위와 동일하게 해주기위해 제곱근을 씌운 손실 함수

<br>

**분류(Classification)**

유한한 모임으로 분류되는 문제

- 교차 엔트로피 함수 (Cross Entropy Function)

![스크린샷 2022-07-25 오후 10 28 18](https://user-images.githubusercontent.com/76269316/180788660-e6097073-f67e-4c7e-bda4-e1ff9ede928e.png)

<img src="https://user-images.githubusercontent.com/76269316/180788803-7e3ed4d1-81bb-414d-aa37-e3ec1061606c.png" alt="스크린샷 2022-07-25 오후 10 28 54" style="zoom:67%;" />

- 이진 교차 엔트로피 함수 (Binary Cross Entropy Function)

![스크린샷 2022-07-25 오후 10 29 31](https://user-images.githubusercontent.com/76269316/180788925-2255bf9c-6d5c-4754-a0f3-c32546e1c700.png)

클래스가 두 개이기 때문에 원-핫 벡터로 표현하지 않고 0, 1로 표현 -> 내적이 다음과 같이 간단하게 표현됨

![스크린샷 2022-07-25 오후 10 30 37](https://user-images.githubusercontent.com/76269316/180789180-c50c3876-991d-4cb8-a0a9-2c7048bcd273.png)

✚ 예측시 소프트맥스 대신 시그모이드를 사용해도 됨

<img src="https://user-images.githubusercontent.com/76269316/180789537-579b856d-36e4-4127-833a-2607de0c08a0.png" alt="스크린샷 2022-07-25 오후 10 32 28" style="zoom:80%;" />

<br>

##### 최적화 (Optimization)

<img src="https://user-images.githubusercontent.com/76269316/180997076-74ef1cbb-3227-410f-9b37-f5403358ce69.png" alt="스크린샷 2022-07-26 오후 8 37 59" style="zoom:80%;" />

모델에 입력 데이터를 넣으면 예측값을 출력 -> 예측값과 실제값의 차이를 나타내는 척도가 Loss -> loss 값이 작게 나올수록 예측이 잘되는 것 -> Loss Function을 작게하는 파라미터 w 값을 찾는 것이 목표! (w 초기 값은 임의의 값으로 세팅)

cf) Maximum으로 정의된 Loss Function이 있음 -> 앞에 -를 붙여주면 Minimum Loss Function과 동일해짐

<br>

##### 하강법 (Descent Method)

![image](https://user-images.githubusercontent.com/76269316/180998040-3bed1040-2365-4c4c-9a3f-f9ac06d668e3.png)

![스크린샷 2022-07-26 오후 8 44 37](https://user-images.githubusercontent.com/76269316/180998084-d1e33bb8-faae-43c2-89a5-eb6062ba4a19.png) 👈 이 식을 만족하는 방향으로 이동

- 초기 값에 따라 global minimum을 찾지 못할 수도 있음

<br>

![스크린샷 2022-07-26 오후 8 47 36](https://user-images.githubusercontent.com/76269316/180998624-501fa9d5-85da-4562-bda5-1ea9ef150dfb.png)

a·(-a) < 0이므로 (a가 실수일 때), <img src="https://user-images.githubusercontent.com/76269316/180999104-24ae7683-ad43-419a-92fd-96fad563b1b0.png" alt="스크린샷 2022-07-26 오후 8 50 23" style="zoom:67%;" />

<img src="https://user-images.githubusercontent.com/76269316/180999312-59cebb7b-927c-4ca0-be53-849098a770dd.png" alt="스크린샷 2022-07-26 오후 8 51 26" style="zoom:67%;" />를 치환하면 <img src="https://user-images.githubusercontent.com/76269316/180999269-43db5f22-7828-4004-b0e4-75fa43967e85.png" alt="스크린샷 2022-07-26 오후 8 51 14" style="zoom:67%;" />

<br>

![스크린샷 2022-07-26 오후 8 52 48](https://user-images.githubusercontent.com/76269316/180999543-4ee83108-15dd-425c-830c-8edec3e95d29.png)

<br>

![스크린샷 2022-07-26 오후 8 59 02](https://user-images.githubusercontent.com/76269316/181000696-b6c0efe4-2e42-4728-bba4-f5dc580e948d.png)

<br>

![스크린샷 2022-07-26 오후 8 59 17](https://user-images.githubusercontent.com/76269316/181000736-14c5e138-2fad-4722-a590-0de28aeebf82.png)

<br>

##### 확률적 경사 하강법 (Stochastic Gradient Descent)

<img src="https://user-images.githubusercontent.com/76269316/181000990-369b541e-4a3f-4814-b4f3-9c14cd9abedc.png" alt="스크린샷 2022-07-26 오후 9 00 35" style="zoom:67%;" />

기존 경사하강법은 데이터 전체를 사용하기 때문에 연산량이 많아지는 문제점이 있음

<br>

<img src="https://user-images.githubusercontent.com/76269316/181001144-8caeefc3-551e-4b1b-96cb-47cdd4742244.png" alt="스크린샷 2022-07-26 오후 9 01 40" style="zoom:67%;" />

이를 해결하기 위해 데이터 전체를 미니 배치로 쪼갠 뒤, 무작위로 섞어 사용 -> 미니배치를 사용해서 업데이트를 진행

<br>

학습률이 상수이기 때문에, 유연하게 학습을 진행하기 어려움 -> 모멘텀 기반 / 가변학습률

![스크린샷 2022-07-26 오후 9 04 12](https://user-images.githubusercontent.com/76269316/181001583-dd97e84d-04f9-434f-ad57-a78dc9e195a7.png)

- 모멘텀 기반: <img src="https://user-images.githubusercontent.com/76269316/181001792-9040dcad-dd8e-4014-a224-dacaf5677e40.png" alt="스크린샷 2022-07-26 오후 9 05 41" style="zoom:67%;" />에 탄력에 대한 식 <img src="https://user-images.githubusercontent.com/76269316/181001840-da3d6116-a8ef-4009-becd-579428cf5e9d.png" alt="스크린샷 2022-07-26 오후 9 05 58" style="zoom:67%;" />를 붙여줌 (현재 스텝 속도를 저장했다가 다음 속도를 계산해 그 값을 반영시킴)
- 가변학습률(Adaptive learning rate): 고정된 학습률 대신 가변학습률을 적용
- 모멘텀 + 가변학습률 개념을 합친 방법론 ➡️ Adam

<br>

##### ADAM (Adaptive Moment Estimation)

RMSProp과 Momentum 개념을 합친 방법론

![스크린샷 2022-07-26 오후 9 12 12](https://user-images.githubusercontent.com/76269316/181002936-d8bc1beb-5ee5-458c-8e50-c3ad2f60b540.png)

gradient와 step size에 이전 정보를 기억해서 현재에 반영

<br>

##### Scheduling

스텝 사이즈를 특정 시간에 따라 조절하는 방법론

- StepLR
- ExponentialLR
- Cosine Annealing

<br>

##### Vanishing Gradient

![스크린샷 2022-07-26 오후 9 21 03](https://user-images.githubusercontent.com/76269316/181004431-2a93b47d-ea49-44a8-8bb7-3b6499014c10.png)

가중치는 backpropagation을 통해 업데이트되게 됨

<br>

![스크린샷 2022-07-26 오후 9 21 58](https://user-images.githubusercontent.com/76269316/181004606-417b54a7-8f6c-49cb-b068-1f43b55e9f67.png)

앞서 살펴본 활성함수 중 시그모이드는 미분시 0 ~ 1 사이의 값을 갖게 되는데, backpropagation 과정에서 연쇄 법칙을 통해 계산되면 미분값이 0에 가까워지는 문제점이 생김 -> 가중치가 업데이트 되지 않는 문제가 생김

<br>

![스크린샷 2022-07-26 오후 9 23 50](https://user-images.githubusercontent.com/76269316/181004966-3100e944-24ab-4872-980f-5f7da50f5765.png)

ReLU나 Leaky ReLU에서는 이를 방지할 수 있음

<br>

##### 하강법의 한계

<img src="https://user-images.githubusercontent.com/76269316/181005278-1194ee15-80b8-4939-b8d1-740542807cbe.png" alt="스크린샷 2022-07-26 오후 9 25 47" style="zoom:67%;" />

시작점에 따라 local minimum에 빠질 수 있음 -> 손실함수 설계시 convex function(볼록 함수)으로 설계하는 것이 좋음

[Convex Optimization](http://sanghyukchun.github.io/63/)

<br>

![스크린샷 2022-07-26 오후 9 31 48](https://user-images.githubusercontent.com/76269316/181006393-029cb388-cc19-441f-a185-6061b258684d.png)

concave한 형태는 maximize function -> -를 붙임으로써 minimization으로 변경할 수 있음 (convex로 변경됨)

<br>

##### Convexity

![스크린샷 2022-07-26 오후 9 28 37](https://user-images.githubusercontent.com/76269316/181005824-d8e2dc96-c197-4043-9922-65bf567941c8.png)

<br>

![스크린샷 2022-07-26 오후 9 35 35](https://user-images.githubusercontent.com/76269316/181007081-2eae5e8e-95fd-4aad-83c0-132d52758241.png)
