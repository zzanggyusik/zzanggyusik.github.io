---
title: "[대학원 면접] 대학원 면접 질문 & 답변"
excerpt: "대학원 면접 질문 & 답변 정리"
toc: true
toc_label: "대학원 면접 질문 & 답변"
toc_sticky: true

categories:
  - MS
tags:
  - 대학원
last_modified_at: 2022-07-21
---

### Machine Learning

##### 정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?

정규화(Normalization)는 개별 피처의 크기를 모두 같은 단위로 변경하는 것을 의미합니다.

데이터에는 다양한 크기의 피처들이 존재하는데, 이 때 더 큰 스케일을 가진 피처가 더 중요하다고 여겨질 수 있습니다.

이 때 정규화를 적용함으로써 서로 다른 스케일을 갖는 피처들을 모두 동일한 정도의 스케일(중요도)로 반영되도록 할 수 있습니다.

정규화의 대표적인 방법으로 **Min-Max Normalization**가 있습니다.

<br>

**Min-Max Normalization**

각 피처의 최솟값을 0, 최댓값을 1로 변환하는 방법으로 다음과 같이 계산할 수 있습니다.

<img width="311" alt="스크린샷 2022-07-21 오후 10 01 22" src="https://user-images.githubusercontent.com/76269316/180219745-0ce68738-fbf8-4a46-8952-85df0f2c5192.png">

❗️Min-Max Normalization의 단점

Min-Max Normalization은 이상치(Outlier)를 잘 처리하지 못합니다.

100개의 데이터가 있는데 그 중 99개는 0 ~ 40 사이에 있고, 나머지 하나만 100일 때, 이에 최소-최대 정규화를 적용하면 다음과 같이 99개의 값은 0 ~ 0.4 사이에 존재하는데 하나의 값만 1이 되게 됩니다. (y축에서는 정규화가 잘 적용됐지만, x축에는 그렇지 못함 -> x축의 영향이 지배적)

<img src="https://user-images.githubusercontent.com/76269316/180220489-1c15a002-01d0-4878-88e9-15e22a5b8eba.png" alt="image" style="zoom:67%;" />

<br>

cf) 표준화 (Standardization)

표준화는 평균이 0이고 표준편차가 1인 표준정규분포(가우시안 분포)로 변환하는 것을 의미합니다.

**Z-Score Normalization**

<img width="228" alt="스크린샷 2022-07-21 오후 10 00 45" src="https://user-images.githubusercontent.com/76269316/180219662-faa54f5a-1ac0-4daf-bd2b-693ea23950f9.png">

<br>

아래는 최대-최소 정규화를 적용한 데이터에 그대로 Z-Score Normalization을 적용한 결과입니다.

<img src="https://user-images.githubusercontent.com/76269316/180221151-2e57f9c1-c93c-4741-b36c-eb868e2f9960.png" alt="image" style="zoom:67%;" />

여전히 outlier로 보이는 값이 하나 존재하지만, y축을 기준으로 봤을 때 모든 값이 -2 ~ 2사이에 존재하는 것을 확인할 수 있습니다.

<br>

**Reference**

- [정규화(Normalization) 쉽게 이해하기](https://hleecaster.com/ml-normalization-concept/)

<br><br>

### Deep Learning

##### 딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는?

딥러닝은 인공신경망(ANN)에 기반한 기계 학습 방법의 일부로 머신러닝에서는 데이터의 여러 특징 중 어떤 특징을 사용할지를 사람이 직접 분석하고 판단해야 하지만, 딥러닝에서는 machine이 스스로 데이터에서 특징을 추출해 학습합니다.

따라서 정형 데이터(tabular data)는 주로 머신러닝을, 비정형데이터(image, text)는 딥러닝 방식을 사용합니다.

<br>

##### 알고있는 Activation Function에 대해 설명해주세요.

**Sigmoid**

<img width="159" alt="스크린샷 2022-07-22 오후 10 50 49" src="https://user-images.githubusercontent.com/76269316/180453429-3297023e-459c-45b6-9198-15136a2380a0.png">

<img src="https://user-images.githubusercontent.com/76269316/180452985-eb6ccd9a-3954-48a6-ad13-8682c1efd0f9.png" alt="image" style="zoom: 67%;" />

Sigmoid 함수는 Multi Perceptron에서 비선형 값을 얻기 위해 사용됐는데, 최근에는 아래 단점들 때문에 사용하지 않습니다.

- 함숫값 중심

시그모이드에서는 함숫값 중심이 0이 아니라 1/2이기 때문에 학습이 느려질 수 있습니다.

모든 x 값들을 같은 부호라고 가정하고(여기서는 양수로 가정) 파라미터 w에 대한 미분함수식을 계산해보면, <img width="135" alt="스크린샷 2022-07-22 오후 11 03 06" src="https://user-images.githubusercontent.com/76269316/180455810-1e755030-56ab-4c21-b3bc-24750aa7d82e.png">에서 <img width="87" alt="스크린샷 2022-07-22 오후 11 03 48" src="https://user-images.githubusercontent.com/76269316/180455955-16379eed-8ed4-4094-83bb-072f7d9bb8ee.png">이기 때문에<img width="206" alt="스크린샷 2022-07-22 오후 11 04 28" src="https://user-images.githubusercontent.com/76269316/180456092-bf98fbf9-c3d1-47e7-a1ba-304032c8097a.png">가 되게됩니다.

x값들이 모두 양수이기 때문에 <img width="52" alt="스크린샷 2022-07-22 오후 11 06 23" src="https://user-images.githubusercontent.com/76269316/180456388-d38f06ca-d029-40e3-a77c-fd9330e07b84.png" style="zoom:67%;" > 부호에 따라 방향이 결정되게 됩니다. 결과적으로 한 노드에서의 모든 파라미터 w의 미분값은 같은 방향으로 업데이트 되게 되므로(zig-zag update) 학습이 느려지게 됩니다.

- Gradient Vanishing

<img width="214" alt="스크린샷 2022-07-22 오후 10 52 14" src="https://user-images.githubusercontent.com/76269316/180453697-2d912640-f7b8-42c0-9aff-5d2589f496d2.png">

<img src="https://user-images.githubusercontent.com/76269316/180453762-efafb0ed-f9a4-49fc-bab0-6fd8c43a5358.png" alt="image" style="zoom:67%;" />

시그모이드 함수를 미분하면 |x|가 커질수록 미분값이 0에 수렴하게 되는데, 이로 인해 Backpropagation 과정에서 미분값이 손실되는 Gradient Vanishing 현상이 발생하게 된다.

<br>

**Hyperbolic Tangent**

<img width="203" alt="스크린샷 2022-07-22 오후 11 10 43" src="https://user-images.githubusercontent.com/76269316/180457230-72cc149e-5e5f-4e8d-85d8-894f8c7301d5.png">

하이퍼볼릭 탄젠트는 시그모이드 함수를 transformation해서도 표현할 수 있습니다.

<img width="223" alt="스크린샷 2022-07-22 오후 11 11 34" src="https://user-images.githubusercontent.com/76269316/180457420-0c5d2655-dd6e-4eee-a4a4-bd45e05e6355.png">

<img src="https://user-images.githubusercontent.com/76269316/180456964-6ff1b0a2-8184-46ab-97d7-4bdedd59c182.png" alt="image" style="zoom:67%;" />

하이퍼볼릭 탄젠트는 함수 중심값을 0으로 옮김으로써 sigmoid의 최적화 과정이 느려지는 문제를 해결했습니다.

하지만 gradient vanishing 문제는 해결하지 못했습니다.

<br>

**Rectified Linear Unit**

<img width="167" alt="스크린샷 2022-07-22 오후 11 13 50" src="https://user-images.githubusercontent.com/76269316/180457885-83ab9430-63ab-4284-8a78-b6f7e921daa2.png">

<img src="https://user-images.githubusercontent.com/76269316/180457731-c3d73c56-5c2a-4ed3-b3eb-364d4ea6390c.png" alt="image" style="zoom: 67%;" />

ReLU 함수는 x > 0이면 기울기가 x 값을 그대로 반환하고 x < 0이면 0을 반환합니다.

연산 비용이 크지 않고, 구현이 간단해 sigmoid나 tanh보다 학습 속도가 빠릅니다.

하지만 x가 0 이하인 값들에 대해서는 뉴런이 죽는(기울기가 0이기 때문에) Dying ReLu 문제가 존재합니다.

<br>

Leaky ReLU

<img width="205" alt="스크린샷 2022-07-22 오후 11 18 59" src="https://user-images.githubusercontent.com/76269316/180458924-9a970a31-664b-4c93-be74-d256973bfd10.png">

Leakly ReLU는 Dying ReLU 현상을 해결하기 위해 나온 함수로, x가 0 이하일 때 함숫값으로 0을 반환하는 것이 아닌 0.01을 곱한 작은 값을 반환합니다.

음수 x에 대한 미분값이 0이 되지 않는 점만 제외하고 ReLU와 동일한 특징을 갖고 있습니다.

**Reference**

- [딥러닝에서 사용하는 활성화 함수](https://reniew.github.io/12/)

<br><br>

### Mathematics

##### 평균(mean)과 중앙값(median) 중 어떤 케이스에서 뭐를 써야할까요?

- 평균(mean): 모든 관측값의 합을 관측값의 개수로 나눈 값
- 중앙값(median): 전체 관측값을 정렬했을 때, 가운데 위치하는 값

평균은 전체 관측값이 반영되기 때문에 평균 근처에 표본이 몰려 있는 상황에서 사용합니다. 하지만 극단적인 값(Outlier - 아주 크거나 아주 작은 값)에 영향을 많이 받는다는 단점이 있습니다.

중앙값은 관측값의 위치만을 고려하기 때문에 평균과 달리 outlier에 영향을 받지 않습니다. 따라서 표본의 편차가 큰 경우에 사용합니다.

하지만 관측치가 많아지면 해당 집단을 대표하는 대표성이 사라진다는 단점이 존재합니다.

ex)  주문 횟수 데이터 = [1, 1, 1, 1, 1, 15, 30] ➡️ 중앙값: 1

<br>

##### 공분산과 상관계수는 무엇일까요? 수식과 함께 표현해주세요

공분산은 확률변수 X의 편차(평균으로부터 얼마나 떨어져 있는지)와 확률변수 Y의 편차를 곱한 것의 평균값으로, 두 확률 변수간의 상관 정도를 나타내는 값으로 다음과 같이 계산합니다.

<img width="512" alt="스크린샷 2022-07-21 오후 11 08 50" src="https://user-images.githubusercontent.com/76269316/180234324-bdac805b-0161-45e4-98ae-726b1f952336.png">

cf) 이 때 각각의 확률변수 X와 Y의 평균은 다음과 같습니다. <img width="270" alt="스크린샷 2022-07-21 오후 11 10 43" src="https://user-images.githubusercontent.com/76269316/180234715-0691ac8f-251e-48d3-96de-f0f5ffe5151b.png">

두 확률변수의 관계에 따라 공분산은 다음과 같은 값을 갖게됩니다.

- X가 증가할 때, Y도 증가 (X가 감소하면 Y도 감소) ➡️ Cov(X,Y) > 0
- X가 증가할 때, Y는 감소 (X가 감소하면 Y는 증가) ➡️ Cov(X,Y) < 0
- 두 확률변수는 아무런 선형관계가 없음 ➡️ Con(X,Y) = 0

공분산은 확률 변수의 단위 크기에 영향을 받기 때문에 공분산만으로는 상관관계의 방향성만을 알 수 있고, 상관관계의 정도는 알 수 없습니다. (각 확률 변수의 단위 크기가 크면 무조건 공분산의 크기가 크게 나오므로)

이를 극복하기 위해 등장한 것이 상관 계수(Correlation Coefficient)입니다.

상관계수는 확률 변수의 절대적 크기에 영향을 받지 않도록 공분산에다가 각 확률 변수의 분산을 나눈 것입니다. (공분산을 정규화시킨 것)

<img width="437" alt="스크린샷 2022-07-21 오후 11 27 58" src="https://user-images.githubusercontent.com/76269316/180239171-ef65e5ba-5e46-4f48-a12a-f0fb551ddb74.png">

상관계수는 -1과 1 사이의 값을 가지며, -1에 가까울 수록 음의 상관관계, 1에 가까울 수록 양의 상관관계를 의미합니다.

상관계수가 0이면 두 변수는 상관관계가 없다고 간주합니다.

![image](https://user-images.githubusercontent.com/76269316/180235558-de0ae384-bb49-48a3-be75-d9f6d24e8ff2.png)
