---
title: "[대학원 면접] 자소서 관련 논문 정리"
excerpt: "자소서 관련 논문 정리"
toc: true
toc_label: "자소서 관련 논문 정리"
toc_sticky: true

categories:
  - MS
tags:
  - 대학원
last_modified_at: 2022-07-31
---

### ROCT

##### [Radius-based Class Overlap Cleaning Technique](https://ieeexplore.ieee.org/document/9529661)

2021 IEEE COMPSAC Wuhan University

software defect prediction 데이터셋 사용

Class Overlap Problem이 Class Imbalance Problem보다 더 큰 성능 저하의 원인인데, 많은 연구들이 COP보다 CIP 문제에 치중돼 있고, 또한 COP cleaning 테크닉들은 하이퍼 파라미터에 의존적이라는 문제가 존재 (최적의 하이퍼 파라미터를 찾는 것이 어렵기 때문)

1. 각 인스턴스를 hypersphere의 중심으로 두고, 최적의 radius를 찾음 (Differential Evolution 알고리즘 사용)
2. 해당 hypersphere 내에 있는 인스턴스 중, 중심 클래스와 반대되는 클래스는 overlap 돼 있다고 판단해 제거

<img width="464" alt="스크린샷 2022-07-30 오후 1 56 41" src="https://user-images.githubusercontent.com/76269316/181875352-1c7f2d1b-fcd7-4281-a33b-f9d7d2b2c5be.png">

<br>

##### [Differential Evolution](https://www.kgsjournal.org/articles/xml/2b7B/)

비선형, 미분 불가능한 연속공간함수를 빠르게 탐색하여 최적해(optimal minimum)를 구하기 위한 목적으로 제안

초기 개체군을 랜덤하게 생성하고 돌연변이(Mutation), 교배(Crossover), 선택(Selection) 등의 단계를 거쳐 다음 세대를 구성

지정된 세대수에 도달하거나, 새로운 개체가 최적값을 만족할 때까지 진화 연산이 반복 수행됨

scipy 메소드로 구현

<br>

**돌연변이 (Mutation)**

이전 세대의 개체 집단 <img src="https://user-images.githubusercontent.com/76269316/181875854-25a64d63-3de5-49a8-b953-380acbb14e68.png" alt="image" style="zoom: 50%;" />에서 임의의 개체 3개를 선택하여 교배용 벡터를 생성(F는 돌연변이 상수로 0~2 사이의 값을 갖는다)

<img width="255" alt="스크린샷 2022-07-30 오후 2 21 11" src="https://user-images.githubusercontent.com/76269316/181875976-661f2288-7770-411a-9a40-338249e40142.png">

<br>

**교배 (Crossover)**

시행벡터(trial vector)를 생성하기 위해 부모 개체와 돌연변이 벡터(mutant vector)가 교배됨(CR은 교배상수로 0~1 사이의 값을 가짐, <img src="https://user-images.githubusercontent.com/76269316/181876078-595cab8d-4b93-45dc-8cf0-f98f51f9d77d.png" alt="image" style="zoom:50%;" />는 [1, 2, ··· D]의 무작위 정수)

<img width="367" alt="스크린샷 2022-07-30 오후 2 22 55" src="https://user-images.githubusercontent.com/76269316/181876028-a703a8f5-35da-4caf-9dec-4411a0141f2b.png">

<br>

**선택 (Selection)**

<img width="310" alt="스크린샷 2022-07-30 오후 2 26 05" src="https://user-images.githubusercontent.com/76269316/181876126-db86e73f-5893-4871-aaf6-489d23726b9a.png">

목표벡터(target vector) <img src="https://user-images.githubusercontent.com/76269316/181876203-acb0750d-eb4c-4916-bea0-fedff425ab5a.png" alt="image" style="zoom:50%;" />와 시행벡터(trial vector) <img src="https://user-images.githubusercontent.com/76269316/181876230-344ad0c3-9c0c-4e13-9d86-6133e733e7cb.png" alt="image" style="zoom:50%;" />와 비교해 교배상수로 다음 세대를 구성할 개체를 선택함

<br><br>

### Modeling Tabular Data using Conditional GAN

Tabular data는 discrete와 continuous한 columns를 학습시킬 때, 다양한 문제가 발생

- continuous data: 확률 분포가 여러 개의 봉우리를 갖게 되는 multimodal distribution 문제
- discrete data:카테고리별 빈도수가 다른 문제 (imbalance)

➡️ tabular data를 보다 좋은 성능으로 생성할 수 있는 방법 및 데이터 생성 알고리즘의 성능을 평가할 수 있는 benchmarking 시스템 제안

<br>

Generator G가 기존 테이블 T(Nc개의 continuous columns, Nd개의 discrete columns를 갖고 있음, 각각의 컬럼은 joint distribution)를 기반으로 T sync를 생성 ➡️ 두가지 metric으로 T sync를 평가

- Likelihood fitness: T sync에 있는 columns가 Ttrain에 있는 columns의 joint distribution을 잘 따르는가?

이를 평가하기 위해 simulated data를 사용 -> 데이터셋의 실제 분포를 정확하게 알 수 있음

![스크린샷 2022-07-31 오전 9 48 35](https://user-images.githubusercontent.com/76269316/182004994-9607e383-2bf3-4921-9a9e-d89adead25c1.png)

L(sync)는 생성 모델이 overfitting될수록 잘 나오기 때문에, 이를 피하기 위해 test데이터와 synthesize 데이터 사이의 likelihood를 구한 L(test)도 사용하게 된다.

- Machine learning efficacy: Ttrain을 통해 생성된 T sync를 학습한 ML 모델의 성능과 실제 데이터인 Ttest를 통해 학습한 ML 모델의 성능은 얼마나 비슷한지?

ML efficacy를 평가하기 위해 이번에는 real data를 사용

![스크린샷 2022-07-31 오전 9 53 20](https://user-images.githubusercontent.com/76269316/182005076-3a961b82-15a2-4dd6-bd0a-fccb63482d64.png)

<br>

이러한 metric이 좋은 성능을 내기 위해서는 tabular data에의 Mixed data types(discrete & continuous columns), Non-Gaussian distributions(continuous data의 경우), Multimodal distributions(확률분포가 여러 봉우리 가짐), Learning from sparse one-hot-encoded vectors, Highly imbalanced categorical columns (mode collapse 발생) 등의 문제를 해결해야 한다.

<br>

**Mode-specific Normalization**

tabular data를 생성하기 전, normalization 과정을 거쳐야 한다. dicsrete 변수는 단순히 전체 카테고리 개수만큼 one-hot encoding을 진행한다.

continuous 변수에 속한 데이터들의 확률 분포는 여러 개의 봉우리(Gaussian Mixture)로 표현된 경우가 많은데, 이 경우 데이터를 생성하기 어렵기 때문에 normalization 과정을 거쳐야 한다.

<p align = "center">
  <img src = "https://user-images.githubusercontent.com/76269316/182003032-13ed2f9a-9907-456b-9819-47e4a617ca1d.png">
</p>
<p align = "center">
  Gaussian Mixture
</p>


▶️ 봉우리 개수만큼의 gaussian 확률 분포를 따르는 여러개의 sub distribution으로 나눠줌 (Gaussian Mixture Model인 VGM이 사용됨)

![스크린샷 2022-07-31 오전 8 09 54](https://user-images.githubusercontent.com/76269316/182003087-ab51fa45-ac36-45c8-bd83-de0a4a4c0f05.png)

sub distribution의 표준 편차, weight를 구한다.

이후 i번째 column, j번째 row에 해당하는 데이터인 C(i,j)의 확률을 계산해(가운데 그림) 가장 확률이 높게 나오는 sub distribution을 구해 이를 one-hot encoding [0, 0, 1]로 표현한다. (3번째 mode가 가장 확률이 높으므로)

또한 각 sub distribution의 평균(<img src="https://user-images.githubusercontent.com/76269316/182003595-f0252b87-791e-406d-b252-aa1b54c2f941.png" alt="스크린샷 2022-07-31 오전 8 38 03" style="zoom:50%;" />), 표준편차(<img src="https://user-images.githubusercontent.com/76269316/182003601-c0a6a4f6-c23a-4ae6-87c3-e3a154379289.png" alt="스크린샷 2022-07-31 오전 8 38 28" style="zoom:50%;" />)를 통해 scalar로 표현된 가중치 값 알파를 구한 뒤, continuous columns와 discrete columns를 concatenate함으로써 각 rows를 normalization 해준다.

![스크린샷 2022-07-31 오전 8 14 17](https://user-images.githubusercontent.com/76269316/182003183-eb604672-004b-44a8-aa8e-096ec55c9001.png)

<br>

**Conditional Generator and Training by Sampling**

discrete 변수의 경우 각 카테고리마다 빈도가 다른데, 이를 고려하지 않고 학습시킬 경우 원래 데이터의 특징이 사라지게 된다. 이를 해결하기 위해 **Training by Sampling** 개념이 도입되었다.

1. Nd개의 discrete columns 중 한 개를 랜덤으로 선택 (이를 i\*라고 표현)
2. 선택된 column(i*)에 대해 PMF(Probability Mass Function, 확률 질량 함수)를 구한다.
3. PMF를 따르는 확률 분포에 따라 값 하나를 선택한다. (이를 k\*번째 값이라고 표현)
4. conditional vector를 i\*와 k\* 값을 고려하여 생성한다.

![스크린샷 2022-07-31 오전 8 48 52](https://user-images.githubusercontent.com/76269316/182003822-4c86bb74-023d-40f9-bb11-3f896bdfedd2.png)

이렇게 taining-by-sampling 을 통해 학습을 진행할 경우, discrete column에 대하여 각 category 별로 기존 데이터의 빈도와 비슷하게 학습이된다.

<br>

**Network Structure**

- Generator

![스크린샷 2022-07-31 오전 9 35 43](https://user-images.githubusercontent.com/76269316/182004742-6fbf4c82-4bc1-4ddc-a7a7-cb1091cf75f8.png)

latent vector를 입력으로 2개의 hidden layer를 거친 뒤, 알파, 베타, d 값을 구한다.

알파값은 scalar이기 때문에 hyperbolic tangent를 activation function으로 사용했고, 베타와 d는 벡터이므로 다중 클래스에 대한 classification이 가능한 gumbel softmax를 사용했다.

학습에 사용된 loss는 Generator loss로 one-hot encoding된 벡터 m과 d 사이의 cross-entropy loss를 사용한다.

<br>

- Discriminator

![스크린샷 2022-07-31 오전 9 35 53](https://user-images.githubusercontent.com/76269316/182004745-9d0f720b-316d-4f95-b179-5863d5f751a3.png)

mode collapse(Generator가 비슷한 이미지만 계속 생성하는 것)를 막기 위해 10개의 sample이 한 번에 들어가며, 10개의 conditional vecotr도 함께 들어간다. (PacGAN 구조)

마지막 레이어에서 1개의 노드가 real 데이터라면 1, fake라면 0으로 예측 ➡️ WGAN loss가 사용되며, optimizer는 Adam이 사용됨
