---
title: "Self Supervised Learning"
excerpt: "Self Supervised Learning 정리"
toc: true
toc_label: "Self Supervised Learning"
toc_sticky: true

categories:
  - MS
tags:
  - 대학원
last_modified_at: 2022-07-12
---

### Self Supervised Learning(SSL)이란?

Self Supervised Learning은 직역하자면 자기 지도 학습으로, 레이블이 없는 데이터를 활용하여 모델이 좋은 특징(representation)을 추출할 수 있도록 학습하는 방법론입니다.

연구 초기에는 Unsupervised Learning(비지도 학습)의 하위 개념 정도로 분류 됐었는데, 현재 Meta AI Research인 Yann LeCun 교수님이 Self Supervised Learning의 개념을 구체화하면서 2019년부터 각각의 독립된 학습 방식으로 나뉘게 됩니다.

![스크린샷 2022-07-12 오후 11 47 43](https://user-images.githubusercontent.com/76269316/178518595-b1c9ee91-09b0-4adf-9675-f8e66742ab19.png)

<br>

##### Self Supervised Learning vs Unsupervised Learning

Self Supervised Learning과 Unsupervised Learning의 차이는 supervision(label이라고 생각하면 좋을 듯)의 유무입니다.

Unsupervised Learning은 레이블이 없는 데이터에서 패턴을 학습하는 방식으로 (e.g. [군집화](https://seominseok4834.github.io/machine%20learning/7.clustering/)) supervision이 아예 없습니다.

하지만 Self Supervised Learning(이하 SSL)은 아래에서 설명하겠지만, 원본 데이터에서 스스로 supervision을 생성해 학습을 진행하는 방식이기 때문에 이를 unsupervised learning의 일종으로 보는 것은 맞지 않다는 것이 Yann LeCun 교수님의 주장입니다.

<br>

SSL의 연구 과정은 다음과 같습니다.

1. Label이 없는 데이터를 가지고, 좋은 representation을 추출할 수 있도록 Backbone 모델을 훈련한다.
2. 1에서 훈련한 backbone 모델을 가지고, 적용하고자하는 downstream task로 transfer learning을 진행한다.

downstream task는 실제로 우리가 하고자 하는 태스크로, image classification이나 object detection 같은 걸 의미합니다. 2에서 transfer learning을 하는 이유는 pretext task를 학습한 모델은 object detection이나 segmentation을 할 수 없기 때문에 다시 학습을 진행하는 것입니다.

![스크린샷 2022-07-13 오후 9 21 00](https://user-images.githubusercontent.com/76269316/178732143-dd0803c3-aa05-4a08-8841-2ca0670d3f51.png)

SSL 연구는, Backbone 모델에서 더 좋은 feature(Representation)을 추출할 수 있도록하여 차후 task에서 성능을 올리는 것(Supervised Learning으로 학습한 모델보다)이 주된 목표인 듯합니다.

현재 연구 동향은 Pretext Task, Contrastive Learning 두 가지로 나뉘는데 레이블이 없는 데이터에서 좋은 representation을 뽑는 backbone 모델을 훈련할 때 pretext task로 할 것인지, contrastive learning으로 진행하는지의 차이입니다.

먼저 Pretext Task에 대해 알아보겠습니다.

<br><br>

### Pretext Task

SSL 초기 연구에서 진행됐던 방법으로, 연구자가 직접 만든 pretext task로 모델을 훈련한 뒤 이를 downstream task로 transfer learning하는 방식입니다.

매우 다양한 pretext task가 있는데, 몇 가지만 소개하겠습니다.

##### Exemplar, 2014 NIPS

[Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks](https://arxiv.org/abs/1406.6909) 논문에서 제안한 pretext task로

![스크린샷 2022-07-13 오전 12 35 28](https://user-images.githubusercontent.com/76269316/178529621-bd091ac5-7af7-4c47-80f9-767ffd12c09b.png)

96x96의 이미지에서 object가 있을만한 곳을 crop하여 32x32 크기의 seed patch를 생성합니다. (위 그림에서는 왼쪽 최상단)

이후 seed patch에 augmentation을 적용해, 24개의 이미지를 확보한 뒤 classifier가 24개의 patch들과 seed patch를 동일한 클래스로 예측하도록 학습을 진행하는 방식입니다.

<br>

##### Context Prediction

다음으로 소개할 pretext task는  [Unsupervised Visual Representation Learning by Context Prediction](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf)에서 제안한 방식으로, 

![스크린샷 2022-07-13 오전 12 44 12](https://user-images.githubusercontent.com/76269316/178532992-9b463829-2854-497e-88c2-b965ed5f1786.png)

이미지에서 9개의 patch를 가져와 중간의 patch와 다른 1개의 patch를 주고, 다른 1개의 patch의 위치가 어느 방향에 위치할지를 찾는 문제입니다.

<br>

하지만 이러한 방식은 이미지 하나하나가 하나의 클래스를 부여받는다는 문제점이 존재했습니다. 데이터 개수가 클래스 개수가 되니 메모리 문제 등으로 큰 데이터셋에 대해서는 이를 적용하기 어려웠고, Contrastive Learning이 주목받게 됩니다.

<br><br>

### Contrastive Learning

##### Motivation

Image classification에서 레오폴드 이미지를 분류한 뒤 예측 확률 값을 확인해 보았더니, 재규어나 치타 같이 레오폴드와 유사한 동물의 클래스 값이 높았다고 합니다.

레오폴드가 재규어나 치타랑 유사하다는 정보는 전혀 주어지지 않았는데도(레이블 정보는 one-hot 벡터로 들어가기 때문에) 저렇게 예측했다는 것은 이미지끼리 공유하는 특징이 있다는 것이고, 이러한 관측치 사이의 유사성을 기반으로 해서 학습한다면 좋은 representation을 얻을 수 있지 않을까하는 동기에서 contrastive learning 연구가 시작됐다고 합니다.

[Unsupervised Feature Learning via Non-Parametric Instance Discrimination](https://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0801.pdf)

![스크린샷 2022-07-13 오전 12 59 13](https://user-images.githubusercontent.com/76269316/178538522-1abd1b2e-ca42-4f4b-87fe-b5182cb2b9bd.png)

<br>

하지만 MNIST만 하더라도 관측치가 60,000개

<img src="https://user-images.githubusercontent.com/76269316/178719166-180979d0-ee47-4af6-a5ae-9d4aa627f918.png" alt="image"  />

ImageNet 같은 경우는 관측치가 1,200,000개 이상이기 때문에 소프트맥스를 사용하기 어렵습니다.

<img src="https://user-images.githubusercontent.com/76269316/178719562-ad2cbc98-c189-4466-91fc-0e05bd4c9786.png" alt="image" style="zoom:67%;" />

<br>

이러한 문제와 유사한 것이 word embedding인데, word embedding은 단어 사이의 관계를 학습하기 위해 context 뒤에 올 단어를 예측하면서 representation을 학습합니다.

word embedding에서도 단어의 개수가 클래스 개수가 되다보니 전체를 softmax로 계산할 수 없게 되는데, 일부만 샘플링하여 계산하는 방법을 사용하였습니다.

<img src="https://user-images.githubusercontent.com/76269316/178744096-5d4e4971-25ad-400e-9c86-edb074913820.png" alt="스크린샷 2022-07-13 오후 10 23 50" style="zoom:67%;" />

<br>

Contrastive Learning에서는 memory bank를 사용하여 이와 유사한 방식을 적용하였습니다.

![스크린샷 2022-07-13 오전 1 34 13](https://user-images.githubusercontent.com/76269316/178545812-f0a4834f-9c0c-41ac-9503-79cb6003063b.png)

먼저, 이미지는 고차원이기 때문에 128차원으로 임베딩해서 사용합니다.

이후, Iteration에서 얻은 모든 임베딩 벡터를 저장하는 memory bank에서 현재 target 이미지인 강아지(anchor)와 매칭되는 임베딩 정보와 나머지 이미지들을 샘플링합니다. (분포를 가정하지 않고 랜덤하게 샘플링)

매칭되는 임베딩 정보는 positive example, 나머지 이미지들은 negative example로 놓고 anchor와 positive의 코사인 유사도는 가깝게 anchor와 negative pair의 코사인 유사도는 멀어지도록 학습합니다.

<br>

<img src="https://user-images.githubusercontent.com/76269316/178724408-6e93294c-5256-4dc5-8e69-3a1acb1d1c22.png" alt="image" style="zoom: 50%;" />

이러한 과정을 거쳐 representation을 잘 학습한다면 다음과 같이 관측치가 배치된다고 합니다.

<br>

##### MoCo

[Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/pdf/1911.05722.pdf)

MoCo는 Facebook에서 발표한 논문으로 앞서 설명한 contrastive learning에서 memory bank의 문제점을 개선한 방식입니다.

memory bank는 관측치 전체에 대한 임베딩 정보를 계속 저장하고 있어야 하고, negative example을 랜덤 샘플링 하다보니 학습에 한 번도 사용하지 않는 데이터가 존재할 수 있다는 문제점이 있습니다.

MoCo에서는 Memory Queue를 사용해 모든 샘플이 동등한 횟수로 학습에 이용되며, 사용할때마다 임베딩하여 메모리 이슈도 해결하였습니다.  

![스크린샷 2022-07-13 오후 8 40 48](https://user-images.githubusercontent.com/76269316/178725495-c407d921-c9d3-4f46-a0e6-a597984eca0b.png)

<br>

Memory Queue에서 임베딩 벡터를 만들 때, Momentum Encoder가 네트워크로 사용됩니다.

<img src="https://user-images.githubusercontent.com/76269316/178725810-33041276-634d-4169-aa70-aa79a84b1c3c.png" alt="스크린샷 2022-07-13 오후 8 42 53" style="zoom:67%;" />

Target 네트워크는 stochastic gradient로 학습되지만, Momentum Encoder는 과거 target 네트워크 모델 weight의 가중 평균으로 업데이트 됩니다.

이러한 방식은 Semi Supervised Learning의 [Mean Teacher](https://arxiv.org/pdf/1703.01780.pdf)에서 차용한 것으로 과거 모델 weight들의 가중 평균을 이와 비슷하게 만들어 감으로써 모델을 학습시키는 방식입니다.

<br>

##### SimCLR

[A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709.pdf)

SimCLR는 구글에서 제안한 논문으로 '배치 사이즈를 크게 가져가면 충분한 양의 negative 샘플을 뽑을 수 있지 않을까?'하는 개념에서 시작됐습니다.

SimCLR에서는 관측치별로 2번의 augmentation을 적용한 뒤 anchor와 같은 이미지에서 나온 이미지를 positive example, 나머지를 negative example로 정의한 뒤 학습을 진행합니다.

<img src="https://user-images.githubusercontent.com/76269316/178730186-587b7ff8-fadd-48b9-a9e8-06ec58ccb53a.png" alt="스크린샷 2022-07-13 오후 9 09 31" style="zoom:67%;" />

<br>

실험 결과 2048개 ~ 4096개의 배치 사이즈를 사용했을 때 충분한 성능 향상이 이루어졌다고 합니다.

![스크린샷 2022-07-13 오후 9 11 57](https://user-images.githubusercontent.com/76269316/178730566-d6a436cb-21d7-4c1d-b783-32fc9c0510bd.png)

하지만 4096개의 배치 사이즈를 사용한다고 하면, 2번의 augmentation을 적용해 8192개의 샘플에 대해 연산을 해야하므로 많은 자원(gpu)이 필요하게 됩니다.

또한 다양한 data augmentation 기법과 조합을 사용해 실험을 진행했습니다.

![스크린샷 2022-07-13 오후 9 14 33](https://user-images.githubusercontent.com/76269316/178730961-3fcc4a9f-0ff8-4cd6-a928-24a059abe968.png)

<br>

### BYOL

Contrastive Learning은 pretext task보다 더 높은 성능을 달성할 수 있었지만, 여전히 한계점이 존재했습니다.

1.Require careful treatment of negative pairs: negative example을 확보하기 위해 많은 노력이 필요함

2.Choice of Image Augmentation: augmentation 조합에 따라 모델의 성능이 크게 좌우됨

![스크린샷 2022-07-13 오후 9 26 29](https://user-images.githubusercontent.com/76269316/178733120-03e53c20-5d7f-4729-b4bd-6bc38902a10e.png)

3.Requires comparing each representation of an augmented view with many negative examples: 수많은 negative 샘플을 비교하는 과정이 필요함

BYOL은 positive pair만을 사용하여 이러한 한계점을 극복하고자 하였습니다.

<br>

BYOL은 Online Network, Target Network로 구성돼 있습니다.

![스크린샷 2022-07-13 오후 9 38 13](https://user-images.githubusercontent.com/76269316/178735300-272c8106-fc8b-4cbc-ac34-4da977635afd.png)

각각의 네트워크는 하나의 이미지에서 다른 augmentation이 적용된 이미지를 입력으로 받습니다.

이후 Encoder에서 이미지의 특징을 담은 representation 벡터를 추출하고, Projector에서 representation 벡터를 projection 벡터로 변환하여 loss 값을 계산하기 좋게 변환합니다.

이후 online network에서 나온 projection 벡터로, target network에서 나온 projection 벡터를 예측하는 과정을 통해 학습하기 때문에 Predictor는 online network에만 존재합니다.

이후 online network에서는 손실 함수에서 계산한 gradient로 파라미터를 업데이트하고, target network에서는 online network와 target network의 파라미터를 가중 평균한 값으로 업데이트합니다. 이를 Exponential Moving Average라고 합니다.

또한 negative pair를 사용하지 않기 때문에 Contrastive Loss가 아닌, L2 loss를 사용합니다.

<br>

##### Collapsed Representation

![스크린샷 2022-07-13 오후 9 50 36](https://user-images.githubusercontent.com/76269316/178737614-1ec1a3cd-c81f-4af5-b838-fbce918a8d17.png)

BYOL에서는 negative pair를 전혀 사용하지 않기 때문에, 모델이 고정된 값을 갖는 constant vector를 출력하는 representation collapse 문제가 발생할 수 있는데, BYOL에서는 이를 해결했다고 한다.

<br>

<img src="https://user-images.githubusercontent.com/76269316/178738550-1e4c23c5-e1ca-4e43-8fbf-5dcb93947388.png" alt="image" style="zoom:50%;" />

그런데 어떻게 방지했는지는 모른다고 한다 .. ¿¿

**batch normalization 때문이다 -> 아니다 -> batch statistics를 통한 implict constrastive 효과를 받지 않는 이상 높은 성능을 발휘 할 수 없다 -> 아니다**

현재도 많은 논쟁이 오가고 있는 것 같다. 추후에 관련 논문들을 읽어봐야겠다.

<img width="613" alt="스크린샷 2022-07-13 오후 10 22 44" src="https://user-images.githubusercontent.com/76269316/178743874-0817c7ae-bc26-4d5b-a8b9-32c081f328a9.png">

[Understanding Self-Supervised Learning Dynamics without Contrastive Pairs](https://arxiv.org/pdf/2102.06810.pdf)

<br><br>

### Reference

**고려대학교 DQMA 연구실**

- [Self-Supervised Representation Learning](http://dmqm.korea.ac.kr/activity/seminar/284)
- [Towards Contrastive Learning](http://dmqm.korea.ac.kr/activity/seminar/308)
- [Dive into BYOL](http://dmqm.korea.ac.kr/activity/seminar/310)