---
title: "[대학원 면접 준비] ML Sampling 정리"
excerpt: "샘플링 정리"
toc: true
toc_label: "ML Sampling 정리"
toc_sticky: true

categories:
  - MS
tags:
  - 대학원
last_modified_at: 2022-07-19
---

### Class Imbalance

학습 데이터의 클래스가 균일하게 분포하지 않고, 하나의 클래스 값에 치우쳐 편항되게 학습되는 것을 **클래스 불균형(Class Imbalance)**이라고 합니다.

![image](https://user-images.githubusercontent.com/76269316/179516000-c87b1b27-0039-497e-b5f4-703e53c3b65f.png)

위 이미지는 캐글 [Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) 데이터셋의 클래스 분포를 시각화한 것입니다.

Not Fraud가 정상적인 신용카드 트랜잭션 데이터, Fraud가 신용카드 사기 트랜잭션을 의미합니다. 전체 데이터의 약 0.172%만이 사기 트랜잭션입니다.

사기가 발생하는 경우가 일반적이진 않기 때문에 이렇게 불균형한 분포를 띄게 됩니다.

실생활에서도 이와 같이 불균형한 클래스 분포를 갖는 데이터들이 많습니다. (암 예측, 스팸 필터링 등)

<br>

이렇게 불균형한 데이터셋으로 학습할 경우 다수 클래스로 치우치게 학습돼 편향된 모델이 만들어지게 됩니다.

그럴 경우 모델이 다수 클래스로 예측하는 경향성이 높아져 성능이 왜곡되게 됩니다.

👉  [Classification Metric](https://seominseok4834.github.io/ms/4.classification-metric-interview/) 참고

<br>

이번 포스팅에선 클래스 불균형 문제를 해결하기 위한 Undersampling / Oversampling에 대해 알아보겠습니다.



![image](https://user-images.githubusercontent.com/76269316/179519114-baee0e95-096d-44ae-b945-804f30711b04.png)

- Undersampling: 많은 데이터 세트를 적은 데이터 세트 수준으로 감소시키는 방식

  정상 레이블 데이터가 10,000건, 이상 레이블을 가진 데이터가 100건이 있으면 정상 레이블 데이터를 100건으로 줄여버리는 방식

  과도하게 정상 레이블로 학습/예측하는 부작용을 개선할 수 있지만, 너무 많은 정상 레이블 데이터를 감소시키기 때문에 정상 레이블의 경우 오히려 제대로 된 학습을 수행할 수 없다는 단점이 있어 잘 적용하지 않습니다.

- Oversampling : 이상 데이터와 같은 적은 데이터 세트를 증식하여 학습을 위한 충분한 데이터를 확보하는 방식

  동일한 데이터를 단순히 증식시키는 방법은 과적합이 되기 때문에 원본 데이터의 피처 값들을 아주 약간만 변경하여 증식

<br><br>

### Undersampling

##### Random Undersampling

다수 클래스에 있는 관측치를 무작위로 샘플링하는 방식으로, 수행할 때마다 다른 결과가 도출됩니다.

![image](https://user-images.githubusercontent.com/76269316/179521416-97a107f0-15a4-49f6-9038-23eab37f7f43.png)

출처: [Machine learning prediction of susceptibility to visceral fat associated diseases](https://www.researchgate.net/publication/343326638_Machine_learning_prediction_of_susceptibility_to_visceral_fat_associated_diseases)

<br><br>

##### Tomek Links

![image](https://user-images.githubusercontent.com/76269316/179529457-6ff26fbb-2aab-4b6e-8a34-11e1d1500173.png)

다수 클래스에서 추출한 포인트를 <img src="https://user-images.githubusercontent.com/76269316/179526117-a250b1a0-06f2-414c-b70b-9c87e2ce8050.png" alt="image" style="zoom:80%;" />, 소수 클래스에 추출한 포인트를 <img src="https://user-images.githubusercontent.com/76269316/179526328-7dc3726f-d21a-4f84-ad4b-7071147cafb0.png" alt="image" style="zoom:80%;" />라고 할 때, <img src="https://user-images.githubusercontent.com/76269316/179526117-a250b1a0-06f2-414c-b70b-9c87e2ce8050.png" alt="image" style="zoom:80%;" />와 <img src="https://user-images.githubusercontent.com/76269316/179526328-7dc3726f-d21a-4f84-ad4b-7071147cafb0.png" alt="image" style="zoom:80%;" /> 사이에 더 가까운 데이터 <img src="https://user-images.githubusercontent.com/76269316/179526562-100a44c1-a1ee-447c-95fa-594703d7b7d7.png" alt="image" style="zoom:80%;" />가 존재하지 않는다면 두 관측치는 Tomek link가 되게 됩니다.

이를 수학적으로 나타내면 다음과 같습니다.

![image](https://user-images.githubusercontent.com/76269316/179527927-d768dae1-0e67-4f65-92ce-8c14af593399.png)

<br>

![image](https://user-images.githubusercontent.com/76269316/179533429-d006826c-c0c9-46e6-b2ab-43294f0ff335.png)

이렇게 Tomek link를 형성한 두 샘플(초록색 동그라미) 중 하나는 노이즈이거나, 클래스 경계선 근처에 있으므로 다수 클래스에 속하는 샘플들을 제거합니다.

![image](https://user-images.githubusercontent.com/76269316/179533296-2f790b3c-6293-4fbc-aabb-f7c63e6da540.png)

이렇게 다수 클래스에 속하는 데이터를 제거함으로써 클래스 경계선을 다수 클래스 쪽으로 밀어붙이는 효과를 갖게 됩니다.

<br><br>

##### Condensed Nearest Neighbor

다음과 같은 데이터가 있다고 할 때,

![image](https://user-images.githubusercontent.com/76269316/179537121-7a21afea-4873-452c-bb2a-bfc1b8f7c04b.png)

<br>

![image](https://user-images.githubusercontent.com/76269316/179537204-a76c2b74-276e-4eaa-b09b-8b1a58c6146d.png)

먼저 소수 클래스를 모두 서브 데이터 집합 S (빨강색)에 포함시킵니다.

<br>

![image](https://user-images.githubusercontent.com/76269316/179537567-58782b26-26e2-4a7d-a48c-a88d4440b9c0.png)

이후 다수 클래스에서 하나의 샘플을 추출한 뒤 다수 클래스에 가까우면 집합 S에 포함시키지 않고, 아니라면 집합 S에 포함시킨다.

이 과정을 아래 그림처럼 선택되는 데이터가 없을 때까지 반복한다.

![image](https://user-images.githubusercontent.com/76269316/179537815-ac3242b1-c2a6-4115-8519-3f9e28db0ec1.png)

<br>

이후 집합 S에 속한 데이터만 남기고, 나머지는 제거한다.

![image](https://user-images.githubusercontent.com/76269316/179538190-bb0ccf29-d5c3-4d6e-9270-5dc621db2ed8.png)

CNN은 이렇게 소수 클래스에 가까운 다수 클래스를 선별하고 나머지는 모두 제거함으로써 분류 경계선을 재설정한다.

<br><br>

##### One Sided Selection

One Sided Selection은 Tomek Links와 Condensed Nearest Neighbor 기법을 합친 방법으로, Tomek link로 클래스 경계에 있는 데이터를 제거하고 CNN으로는 경계에서 멀리 떨어진 다수 범주의 데이터를 제거합니다.

아래 그림으로 자세히 살펴보겠습니다.

<br>

![image](https://user-images.githubusercontent.com/76269316/179750488-febc84ed-2460-40fc-bd7b-012f0f6d17f1.png)

Tomek Link를 형성한 두 샘플에서 아래와 같이 다수 클래스를 제거한 뒤,

![image](https://user-images.githubusercontent.com/76269316/179750685-dc08e03f-55aa-42c0-a262-a3c5c3e1aa3d.png)

나머지 데이터에 대해서는 Condensed Nearest Neighbor 방식으로 제거합니다.

![image](https://user-images.githubusercontent.com/76269316/179751973-82a2348f-eee3-400e-a5be-18a58b07e98c.png)

<br><br>

### Oversampling

##### Synthetic Minority Oversampling Technique (SMOTE)

SMOTE는  소수 클래스의 관측치에서 K 최근접 이웃 (K Nearest Neighbor)을 찾아 해당 관측치와 K개 이웃들의 차이를 일정 값으로 만들어서 기존 데이터와 약간 차이가 나는 새로운 데이터들을 생성하는 방식입니다.

<br>

![image](https://user-images.githubusercontent.com/76269316/179757027-475cbff0-0534-4bcb-811f-9583173f4dd0.png)

다음 분포를 갖는 데이터가 있다고 할 때,

<br>

![image](https://user-images.githubusercontent.com/76269316/179757514-f747b284-a36b-4eee-8cc0-e770e6fb2862.png)

소수 클래스에서 관측치 하나를 임의로  선정한 뒤, K개(K는 3으로 가정)의 인접한 다른 소수 클래스 관측치를 선택합니다.

<br>

![image](https://user-images.githubusercontent.com/76269316/179757965-0619c57a-1401-4e19-ba4e-466af911be0c.png)

선택된 K개의 관측치 중 다시 하나를 랜덤하게 선택한 뒤, 아래 식을 통해 새로운 데이터를 하나 생성합니다.

![image](https://user-images.githubusercontent.com/76269316/179759420-f0fe78c6-869a-481c-98f3-0f40c2719863.png)

<br>

![image](https://user-images.githubusercontent.com/76269316/179760141-b65298f3-dd03-45aa-b354-c86b6fd8b23b.png)

위 식을 간단하게 설명하기 위해 데이터가 다음과 같이 2차원 상에 있다고 가정해보겠습니다.

<br>

![image](https://user-images.githubusercontent.com/76269316/179760325-9d39a5ed-599d-4118-b97a-41e6a1ce9b82.png)

이 부분을 먼저 계산하면 (2, 3) - (5, 1) = (-3, 2)가 되게 되므로 전체 식은 다음과 같이 되게 됩니다.

![image](https://user-images.githubusercontent.com/76269316/179760755-5bac3af0-7da2-4b0d-a5d5-506bc5e9056d.png)

*u(0,1)은 0에서 1 사이의 값을 가짐*

<br>

![image](https://user-images.githubusercontent.com/76269316/179761590-320c9487-04fb-471b-9745-68f6c6e5793d.png)

위 식에 따르면, 결국 두 관측치를 잇는 직선 사이에 새로운 관측치가 생성되게 됩니다.

이러한 방식을 모든 소수 클래스의 관측치에 대해 반복함으로써, 가상의 관측치를 생성하게 됩니다.

<br>

만약 K를 1로 설정한다면, 주변 관측치가 변하지 않기 때문에 

![image](https://user-images.githubusercontent.com/76269316/179762130-c4faa50b-4ab0-4c79-adbf-88d4fdb81ba9.png)

계속해서 고정된 직선 사이에서만 관측치가 생기게 되고, 아래와 같이 오버 샘플링이 되게 됩니다.

<img src="https://user-images.githubusercontent.com/76269316/179762378-e714114b-cdd4-4d3b-a8dc-cfba6e502269.png" alt="image" style="zoom:50%;" />

<br><br>

##### Borderline SMOTE

앞서 살펴본 SMOTE 방식을 두 클래스의 경계 부분에 있는 관측치에만 적용한 방법입니다.

먼저 경계를 찾기 위해 소수 클래스 관측치를 모두 순회하며, k개의 이웃 관측치를 확인합니다.

![image](https://user-images.githubusercontent.com/76269316/179763324-3369484c-0f31-447d-ae3a-755b6b6ab80c.png)

K=3이라고 가정했을 때, 다음과 같이 3개의 클래스가 모두 소수 클래스인 경우에는 경계선이 아니라고 판별하고, 이 때의 이웃 관측치를 **Safe 관측치**라고 합니다.

<br>

![image](https://user-images.githubusercontent.com/76269316/179764575-76d8956e-1a1d-44e9-8fe8-0d357ad46546.png)

또 다른 소수 클래스 관측치를 살펴보면, 다음과 같이 2개는 다수 클래스, 1개는 소수 클래스에 속하는 경우 경계선으로 판별하고 이 때의 이웃 관측치를 **Danger 관측치**라고 합니다.

<br>

![image](https://user-images.githubusercontent.com/76269316/179764946-3c86bd6e-5d26-48e2-b22b-9bc452c65c2d.png)

다음과 같이 이웃 관측치가 모두 다수 클래스인 경우에는 이상치로 판단하여, **Noise 관측치**라 하고 이 때도 경계선이 아니라고 판단합니다.

<br>

논문에서는 다음과 같이 분류합니다.

- K개가 모두 다수 클래스인 경우: Noise 관측치
- 다수 클래스가 K/2 이하인 경우: Safe 관측치
- 다수 클래스가 K/2보다 크고 K보다 작은 경우 Danger 관측치

어느정도 다수 관측치가 있는 경우만 경계 라인에 있다고 판단합니다.

Borderline SMOTE는 Danger 관측치에만 SMOTE를 적용해 오버 샘플링을 진행합니다.

![image](https://user-images.githubusercontent.com/76269316/179766526-bc53203b-7f7a-4e4a-9246-5a166e13e92e.png)

출처: [Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning](https://sci2s.ugr.es/keel/keel-dataset/pdfs/2005-Han-LNCS.pdf)

<br><br>

##### Adaptive Synthetic Sampling (ADASYN)

ADASYN은 Borderline SMOTE와 유사한데, 샘플링하는 위치에 따라 샘플링 개수를 다르게 하는 방식입니다.

아래 정의한 식에 따라 i번째 위치에서는 ![image](https://user-images.githubusercontent.com/76269316/179768048-bf7d9cc1-d743-49fe-87db-5e1b2178c772.png)개를 샘플링하고, j번째 위치에서는 ![image](https://user-images.githubusercontent.com/76269316/179768132-60550be6-717b-48b1-8e99-f6446d351b5a.png)개를 샘플링합니다.

![image](https://user-images.githubusercontent.com/76269316/179767836-e589d696-ef65-4db7-85b2-3907db378c5c.png)

![image](https://user-images.githubusercontent.com/76269316/179768446-aebf05d3-9d68-4053-9620-859f0114cb34.png)는 소수 클래스 ![image](https://user-images.githubusercontent.com/76269316/179768989-b622a9f9-ee78-41e0-a2e8-d350d8208e40.png)의 K개의 인접 관측치 중 다수 클래스의 개수이고, m은 소수 클래스 관측치의 총 개수로

위 식은 각 소수 클래스의 관측치 주변에 얼만큼 많은 다수 클래스 관측치가 존재하는가를 정량화 한 지표입니다.

<br>

![image](https://user-images.githubusercontent.com/76269316/179763324-3369484c-0f31-447d-ae3a-755b6b6ab80c.png)

위 그림에서의 소수 클래스 관측치를 예로 들면(K=3), 주변에 다수 클래스가 없으므로 ![image](https://user-images.githubusercontent.com/76269316/179768048-bf7d9cc1-d743-49fe-87db-5e1b2178c772.png) = 0 / 3 = 0이 되게 됩니다.

<br>

![image](https://user-images.githubusercontent.com/76269316/179764575-76d8956e-1a1d-44e9-8fe8-0d357ad46546.png)

다음 관측치에서는 다수 클래스가 1개 있으므로, ![image](https://user-images.githubusercontent.com/76269316/179768048-bf7d9cc1-d743-49fe-87db-5e1b2178c772.png) = 1 / 3 = 0.3이 되게 됩니다.

이렇게 모든 소수 클래스에 대해 ![image](https://user-images.githubusercontent.com/76269316/179768048-bf7d9cc1-d743-49fe-87db-5e1b2178c772.png)를 계산한 뒤, 다음 식으로 스케일링 해줍니다.

<img src="https://user-images.githubusercontent.com/76269316/179769797-5b70b864-eab9-45c2-a4da-4e911fd17ae0.png" alt="image" style="zoom:50%;" />

<br>

이후 스케일링 해준 값에 G(다수 클래스 개수 - 소수 클래스 개수)를 곱해준 뒤, 반올림해서 정수로 만든 뒤 해당 정수 값만큼 오버 샘플링 해줍니다.

<br><br>

### Reference

- [[핵심 머신러닝] 불균형 데이터 분석을 위한 샘플링 기법](https://www.youtube.com/watch?v=Vhwz228VrIk)

- [[인사이드 머신러닝] 불균형 데이터 샘플링](https://velog.io/@cleansky/%EC%9D%B8%EC%82%AC%EC%9D%B4%EB%93%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EB%B6%88%EA%B7%A0%ED%98%95-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%83%98%ED%94%8C%EB%A7%81)