---
title: "[대학원 면접 준비] ML Sampling 정리"
excerpt: "샘플링 정리"
toc: true
toc_label: "ML Sampling 정리"
toc_sticky: true

categories:
  - MS
tags:
  - 대학원
last_modified_at: 2022-07-18
---

### Class Imbalance

학습 데이터의 클래스가 균일하게 분포하지 않고, 하나의 클래스 값에 치우쳐 편항되게 학습되는 것을 **클래스 불균형(Class Imbalance)**이라고 합니다.

![image](https://user-images.githubusercontent.com/76269316/179516000-c87b1b27-0039-497e-b5f4-703e53c3b65f.png)

위 이미지는 캐글 [Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) 데이터셋의 클래스 분포를 시각화한 것입니다.

Not Fraud가 정상적인 신용카드 트랜잭션 데이터, Fraud가 신용카드 사기 트랜잭션을 의미합니다. 전체 데이터의 약 0.172%만이 사기 트랜잭션입니다.

사기가 발생하는 경우가 일반적이진 않기 때문에 이렇게 불균형한 분포를 띄게 됩니다.

실생활에서도 이와 같이 불균형한 클래스 분포를 갖는 데이터들이 많습니다. (암 예측, 스팸 필터링 등)

<br>

이렇게 불균형한 데이터셋으로 학습할 경우 다수 클래스로 치우치게 학습돼 편향된 모델이 만들어지게 됩니다.

그럴 경우 모델이 다수 클래스로 예측하는 경향성이 높아져 성능이 왜곡되게 됩니다.

👉  [Classification Metric](https://seominseok4834.github.io/ms/4.classification-metric-interview/) 참고

<br>

이번 포스팅에선 클래스 불균형 문제를 해결하기 위한 Undersampling / Oversampling에 대해 알아보겠습니다.



![image](https://user-images.githubusercontent.com/76269316/179519114-baee0e95-096d-44ae-b945-804f30711b04.png)

- Undersampling: 많은 데이터 세트를 적은 데이터 세트 수준으로 감소시키는 방식

  정상 레이블 데이터가 10,000건, 이상 레이블을 가진 데이터가 100건이 있으면 정상 레이블 데이터를 100건으로 줄여버리는 방식

  과도하게 정상 레이블로 학습/예측하는 부작용을 개선할 수 있지만, 너무 많은 정상 레이블 데이터를 감소시키기 때문에 정상 레이블의 경우 오히려 제대로 된 학습을 수행할 수 없다는 단점이 있어 잘 적용하지 않습니다.

- Oversampling : 이상 데이터와 같은 적은 데이터 세트를 증식하여 학습을 위한 충분한 데이터를 확보하는 방식

  동일한 데이터를 단순히 증식시키는 방법은 과적합이 되기 때문에 원본 데이터의 피처 값들을 아주 약간만 변경하여 증식

<br><br>

### Undersampling

##### Random Undersampling

다수 클래스에 있는 관측치를 무작위로 샘플링하는 방식으로, 수행할 때마다 다른 결과가 도출됩니다.

![image](https://user-images.githubusercontent.com/76269316/179521416-97a107f0-15a4-49f6-9038-23eab37f7f43.png)

출처: [Machine learning prediction of susceptibility to visceral fat associated diseases](https://www.researchgate.net/publication/343326638_Machine_learning_prediction_of_susceptibility_to_visceral_fat_associated_diseases)

<br><br>

##### Tomek Links

![image](https://user-images.githubusercontent.com/76269316/179529457-6ff26fbb-2aab-4b6e-8a34-11e1d1500173.png)

다수 클래스에서 추출한 포인트를 <img src="https://user-images.githubusercontent.com/76269316/179526117-a250b1a0-06f2-414c-b70b-9c87e2ce8050.png" alt="image" style="zoom:80%;" />, 소수 클래스에 추출한 포인트를 <img src="https://user-images.githubusercontent.com/76269316/179526328-7dc3726f-d21a-4f84-ad4b-7071147cafb0.png" alt="image" style="zoom:80%;" />라고 할 때, <img src="https://user-images.githubusercontent.com/76269316/179526117-a250b1a0-06f2-414c-b70b-9c87e2ce8050.png" alt="image" style="zoom:80%;" />와 <img src="https://user-images.githubusercontent.com/76269316/179526328-7dc3726f-d21a-4f84-ad4b-7071147cafb0.png" alt="image" style="zoom:80%;" /> 사이에 더 가까운 데이터 <img src="https://user-images.githubusercontent.com/76269316/179526562-100a44c1-a1ee-447c-95fa-594703d7b7d7.png" alt="image" style="zoom:80%;" />가 존재하지 않는다면 두 관측치는 Tomek link가 되게 됩니다.

이를 수학적으로 나타내면 다음과 같습니다.

![image](https://user-images.githubusercontent.com/76269316/179527927-d768dae1-0e67-4f65-92ce-8c14af593399.png)

<br>

![image](https://user-images.githubusercontent.com/76269316/179533429-d006826c-c0c9-46e6-b2ab-43294f0ff335.png)

이렇게 Tomek link를 형성한 두 샘플(초록색 동그라미) 중 하나는 노이즈이거나, 클래스 경계선 근처에 있으므로 다수 클래스에 속하는 샘플들을 제거합니다.

![image](https://user-images.githubusercontent.com/76269316/179533296-2f790b3c-6293-4fbc-aabb-f7c63e6da540.png)

이렇게 다수 클래스에 속하는 데이터를 제거함으로써 클래스 경계선을 다수 클래스 쪽으로 밀어붙이는 효과를 갖게 됩니다.

<br><br>

##### Condensed Nearest Neighbor

다음과 같은 데이터가 있다고 할 때,

![image](https://user-images.githubusercontent.com/76269316/179537121-7a21afea-4873-452c-bb2a-bfc1b8f7c04b.png)

<br>

![image](https://user-images.githubusercontent.com/76269316/179537204-a76c2b74-276e-4eaa-b09b-8b1a58c6146d.png)

먼저 소수 클래스를 모두 서브 데이터 집합 S (빨강색)에 포함시킵니다.

<br>

![image](https://user-images.githubusercontent.com/76269316/179537567-58782b26-26e2-4a7d-a48c-a88d4440b9c0.png)

이후 다수 클래스에서 하나의 샘플을 추출한 뒤 다수 클래스에 가까우면 집합 S에 포함시키지 않고, 아니라면 집합 S에 포함시킨다.

이 과정을 아래 그림처럼 선택되는 데이터가 없을 때까지 반복한다.

![image](https://user-images.githubusercontent.com/76269316/179537815-ac3242b1-c2a6-4115-8519-3f9e28db0ec1.png)

<br>

이후 집합 S에 속한 데이터만 남기고, 나머지는 제거한다.

![image](https://user-images.githubusercontent.com/76269316/179538190-bb0ccf29-d5c3-4d6e-9270-5dc621db2ed8.png)

CNN은 이렇게 소수 클래스에 가까운 다수 클래스를 선별하고 나머지는 모두 제거함으로써 분류 경계선을 재설정한다.

### Reference

- [[인사이드 머신러닝] 불균형 데이터 샘플링](https://velog.io/@cleansky/%EC%9D%B8%EC%82%AC%EC%9D%B4%EB%93%9C-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EB%B6%88%EA%B7%A0%ED%98%95-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%83%98%ED%94%8C%EB%A7%81)

- [비대칭 데이터 문제](https://datascienceschool.net/03%20machine%20learning/14.02%20%EB%B9%84%EB%8C%80%EC%B9%AD%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EB%AC%B8%EC%A0%9C.html#id1)