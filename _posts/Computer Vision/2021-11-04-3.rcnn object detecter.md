---
title:  "3.RCNN Object Detecter"
excerpt: "RCNN 계열 Object Detecter"
toc: true
toc_label: "RCNN Object Detecter"
toc_sticky: true
published: true

categories:
  - Computer Vision
tags:
  - 컴퓨터 비전
last_modified_at: 2021-11-06
---

### RCNN의 이해 01 - Region Proposal 기반의 Object Detection 모델

Object Localization/Detection과 Image Classification의 가장 큰 차이는 이미지에서 특정 object의 위치를 찾는 것입니다.

<img src="https://user-images.githubusercontent.com/76269316/140601172-28474f3b-5988-4c36-99f4-e140189312af.png" alt="image" style="zoom: 67%;" />

빨간색 부분은 image classification과 동일한 architecture이고, object가 위치한 좌표(bounding box)를 찾기 위한 regression 부분(파란색 부분)이 추가됩니다.

→ classification과 regression 두 개의 모델을 갖고 있는 것이 object localization/detection

<br>

Localization과 다르게 Detection은 한 이미지 안에 두 개 이상의 object들이 존재하기 때문에 단순히 위에 있는 네트워크로 적용하기에는 여러 [문제점](https://seominseok4834.github.io/computer%20vision/1.understanding-object-detection/#object-localization-%EC%98%88%EC%B8%A1-%EA%B2%B0%EA%B3%BC)들이 있었음

<br>

Object Detection에서는 먼저 object가 있을만한 위치를 예측해야하는데, Region Proposal이라는 알고리즘을 통해  object가 있을만한 위치를 찾습니다.

→ 초기 Region Proposal의 대표적인 방식이 [selective search](https://seominseok4834.github.io/computer%20vision/1.understanding-object-detection/#region-proposal%EC%98%81%EC%97%AD-%EC%B6%94%EC%A0%95%EC%9D%98---selective-search-%EA%B8%B0%EB%B2%95)

▶ Region Proposal 방식에 기반한 Object Detection이 RCNN

<br>

##### Region Proposal 방식에 기반한 Object Detection - RCNN

<img src="https://user-images.githubusercontent.com/76269316/140601630-f54f2290-6172-4f4b-842b-c9fe8ec620db.png" alt="image" style="zoom:80%;" />

1. Selective Search를 통해 2,000개의 object가 있을만한 위치(영역)를 제안
2. 2,000개의 영역에서 제안된 이미지의 크기를 맞춤
3. 2,000개의 이미지를 네트워크에 집어 넣어 학습을 시킴

<br>

*이미지의 크기를 동일하게 맞춰줘야 하는 이유?*

1D Flattened Fully Connected Input의 사이즈가 명시돼 있기 때문에

<br>

원래 딥러닝에서는 FC Layer 이후 soft max를 통해 classification 하는데 RCNN에서는 SVM classifier를 사용합니다.

→ 딥러닝 모델은 FC Layer에서 끝이고 딥러닝 모델에서 나오는 최종 FC Layer를 기반으로 다시 SVM Classifier를 적용하겠다. (Feature Extractor level에서만 CNN을 활용)

SVM을 적용한 이유는? 그냥 해봤더니 성능이 좋아서

<br>

##### RCNN 개요

<img src="https://user-images.githubusercontent.com/76269316/140601728-94a0ee24-8e2b-4688-a66b-0a3d96706e51.png" alt="image" style="zoom:80%;" />

1. Input Image를 받아서 region proposal 영역으로 쪼갬
2. CNN의 입력으로 사용하기 위해 입력 크기를 맞춤 (원본 이미지의 비율과 안 맞음 -  warped region)
3. classification, regression 진행

<br>

하나의 이미지에서 뽑아낸 2,000개의 object에 대해서 각각 적용해야하기 때문에 학습 시간이 오래걸림

<br><br>

### RCNN의 이해 02 - RCNN Training과 Loss

##### RCNN Training - Classification

<img src="https://user-images.githubusercontent.com/76269316/140601962-979a4ed2-0f54-4c98-8e5e-fb031ee7d1c6.png" alt="image" style="zoom:80%;" />

Pascal VOC 데이터 세트에 대해서 image detection 한다고 할 때, 먼저 ImageNet 데이터 세트로 Feature Extractor<img src="https://user-images.githubusercontent.com/76269316/140602225-64cfb848-799a-4177-951b-904c3aef6047.png" alt="image" style="zoom: 25%;" />까지 미리 pre-train 시켜놓습니다.

그런 다음 pre-trained된 모델 위에다가 fine-tuning(사전학습된 모델을 활용하여 새로운 모델을 학습하는 과정)을 진행하는데,

우리가 예측한 영역(selective serach predicted)과 실제 영역(ground truth) 중에 겹치는 IOU가 0.5 이상인 경우 해당 클래스로 다시 학습시키고, IOU가 0.5 미만인 경우 background로 학습을 시킵니다.

그러면 피처(1차원 피처 맵)가 생성되게 되고 이를 SVM으로 학습합니다.

ground truth 데이터를 기반으로 클래스 결정 (IOU가 0.3 이하인 경우 background로 설정, 0.3 이상이지만 GT가 아닌 경우 빼버림 - 조금 더 여유를 가짐)

<br>

##### Bounding Box Regression

<img src="https://user-images.githubusercontent.com/76269316/140602413-b5c6b7fd-f752-443c-8b2b-28dd0e56f7b1.png" alt="image" style="zoom:80%;" />

selective search로 proposal 된 영역의 중심점(Px, Py)과 ground truth의 중심점의 거리와 width 차이가 최소가 돼야 함 

<br>

##### R-CNN 성능 비교

<img src="https://user-images.githubusercontent.com/76269316/140602725-e1a87e9d-fc18-494e-bd3f-92650572f3c8.png" alt="image" style="zoom:80%;" />

기존의 딥러닝을 쓰지 않은 알고리즘 대비 높은 성능이 나옴

<br>

##### R-CNN 장단점

성능적인 측면에선 굉장한 장점이 있었지만 많이 복잡하고 느림

동시대의 다른 알고리즘 대비 매우 높은 detection 정확도 하지만 너무 느린 detection 시간과 복잡한 아키텍처 및 학습 프로세스

→ 1장의 이미지를 object detection 하는데 약 50초 소요

<br>

- 하나의 이미지마다 selective search를 수행하여 2,000개의 region 영역 이미지 도출
- 개별 이미지별로 2,000개씩 생성된 region 이미지를 CNN Feature map 생성
- 각각 따로 노는 구성 요소들 (Selective Search, CNN Feature Extractor, SVM, Bounding box regressor로 구성된 복잡한 프로세스를 거쳐 학습 및 object detection 돼야 함)

<br>

##### R-CNN 이후 Object Detection 연구 방향성

Deep Learning과 Region Proposal 기반 Object Detection 성능을 입증했기 때문에 detection 수행 시간을 줄이고 복잡하게 분리된 개별 아키텍처를 통합할 수 있는 방안 연구

<br><br>

### SPPNet의 이해 01 - RCNN 문제점과 Spatial Pyramid Matching 이해

##### R-CNN의 주요 문제점

- 2,000개의 region 영역 이미지가 CNN으로 입력되면서 object detection 수행 시간이 오래 걸림

- region 영역의 이미지가 crop/warp 되어야 함

<br>

##### R-CNN 개선 방안

<img src="https://user-images.githubusercontent.com/76269316/140604261-9eeb8671-e7a7-49d5-b28d-a48acb5931c1.png" alt="image" style="zoom:80%;" />

원본 이미지에서 selective search로 추출

추출한 object가 아닌 원본 이미지를 VGG에 넣어서 feature map 생성한 다음 selective search에서 찾은 추천된 영역을 feature map에 mapping 하면되지 않을까하고 생각했는데 **R-CNN이 등장했던 2013년 메커니즘에서는 이러한 방식을 사용할 수 없었습니다.**

feature map(3차원)에 있는 걸 학습시켜서 classification layer로 가려면 1D Flattened FC Input(Dense Layer) 를 통과해1차원 형태가 돼야 하는데 사이즈가 명확하게 정해져 있어 각각 크기가 다른 추천된 영역이 dense layer를 통과할 수 없었기 때문입니다.

<img src="https://user-images.githubusercontent.com/76269316/140604426-db49b1ab-55ea-4c2a-83ad-ec60db90b9de.png" alt="image" style="zoom:50%;" />

<br>

##### 서로 다른 크기의 Region Proposal 이미지 개선 방안

<img src="https://user-images.githubusercontent.com/76269316/140604576-41f58224-cab2-4b43-a4ae-db70cf908f59.png" alt="image" style="zoom:80%;" />

Feature map으로 투영된 서로 다른 크기를 가진 region proposal 이미지를 SPP Net의 고정된 크기 벡터로 변환하여 FC에 1D Flattened 된 ipnut 제공

<br>

##### SPP(Spatial pyramid Pooling)

SPP는 CNN Image Classification에서 서로 다른 이미지의 크기를 고정된 크기로 변환하는 기법으로 사용됐는데, 이를 object detection에 적용한 것입니다.

<img src="https://user-images.githubusercontent.com/76269316/140604634-b0cb5e37-f319-495d-8cb5-24662145656c.png" alt="image" style="zoom:80%;" />

기존에는 원본 이미지를 네트워크에 집어넣기 전에 고정된 사이즈가 나와야하므로 이미지를 crop/warp 해야 했고 따라서 이미지의 손실이 불가피 했는데 SPP가 등장함에 따라서 feature map(conv layers)과 dense layer(fc layers)를 유연하게 연결할 수 있게 됐습니다.

<br>

##### Bag of Visual words → Spatial Pyramid Matching

**Bag of Words**

[NLP](https://seominseok4834.github.io/machine%20learning/8.text-analytics/#bag-of-words---bow)(National Language Processing - 자연어 처리)에서 주로 사용하는 용어로 신문기사에 있는 단어들을 추출해 모은 다음 단어의 출현 빈도 수에 따라 어떤 기사인지 파악하는 방식을 의미

<img src="https://user-images.githubusercontent.com/76269316/140604785-268eb579-cc88-4f27-afcb-b2a2d8be89e7.png" alt="image" style="zoom:80%;" />

Bag of Visual words도 마찬가지로 이미지를 분류할 때 특징들을 쪼개서 모은 다음 어떤 이미지인지 분류하는 방식을 의미합니다.

→ 원본이 갖고 있는 정보를 새로운  mapping 정보(histogram)로 변환 (비정형적인 데이터를 정형화된 피처 속성으로 만드는 것)

▶ 하지만 Bag of Words 방식은 문맥이 없기 때문에 정확성이 떨어집니다. (Bag of Visual Words도 마찬가지)

<br>

**Spatial Pyramid Matching**

Bag of Words 방식의 경우 문맥 정보가 없기 때문에 모래가 조금 나오거나 하늘이 조금 나오면 해변으로 분류할 수 없습니다.

이를 해결하기 위해 이미지의 위치 정보들을 어느정도 유지하면서 다른 피처 레벨의 정보를 추출해 그걸 기반으로 분류하는 기법 SPM(Spatial Pyramid Matching)이 등장했습니다.

<img src="https://user-images.githubusercontent.com/76269316/140604980-ada596fe-c6dd-4395-a38e-86f047522d09.png" alt="image" style="zoom:80%;" />

<br>

**Spatial Pyramid Matching (SPM) 개요**

<img src="https://user-images.githubusercontent.com/76269316/140605043-b96553fc-2889-408b-9a5a-1d8b01b1fee1.png" alt="image" style="zoom:80%;" />

 피처들이 해당 위치에 어떻게 histogram을 갖고 있는지를 기반으로 새로운 피처 값들이 생기게되고 이를 가지고 분류 (위치 정보를 감안하기 위해 분면으로 쪼갬)

- level 0: 위치적인 정보를 감안하지 않음
- level 1: 4분면으로 쪼갰을 때의 histogram
- level 2: 16분면으로 쪼갰을 때의 histogram

<br><br>

### SPPNet의 이해 02 - Spatial Pyramid Pooling을 활용한  Object Detection

##### SPM으로 서로 다른 크기의 Feature Map을 균일한 크기의 vector로 표현

<img src="https://user-images.githubusercontent.com/76269316/140605248-c3cfb372-ebf8-47ea-9300-8268901f1df6.png" alt="image" style="zoom:80%;" />

피처가 3개가 있을 경우 level 0에서는 3개의 histogram이 만들어질 수 있고, level 1에서는 12개, level 2에서는 16개의 histogram이 만들어질 수 있습니다.

→ feature map이 8x8이 아니더라도 4분면으로 나눠지기만 하면 똑같이 63개의 벡터로 표현할 수 있음 (분면을 나누는 것은 CNN의 pooling 개념과 거의 유사합니다.)

<br>

##### Spatial Pyramid Pooling

<img src="https://user-images.githubusercontent.com/76269316/140605398-ca30ceeb-e555-48e7-aad9-e2dc6807b815.png" alt="image" style="zoom:80%;" />

max pooling(가장 큰 값 하나만 뽑음)을 적용했기 때문에 21개의 벡터 값으로 표현됨

<br>

##### SPP-Net (Image Classification)

<img src="https://user-images.githubusercontent.com/76269316/140605505-f2cd3afd-42cc-47f1-b92e-640b084e3daf.png" alt="image" style="zoom:80%;" />

나누는 분면에 따라서 벡터 개수가 정해지기 때문에 Input Image의 사이즈가 변하더라도 표현되는 벡터의 개수는 변하지 않습니다.

<br>

*원본 feature map이 있는데 이렇게까지 가공해도 되는가?*

feature extractor level에서도 pooling을 적용하는데 여기서 다시 pooling을 하면 손실이 있지 않을까 생각할 수 있는데, 원본 이미지에 crop/warp를 적용해 이미지 본연의 모습이 달라지는 것보다 좋은 성능 효과를 거두었기 때문에 이 방식을 채용했다고 합니다.

<br>

##### SPP-Net (Obejct Detection)

<img src="https://user-images.githubusercontent.com/76269316/140605665-22d01342-457c-4d15-a896-2c0779c963f2.png" alt="image" style="zoom:80%;" />

Selective Search에서 가져온 영역에 SPP Layer를 적용하면 분면을 쪼개서 가져가기 때문에 fixed-length는 변하지 않으니까 그대로 Dense Layer(FC Layer)에 적용할 수 있지 않을까? 하고 SPP-Net을 R-CNN에 적용

<br>

##### SPP-Net을 R-CNN에 적용

<img src="https://user-images.githubusercontent.com/76269316/140605723-fa1c52fa-f087-4866-a63d-f659bae283fb.png" alt="image" style="zoom:80%;" />

이전에는 object가 있을만한 위치에 있는 것들 2,000개를 227x227로 변환해 학습을 진행시켰던 반면에, 이제는 원본 이미지만 학습을하고 feature map에 selective search가 mapping을 하면 mapping 된 feature map을 SPP Layer에서 fixed length 벡터로 변환해 학습을 진행합니다.

<img src="https://user-images.githubusercontent.com/76269316/140605820-776d25a6-295f-4169-aa95-bb4032dfe293.png" alt="image" style="zoom:80%;" />

<img src="https://user-images.githubusercontent.com/76269316/140633965-848bfcf8-7cd1-4791-8875-e787421a73e8.png" alt="image" style="zoom:80%;" />

RCNN과 거의 동일한 mAP 성능을 가졌는데도 64배 이상의 속도 향상을 보여줍니다.

<br>
<br>

### Fast RCNN의 이해

##### RCNN의 발전

<img src="https://user-images.githubusercontent.com/76269316/140605960-8c4a8306-b76c-46a2-8b42-f89f97a92cb2.png" alt="image" style="zoom:80%;" />

R-CNN의 경우 2,000개의 object에 대해서 각각 학습을 진행했기 때문에 속도가 굉장히 느렸습니다.

Fast R-CNN은 SPP-Net과 비슷하게 원본 이미지만 학습을 하고, 학습을 통해 만들어진 피처들에는 ROI pooling을 적용했습니다.

또한 R-CNN의 경우 딥러닝 로직에서 classifcation이 수행되는게 아닌 별도의 SVM을 적용했어야 했는데, Fast R-CNN에서는 바로 soft max를 적용했습니다.

+Fast R-CNN까지 여전히 selective search가 딥러닝 네트워크 밖에 존재했는데, Faster R-CNN에서야 딥러닝 네트워크 안으로 가지고 왔습니다.(RPN)

<br>

##### Fast R-CNN 주요 특징

<img src="https://user-images.githubusercontent.com/76269316/140606069-c58e65ea-c3d0-4e88-9b79-fe0ad72b5d21.png" alt="image"  />

- SPP Layer를 ROI Pooling Layer로 변경
- End-to-End Network Learning
  - 딥러닝으로 들어오지 않았던 별도의 머신러닝 알고리즘(SVM)을 softmax로 변환해서 딥러닝 안으로 들여옴
  - Multi-task loss 함수로 classification과 regression을 함께 최적화 (softmax가 안으로 들어왔으니까 total loss를 만들 수 있음)
    ![image](https://user-images.githubusercontent.com/76269316/140606151-6f26d77e-1dda-4f8e-935e-33eca8766b8a.png)

<br>

##### ROI Pooling

<img src="https://user-images.githubusercontent.com/76269316/140606217-f70ebbec-3067-4afd-9aec-4a816b73b92f.png" alt="image" style="zoom:80%;" />

14x14 feature map에 selective search를 통해 두 개의 영역이 뽑혔을 때(빨간색 네모), 2x2로 pooling 한다고 하면 4등분 해서 max pooling을 적용할 수 있습니다.

<br>

##### Fast R-CNN에서 ROI Pooling

<img src="https://user-images.githubusercontent.com/76269316/140606326-472a4950-d59a-4124-a2a3-3ebde1ed1ba0.png" alt="image" style="zoom:80%;" />

14x7의 경우 세로 2칸, 가로 1칸으로 ROI pooling을 적용할 수 있는데, 8x4처럼 정수형으로 나누어 떨어지지 않는 경우에는 보간법을 적용합니다.

→ 세로의 경우 마지막 두 칸을 하나로 채우고, 가로의 경우 4칸을 7칸으로 늘려서 계산하거나, 이미지 resize 하듯이 8x4를 7x7로 mapping을 시켜버림

<br>

##### ROI Pooling을 적용한 Fast R-CNN 구조

<img src="https://user-images.githubusercontent.com/76269316/140634162-fe106fcf-99bf-4035-93b9-ccc0533ddc7a.png" alt="image" style="zoom:80%;" />

원본 이미지에 selective seach 적용해서 추출한 영역을 feature map에 mapping한 다음 ROI pooling 적용합니다.

ROI Pooling 적용 후 depth 개수는 Feature Map depth 개수와 동일 (SPP Net은 채널까지 포함해서 1차원으로 만들지만 ROI Pooling은 size(7x7)만 고정시키고 채널 depth는 똑같이 가져감)

batch size가 4 (이미지 4개)이고, feature map depth가 256인 경우 ROI Pooling을 통과하면 (4, 2000, 7, 7, 256)이 됨 *(2,000은 selective search된 object)*

<br>

<img src="https://user-images.githubusercontent.com/76269316/140634226-79b07268-069d-4eb4-81ce-8f40ae22345b.png" alt="image" style="zoom:80%;" />

→ classification과 regression loss를 함께 반영해서 loss 함수를 만들 수 있게됨

→ backpropagation이 전반적으로 가능하게됨

 <br>

##### Multi-task loss

<img src="https://user-images.githubusercontent.com/76269316/140634288-f2e51b07-dcf4-4cbf-928b-ed0d1a6f1a5a.png" alt="image" style="zoom:80%;" />

L1 loss: 실제값과 예측값의 차이에 절댓값을 취한 것

L2 loss: 실제값과 예측값의 차이에 제곱을 취한 것

smooth L1 loss: 실제값과 예측값의 차이가 1보다 작으면 0.5x^2을 시키고 1보다 크거나 같으면 x - 0.5를 함 (1보다 작을 때는 급격하게 loss를 줄여줌)

 <br>

##### Fast RCNN 성능 비교

<img src="https://user-images.githubusercontent.com/76269316/140634354-a5f58758-66cd-466f-9c45-0cc4c7c779f7.png" alt="image" style="zoom:80%;" />

<br><br>

### Faster RCNN의 이해 01 - Anchor Box

##### Faster RCNN 개요

Object Detection을 구성하는 모든 요소들을 딥러닝만으로 구현한 첫 번째 Object Detection 알고리즘이 **Faster RCNN**입니다.

Fast RCNN에서는 Selective Search를 통해 RoI(Region of Interest) Proposal 하는 부분이 딥러닝 영역 안으로 들어오지 못했는데 이를 Faster RCNN에서 RPN으로 구성했습니다.

Faster RCNN = RPN + Fast RCNN

<img src="https://user-images.githubusercontent.com/76269316/140634544-a6007d46-74e2-4f4a-9001-5d19657ba027.png" alt="image" style="zoom:80%;" />

<br>

##### Faster RCNN 구조

Faster RCNN은 Convolutional network를 통과해 생성된 feature map이 RPN(Region Proposal Network)로 가서 추출된 영역에 RoI Pooling 적용한 이후는 Fast RCNN과 동일하게 classification과 regression 진행합니다.

<img src="https://user-images.githubusercontent.com/76269316/140634700-cd6e1dc2-604f-4cb4-ab7c-f851cc17cbb5.png" alt="image" style="zoom:80%;" />

<br>

##### Region Proposal Network 구현 이슈

Selective Search는 이미지가 들어가면 무늬, 색깔, 경계선, 픽셀의 강도를 갖고 object가 있을만한 2,000개의 위치를 뽑아내는데,

Selective Search를 대체하기 위한 Region Proposal Network를 어떻게 구현할 지에 대한 고민이였습니다.

*데이터로 주어지는 피처 (pixel 값)와 target (ground truth bounding box)만 가지고는 불가능한데 어떡하지?* 

→ Anchor Box 도입

<img src="https://user-images.githubusercontent.com/76269316/140634773-6838889c-91a2-41b0-b74f-a87abd806eb7.png" alt="image" style="zoom:80%;" />

위 사진처럼 이미지가 들어오면 박스들을 포인트별로 촘촘하게 채움

처음 anchor box에는 그냥 값만 있는데 데이터를 가지고 ground truth 기반으로 학습을 시킬 수 있음 (딥러닝은 데이터가 들어오면 데이터를 가지고 학습을 하면서 최적의 값을 찾는거니까!)

→ 학습을 진행할 수록 이 anchor box가 도드라지게 되는데, 이 anchor box를 기반으로 위치를 찾아보자는 것이  RPN입니다.

<br>

##### Anchor Box 구성

Faster RCNN에서는 사이즈, 비율이 다른 9개의 anchor box로 구성합니다.

why? object 형태가 다르기 때문에

<img src="https://user-images.githubusercontent.com/76269316/140634927-3ef754aa-1883-4895-9bc3-ef99dfea0846.png" alt="image"  />

<br>

<img src="https://user-images.githubusercontent.com/76269316/140634901-cfe40c09-e6c3-4af2-9dfd-6b5ebe573e73.png" alt="image" style="zoom:80%;" />

쉽게 설명하기 위해 이미지 크기로 anchor box 크기를 나타냈는데(128x128, 256x256, 512x512), 실제 anchor box는 이미지에 mapping 시키는 것이 아닌 feature map에 mapping 시키는 것이기 때문에 그림의 사이즈보다 1/16로 줄어듧니다.

<br>

##### Anchor Box 특징

![image](https://user-images.githubusercontent.com/76269316/140635120-1aec0e48-b9c9-4162-b6b5-bb29ce297486.png)

anchor box 안에 object가 얼마나 들어와있느냐(IOU)에 따라 평가하기 때문에 만약 장사각형 anchor box만 사용했다면 사람하고 겹치는 anchor box를 찾을 수 없습니다. (사람은 직사각형인데, anchor box는 정사각형이므로)

<br>

또한 하나의 anchor box만 있으면 자동차도 있고 사람도 있으면 사람도 찾아야 하는데 못 찾기 때문에  중심점을 기준으로 다른 크기, 비율을 갖는 anchor box를 촘촘히 가져야합니다.

<br>

##### Feature Map에서 anchor box 매핑

이해를 위해서 이미지에다가 anchor box를 mapping 시켰지만 **실제로는 feature map에 mapping!!**

<img src="https://user-images.githubusercontent.com/76269316/140635141-aa106c8e-ebe6-4eb3-be39-ef61322aeb0c.png" alt="image" style="zoom:80%;" />

<br>

 <img src="https://user-images.githubusercontent.com/76269316/140635173-50d8b334-fd20-4caf-907e-4cc7fd260922.png" alt="image" style="zoom:80%;" />

원본 이미지가 vgg net을 통과하면 1/16 크기로 down sampling 됨 (Hegiht 800 → 50, Width 600 → 38)

50x38개의 gridpoint에 anchor box가 생김

<br>

<img src="https://user-images.githubusercontent.com/76269316/141683306-8f9f8312-40d7-4e01-b142-d01c8d761885.png" alt="image" style="zoom:80%;" />

50 * 38 * 9 = 17,100개의 anchor box 생성

<img src="https://user-images.githubusercontent.com/76269316/141683385-b5118df4-8935-4484-817d-a4b97294e46b.png" alt="image" style="zoom:80%;" />

<br><br>

### Faster RCNN의 이해 02 - Anchor Box를 활용한  RPN 구성

##### RPN 개요

<img src="https://user-images.githubusercontent.com/76269316/140635379-4eb7e40e-55df-4be3-a6fd-ec5bc3271d92.png" alt="image" style="zoom:80%;" />

<br>

##### RPN 네트워크 구성

50x40x512

<img src="https://user-images.githubusercontent.com/76269316/141683795-b2f1bb53-9c45-4a97-978a-61797b961236.png" alt="image" style="zoom:80%;" />

feature map에 3x3 conv을 적용(padding=1로 줘서 size를 유지)한 다음, 

1x1 conv을 적용해서 foreground(obj)인지 background(no obj)인지 이진 분류 적용 (Sigmod Classification)하고 또 1x1 conv을 적용해서 bounding box regression 진행

- Sigmoid Classification

*1x1 convolution은 feature map의 파라미터를 대폭 줄이는데, 이 때  모델 성능에 최소한으로 영향을 끼치며 차원을 축소하는 기능을 함*

*1x1을 곱했기 때문에 size는 변하지 않지만 채널 수를 9로 변환 (anchor box 수) → 40x50x9개의 anchor box가 object인지 아닌지 분류*

- Bounding box Regression

*bounding box regression에서는 채널 수를 4(x1, y1, w, h)x9로 변환 → 40x50x4x9개만큼의 정보를 갖게됨*

```python
def rpn(base_layers, num_anchors):  #base_layers: feature map
    x = Convolution2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)
    
    #40x50x9 = 18,000개의 anchor box가 object인지 아닌지 판단
    x_class = Convolution2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)
    #4 - (x1, y1), w, h 계산
    x_regr = COnvolution2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)
    
    return [x_class, x_regr, base_layers]
```

<br>

##### RPN Bounding Box Regression

<img src="https://user-images.githubusercontent.com/76269316/141683976-f27b3805-cdaa-4649-bba7-a9cc86b771b9.png" alt="image" style="zoom:80%;" />

Anchor Box와 Ground Truth의 차이와 Predicted와 Ground Truth와 최대한 동일하게 예측될 수 있어야 한다는데

이 부분 이해가 안되네요,,

<br><br>

### Faster RCNN의 이해 03 - RPN과 Faster RCNN Training 및 성능 비교

##### Positive Anchor Box, Negative Anchor Box

<img src="https://user-images.githubusercontent.com/76269316/141780301-79bb1122-f317-4c3e-a955-6cf62c00a70e.png" alt="image" style="zoom:80%;" />

Ground Truth와 Anchor Box가 얼마나 겹치는가에 따라서 positive와 negative로 나뉘는데 가장 높은 IOU, IOU가 0.7 이상인 anchor box는 positive이고 0.3보다 낮으면 negative로 분류합니다. (negative까지 학습에 포함시키는 이유는 object가 아닌 것들을 확실한게 분류하기 위해서)

<br>

*0.7에서 0.3 사이에 있는 것들은 어떻게 할까?*

학습에 포함시키지 않고 filtering out 시켜버립니다.

<br>

##### Anchor Box를 reference로 한 Bounding Box Regression

<img src="https://user-images.githubusercontent.com/76269316/141780898-4de2603b-5abd-4c73-89f1-e32a85fb3e81.png" alt="image" style="zoom:80%;" />

예측한 bounding box와 positive anchor box와의 차이가 ground truth와 positive anchor box와의 차이가 동일하게 된다면 예측한 bounding box는 ground truth와 유사하게 되므로, 동일하게 될 수 있도록 regression을 학습시켜야 합니다.

+Ground Truth와 Positive anchor box의 차이는 둘 다 고정된 위치에 있기 때문에 고정된 값임

<br>

##### Anchor Box에 따른 RPN Output

<img src="https://user-images.githubusercontent.com/76269316/141781984-229754b6-f958-4c10-b273-6f9ccd85db35.png" alt="image" style="zoom:80%;" />

k = anchor box의 개수인데 2k scores는 classification, 4k는 regression을 나타내는데,

[위에서](https://seominseok4834.github.io/computer%20vision/3.rcnn-object-detecter/#rpn-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EA%B5%AC%EC%84%B1) sigmoid classification이라고 했는데, sigmoid의 경우 anchor 개수인 k개만 하면되는데 논문에서는 2k로 나타냈기 때문에 soft max를 사용한 것 같습니다.

<br>

##### RPN Loss 함수

<img src="https://user-images.githubusercontent.com/76269316/141783147-96e0d8fa-3986-45c7-8519-ba93a7b5efce.png" alt="image" style="zoom: 50%;" />

i는 anchor box의 개수를 나타냅니다.

위 식은 total loss이기 때문에 classification 부분과 regression 부분의 loss가 둘 다 작아져야 하는데 classification에는 positive와 negative 둘 다 들어가는데 이미지에는 negative가 더 많기 때문에 Mini Batch를 사용해서 balance를 맞춰주는 과정이 들어감

<img src="https://user-images.githubusercontent.com/76269316/141783585-ba74c29c-5393-42f5-bd03-946647947cf3.png" alt="image" style="zoom:80%;" />

<br>

##### Faster RCNN Training

![image](https://user-images.githubusercontent.com/76269316/141783734-e2ba6cd0-5f95-4968-baa7-3318cf42afba.png)

1. RPN을 먼저 학습
2. Fast RCNN Classification/Regression 학습
3. RPN Fine Tuning
4. Fast RCNN Fine Tuning

<br>

##### Faster RCNN Detection 성능 비교

![image](https://user-images.githubusercontent.com/76269316/141784730-dc59af7d-7bae-44ab-84bb-7a3f06e3f574.png)

<br>

##### Faster RCNN 수행 시간 비교

![image](https://user-images.githubusercontent.com/76269316/141784785-07baa754-e4ac-4ff5-80c7-44a6d28b589b.png)

<br><br>

### OpenCV의 DNN을 이용한 Object Detection 구현 개요

##### OpenCV DNN 장단점

**장점**

- 딥러닝 개발 프레임워크 없이 쉽게 inference 구현 가능
- OpenCV에서 지원하는 다양한 Computer Vision 처리 API와 Deep Learning을 쉽게 결합

<br>

**단점**

- GPU 지원 기능이 약함
- DDN 모듈은 과거에 NVDIA GPU 지원 안됨
- OpenCV는 모델을 학습할 수 있는 방법을 제공하지 않으며 오직 inference만 가능
- CPU 기반에서 inference 속도가 개선됐으나, GPU가 지원되지 않아 타 deep learning framework 대비 inference 속도가 크게 저하됨

<br>

##### 기존 Deep Learning Frame과의 연동

환경 파일은 타 프레임워크 모델 파일의 환경(config) 파일을 DNN 패키지에 맞게 다시 변환해야함

<img src="https://user-images.githubusercontent.com/76269316/141789501-e997eab0-3962-4593-ad94-12067b6b8f3e.png" alt="image" style="zoom:80%;" />

<br>

##### OpenCV 지원 Tensorflow 모델

https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API

<img src="https://user-images.githubusercontent.com/76269316/141789937-a1796cf0-fbee-476d-8d8a-cd5c161b27e5.png" alt="image" style="zoom:80%;" />

<br>

##### OpenCV DNN을 이용한 Inference 수행 절차

1.가중치 모델 파일과 환경 설정 파일을 로드하여 inference 네트워크 모델 생성, Input 이미지를 image array로 변환

```python
cvNet = cv2.dnn.readNetFromTensorflow('frozen_inference_graph.pb', 'graph.pbtxt')

img = cv2.imread('img.jpg')
rows, cols, channels = img.shape
```



2.입력 이미지를 preprocessing하여 위에서 생성한 네트워크에 입력

```python
cvNet.setInput(cv2.dnn.blobFromImage(img, size=(300, 300), swapRB=True, crop=False))
```



3.Inference 네트워크에서 output 추출 (detect된 정보들이 networkOutput으로 출력됨)

```python
networkOutput = cvNet.forward()
```



4.추출된 output에서 detect 정보를 기반으로 원본 이미지 위에 object detection 시각화

```python
for detection in networkOutput[0, 0]:
    #object detected된 결과, bounding box 좌표, 예측 레이블들을 원본 image 위에 시각화
```

<br>

##### OpenCV blobFromImage()

![image](https://user-images.githubusercontent.com/76269316/141791378-48516cb2-dfb0-48e1-827e-ac52a54be4ae.png)

Image에 preprocessing을 수행하여 네트워크에 입력할 수 있게 제공

1. 이미지 사이즈 고정
2. 이미지 값 스케일링
3. BGR을 RGB로 변경, 이미지를 cropt 할 수 있는 옵션 제공

<br>

<img src="https://user-images.githubusercontent.com/76269316/141791481-f9a4f7ce-d557-4c77-864d-ed3fec20ab06.png" alt="image" style="zoom:80%;" />

OpenCV는 RGB 이미지를 BGR 형태로 저장하므로 원본 이미지 그대로 저장하려면 아래처럼 cvtColor()를 이용해서 RGB로 변환하거나,

```python
Img_bgr = cv2.imread('원본 image')
Img_rgb = cv2.cvtColor(img_bgr, cv2.Color_BGR2RGB)
```



blobFromImage()의 swapRB=True를 사용해서 변환할 수 있습니다.

```python
Img_bgr = cv2.imread('원본 image')
cvNet.setInput(cv2.dnn.blobFromImage(img_bgr, size=(300, 300), swapRB=True, crop=False))
```

<br>

##### OpenCV  Video Stream Caputre를 이용한 Video Object Detection

OpenCV의 VideoCapture() APi를 이용해 Video Stream을 frame by frame 별로 capture한 image에 object detection을 수행하는 방식

<img src="https://user-images.githubusercontent.com/76269316/141792119-139be5c5-4ab3-4eb2-ae7b-a377d18de099.png" alt="image" style="zoom:80%;" />

<br><br>

##### OpenCV DNN으로 Faster RCNN Object Detection 실습 01

##### MS-COCO Dataset 오브젝트 카테고리

![image](https://user-images.githubusercontent.com/76269316/141794798-ce56b43f-f206-4932-b002-338c6a439b2d.png)

91개의 클래스 id가 있지만 이 중 11개가 없어서 실제로는 80개의 object 카테고리만 존재

<br>

##### OpenCV와 Tensorflow의 coco 클래스 id와 name mapping

OpenCV를 Tensorflow를 import할 때 coco 클래스 mapping이 알고리즘별로 조금씩 다름

![image](https://user-images.githubusercontent.com/76269316/141795104-ccb3a6d6-a65f-46a0-ba96-c5a80c0abb22.png)

![image](https://user-images.githubusercontent.com/76269316/141795328-09168523-c2fc-47fa-b584-6b5bafffd414.png)

<br><br>

### Modern Object Detection 모델 아키텍처

![image](https://user-images.githubusercontent.com/76269316/142344245-32329b9a-9a75-4cf8-9e71-86df0f9e69bd.png)

Backbone은 원본 이미지를 받아 다양한 정보가 추상화된 feature map을 생성하는 역할을 합니다.

Backbone에서는 feature map이 원본 이미지 대비 크기는 점점 작아지지면서 두께(depth)는 계속 두꺼워집니다.

Neck은 backbone과 head를 연결하면서 head가 feature map의 특성을 보다 잘 해석하고 처리할 수 있도록 정제작업을 수행합니다.

이전까지는 가장 마지막에 있는 feature map만 가지고 object detection을 수행했는데, neck이 등장하면서 다양한 사이즈의 feature map을 활용할 수 있게 되면서 작은 object들을 보다 잘 detect 할 수 있게 됐습니다.

Head에선느 feature map에서 object의 위치와 classification을 처리합니다.

<br><br>

##### Feature Pyramid Network

![image](https://user-images.githubusercontent.com/76269316/142345052-19a21184-663d-44e5-8cfe-8b5834ee7581.png)

작은 object들을 보다 잘 detect하기 위해 다양한 feature map을 활용하기 위해 Feature Pyramid Network가 등장했습니다.

FPN은 상위 feature map의 추상화된 정보와 하위 feature map의 정보를 결합하기 위해 feature map을 2배로 subsampling한 다음 Backbone의 바로 밑에 있는 동일한 사이즈의 feature map과 더합니다.

![image](https://user-images.githubusercontent.com/76269316/142346179-730299db-17dc-47f5-9355-c8e0610e9361.png)

<br><br>

##### MMDetection 패키지의 모델 아키텍처

<img src="https://user-images.githubusercontent.com/76269316/142346309-77aa2062-4fa9-44da-881d-ecdcf646db70.png" alt="image" style="zoom:80%;" />
