---
title:  "3.RCNN Object Detecter"
excerpt: "RCNN 계열 Object Detecter"
toc: true
toc_label: "RCNN Object Detecter"
toc_sticky: true
published: true

categories:
  - Computer Vision
tags:
  - 컴퓨터 비전
last_modified_at: 2021-11-06
---

### RCNN의 이해 01 - Region Proposal 기반의 Object Detection 모델

Object Localization/Detection과 Image Classification의 가장 큰 차이는 이미지에서 특정 object의 위치를 찾는 것임

<img src="https://user-images.githubusercontent.com/76269316/140601172-28474f3b-5988-4c36-99f4-e140189312af.png" alt="image" style="zoom: 67%;" />

빨간색 부분은 image classification과 동일한 architecture이고, object가 위치한 좌표(bounding box)를 찾기 위한 regression 부분(파란색 부분)이 추가됨

→ classification과 regression 두 개의 모델을 갖고 있는 것이 object localization/detection

<br>

Localization과 다르게 Detection은 한 이미지 안에 두 개 이상의 object들이 존재하기 때문에 단순히 위에 있는 네트워크로 적용하기에는 여러 [문제점](https://seominseok4834.github.io/computer%20vision/1.understanding-object-detection/#object-localization-%EC%98%88%EC%B8%A1-%EA%B2%B0%EA%B3%BC)들이 있었음

<br>

Object Detection에서는 먼저 object가 있을만한 위치를 예측해야함 → Region Proposal

초기 Region Proposal의 대표적인 방식이 [selective search](https://seominseok4834.github.io/computer%20vision/1.understanding-object-detection/#region-proposal%EC%98%81%EC%97%AD-%EC%B6%94%EC%A0%95%EC%9D%98---selective-search-%EA%B8%B0%EB%B2%95)

▶ Region Proposal 방식에 기반한 Object Detection이 RCNN

<br>

##### Region Proposal 방식에 기반한 Object Detection - RCNN

<img src="https://user-images.githubusercontent.com/76269316/140601630-f54f2290-6172-4f4b-842b-c9fe8ec620db.png" alt="image" style="zoom:80%;" />

1. Selective Search를 통해 2,000개의 object가 있을만한 위치(영역)를 제안
2. 2,000개의 영역에서 제안된 이미지의 크기를 맞춤
3. 2,000개의 이미지를 네트워크에 집어 넣어 학습을 시킴

<br>

*이미지의 크기를 동일하게 맞춰줘야 하는 이유?*

1D Flattened Fully Connected Input의 사이즈가 명시돼 있기 때문에

<br>

원래 딥러닝에서는 FC Layer 이후 soft max를 통해 classification 하는데 RCNN에서는 SVM classifier를 사용 → 딥러닝 모델은 FC Layer에서 끝이고 딥러닝 모델에서 나오는 최종 FC Layer를 기반으로 다시 SVM Classifier를 적용하겠다. (Feature Extractor level에서만 CNN을 활용)

SVM을 적용한 이유는? 그냥 해봤더니 성능이 좋아서

<br>

##### RCNN 개요

<img src="https://user-images.githubusercontent.com/76269316/140601728-94a0ee24-8e2b-4688-a66b-0a3d96706e51.png" alt="image" style="zoom:80%;" />

1. Input Image를 받아서 region proposal 영역으로 쪼갬
2. CNN의 입력으로 사용하기 위해 입력 크기를 맞춤 (원본 이미지의 비율과 안 맞음 -  warped region)
3. classification, regression 진행

<br>

하나의 이미지에서 뽑아낸 2,000개의 object에 대해서 각각 적용해야하기 때문에 학습 시간이 오래걸림

<br><br>

### RCNN의 이해 02 - RCNN Training과 Loss

##### RCNN Training - Classification

<img src="https://user-images.githubusercontent.com/76269316/140601962-979a4ed2-0f54-4c98-8e5e-fb031ee7d1c6.png" alt="image" style="zoom:80%;" />

Pascal VOC 데이터 세트에 대해서 image detection 한다고 할 때 먼저 ImageNet 데이터 세트로 Feature Extractor<img src="https://user-images.githubusercontent.com/76269316/140602225-64cfb848-799a-4177-951b-904c3aef6047.png" alt="image" style="zoom: 25%;" />까지 pre train 시킴

그런 다음 Pascal VOC 데이터 세트로 pre train된 모델 위에다가 fine-tuning을 진행함

*우리가 예측한 영역(selective serach predicted)과 실제 영역(ground truth) 중에 겹치는 IOU가 0.5 이상인 경우 해당 클래스로 다시 학습시키고, IOU가 0.5 미만인 경우 background로 학습을 시킴*

그러면 피처(1차원 피처 맵)가 생성되고 이를 SVM으로 학습

ground truth 데이터를 기반으로 클래스 결정 (IOU가 0.3 이하인 경우 background로 설정, 0.3 이상이지만 GT가 아닌 경우 빼버림 - 조금 더 여유를 가짐)

<br>

##### Bounding Box Regression

<img src="https://user-images.githubusercontent.com/76269316/140602413-b5c6b7fd-f752-443c-8b2b-28dd0e56f7b1.png" alt="image" style="zoom:80%;" />

selective search로 proposal 된 영역의 중심점과 ground truth의 중심점의 거리와 width 차이가 최소가 돼야 함 

<br>

##### R-CNN 성능 비교

<img src="https://user-images.githubusercontent.com/76269316/140602725-e1a87e9d-fc18-494e-bd3f-92650572f3c8.png" alt="image" style="zoom:80%;" />

기존의 딥러닝을 쓰지 않은 알고리즘 대비 높은 성능이 나옴

<br>

##### R-CNN 장단점

성능적인 측면에선 굉장한 장점이 있었지만 많이 복잡하고 느림

동시대의 다른 알고리즘 대비 매우 높은 detection 정확도 하지만 너무 느린 detection 시간과 복잡한 아키텍처 및 학습 프로세스

→ 1장의 이미지를 object detection 하는데 약 50초 소요

<br>

- 하나의 이미지마다 selective search를 수행하여 2,000개의 region 영역 이미지 도출
- 개별 이미지별로 2,000개씩 생성된 region 이미지를 CNN Feature map 생성
- 각각 따로 노는 구성 요소들 (Selective Search, CNN Feature Extractor, SVM, Bounding box regressor로 구성된 복잡한 프로세스를 거쳐 학습 및 object detection 돼야 함)

<br>

##### R-CNN 이후 Object Detection 연구 방향성

Deep Learning과 Region Proposal 기반 Object Detection 성능을 입증했기 때문에 detection 수행 시간을 줄이고 복잡하게 분리된 개별 아키텍처를 통합할 수 있는 방안 연구

<br><br>

### SPPNet의 이해 01 - RCNN 문제점과 Spatial Pyramid Matching 이해

##### R-CNN의 주요 문제점

- 2,000개의 region 영역 이미지가 CNN으로 입력되면서 object detection 수행 시간이 오래 걸림

- region 영역의 이미지가 crop/warp 되어야 함

<br>

##### R-CNN 개선 방안

<img src="https://user-images.githubusercontent.com/76269316/140604261-9eeb8671-e7a7-49d5-b28d-a48acb5931c1.png" alt="image" style="zoom:80%;" />

원본 이미지에서 selective search로 추출

추출한 object가 아닌 원본 이미지를 VGG에 넣어서 feature map 생성한 다음 selective search에서 찾은 추천된 영역을 feature map에 mapping 하면되지 않을까?

→ R-CNN이 등장했던 2013년 메커니즘에서는 안됐음

feature map(3차원)에 있는 걸 학습시켜서 classification layer로 가려면 1D Flattened FC Input(Dense Layer) 를 통과해1차원 형태가 돼야 하는데 사이즈가 명확하게 정해져 있어 각각 크기가 다른 추천된 영역이 dense layer를 통과할 수 없었기 때문

<img src="https://user-images.githubusercontent.com/76269316/140604426-db49b1ab-55ea-4c2a-83ad-ec60db90b9de.png" alt="image" style="zoom:50%;" />

<br>

##### 서로 다른 크기의 Region Proposal 이미지 개선 방안

<img src="https://user-images.githubusercontent.com/76269316/140604576-41f58224-cab2-4b43-a4ae-db70cf908f59.png" alt="image" style="zoom:80%;" />

Feature map으로 투영된 서로 다른 크기를 가진 region proposal 이미지를 SPP Net의 고정된 크기 벡터로 변환하여 FC에 1D Flattened 된 ipnut 제공

<br>

##### SPP(Spatial pyramid Pooling)

SPP는 CNN Image Classification에서 서로 다른 이미지의 크기를 고정된 크기로 변환하는 기법으로 사용 됐는데 이를 object detection에 함께 적용했음

<img src="https://user-images.githubusercontent.com/76269316/140604634-b0cb5e37-f319-495d-8cb5-24662145656c.png" alt="image" style="zoom:80%;" />

기존에는 원본 이미지를 네트워크에 집어넣기 전에 고정된 사이즈가 나와야하므로 이미지를 crop/warp 해야 했고 따라서 이미지의 손실이 불가피 했음

SPP가 등장함에 따라서 feature map(conv layers)과 dense layer(fc layers)를 유연하게 연결할 수 있게됨

<br>

##### Bag of Visual words → Spatial Pyramid Matching

**Bag of Words**

[NLP](https://seominseok4834.github.io/machine%20learning/8.text-analytics/#bag-of-words---bow)(National Language Processing - 자연어 처리)에서 주로 사용하는 용어로 신문기사에 있는 단어들을 추출해 모은 다음 단어의 출현 빈도 수에 따라 어떤 기사인지 파악하는 방식을 의미

<img src="https://user-images.githubusercontent.com/76269316/140604785-268eb579-cc88-4f27-afcb-b2a2d8be89e7.png" alt="image" style="zoom:80%;" />

Bag of Visual words도 마찬가지로 이미지를 분류할 때 특징들을 쪼개서 모은 다음 어떤 이미지인지 분류하는 방식을 의미함

원본이 갖고 있는 정보를 새로운  mapping 정보(histogram)로 변환 (비정형적인 데이터를 정형화된 피처 속성으로 만드는 것)

▶ Bag of Words 방식은 문맥이 없기 때문에 정확성이 떨어짐 (Bag of Visual Words도 마찬가지)

<br>

**Spatial Pyramid Matching**

Bag of Words 방식의 경우 문맥 정보가 없기 때문에 모래가 조금 나오거나 하늘이 조금 나오면 제대로 분류할 수 없음

→ 이미지의 위치 정보들을 어느정도 유지하면서 다른 피처 레벨의 정보를 추출해 그걸 기반으로 분류하는 기법

<img src="https://user-images.githubusercontent.com/76269316/140604980-ada596fe-c6dd-4395-a38e-86f047522d09.png" alt="image" style="zoom:80%;" />

<br>

**Spatial Pyramid Matching (SPM) 개요**

<img src="https://user-images.githubusercontent.com/76269316/140605043-b96553fc-2889-408b-9a5a-1d8b01b1fee1.png" alt="image" style="zoom:80%;" />

 피처들이 해당 위치에 어떻게 histogram을 갖고 있는지를 기반으로 새로운 피처 값들이 생기게되고 이를 가지고 분류 (위치 정보를 감안하기 위해 분면으로 쪼갬)

- level 0: 위치적인 정보를 감안하지 않음
- level 1: 4분면으로 쪼갰을 때의 histogram
- level 2: 16분면으로 쪼갰을 때의 histogram

<br><br>

### SPPNet의 이해 02 - Spatial Pyramid Pooling을 활용한  Object Detection

##### SPM으로 서로 다른 크기의 Feature Map을 균일한 크기의 vector로 표현

<img src="https://user-images.githubusercontent.com/76269316/140605248-c3cfb372-ebf8-47ea-9300-8268901f1df6.png" alt="image" style="zoom:80%;" />

피처가 3개가 있을 경우 level 0에서는 3개의 histogram이 만들어질 수 있고, level 1에서는 12개, level 2에서는 16개의 histogram이 만들어질 수 있음

 feature map이 8x8이 아니더라도 4분면으로 나눠지기만 하면 똑같이 63개의 벡터로 표현할 수 있음

→ 분면을 나누는 것은 CNN의 pooling 개념과 거의 유사함

<br>

##### Spatial Pyramid Pooling

<img src="https://user-images.githubusercontent.com/76269316/140605398-ca30ceeb-e555-48e7-aad9-e2dc6807b815.png" alt="image" style="zoom:80%;" />

max pooling(가장 큰 값 하나만 뽑음)을 적용했기 때문에 21개의 벡터 값으로 표현됨

<br>

##### SPP-Net (Image Classification)

<img src="https://user-images.githubusercontent.com/76269316/140605505-f2cd3afd-42cc-47f1-b92e-640b084e3daf.png" alt="image" style="zoom:80%;" />

나누는 분면에 따라서 벡터 개수가 정해지기 때문에 Input Image의 사이즈가 변하더라도 표현되는 벡터의 개수는 변하지 않음

<br>

*원본 feature map이 있는데 이렇게까지 가공해도 되는가?*

feature extractor level에서도 pooling을 적용하는데 여기서 다시 pooling을 하면 손실이 있지 않을까 생각할 수 있는데, 원본 이미지에 crop/warp를 적용해 이미지 본연의 모습이 달라지는 것보다 좋은 성능 효과를 거두었기 때문에 이 방식을 채용함

<br>

##### SPP-Net (Obejct Detection)

<img src="https://user-images.githubusercontent.com/76269316/140605665-22d01342-457c-4d15-a896-2c0779c963f2.png" alt="image" style="zoom:80%;" />

Selective Search에서 가져온 영역에 SPP Layer를 적용하면 분면을 쪼개서 가져가기 때문에 fixed-length는 변하지 않으니까 그대로 Dense Layer(FC Layer)에 적용할 수 있지 않을까? 하고 SPP-Net을 R-CNN에 적용해봄

<br>

##### SPP-Net을 R-CNN에 적용

<img src="https://user-images.githubusercontent.com/76269316/140605723-fa1c52fa-f087-4866-a63d-f659bae283fb.png" alt="image" style="zoom:80%;" />

이전에는 object가 있을만한 위치에 있는 것들 2,000개를 227x227로 변환해 학습을 진행시켰던 반면에 이제는 원본 이미지만 학습을하고 feature map에 selective search가 mapping을 하면 mapping 된 feature map을 SPP Layer에서 fixed length 벡터로 변환해 학습을 진행

<img src="https://user-images.githubusercontent.com/76269316/140605820-776d25a6-295f-4169-aa95-bb4032dfe293.png" alt="image" style="zoom:80%;" />

<img src="https://user-images.githubusercontent.com/76269316/140633965-848bfcf8-7cd1-4791-8875-e787421a73e8.png" alt="image" style="zoom:80%;" />

RCNN과 거의 동일한 mAP 성능을 가졌는데도 64배 이상의 속도 향상을 보여줌

<br>
<br>

### Fast RCNN의 이해

##### RCNN의 발전

<img src="https://user-images.githubusercontent.com/76269316/140605960-8c4a8306-b76c-46a2-8b42-f89f97a92cb2.png" alt="image" style="zoom:80%;" />

R-CNN의 경우 2,000개의 object에 대해서 각각 학습을 진행했기 때문에 속도가 굉장히 느렸음

Fast R-CNN은 SPP-Net과 비슷하게 원본 이미지만 학습을 하고, 학습을 통해 만들어진 피처들에 ROI pooling을 적용함

Fast R-CNN까지 여전히 selective search를 사용했는데 Faster R-CNN의 경우 selective search 부분을 딥러닝 네트워크 안으로 가지고 옴(RPN)

<br>

또한 R-CNN의 경우 딥러닝 로직에서 classifcation이 수행되는게 아닌 별도의 SVM을 적용했어야 했는데, Fast R-CNN의 경우 바로 soft max를 적용

<br>

##### Fast R-CNN 주요 특징

<img src="https://user-images.githubusercontent.com/76269316/140606069-c58e65ea-c3d0-4e88-9b79-fe0ad72b5d21.png" alt="image"  />

- SPP Layer를 ROI Pooling Layer로 바꿈
- End-to-End Network Learning
  - 딥러닝으로 들어오지 않았던 별도의 머신러닝 알고리즘(SVM)을 softmax로 변환해서 딥러닝 안으로 들여옴
  - Multi-task loss 함수로 classification과 regression을 함께 최적화 (softmax가 안으로 들어왔으니까 total loss를 만들 수 있음)
    ![image](https://user-images.githubusercontent.com/76269316/140606151-6f26d77e-1dda-4f8e-935e-33eca8766b8a.png)

<br>

##### ROI Pooling

<img src="https://user-images.githubusercontent.com/76269316/140606217-f70ebbec-3067-4afd-9aec-4a816b73b92f.png" alt="image" style="zoom:80%;" />

14x14 feature map에 selective search를 통해 두 개의 영역이 뽑혔을 때(빨간색 네모), 2x2로 pooling 한다고 하면 4등분 해서 max pooling을 적용할 수 있음

<br>

##### Fast R-CNN에서 ROI Pooling

<img src="https://user-images.githubusercontent.com/76269316/140606326-472a4950-d59a-4124-a2a3-3ebde1ed1ba0.png" alt="image" style="zoom:80%;" />

14x7의 경우 세로 2칸, 가로 1칸으로 ROI pooling을 적용할 수 있는데, 8x4처럼 정수형으로 나누어 떨어지지 않는 경우에는 보간법을 적용

→ 세로의 경우 마지막 두 칸을 하나로 채우고, 가로의 경우 4칸을 7칸으로 늘려서 계산하거나 이미지 resize 하듯이 8x4를 7x7로 mapping을 시켜버림

<br>

##### ROI Pooling을 적용한 Fast R-CNN 구조

<img src="https://user-images.githubusercontent.com/76269316/140634162-fe106fcf-99bf-4035-93b9-ccc0533ddc7a.png" alt="image" style="zoom:80%;" />

원본 이미지에 selective seach 적용해서 추출한 영역을 feature map에 mapping한 다음 ROI pooling 적용

ROI Pooling 적용 후 depth 개수는 Feature Map depth 개수와 동일 (SPP Net은 채널까지 포함해서 1차원으로 만들지만 ROI Pooling은 size(7x7)만 고정시키고 채널 depth는 똑같이 가져감)

<br>

<img src="https://user-images.githubusercontent.com/76269316/140634226-79b07268-069d-4eb4-81ce-8f40ae22345b.png" alt="image" style="zoom:80%;" />

→ classification과 regression loss를 함께 반영해서 loss 함수를 만들 수 있게됨

→ backpropagation이 전반적으로 가능하게됨

 <br>

##### Multi-task loss

<img src="https://user-images.githubusercontent.com/76269316/140634288-f2e51b07-dcf4-4cbf-928b-ed0d1a6f1a5a.png" alt="image" style="zoom:80%;" />

L1 loss: 실제값과 예측값의 차이에 절댓값을 취한 것

L2 loss: 실제값과 예측값의 차이에 제곱을 취한 것

smooth L1 loss: 실제값과 예측값의 차이가 1보다 작으면 0.5x^2을 시키고 1보다 크거나 같으면 x - 0.5를 함 (1보다 작을 때는 급격하게 loss를 줄여줌)

 <br>

##### Fast RCNN 성능 비교

<img src="https://user-images.githubusercontent.com/76269316/140634354-a5f58758-66cd-466f-9c45-0cc4c7c779f7.png" alt="image" style="zoom:80%;" />

<br><br>

### Faster RCNN의 이해 01 - Anchor Box

##### Faster RCNN 개요

Object Detection을 구성하는 모든 요소들을 딥러닝만으로 구현한 첫 번째 Object Detection 알고리즘

Fast RCNN에서는 Selective Search를 통해 RoI(Region of Interest) Proposal 하는 부분이 딥러닝 영역 안으로 들어오지 못했는데 Faster RCNN에서 RPN으로 구성했음

Faster RCNN = RPN + Fast RCNN

<img src="https://user-images.githubusercontent.com/76269316/140634544-a6007d46-74e2-4f4a-9001-5d19657ba027.png" alt="image" style="zoom:80%;" />

<br>

##### Faster RCNN 구조

Convolutional network를 통과해 생성된 feature map이 RPN(Region Proposal Network)로 가서 추출된 영역에 RoI Pooling 적용, 이후는 Fast RCNN과 동일

<img src="https://user-images.githubusercontent.com/76269316/140634700-cd6e1dc2-604f-4cb4-ab7c-f851cc17cbb5.png" alt="image" style="zoom:80%;" />

<br>

##### Region Proposal Network 구현 이슈

Selective Search를 대체하기 위한 Region Proposal Network를 어떻게 구현할 지에 대한 고민

*데이터로 주어지는 피처 (pixel 값)와 target (ground truth bounding box)만 가지고는 불가능한데 어떡하지?* 

→ Anchor Box 도입

<img src="https://user-images.githubusercontent.com/76269316/140634773-6838889c-91a2-41b0-b74f-a87abd806eb7.png" alt="image" style="zoom:80%;" />

딥러닝은 데이터가 들어오면 데이터를 가지고 학습을 하면서 최적의 값을 찾는 것

처음 anchor box에는 그냥 값만 있는데 데이터를 가지고 ground truth 기반으로 학습을 시킬 수 있음 → 학습을 진행할 수록 이 anchor box가 도드라지네? anchor box를 기반으로 위치를 찾아보자는 것이  RPN

<br>

##### Anchor Box 구성

<img src="https://user-images.githubusercontent.com/76269316/140634901-cfe40c09-e6c3-4af2-9dfd-6b5ebe573e73.png" alt="image" style="zoom:80%;" />

실제 anchor box는 이미지에 mapping 시키는 것이 아닌 feature map에 mapping 시키는 것이기 때문에 그림의 사이즈보다 크기가 줄어들 수 있음

<br>

<img src="https://user-images.githubusercontent.com/76269316/140634927-3ef754aa-1883-4895-9bc3-ef99dfea0846.png" alt="image"  />

크기, 비율이 다른 9개의 anchor box로 구성해서 region proposal

<br>

##### Anchor Box 특징

![image](https://user-images.githubusercontent.com/76269316/140635120-1aec0e48-b9c9-4162-b6b5-bb29ce297486.png)

anchor box 안에 object가 얼마나 들어와있느냐(IOU)에 따라 평가하기 때문에 만약 검은색 3x3 정사각형 anchor box만 사용했다면 사람하고 겹치는 anchor box를 찾을 수 없음 (사람은 직사각형인데, anchor box는 정사각형이므로) → 3개의 ration 사용

<br>

또한 하나의 anchor box 안에 자동차도 있고 사람도 있으면 사람도 찾아야 하는데 못 찾음  → 중심점을 기준으로 다른 크기, 형태를 갖는 anchor box를 촘촘히 가져야함

<br>

##### Feature Map에서 anchor box 매핑

이해를 위해서 이미지에다가 anchor box를 mapping 시켰지만 **실제로는 feature map에 mapping!!**

<img src="https://user-images.githubusercontent.com/76269316/140635141-aa106c8e-ebe6-4eb3-be39-ef61322aeb0c.png" alt="image" style="zoom:80%;" />

<br>

 <img src="https://user-images.githubusercontent.com/76269316/140635173-50d8b334-fd20-4caf-907e-4cc7fd260922.png" alt="image" style="zoom:80%;" />



원본 이미지가 1/16 크기로 down sampling (Hegiht 800 → 50, Width 600 → 38)

50x38개의 grid point에 anchor box가 생김

<br>

<img src="https://user-images.githubusercontent.com/76269316/140635225-3f6afc2c-90cf-4264-8131-28da61f06826.png" alt="image" style="zoom:80%;" />

50 * 38 * 9 = 17,100개의 anchor box 생성

<img src="https://user-images.githubusercontent.com/76269316/140635308-88b4c320-71b9-4695-b2b3-12c444a66289.png" alt="image" style="zoom:80%;" />

+흰색 부분은 이미지 바깥 부분

<br><br>

### Faster RCNN의 이해 02 - Anchor Box를 활용한  RPN 구성

##### RPN 개요

<img src="https://user-images.githubusercontent.com/76269316/140635379-4eb7e40e-55df-4be3-a6fd-ec5bc3271d92.png" alt="image" style="zoom:80%;" />

<br>

##### RPN 네트워크 구성

<img src="https://user-images.githubusercontent.com/76269316/140635468-d515348b-c7f6-4de0-993e-385210c4aaf2.png" alt="image" style="zoom:80%;" />

feature map에 3x3 conv을 적용한 다음, 1x1 conv을 적용해서 foreground(obj)인지 background(no obj)인지 이진 분류 적용 (Sigmod Classification)

※ 1x1 convolution은 feature map의 파라미터를 대폭 줄이는데, 이 때  모델 성능에 최소한으로 영향을 끼치며 차원을 축소하는 기능을 함

1x1을 곱했기 때문에 size는 변하지 않지만 채널수가 9로 변함 (anchor box 수)

```python
def rpn(base_layers, num_anchors):  #base_layers: feature map
    x = Convolution2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)
    
    #40x50x9 = 18,000개의 anchor box가 object인지 아닌지 판단
    x_class = Convolution2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)
    #(x1, y1), w, h 계산
    x_regr = COnvolution2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)
    
    return [x_class, x_regr, base_layers]
```

<br>

##### RPN Bounding BOx Regression

