---
title:  "1.Understanding Object Detection"
excerpt: "Object Detection 이해"
toc: true
toc_label: "Understanding Object Detection"
toc_sticky: true
published: true

categories:
  - Computer Vision
tags:
  - 컴퓨터 비전
  - 객체 탐지
last_modified_at: 2021-11-03
---

### Object Detection과 Segmentation 개요

딥러닝이 컴퓨터 비전 영역에서 가장 큰 변곡점이 된 시점 → 2012년

AlexNet이 ImageNet(Image Classification Competition)에서 딥러닝 기반의 CNN 방식으로 우승한 이후 Image Classification 영역에서 사용된 딥러닝 기법을 Object Detection 분야에서 활용하기 시작함

![image](https://user-images.githubusercontent.com/76269316/137617207-c326287a-7cb3-499e-bc7c-b13482ca4acb.png)

PASCAL VOC(Object Detection Competition)에서 mAP(Object Detection 성능 측정 지표)가 2012년을 기점으로 크게 향상됨

<br>

##### Classification / Localization

![image](https://user-images.githubusercontent.com/76269316/137617460-98304cd6-5d3b-4363-85c3-c976a3e08dc3.png)

Classification: 이미지에 CNN을 적용해 feature map 특성을 기반으로 분류

Localization: 하나의 이미지 안에서 하나의 object를 bounding box로 지정해 찾음

<br>

##### Detection / Segmentation

![image](https://user-images.githubusercontent.com/76269316/137617549-6779ed0b-db16-414b-a212-f5c90bd7b739.png)

Detection: 하나의 이미지 안에 여러 object들이 존재하고, 그 object들을 bounding box로 찾음

Segmentation: 하나의 이미지 안에 여러 object들이 존재하고, 그 object들을 개별 pixel level로 detection 수행

<br>

- localization, object detection, object segmentation은 object의 위치를 찾아내는 것

- classification은 이미지가 무엇인지를 분류하는 것

<br>

localization/detection은 해당 object의 위치를  bounding box로 찾고 bounding box내의 object를 classification함

▶ bounding box regression(box의 좌표값을 예측) + classification

<br>

##### Object Detection History

![image](https://user-images.githubusercontent.com/76269316/137617821-3a29d9cf-b417-451c-a5b1-69b1a7446dd5.png)

2012년 이전까지는 알고리즘 기반

2012년 이후 AlexNet과 같은 딥러닝 기반 framework를 어떻게 object detection으로 끌어올 것인가 고민

- One Stage detector: 바로 detect 적용

초기에는 수행 시간만 집중한 나머지 빨랐지만 성능은 안 좋았음

점점 수행 시간과 수행 성능도 좋아짐

- Two-Stage detector: object가 있을만한 위치를 먼저 참조해서 만들어 놓음

성능은 좋지만 이미지를 detect하는데 시간이 많이 소요됨 → 실시간 적용이 어려움

<br><br>

### Object Detection 주요 구성 요소 및 왜 Object Detection은 어려운가?

##### 일반적인 Object Detection 모델

![image](https://user-images.githubusercontent.com/76269316/138263334-ba397dec-5e14-459d-8976-9e89428b26c6.png)

<br>

##### Object Detection의 난제

![image](https://user-images.githubusercontent.com/76269316/138263692-ed88ea81-26b3-4b6c-8870-04a58d1de2aa.png)

1. Classification과 Regression 둘 다 loss가 좋아야 함 → loss 함수 식이 일반적인 Image Classification보다 복잡
2. 이미지 안에 다양한 크기와 여러가지 유형의 object가 섞여 있음 (딥러닝에서 object detection 적용시 feature extrator 통과해서 만들어진 feature map에 대해서 detect 수행하는데 feature map이 원본 크기보다 작기 때문에 다양한 유형의 object를 detect하는게 어려움)
3. Detect 시간도 중요 (알고리즘의 딜레마: 복잡한 로직으로 알고리즘 성능을 좋게 하면 수행 시간이 떨어짐)
4. Detect 해야할 Object가 차지하는 비중이 높지 않은 경우 (전체 이미지에서 배경이 차지하는 비중이 높음)
5. annotation을 만들어야하기 때문에 상대적으로 데이터 세트가 부족함

<br><br>

### Object Localization과 Detection의 이해

##### Object Localization 개요

하나의 이미지 안에서 하나의 object를 bounding box로 지정해 찾는 것

![image](https://user-images.githubusercontent.com/76269316/138264632-e4a553f6-961b-4e3a-b921-5a1bbe91094e.png)

- Image Classification

**원본이미지 (224x224) → Feature Extrator(Backbone) → Featur Map으로 학습 (이미지-레이블) → Dense Layer(Fully Connected Layer) → Soft max를 적용해 분류**

Feature Extrator: 이미지에서 중요한 피처들을 뽑아냄 (Feature Map 생성 - 사이즈는 줄지만 채널 수는 증가, 추상화된 이미지를 갖고 있음)

<br>

- Object Localization

Bounding Box에 대한 좌표값을 Annotation으로 갖고 있음

Object Localization은 Bounding Box Regression layer가 또 있음

feature map에서 학습한 특성이 예측하려는 이미지에서 나오면 바로 "이 bounding box다!" 하면서 mapping → 비교적 결과가 잘 나옴

<br>

##### Object Localization - Bounding Box 학습

![image](https://user-images.githubusercontent.com/76269316/138266188-d3e9807a-a6c6-4290-9b20-dac8bda5136c.png)

<br>

##### Object Localization 예측 결과

![image](https://user-images.githubusercontent.com/76269316/138266494-e30da1e9-07f1-4091-81a6-3b1e0832475d.png)

object localization은 일반적인 image classification network 모델에다가 bounding box regression을 붙이면 되는데 이런 로직을 바로 object  detection에 적용하면 문제가 생김

<br>

![image](https://user-images.githubusercontent.com/76269316/139690830-4d2a2511-938d-46b9-95bd-e8fa18879d74.png)

object localization처럼 적용할 경우 주황색 bounding box로 예측함 (bounding box만 모델에 넣어서는 inference가 어려움)

파랑색 모델x와 빨강색 모델3는 feature map으로 봤을 때는 같은 특성 → 모델x 피처로 봤을 때는 파랑색 영역에 있고, 모델3 피처로 봤을 때는 빨강색 영역에 있음 
"비슷한데? 모르겠다~" 하고 주황색 영역으로 예측함

<br><br>

### Region Proposal(영역 추정)의 이해와 슬라이딩 윈도우와의 비교

object가 있을만한 위치를 먼저 찾은 다음 그 위치에 있는 object에 대해 detection을 수행하는게 일반적인 방법임

이미지에서 detect하려는 object가 어디에 있는지를 찾는 것이 object detection에서 넘어야 할 첫번째 알고리즘임

<br>

##### 슬라이딩 윈도우

![image](https://user-images.githubusercontent.com/76269316/139692474-02ecd46d-4ae4-45ce-b986-770898c43fd8.png)

이미지의 특정 영역에서 윈도우를 한 칸씩 움직이면서 object를 찾은 다음, 찾은 object를 학습된 feature map과 mapping을 해서 bounding box regression을 통해서 object의 위치를 찾는 방식

윈도우를 왼쪽 상단에서부터 오른쪽 하단으로 이동시키면서 object를 detection하는 방식

![image](https://user-images.githubusercontent.com/76269316/139692853-f63fd7f5-29e2-45e0-8706-bdd04c779395.png)

case by case라는게 문제임 (윈도우 크기·형태에 따라 detect가 잘되지 않을 수도 있음)

▶ 해결책

1. 다양한 형태의 window를 각각 sliding 시키는 방식

![image](https://user-images.githubusercontent.com/76269316/139693432-7b46276b-af65-42a4-835e-bbcb469f87bc.png)

2. window scale은 고정하고 scale을 변경한 여러 이미지를 사용하는 방식

![image](https://user-images.githubusercontent.com/76269316/139693565-08e4049e-2a44-4d21-b0fb-936a1982afcf.png)

이렇게하면 시간이 오래 걸리게 됨.

- Object Detection의 초기 기법으로 활용
- Object가 없는 영역도 무조건 슬라이딩 해야하며 여러 형태의 window와 여러 scale을 가진 이미지를 스캔해서 검출해야 하므로 수행 시간이 오래 걸리고 검출 성능이 상대적으로 낮음
- Region Proposal(영역 추정) 기법의 등장으로 활용도는 떨어졌지만 Object Detection 발전을 위한 기술적 토대 제공

<br>

첫번째 해결책은 region proposal에서 anchor box로, 두번째 해결책은 FPN으로 발전 

<br>

##### 이미지 scale 조정에 따른 여러 크기의 object detection

![image](https://user-images.githubusercontent.com/76269316/139695324-e5e0d02c-6c6b-486c-9b34-4cd952f3fae8.png)

첫번째 경우 윈도우 scale이 작아서 오드리 헵번을 detect 하지 못함

오드리 헵번을 detect하기 위해 윈도우 scale은 유지한채 이미지 scale을 줄였음 → 그레고리 펙이 detect가 안됨

다시 이미지 scale을 줄였음 → 너무 줄여서 하나의 윈도우 안에 object들이 너무 많이 들어와서 detect하기가 쉽지 않음

<br><br>

### Region Proposal(영역 추정)의 - Selective Search 기법

Region Proposal은 object가 있을만한 후보 영역을 찾는 것 (Sliding Window는 그냥 윈도우를 순차적으로 슬라이딩 해가면서 찾는 방식이기 때문에 region proposal로 부르기엔 조금 그럼)

Selective Search는 region proposal의 대표적인 기법

<img src="https://user-images.githubusercontent.com/76269316/139754623-4b6d09d8-bb4d-44b3-a4fc-24112f078258.png" alt="image" style="zoom:67%;" />

우리가 이미지를 봤을 때는 즉각적으로 알 수 있지만 컴퓨터는 그렇지 않음

*직관적으로 구분할 수 있는 방법이 뭘까?*

▶ 사람과 벽의 경우 밝기, 색상, 경계선의 유형이 다르니까 이러한 특성을 결합해서 object가 있을만한 후보 영역을 찾는 알고리즘이 Selective Search

<br>

<img src="https://user-images.githubusercontent.com/76269316/139754992-f2bf289e-2f57-4901-a4d7-59d0bbe2da8e.png" alt="image" style="zoom:67%;" />

object별 차이점들을 segmentation화 시킨 뒤 segmentation을 기반으로 후보 object 영역들을 찾음

빠른 detection과 높은 recall 예측 성능을 동시에 만족하는 **별도의 알고리즘**

→ selective search가 딥러닝 알고리즘에 합쳐지기까지 시간이 걸렸음

<br>

<img src="https://user-images.githubusercontent.com/76269316/139850748-4b6c7dde-a960-4b2c-af60-4f36115caad6.png" alt="image" style="zoom:67%;" />

왼쪽 상단은 어두운 색이라 픽셀 값이 낮고, 여자 옆의 부분은 회색으로 픽셀 값을 200정도로 봄 → 픽셀의 intensity를 통해 색깔별로 구분을 할 수 있음

<br>

<img src="https://user-images.githubusercontent.com/76269316/139851444-91b9924a-6f73-4bc6-8896-fca482a991d6.png" alt="image" style="zoom:67%;" />

경계션별로 edge detector를 사용해서 edge를 detect → edge를 경계로 봤을 때 경계 좌우의 차이로도 구분할 수 있음

<br>

<img src="https://user-images.githubusercontent.com/76269316/139851810-67083d39-a033-4681-9fa4-b70a44a29a80.png" alt="image" style="zoom:67%;" />

edge detector로 찾은 크기, 형태로도 구분할 수 있음

<img src="https://user-images.githubusercontent.com/76269316/139852269-68658e5c-2017-420b-83eb-493a1bd06c94.png" alt="image" style="zoom:67%;" />

초기에는 해당 영역과 비슷한 픽셀 영역을 masking해서 굉장히 촘촘하게 찾아냄 (Over Segmentation)

<img src="https://user-images.githubusercontent.com/76269316/139852576-d540b03d-f391-4b3a-b27c-c397fe8d460e.png" alt="image" style="zoom:67%;" />

segmentation을 bounding box로 변환 → 너무 많으니까 이렇게 만든 초기 영역을 계층적 그룹핑 방법으로 합침 (비슷한 애들을 합침)



개별 segment된 모든 부분들을 bounding box로 만들어서 region proposal 리스트로 추가하고, 컬러, 무늬, 크기, 형태에 따라 유사도가 비슷한 segemt들을 그룹핑하는 과정을 계속 반복하면서 region proposal 수행

![image](https://user-images.githubusercontent.com/76269316/139853577-3b8756e0-7c6a-46ae-8f3b-a9bcbbb748f5.png)

<br><br>

### IOU(Intersection over Union)의 이해와 구현 실습

<img src="https://user-images.githubusercontent.com/76269316/139860269-1379077a-5180-4dd8-810e-52612934e3f7.png" alt="image" style="zoom:67%;" />

IoU(Intersection over Union): 모델이 예측한 결과와 실측(Ground Truth) box가 얼마나 정확하게 겹치는가를 나타내는 지표

<br>

![2021-11-02 225253](https://user-images.githubusercontent.com/76269316/139860588-ea0bf420-ec48-4cd8-889f-d9af0fb44ec9.png)



<img src="https://user-images.githubusercontent.com/76269316/139860957-dc05a549-92ce-4929-8830-2a84dcc31736.png" alt="image" style="zoom: 80%;" />

거의 완벽하게 detect 했다면 1에 가까운 수치가 나오게 됨

<br>

##### IoU에 따른 Detection 성능

<img src="https://user-images.githubusercontent.com/76269316/139861220-16a5e8c7-937e-4f9a-b20b-38497b1abe95.png" alt="image" style="zoom:80%;" />

<br><br>

### NMS(Non Max Suppression)의 이해

![image](https://user-images.githubusercontent.com/76269316/139864944-23cfe7ac-d021-4329-b4bb-2e5bc31ba3e0.png)

bounding box들 중 가장 확실한 bounding box를 제외한 비슷한 bounding box를 모두 눌러주는(suppression) 기법

<br>

*왜 하나의 object를 정확하게 detect하지 못하고 여러 개의 bounding box detection이 나올까?*

object가 있는 곳을 놓쳐서는 안되기 때문에 비슷하다고 생각되면 계속 추천하기 때문에

<br>

##### NMS 수행 로직

1.Detected 된 bounding box별로 특정 confidence threshold 이하 bounding box는 먼저 제거 (confidence score < 0.5)

NMS는 두 가지 threshold 값을 가짐

①confidence score threshold: objcet detect 확률 (차라고 분류할 확률)

②IOU threshold

<br>

2.가장 높은 confidence score를 가진 box 순으로 내림차순 정렬하고 아래 로직을 모든 box에 순차적으로 적용

- 높은 confidence score를 가진 box와 겹치는 다른 box를 모두 조사하여 IOU가 특정 threshold 이상인 box를 모두 제거 (예: IOU threshold > 0.4)

<br>

3.남아있는 box만 선택

▶ confidence score가 높을수록, IOU Threshold가 낮을수록 많은 box가 제거됨 (IOU Threshold로 많이 조절함)

<br><br>

### Object Detection 성능 평가 지표 mAP의 이해 01 - 정밀도와 재현율

##### mAP

실제 object가 dedtected된 재현율(Recall)의 변화에 따른 정밀도(Precision)의 값을 평균한 성능 수치

<br>

[정밀도와 재현율](https://seominseok4834.github.io/machine%20learning/3.evaluation/#3-%EC%A0%95%EB%B0%80%EB%8F%84%EC%99%80-%EC%9E%AC%ED%98%84%EC%9C%A8-precision-recall)

정밀도와 재현율은 주로 이진 분류에서 사용되는 성능 지표로

정밀도는 예측을 positive로 한 대상 중에 예측과 실제 값이 positive로 일치한 데이터의 비율을 의미하고

재현율은 실제 값이 positive인 대상 중에 예측과 실제 값이 positive로 일치한 데이터의 비율을 의미함

<br>

Object Detection에서 정밀도는 검출 알고리즘이 예측한 결과가 실제 object들과 얼마나 일치하는지를 나타내는 지표이고,

재현율은 검출 알고리즘이 실제 object들을 빠뜨리지 않고 얼마나 정확히 검출 예측하는지를 나타내는 지표임

▶ 좋은 알고리즘이라면 정밀도와 재현율이 모두 좋아야함

![image](https://user-images.githubusercontent.com/76269316/139969416-f8dc3068-9890-4f98-a6ea-6307cf76fbf2.png)

<br>

*Object Detection에서 어떤 걸 예측 성능으로 볼 것인가?*

Object Detection에서 개별 Object에 대한 검출 예측이 성공했는지 여부를 IOU로 결정

<img src="https://user-images.githubusercontent.com/76269316/139969486-a7d5383f-5142-4522-8ca5-bd69f1cefecf.png" alt="image" style="zoom:80%;" />

<br>

##### 오차행렬 (Confusion Matrix)

이진 분류의 예측 오류가 얼마인지와 더불어 어떠한 유형의 예측 오류가 발생하고 있는지를 함께 나타내는 지표

<img src="https://user-images.githubusercontent.com/76269316/139969579-09a825cd-b30b-4e61-a568-23823e5b2f78.png" alt="image" style="zoom:67%;" />

<br>

##### Object Detection에서의 TP, FP, FN

![image](https://user-images.githubusercontent.com/76269316/139969764-2c16f6ad-4226-493b-97eb-48e1051909a1.png)



![image](https://user-images.githubusercontent.com/76269316/124432235-5e3b7380-ddac-11eb-8d50-af9b7e3d2527.png)

정밀도는  맞은것 / 전체 예측 횟수, 재현율은 맞은 것 / 실제 object

정밀도가 좋아지려면 FP가 작아야 하고, 재현율이 좋아지려면 FN이 작아야 함

<br>

##### 업무에 따른 재현율과 정밀도의 상대적 중요도

- 재현율이 상대적으로 더 중요한 지표인 경우: 실제 positive 데이터를 negative로 잘못 예측하면 업무상 큰 영향이 발생하는 경우
  ex) 암 진단, 금융사기 판별
- 정밀도가 상대적으로 더 중요한 지표인 경우: 실제 negative인 데이터 예측을 positive로 잘못 예측하면 업무상 큰 영향이 발생하는 경우
  ex) 스팸 메일 (일반 메일인데 스팸 메일로 잘못 판단한 경우)

<br>

##### 정밀도와 재현율의 맹점

- 정밀도를 100%로 만드는 방법: 확실한 기준이 되는 경우만 positive로 예측하고 나머지는 모두 negative로 예측
  전체 환자 1000명 중 확실한 positive 징후를 가진 환자가 단 1명이라고 한다면 이 한 명만 positive로 예측하고 나머지는 모두 negative로 예측해도 1 / (1 + 0)이 돼서 100%가 되게 됨
- 재현율을 100%로 만드는 방법: 모든 환자를 positive로 예측
  음성인 환자들이 있더라도 TN(음성으로 예측했는데 실제 음성인 환자들)이 수치에 포함되지 않기 때문에 30/(30+0)이 돼서 100%가 되게 됨

<br>

##### Confidence 임곗값에 따른 정밀도-재현율 변화

![image](https://user-images.githubusercontent.com/76269316/139970747-a6c0256a-c1c9-4503-9674-264cafc22028.png)

<br>

##### 정밀도 재현율의 트레이드 오프

**정밀도 재현율 트레이드 오프 (Precision Recall Trade-off)**

Confidence 임곗값(Threshold)을 조정하면 정밀도 또는 재현율의 수치를 높일 수 있지만 정밀도와 재현율은 상호 보완적인 지표이기 때문에 어느 한쪽을 강제로 높이면 다른 하나의 수치는 떨어지게 됨

![image](https://user-images.githubusercontent.com/76269316/139970893-321a503d-c748-48dc-a0ab-6c8c7fe48fa0.png)

<br>

**정밀도 재현율 곡선 (Precision Recall Curve)**

Recall 값의 변화에 따른 (confidence 값을 조정하면서 얻어진) precision 값을 나타낸 곡선

이렇게 얻어진 precision 값의 평균을 AP라고하며 일반적으로 정밀도 재현율 곡선의 면적 값으로 계산

![image](https://user-images.githubusercontent.com/76269316/139971055-3d445a17-1c5b-405d-8b5c-e78a3f7894a4.png)

<br><br>

### Object Detection 성능 평가 지표 mAP의 이해 02 - mAP 계산하기

##### Confidence에 따른 Precision과 Recall의 변화

![image](https://user-images.githubusercontent.com/76269316/139998494-28e5c417-728c-4323-a5d1-3485263273cd.png)



![image](https://user-images.githubusercontent.com/76269316/139999426-be7e59ea-0dac-40dc-b878-8e57589820eb.png)

<br>

##### AP(Average Precision) 계산하기

confidence 값에 따른 precision과 recall의 변화를 그래프로 나타낸 것

![image](https://user-images.githubusercontent.com/76269316/139999503-3933fe9e-e2c1-434a-8137-f8e635dc0de8.png)

AP 계산시 오른쪽 최대 prcision 값을 연결해서 계산함

<br>

![image](https://user-images.githubusercontent.com/76269316/139999725-2618564c-25e4-4937-9ba8-e399369a0e56.png)

<br>

##### mAP(mean Average Precision)

AP는 한 개 오브젝트에 대한 성능 수치, mAP는 여러 오브젝트들의 AP를 평균한 값임

<br>

##### COCO Challenge에서의 mAP

IOU를 0.5 이상으로 고정한 PASCAL VOC와 달리 COCO Challenge는 IOU를 0.5부터 0.05씩 증가시키며 0.95까지 해당하는 IOU별로 mAP를 계산 (AP@[.50:.05:.95])

또한 크기의 유형(대/중/소)에 따른 mAP도 측정함

<img src="https://user-images.githubusercontent.com/76269316/140000074-b70bca06-e1cf-43a0-8b34-68dce85e3aa2.png" alt="image" style="zoom:80%;" />

<br>

##### 데이터세트와 알고리즘 별 mAP 수치 예시

![image](https://user-images.githubusercontent.com/76269316/140000177-08d3b851-dc00-4773-9e69-9a2161673436.png)

