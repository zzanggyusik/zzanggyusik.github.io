---
title:  "5.MMDetection"
excerpt: "MMDetection"
toc: true
toc_label: "MMDetection"
toc_sticky: true
published: true

categories:
  - Computer Vision
tags:
  - 컴퓨터 비전
last_modified_at: 2021-11-18

---

> <img src="https://user-images.githubusercontent.com/76269316/141792502-d3978a97-7c36-4be7-881e-3e0d03d2e901.png" alt="image" style="zoom: 80%;" />
>
> [[개정판] 딥러닝 컴퓨터 비전 완벽 가이드](https://www.inflearn.com/course/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%BB%B4%ED%93%A8%ED%84%B0%EB%B9%84%EC%A0%84-%EC%99%84%EB%B2%BD%EA%B0%80%EC%9D%B4%EB%93%9C)를 수강하고 공부한 내용을 정리한 포스팅입니다.

<br>

### Tiny Kitti 데이터로 MMDetection Train 실습 - MMDetection Dataset의 이해

##### MMDetection의 주요 구성 요소

<img src="https://user-images.githubusercontent.com/76269316/142376120-da51d416-a2ba-40bb-bf05-5422ce611380.png" alt="image" style="zoom:80%;" />

MMDetection의 주요 구성 요소 중 핵심은 **config**입니다.

데이터 세트를 모델에 입력시켜주면 학습을 통해 weight 값을 갱신하면서 모델을 고도화시켜나가는데, 이 일련의 과정들이 config를 통해서 이루어지기 때문입니다.

결국, config 파일을 보고 MMDetection framework가 움직이게 됩니다.

모델에 custom 데이터 세트를 입력시켜 주기 위해선 MMDetection이 받아들일 수 있는 데이터 유형으로 변환하는 작업이 필요합니다.

<br><br>

##### MMDetection Dataset 지원

MMDetection은 다양한 유형의 Dataset를 지원하기 위해 변환 클래스(Dataset class)를 지원합니다.

<img src="https://user-images.githubusercontent.com/76269316/142376920-cb17015a-7e5a-4596-99f1-2f9517366044.png" alt="image" style="zoom:80%;" />

<br>

https://github.com/open-mmlab/mmdetection/tree/master/mmdet/datasets

![image](https://user-images.githubusercontent.com/76269316/142377615-8f570092-741f-48b1-977e-8bb664474224.png)

<br>

아래처럼 **\__init__.py**에 지원하는 data set가 정의돼 있고,

**\__init__.py**

```python
# Copyright (c) OpenMMLab. All rights reserved.
from .builder import DATASETS, PIPELINES, build_dataloader, build_dataset
from .cityscapes import CityscapesDataset
from .coco import CocoDataset
from .coco_panoptic import CocoPanopticDataset
from .custom import CustomDataset
from .dataset_wrappers import (ClassBalancedDataset, ConcatDataset,
                               MultiImageMixDataset, RepeatDataset)
from .deepfashion import DeepFashionDataset
from .lvis import LVISDataset, LVISV1Dataset, LVISV05Dataset
from .samplers import DistributedGroupSampler, DistributedSampler, GroupSampler
from .utils import (NumClassCheckHook, get_loading_pipeline,
                    replace_ImageToTensor)
from .voc import VOCDataset
from .wider_face import WIDERFaceDataset
from .xml_style import XMLDataset

__all__ = [
    'CustomDataset', 'XMLDataset', 'CocoDataset', 'DeepFashionDataset',
    'VOCDataset', 'CityscapesDataset', 'LVISDataset', 'LVISV05Dataset',
    'LVISV1Dataset', 'GroupSampler', 'DistributedGroupSampler',
    'DistributedSampler', 'build_dataloader', 'ConcatDataset', 'RepeatDataset',
    'ClassBalancedDataset', 'WIDERFaceDataset', 'DATASETS', 'PIPELINES',
    'build_dataset', 'replace_ImageToTensor', 'get_loading_pipeline',
    'NumClassCheckHook', 'CocoPanopticDataset', 'MultiImageMixDataset'
]
```

<br>

각각의 데이터 세트에 대한 클래스는 위 깃허브 링크에 정의돼 있습니다.

![image](https://user-images.githubusercontent.com/76269316/142378371-8f9fae78-8b8f-4cd1-9be4-6efe169b126e.png)

<br><br>

##### Pascal VOC 데이터 세트 특징

![image](https://user-images.githubusercontent.com/76269316/142378634-045e0db9-23e3-4c9e-8c46-b550e8972554.png)

Pascal VOC 데이터 세트는 이미지 개당 1개의 annotation 파일을 갖고 있고, 그 파일에 개별 object들의 정보를 갖고 있습니다.

<br><br>

##### MS-COCO 데이터 세트 특징

<img src="https://user-images.githubusercontent.com/76269316/142378797-b61d1b6f-3d38-4154-ae98-540d76ff0065.png" alt="image" style="zoom:80%;" />

MS-COCO 데이터 세트는 모든 이미지들에 대해서 단 1개의 annotation 파일을 갖습니다.

images에 개별 이미지들의 정보가 있고, annotations에 image_id에 대응되는 이미지의 여러 object들의 정보가 담겨져 있습니다.

<br><br>

##### Custom Dataset에 사용되는 포맷

COCO Dataset 스타일과 유사하게 모든 이미지들에 대한 annotation 정보를 list 객체로 갖고 있습니다.

list내의 개별 원소는 dict로 구성되며 개별 dict는 1개 이미지에 대한 annotation 정보를 가지며,
1개 이미지에 있는 여러 object bounding box가 2차원 array로 주어지고, object label은 1차원 array로 구성됩니다.

<img src="https://user-images.githubusercontent.com/76269316/142379293-2ce3aec9-0666-4b1a-8309-7e5ef52ef1ab.png" alt="image" style="zoom:80%;" />

<br><br>

```python
# inference_detector의 인자로 string(file경로), ndarray가 단일 또는 list형태로 입력 될 수 있음. 
img_path = '/content/mmdetection/demo/demo.jpg'

# results = inference_detector(model, img_arr)  #bgr 이미지를 넣어야 함
results = inference_detector(model, img_path)
```

<br><br>

### tiny kitty 데이터로 MMDetection Train 실습 - Config와 CustomDataset 동작 매커니즘의 이해 01

변환을 위해서는 config와  dataset가 어떻게 상호작용하는지 먼저 알아야합니다.

먼저 custom dataset 구조를 살펴보겠습니다.

##### CustomDataset 구조 개요

<img src="https://user-images.githubusercontent.com/76269316/142399024-5862f6fb-cbdb-4449-8822-44a3d8d12e55.png" alt="image" style="zoom:80%;" />

생성자 \__init__는 mmdetection framework에서 호출하기 때문에 config에 등록해줘야 하는데 **@DATASETS.register_module()**로 등록합니다.

이후 config 파일로부터 객체 생성시 \__init__의 인자가 입력됩니다.

<br>

밑에 def load_annotations는 middle format으로 annotation을 변환하는 함수인데, 이 부분은 저희가 직접 작성해야 합니다.

<br><br>

##### CustomDataset를 상속받아 Dataset 생성

```python
from mmdet.datasets.builder import DATASETS
from mmdet.datasets.custom import CustomDataset

# 반드시 아래 Decorator 설정 할것.@DATASETS.register_module() 설정 시 force=True를 입력하지 않으면 Dataset 재등록 불가. 
@DATASETS.register_module(force=True)
class KittyTinyDataset(CustomDataset):
  CLASSES = ('Car', 'Truck', 'Pedestrian', 'Cyclist')
  
  ##### self.data_root: /content/kitti_tiny/ self.ann_file: /content/kitti_tiny/train.txt self.img_prefix: /content/kitti_tiny/training/image_2
  #### ann_file: /content/kitti_tiny/train.txt
  # annotation에 대한 모든 파일명을 가지고 있는 텍스트 파일을 __init__(self, ann_file)로 입력 받고, 이 self.ann_file이 load_annotations()의 인자로 입력
  def load_annotations(self, ann_file):
    print('##### self.data_root:', self.data_root, 'self.ann_file:', self.ann_file, 'self.img_prefix:', self.img_prefix)
    print('#### ann_file:', ann_file)
    cat2label = {k:i for i, k in enumerate(self.CLASSES)}
    image_list = mmcv.list_from_file(self.ann_file)
    # 포맷 중립 데이터를 담을 list 객체
    data_infos = []
    
    for image_id in image_list:
      filename = '{0:}/{1:}.jpeg'.format(self.img_prefix, image_id)
      # 원본 이미지의 너비, 높이를 image를 직접 로드하여 구함. 
      image = cv2.imread(filename)
      height, width = image.shape[:2]
      # 개별 image의 annotation 정보 저장용 Dict 생성. key값 filename 에는 image의 파일명만 들어감(디렉토리는 제외)
      data_info = {'filename': str(image_id) + '.jpeg',
                   'width': width, 'height': height}
      # 개별 annotation이 있는 서브 디렉토리의 prefix 변환. 
      label_prefix = self.img_prefix.replace('image_2', 'label_2')
      # 개별 annotation 파일을 1개 line 씩 읽어서 list 로드 
      lines = mmcv.list_from_file(osp.join(label_prefix, str(image_id)+'.txt'))

      # 전체 lines를 개별 line별 공백 레벨로 parsing 하여 다시 list로 저장. content는 list의 list형태임.
      # ann 정보는 numpy array로 저장되나 텍스트 처리나 데이터 가공이 list 가 편하므로 일차적으로 list로 변환 수행.   
      content = [line.strip().split(' ') for line in lines]
      # 오브젝트의 클래스명은 bbox_names로 저장. 
      bbox_names = [x[0] for x in content]
      # bbox 좌표를 저장
      bboxes = [ [float(info) for info in x[4:8]] for x in content]

      # 클래스명이 해당 사항이 없는 대상 Filtering out, 'DontCare'sms ignore로 별도 저장.
      gt_bboxes = []
      gt_labels = []
      gt_bboxes_ignore = []
      gt_labels_ignore = []

      for bbox_name, bbox in zip(bbox_names, bboxes):
        # 만약 bbox_name이 클래스명에 해당 되면, gt_bboxes와 gt_labels에 추가, 그렇지 않으면 gt_bboxes_ignore, gt_labels_ignore에 추가
        if bbox_name in cat2label:
          gt_bboxes.append(bbox)
          # gt_labels에는 class id를 입력
          gt_labels.append(cat2label[bbox_name])
        else:
          gt_bboxes_ignore.append(bbox)
          gt_labels_ignore.append(-1)
      # 개별 image별 annotation 정보를 가지는 Dict 생성. 해당 Dict의 value값은 모두 np.array임. 
      data_anno = {
          'bboxes': np.array(gt_bboxes, dtype=np.float32).reshape(-1, 4),
          'labels': np.array(gt_labels, dtype=np.long),
          'bboxes_ignore': np.array(gt_bboxes_ignore, dtype=np.float32).reshape(-1, 4),
          'labels_ignore': np.array(gt_labels_ignore, dtype=np.long)
      }
      # image에 대한 메타 정보를 가지는 data_info Dict에 'ann' key값으로 data_anno를 value로 저장. 
      data_info.update(ann=data_anno)
      # 전체 annotation 파일들에 대한 정보를 가지는 data_infos에 data_info Dict를 추가
      data_infos.append(data_info)

    return data_infos
```

KittyTinyDataset 클래스의 경우 생성자(\__init__)가 없는데 이 경우 상속받은 CustomDataset의 생성자를 그대로 사용합니다.

코드는 아래에서 다시 설명하겠습니다.

<br><br>

##### MMDetection Custom Dataset 생성 주요 로직

<img src="https://user-images.githubusercontent.com/76269316/142402141-d20182a8-8f4b-4fb6-9563-f7c18c3ffb23.png" alt="image" style="zoom:80%;" />

생성자 파라미터로 받은 data_root, ann_file, img_prefix를 알아야 parssing 할 수 있습니다. ( source를 middle form으로 변경하기 위한 중요한 파라미터 3가지임)

1. CustomDataset 객체를 MMDetection Framework에 등록: @DATASETS.register_module()
2. Config에 설정된 주요 값으로 CustomDataset 객체 생성: framework에서 build_dataset()을 호출해 생성 

위 두 작업을 수행하면 source-dataset pipeline이 생겨서 <img src="https://user-images.githubusercontent.com/76269316/142402352-51bf2a32-f643-497f-bddb-caa10e57fed6.png" alt="image" style="zoom:50%;" /> 모델로 집어넣을 수 있게 됩니다.

<br><br><br>

### tiny kitty 데이터로 MMDetection Train 실습 - Config와 CustomDataset 동작 매커니즘의 이해 02

##### data_root, ann_file, img_prefix의 활용

MMDetection의 dataset는 학습용, 검증용, 테스트용으로 각각 만들어질 수 있어야 합니다.

소스 데이터 세트 디렉토리 구조가 다양한데 대표적인 3가지는 다음과 같습니다.

<br>

**소스 데이터들의 학습용, 검증용, 테스트용 분리 유형**

- Image들과 Annotation 파일이 학습용, 검증용, 테스트용 디렉토리로 별도로 분리된 경우

<img src="https://user-images.githubusercontent.com/76269316/142403448-96e4e110-2534-4f79-8129-fa93757835ba.png" alt="image" style="zoom:50%;" />

- 별도의 메타 파일에서 학습용, 검증용 , 테스트용 image들과 annotation 파일을 지정한 경우

<img src="https://user-images.githubusercontent.com/76269316/142404091-0b119604-1d6b-4611-abce-6411c39f8ec1.png" alt="image" style="zoom:80%;" />

- Image들은 학습용, 검증용, 테스트용 디렉토리로 분리돼 있는데 annotation은 학습용, 검증용, 테스트용으로 하나만 갖는 경우

<img src="https://user-images.githubusercontent.com/76269316/142404351-b4e38e3a-30d7-4dd4-82ef-00183462b347.png" alt="image" style="zoom:80%;" />

<br>

소스 데이터들이 mmdetection dataset로 들어가려면 변환을 해야하는데, 이를 위해 **data_root, ann_file, img_prefix** 앞서 본 세 개의 파라미터가  다음과 같이 사용됩니다.

![image](https://user-images.githubusercontent.com/76269316/142404835-cf42e183-5533-435c-87bf-0d50a3a07fc2.png)

img_prefix는 여러 개의 image들을 포함한 디렉토리 형태로 지정되고, ann_file은 파일 하나만 지정되는데 Pascal VOC dataset의 경우 하나의 이미지에 하나의 annotation 파일이 대응되기 때문에 annotation file이 여러 개 있습니다.

*그러면 ann_file을 어떻게 지정하나요?*

▶ 이런 경우 train.txt 같이 annotation file 이름을 list로 갖는 메타 파일을 지정해주면 됩니다.

<img src="https://user-images.githubusercontent.com/76269316/142416637-b846a8aa-0f6c-42dd-b18a-54c14b9c11cf.png" alt="image" style="zoom: 80%;" />

<br>

위 세 개의 파라미터를 config 파일로부터 입력을 받아 생성자의 인자로 입력됩니다.

```python
@DATASETS.register_module()
class CustomDataset(Dataset):
    CLASSES = None
    
    def__init__(self, ann_file, pipeline, classes=None, data_root=None, img_prefix="", ,):
        self.ann_file = ann_file
        self.data_root = data_root
        self.img_prefix = img_prefix
        self.CLASSES = self.get_classes(claases)
        						···
        self.ann_file = os.path.join(self.data_root, self.ann_file)
        self.img_prefix = os.path.join(self.data_root, self.img_prefix)
        						···
        self.pipeline = Compose(pipeline):
            
    def load_annotations(self, ann_file):  #ann_file은 위에서 생성한 self.ann_file이 들어감
        			···
        return mmcv.load(ann_file)
```

<br><br>

##### Mask RCNN inference시 반환 결과 분석 - bbox 정보

results[0]은 bounding box prediction 결과로 (bounding box 좌표, classification confidence score)가 들어가 있습니다.

<img src="https://user-images.githubusercontent.com/76269316/142383160-790dc864-73a0-4eac-8673-105e933d4f2c.png" alt="image" style="zoom:80%;" />

<br>

##### Mask RCNN inference시 반환 결과 분석 - mask 정보

results[1]은 mask prediction 결과로 masking 정보가 들어가 있습니다.

<img src="https://user-images.githubusercontent.com/76269316/142383650-eccad422-c759-4376-9704-43421cc84c50.png" alt="image" style="zoom:80%;" />

