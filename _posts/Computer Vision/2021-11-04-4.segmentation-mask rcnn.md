---
title:  "4.Segmentation-Mask RCNN"
excerpt: "Segmentation-Mask RCNN"
toc: true
toc_label: "Segmentation-Mask RCNN"
toc_sticky: true
published: true

categories:
  - Computer Vision
tags:
  - 컴퓨터 비전
last_modified_at: 2021-11-04
---

### Segmentation 개요

##### Object Detection과 Segmentation

![image](https://user-images.githubusercontent.com/76269316/140112756-8ea7cd4e-5d7a-4ed7-9520-843b6864ab0e.png)

이미지 상에서 특정 object의 위치를 bounding box 형태로 찾아낸 다음 해당 object의 class를 분류하는 것이 Object Detection입니다.

Segmentation은 전체 이미지에 masking 정보를 씌워서 의미가 있는 형태를 찾는 것인데, Semantic Segmentation은 동일 클래스에 대해 합쳐진 형태로 mask가 씌워지고 Instance Segmentation은 개별 object별로 masking 정보가 씌워집니다.

<br>

|                  Semantic Segmentation                   |         Instance Segmentation          |
| :------------------------------------------------------: | :------------------------------------: |
| 동일 클래스에 대해 합쳐지는 형태로 masking 정보가 씌워짐 | 개별 object 별로 masking 정보가 씌워짐 |

Instance Segmentation의 대표적인 기법이 Mask-RCNN인데 Mask-RCNN은 Faster-RCNN과 FCN(Semantic Segmentation 기법)이 결합된 것이기 때문에 먼저 Semantic Segmentation을 알아보겠습니다.

<br>

##### Semantic Segmentation 개요

![image](https://user-images.githubusercontent.com/76269316/140119388-37078966-82c2-41fb-a93b-70c5821d7de3.png)

semantic segmentation은 개별 클래스별로 segmentation화 시키는데, 

이 이미지 안에 있는 개별 픽셀들이 어떤 클래스에 속하느냐를 결정한다면 그 픽셀의 클래스에 맞게 masking을 씌움으로써 (동일 클래스에 대해 동일 masking을 씌움) semantic segmentation이 가능하게되므로 pixel wise classification이라고도 불립니다.

<br>

##### 이미지의 개별 pixel별 classification

[1편: Semantic Segmentation 첫걸음!](https://medium.com/hyunjulie/1%ED%8E%B8-semantic-segmentation-%EC%B2%AB%EA%B1%B8%EC%9D%8C-4180367ec9cb)

<img src="https://user-images.githubusercontent.com/76269316/140124769-3ac01c6c-2f17-41f4-b3f9-b6f2a87fef34.png" alt="image" style="zoom:80%;" />

이렇게 이미지의 개별 픽셀별 classification을 통해 semantic segmentation을 구현할 수 있습니다.

<br>

<img src="https://user-images.githubusercontent.com/76269316/140127124-6ca25ec4-bf9b-484b-96ca-f8354e7e194f.png" alt="image" style="zoom:80%;" />

각 클래스에 대해 출력 채널을 만들어서 semantic map을 생성한 다음, argmax를 통해서 위에 있는 이미지처럼 하나의 결과로 합칩니다.

<br>

##### Semantic Segmentation Network Model

Semantic Segmentation에서는 Encoder - Decoder Model과 비슷한 네트워크 모델 구조를 가집니다.

<img src="https://user-images.githubusercontent.com/76269316/140129316-c35493d7-dcf9-4cfd-aad1-0649dc5fc58e.png" alt="image" style="zoom:80%;" />

Image Classification 할 때나 Object Detection 할 때나 down sampling을 진행해서 사이즈가 작고 채널이 깊은 피처맵을 만드는데 이를 **차원 축소 (Dimension Reduction)**라고 합니다.

차원 축소가 되면서 추상화의 depth가 높아짐에 따라 원본 이미지가 갖고 있는 위치적인 정보는 깨지지만 핵심정인 픽셀적인 가치는 응축이 되게됩니다.

<br>

이 때 **Decoder**를 통해 압축된 추상적인 정보를 가지고 원본 이미지와 유사한 크기로 늘려나가면서 숨겨진 hidden factor를 찾을 수 있습니다.

※ decoding하면서 원본 이미지와 똑같이 복사하는게 아닙니다!! (그러면 네트워크를 구축한 의미가 없음) → 압축된 추상화된 피처맵을 기반으로 원본 이미지에서는 찾을 수 없던 특성을 찾는 것이 목적

FCN이 시초이고, 이후에 Segnet, U-NET, Dilated FCN으로 발전했는데 저희는 FCN만 다루겠습니다.

<br>

##### Mask R-CNN

Mask R-CNN은 Faster R-CNN 기법을 굉장히 많이 가져와 Faster R-CNN과 유사합니다.

<img src="https://user-images.githubusercontent.com/76269316/140141071-0e0148c5-0247-4b79-8e9d-0ae06f05b411.png" alt="image" style="zoom: 80%;" />

- masking을 하기 위해 FCN(Fully Convolutional Network for Semantic Segmentation)을 가져옴

- Faster R-CNN의 경우 ROI Pooling을 했는데 이게 segmentation 하기에는 정확도가 떨어짐 → ROI Align을 적용한게 Mask R-CNN

<br><br>

### Semantic Segmentation FCN 이해

FCN이 나왔을 당시, fully connected layer가 필수였는데 (1차원으로 flat하게 만드는 층) FCN(Fully Convolutional Layer)은 fully connected layer 없이 구성된 모델입니다.

##### Semantic Segmentation Encoder-Decoder Model

- 원본 이미지를 convolution으로 차원 축소(Dimension Reduction)해 응축된 정보(사이즈는 줄지만 채널 깊이는 깊어짐 - 위치정보는 상당 부분 손실되지만 집약적인 정보를 갖고 있음)를 가지고, 이를 다시 upsampling하면서 필요한 정보를 학습하는 모델입니다.

<img src="https://user-images.githubusercontent.com/76269316/140224422-de4566d3-2c91-44a6-9c37-ae9248ee7a40.png" alt="image" style="zoom:80%;" />

*고고학에서 화석(응축된 정보)을 가지고 유전학적 정보, 해부학적 정보등을 얻는 과정과 유사 (고고학은 최대한 똑같이 복원하는게 목표지만, 인코더·디코더는 똑같이 만들지는 않는다는 점에서 차이가 있음)*

▶ 인코더, 디코더의 목표는 재해석·재창조임

<br>

##### Fully Connected Layer vs Fully Convolutional Layer

- Image Classification

<img src="https://user-images.githubusercontent.com/76269316/140632377-7d51c95c-4933-4de8-bb34-add722c8a740.png" alt="image" style="zoom:80%;" />

Image Net 데이터 세트를 사용했다고 가정하면 Image Net 데이터 세트의 경우 1,000개의 클래스를 가지므로 마지막에 1,000개의 depth를 갖고 있습니다.

- Object Detection

<img src="https://user-images.githubusercontent.com/76269316/140632462-c28e2097-ae83-44eb-8d94-3f88af5802a4.png" alt="image" style="zoom:80%;" />

1,000개의 depth를 갖는 layer의 heat map을 보면 개별 grid가 클래스에 대한 확률을 갖고 있습니다. (원본 이미지 사이즈가 아니니까 pixel보다는 grid가 정확한 표현)

→ pixel by pixel로 예측하기 위해서는 upsampling이 필요 (downsampling 하면서 응축된 형태이기 때문에 upsampling 과정을 통해 원본 이미지와 동일하게 만드는게 아닌 재창조를 한다고 생각하면 될 것 같습니다)

<br>

##### FCN(Fully Convolutional Network for Semantic Segmentation)

<img src="https://user-images.githubusercontent.com/76269316/140632738-0e627e1a-10c5-437c-8cb9-12208c860445.png" alt="image" style="zoom:80%;" />

224x224의 원본 이미지를 1/2, 1/4, ··· 1/32까지 줄여 7x7로 만듧니다. (depth는 4096으로 증가)

VOC Dataset를 사용하고 있기 때문에 (20개의 클래스 + 1개의 백그라운드) 21개의 depth로 변경하고 32배로 upsampling을 진행합니다.

이후 ground truth와 비교하며 개별 픽셀을 학습하면서 가중치를 업데이트합니다.

<br>

하지만 이렇게 바로 32배로 upsampling을 하게될 경우 원본이미지와 비교했을 때 뭉뚱그러지는 형태로 예측이되는 문제점이 발생합니다.

<img src="https://user-images.githubusercontent.com/76269316/140632861-ec35e248-1019-4609-94fc-7449af34a603.png" alt="image" style="zoom:80%;" />

→ ROI Align에서 설명할 내용인 보간법을 통해 최대한 비슷하게 복구하는 기법을 사용해 이를 해결했습니다.

<br>

##### Feature Map을 혼합하여 Pixel Wise Prediction

<img src="https://user-images.githubusercontent.com/76269316/140633029-9708366a-d1ab-4955-b19b-5724dd373f37.png" alt="image" style="zoom:80%;" />

<img src="https://user-images.githubusercontent.com/76269316/140633059-0f42a930-0ba9-4f6a-a640-b206f1b36fc8.png" alt="image" style="zoom:80%;" />

보통 FCN이라고 하면 주로 FCN-8s를 기반으로 semantic segmentation을 적용하는 걸 의미합니다.

<br><br>

### Mask RCNN의 이해 01

##### Mask RCNN

Faster RCNN과 FCN 기법을 결합한 것, 비교적 빠른 dtection 시간과 높은 정확도를 가짐

![image](https://user-images.githubusercontent.com/76269316/140640123-f83ab9f8-1e6f-437e-b2d2-3b4d4964fc53.png)

- Faster R-CNN: Region Proposal Network에서 object가 있을만한 영역을 추출해 ROI Align을 적용해 bounding box regression, classification 수행
- FCN: binary mask prediction 수행 (FCN은 픽셀별로 classification 해줬는데, FCN for semantic segmentation에서는 classification 자체를 Faster R-CNN에서 함 - bounding box 안에 있는 object가 무엇인지를 분류)
  bounding box 안에 있는 Faster R-CNN에서 분류한 클래스에 해당하는 픽셀들이 마스킹인지 아닌지 결정하는 것
  *이렇게 하는 것이 classification하는 FCN보다 성능이 개선됐다고 합니다.*

<br>

##### Faster RCNN 구조

<img src="https://user-images.githubusercontent.com/76269316/140640504-0bf379b0-4e0b-4c20-915b-7a817dd92b72.png" alt="image" style="zoom:80%;" />

<br>

Mask RCNN에서는 ROI Pooling이 ROI Align으로 변경됨

<img src="https://user-images.githubusercontent.com/76269316/140640517-bdd5d7c0-59e3-4b1c-849d-bd4cfacb251f.png" alt="image" style="zoom:80%;" />

Quantization

bounding box는 조금 어긋나 여백이 있어도 괜찮은데,  만약 input으로 들어온 feature map과 pooling하려는 feature map의 크기가 안 맞을수 있는데

예를 들어 input이 20x20, pooling 사이즈를 7x7로 하면 2.8x2.8로 딱 들어맞지가 않는 경우 소수점을 버리고 2x2로 맞춰버립니다.

→ 이를 Quantization이라고 하는데, 이러한 문제점을 보완하기 위해 ROI Align 기법이 사용됐습니다.

<br>

##### Segmentation에서 ROI Pooling의 문제점

masking을 해야하기 때문에 보다 정확해야 하는데 quantization을 해버리기 때문에 문제가 발생

<img src="https://user-images.githubusercontent.com/76269316/140640734-cc6b3407-d63f-4692-ac35-776af721d8dc.png" alt="image" style="zoom:80%;" />

원본 이미지 (800x800)와 region proposal 영역(665x665)이 VGG를 통과해서 1/32로 사이즈가 줄어서 원본 이미지는 225x225가 되는데, region proposal 영역은 정확히 나누면 20.78이 됨 → quantization: 소수점을 잘라버리고 20x20으로 계산

다시 7x7로 ROI Pooling을 적용하려 했더니 20/7=2.86이 돼서 2x2가 되게 됨

▶ Quantization이 2번이나 발생

<br>

<img src="https://user-images.githubusercontent.com/76269316/140640823-b15485cd-8acb-4574-975e-d5717c88a2ad.png" alt="image" style="zoom:80%;" />

Region Proposal된 영역이 3x3이라 2x2로 ROI Pooling을 적용할 수 없음 → 균등한 사이즈가 아니라 segmentation에서 정확한 영역을 추정하는데 문제가 발생함

<br>

##### Bilnear Interpolation을 이용한 ROI-Align

grid by grid로 계산하면 소숫점을 잘라야하니까 가상의 소숫점까지 생각한 grid를 생각하자

<img src="https://user-images.githubusercontent.com/76269316/140640898-e781c6b2-c212-433b-93c2-c98d72a804fd.png" alt="image" style="zoom:80%;" />

grid에 맞추지말고 줄어드는 스케일 영역을 일단 소숫점까지 생각해서 맞춘 다음 보간법(Bilinear Interpolation) 적용

<br>

**보간법 (Bilinear Interpolation)**

각 그리드 안에 네 개의 point를 균등하게 배열한 다음 보간법을 사용해 weight sum으로 계산

<img src="https://user-images.githubusercontent.com/76269316/140641036-f562b860-69bf-4a23-b21f-56556b49287c.png" alt="image" style="zoom:80%;" />

확대시키면 당연히 이미지가 깨짐 (빈 칸에 뭐가 들어가야 할지를 모르니까) → 빈 공간을 유추해야 함

<img src="https://user-images.githubusercontent.com/76269316/140641092-885af7e2-4998-4cde-8bfe-d78f169361e5.png" alt="image" style="zoom:80%;" />

노란색 별표 픽셀 값을 찾는다고 한다면 기준점을 갖고 있는 point들과의 거리 비율을 감안해서 계산함

이런 식으로 <img src="https://user-images.githubusercontent.com/76269316/140640898-e781c6b2-c212-433b-93c2-c98d72a804fd.png" alt="image" style="zoom:80%;" /> 각 포인트들의 값을 계산한 다음 max pooling 적용

<br>

<img src="https://user-images.githubusercontent.com/76269316/140641156-ea6f826a-f4ff-48d5-a12d-bc5cafc6a336.png" alt="image" style="zoom:80%;" />



<br>

##### ROI-Align 적용

![image](https://user-images.githubusercontent.com/76269316/140641253-af3696ce-f48e-43b1-868a-b4edbc7b436c.png)

<br>

ROI Align 적용 후 성능 향상

<img src="https://user-images.githubusercontent.com/76269316/140641303-9daf7646-c9c2-4c42-85b9-f45d92e8475f.png" alt="image" style="zoom:80%;" />

AP50보다 AP75의 경우 성능 향상이 더 큼 → 더 정밀하게 찾아야하는 경우 ROI Align을 적용하는 것이 좋음

<br><br>

### Mask RCNN의 이해 02

##### Mask RCNN Feature Extractor

<img src="https://user-images.githubusercontent.com/76269316/140641347-05eb5ce0-a8d5-478c-839e-ed1ccb72a23b.png" alt="image" style="zoom:80%;" />

<br>

#####  Mask RCNN Loss Function

![image](https://user-images.githubusercontent.com/76269316/140641396-9247c2b0-13de-4725-a3d9-a23188d99af8.png)

<br>

- Classification Loss: Multiclass cross-entropy loss (Pascal VOC의 경우20개, COCO의 경우 80개의 soft max로 계산)
- Bounding box Loss: Smooth L1 loss (차이가 1보다 작은 경우 L2 규제, 차이가 1 이상인 경우 L1 규제)

![image](https://user-images.githubusercontent.com/76269316/140641445-fe189780-d49a-4ae2-93ec-a76ef2f44b92.png)

- Mask Loss: Binary cross-entropy loss, K개의 정해진 Class에 대해서 그 class에 pixel이 속하는지 그렇지 않은지를 sigmoid로 결정

<br>

##### Mask Prediction

<img src="https://user-images.githubusercontent.com/76269316/140641506-91d2ad34-542f-4dac-9505-a6bc448ea033.png" alt="image" style="zoom:80%;" />

mask prediction에서는 어떤 클래스인지 찾을 필요는 없고, class인지 아닌지만 판단하면 됨

<br>

<img src="https://user-images.githubusercontent.com/76269316/140641548-70ecdf40-f17d-47f6-9e1e-e20edebeddfd.png" alt="image" style="zoom:80%;" />

mask prediction은 14x14로 되면 이를 원본 이미지 크기에 맞게 resize masking을 한 다음, 원본 이미지와 overlaid

<br>

##### Mask RCNN 성능 비교

<img src="https://user-images.githubusercontent.com/76269316/140641614-c8a9992d-5dd5-4bae-844b-a83720ec6727.png" alt="image" style="zoom:80%;" />

<br><br>

### MS COCO 데이터 포맷 상세 이해

<img src="https://user-images.githubusercontent.com/76269316/140642209-66b5c4ad-446b-4b41-ae8e-a04969c6196c.png" alt="image" style="zoom:80%;" />

<br>

```json
{
    "info": {...},
	"licenses": [...],
	"images": [...],  /* 개별 image들의 정보 */
	"annotations": [...],  /* bounding box, segmentation, annotation과 image id 매핑 정보 */
	"categories": [...],
	"segment_info": [...]  /* panoptic segmentation에서 사용하는 정보 */
}
```

<br>

#####  Images 포맷

file_name의 경우 절대 경로가 아님

<img src="https://user-images.githubusercontent.com/76269316/140642261-d5f995ea-7f32-492e-b713-43dd91939ba5.png" alt="image" style="zoom:80%;" />

<br>

##### Annotations 포맷

<img src="https://user-images.githubusercontent.com/76269316/140642402-303d9857-a406-4553-ab67-662f43c21d5b.png" alt="image" style="zoom:80%;" />

<br>

##### Categories 포맷

<img src="https://user-images.githubusercontent.com/76269316/140642466-9a3e3262-2ea8-4f29-9b64-af44924a196c.png" alt="image" style="zoom:80%;" />

id는 있는데 데이터가 없는 것들도 있어서 id를 0~79로 다시 mapping 시켜서 사용하는 경우도 있음

<br>

##### pycocotools 개요

<img src="https://user-images.githubusercontent.com/76269316/140642543-4e751e00-186d-4f75-92db-339efb804120.png" alt="image" style="zoom:80%;" />

coco 데이터 세트 내 이미지, annotations 등의 정보들을 쉽게 access 할 수 있도록 지원해주는 API

<br><br>

### Polygon과 Mask Segmentation 시각화 - 01

<img src="https://user-images.githubusercontent.com/76269316/140643291-6499160a-1c66-4f80-8bc6-441dbfa90478.png" alt="image" style="zoom:80%;" />

polygon 좌표를 이어서 다각형으로 만드는건 openCV의 polylines를 이용하면 됨

보통 data로 들어가는건 polygon 형태가 아닌 mask 형태(우리가 detect하고자 하는 특정 object 부분만 1로 masking하고 나머지는 모두 0으로 masking한 형태)로 들어감

<img src="https://user-images.githubusercontent.com/76269316/140643717-9ac6545c-ee34-4966-b790-4b6a702f7f81.png" alt="image" style="zoom:50%;" />
